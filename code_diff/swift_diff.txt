commit 6b70b37f2b43c11bf9f404bdf9ba683d5a939c27
Merge: d5e5253cee0 4e1c2b2e478
Author: Slava Pestov <spestov@apple.com>
Date:   Sat Jul 31 11:28:28 2021 -0400

    Merge pull request #38713 from slavapestov/lazy-assoc-type-list-deserialization
    
    More efficient getAssociatedTypeMembers() on serialized ProtocolDecls

commit 499bff25bca9c655277ad94f0eae5458f861370e
Author: Slava Pestov <spestov@apple.com>
Date:   Tue Jul 13 23:12:28 2021 -0400

    RequirementMachine: Implement GenericSignature::getConformanceAccessPath() query
    
    This is just a straight port of the existing code in the GSB, with minimal changes.
    
    It could be made more efficient in the future by trafficking in Terms rather than
    Types, avoiding some intermediate conversion and canonicalization steps.

commit e096b2f14abae0d75191eb30d034aaff83d58ddb
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Thu Jul 1 17:03:54 2021 +0200

    BasicCalleeAnalysis: make the callee list datastructure more efficient
    
    This avoids copying a SmallVector in case of multiple callees in the list.
    Instead, just reference the callee list in the cache.

commit 969ac78eaf97cb1ee92db4d8ed092fa43a830523
Author: Slava Pestov <spestov@apple.com>
Date:   Wed Jun 16 18:36:13 2021 -0400

    RequirementMachine: Compute critical pairs directly
    
    Previously if the left hand side of two rules overlapped, we would
    compute the overlapped term and apply both rules to obtain a
    critical pair.
    
    But it is actually possible to compute the critical pair directly.
    For now this has no effect other than possibly being more efficient,
    but for concrete type terms we will need this formulation for the
    completion procedure to work.

commit 466e26a872b9bcc848bb77891842e887e3f9f0de
Author: Karoy Lorentey <klorentey@apple.com>
Date:   Mon Jun 14 22:05:37 2021 -0700

    [stdlib] Implement _copyContents on internal Array types
    
    `_copyContents(initializing:)` is a core method of Sequence, and it is used surprisingly often to copy stuff out of sequences. Arrayâ€™s internal types currently have explicit implementations of it that trap (to prevent a performance bug due to the default iterator-based implementation. This has proved a bad idea, as not all code paths that end up calling `_copyContents` have actually been expunged â€” so we replaced a performance bug with a catastrophic correctness bug. ðŸ˜¥
    
    Rather than trying to play whack-a-mole with code paths that end up in `_copyContents`, replace the traps with (relatively) efficient implementations, based on the ancient `_copyContents(subRange:initializing)` methods that have already been there all this time.
    
    This resolves https://bugs.swift.org/browse/SR-14663.
    
    I expect specialization will make this fix deploy back to earlier OSes in most (but unfortunately not all) cases.

commit 14422cdb50367387fe65e80edbe64fb444e77c84
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Fri Apr 23 20:26:42 2021 +0200

    libswift: Add the StackList data structure
    
    StackList is a very efficient data structure for worklist type things.
    This is a port of the C++ utility with the same name.
    
    Compared to Array, it does not require any memory allocations.

commit 8da2c377da29027c9d457a1341d02b46169c7fc9
Author: Saleem Abdulrasool <compnerd@compnerd.org>
Date:   Sun May 2 10:02:11 2021 -0700

    IRGen: add support for static linking on Windows
    
    This adjusts the IRGen layer to accommodate the Windows linking model.
    We assume dynamic linking by default.  The static linking is enabled by
    passing `-static` to the driver, which forwards it to the frontend when
    building the module statically.  This has already been required when
    generating libraries, however, the non-Windows targets are more
    forgiving and let it work.  On those platforms, using this hint would
    allow for more efficient code generation, reducing load times and some
    runtime penalties from the PLT and GOT references formed to symbols
    which are module local.
    
    This corrects static linking on Windows, which is one of the last few
    items that are missing on Windows.  It also takes advantage of the hint
    for the one peculiar difference between Windows and non-Windows:
    protocol conformances that span module boundaries are not available as a
    constant.  However, when statically linking, we can enable those
    conformances to be statically resolved.  This should enable the last
    known pattern to work when using static linking.
    
    This support requires further work in the Swift Package Manager to
    actually enable building libraries properly.  However, when building
    with CMake, this should be sufficient to enable static linking.

commit 294977534c9e24d751301d855d61f26a1ee0cec5
Author: Alex Hoppen <ahoppen@apple.com>
Date:   Fri Mar 12 09:56:59 2021 +0100

    [libSyntax] Remove incremental JSON transfer option
    
    We were only keeping track of `RawSyntax` node IDs to incrementally transfer a syntax tree via JSON. However, AFAICT the incremental JSON transfer option has been superceeded by `SyntaxParseActions`, which are more efficient.
    
    So, letâ€™s clean up and remove the `RawSyntax` node ID and JSON incremental transfer option.
    
    In places that still need a notion of `RawSyntax` identity (like determining the reused syntax regions), use the `RawSyntax`â€™s pointer instead of the manually created ID.
    
    In `incr_transfer_round_trip.py` always use the code path that uses the `SyntaxParseActions` and remove the transitional code that was still using the incremental JSON transfer but was never called.

commit efb91f70a15b251607b77edeb682969ffdccbafe
Author: Alex Hoppen <ahoppen@apple.com>
Date:   Tue Mar 9 13:40:49 2021 +0100

    [libSyntax] Handle deferred node data in SyntaxParseActions
    
    By now ParsedRawSyntaxNode does not have any knowledge about deferred
    node data anymore, which frees up SyntaxParseActions (and, in
    particular its sublass SyntaxTreeCreator) to perform optimisations to
    more efficiently create and record deferred nodes.

commit c1d65de89c680333febae18a4dc5a5c07a4bba85
Author: Alex Hoppen <ahoppen@apple.com>
Date:   Fri Jan 29 12:57:08 2021 +0100

    [libSyntax] Optimise layout of RawSyntax to be more space efficient
    
    This decreases the size of RawSyntax nodes from 88 to 64 bytes by
    - Avoiding some padding by moving RefCount further up
    - Limiting the length of tokens and their trivia to 32 bits. We would
      hit this limit with files >4GB but we also hit this limit at other
      places like the TextLength property in the Common bits.

commit caefb9afaa143935f500770962086fc936ed4e6a
Author: Andrew Trick <atrick@apple.com>
Date:   Sat Jan 30 19:26:13 2021 -0800

    Centralize and document low-level OSSA utilities
    
    ...for handling borrow scopes:
    
    - find[Extended]TransitiveGuaranteedUses
    
    - BorrowingOperand::visit[Extended]ScopeEndingUses
    
    - BorrowedValue BorrowingOperand::getBorrowIntroducingUserResult()
    
    And document the logic.
    
    Mostly NFC in this commit, but more RAUW cases should be correctly
    handled now.
    
    Particularly, ensure that we can cleanly handle all manner of
    reborrows. This is necessary to ensure both CanonicalizeOSSA and
    replace-all-uses higher-level utilities handle all cases.
    
    This generalizes some of the logic in CanonicalizeOSSA so it can be
    shared by other high-level OSSA utilities.
    
    These utilities extend the fundamental BorrowingOperand and
    BorrowedValue functionality that has been designed recently. It builds
    on and replaces a mix of piece-meal functionality that was needed
    during bootstrapping. These APIs are now consistent with the more
    recently designed code. It should be obvious what they mean and how to
    use them, should be very hard to use them incorrectly, and should be
    as efficient as possible, since they're fundamental to other
    higher-level utilities.
    
    Most importantly, there were several very subtle correctness issues
    that were not handled cleanly in a centralized way. There are now a
    mix of higher-level utilities trying to use this underlying
    functionality, but it was hard to tell if all those higher-level
    utilities were covering all the subtle cases:
    
    - checking for PointerEscapes everywhere we need to
    
    - the differences between uses of borrow introducers and other
      guaranteed values
    
    - handling the uses of nested borrow scopes
    
    - transitively handling reborrows
    
    - the difference between nested and top-level reborrows
    
    - forwarding destructures (which can cause exponential explosion)
    
    In short, I was fundamentally confused and getting things wrong before
    designing these utilities.

commit 65208c0642d74b05ab1cd7618a3273860f2348ff
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Fri Jan 29 14:11:29 2021 +0100

    SIL: efficiently store SILLocation in SILInstruction
    
    Store the 1-byte kindAndFlags of SILLocation in the instruction's SILNode bitfield and only store SILLocation::storage in SILInstruction directly.
    This reduces the space for the location from 2 to 1 word in SILInstruction.

commit c9c7619f53a6355d3b642a8b1875f52b3e70aa7a
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Thu Jan 21 14:19:21 2021 +0100

    DefiniteInitialization: use BasicBlockData instead of a map.
    
    And reuse this block data for all memory objects which are analyzed.
    This is more efficient and avoids memory allocations.

commit a0884baa3cf2126fe3232a97e8278b948a76200a
Merge: a89c882bdb5 3e8612b0d3d
Author: eeckstein <eeckstein@apple.com>
Date:   Fri Jan 22 08:39:13 2021 +0100

    Merge pull request #35521 from eeckstein/sil-bitfields
    
    SIL: add a utility which can manage per-block bitfields and flags efficiently.

commit 65976fd0c5b94c749bedcf6ee1a4ebeb9c9d455f
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Wed Jan 20 21:10:28 2021 +0100

    SIL: add a utility which can manage per-block bitfields and flags efficiently.
    
    It is very efficient: no memory allocation is needed an initialization is at zero cost.

commit 9c602026a4a32ded866df71723d1f7c957de12cb
Merge: 6c41322f0dc 7108be14420
Author: eeckstein <eeckstein@apple.com>
Date:   Wed Jan 20 18:37:32 2021 +0100

    Merge pull request #35448 from eeckstein/block-data
    
    SIL: add a utility which can manage per-block data efficiently.

commit 273bd350615053baeb853f7a4a0517f8aa858a9d
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Fri Jan 15 18:04:38 2021 +0100

    SIL: add a utility which let's manage per-block data efficiently.
    
    It can be used by transforms to store temporary data per basic block.
    It is very efficient: only a single memory allocation is needed and no maps are used to lookup data.

commit aa38be6d987c9289c24e1455ca54af86779fc455
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Fri Jan 15 09:59:27 2021 -0800

    [inst-simplify] Hide simplifyInstruction in favor of using simplifyAndReplaceAllSimplifiedUsesAndErase.
    
    Currently all of these places in the code base perform simplifyInstruction and
    then a replaceAllSimplifiedUsesAndErase(...). This is a bad pattern since:
    
    1. simplifyInstruction assumes its result will be passed to
       replaceAllSimplifiedUsesAndErase. So by leaving these as separate things, we
       allow for users to pointlessly make this mistake.
    
    2. I am going to implement in a subsequent commit a utility that lifetime
       extends interior pointer bases when replacing an address with an interior
       pointer derived address. To do this efficiently, I want to reuse state I
       compute during simplifyInstruction during the actual RAUW meaning that if the
       two operations are split, that is difficult without extending the API. So by
       removing this, I can make the transform and eliminate mistakes at the same
       time.

commit f22d0855df15f91691367bf8bbf5d4b35287a1a5
Author: Andrew Trick <atrick@apple.com>
Date:   Wed Dec 30 00:12:56 2020 -0800

    Add a CanonicalOSSALifetime utility.
    
    Canonicalizing OSSA provably minimizes the number of retains and
    releases within the boundaries of that lifetime. This eliminates the
    need for ad-hoc optimization of OSSA copies.
    
    This initial implementation only canonicalizes owned values, but
    canonicalizing guaranteed values is a simple extension.
    
    This was originally part of the CopyPropagation prototype years
    ago. Now OSSA is specified completely enough that it can be turned
    into a simple utility instead.
    
    CanonicalOSSALifetime uses PrunedLiveness to find the extended live
    range and identify the consumes on the boundary. All other consumes
    need their own copy. No other copies are needed.
    
    By running this after other transformations that affect OSSA
    lifetimes, we can avoid the need to run pattern-matching optimization
    to SemanticARC to recover from suboptimal patterns, which is not
    robust, maintainable, or efficient.

commit bf2be9eb5d991285284b0a681816ae394ed08bc6
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Tue Nov 17 18:08:06 2020 +0100

    [concurrency] IRGen: update task/executor/context on every suspend point
    
    For this, store those 3 values on the stack at function entry and update them with the return values of coro_suspend_async intrinsic calls.
    
    This fixes a correctness issue, because the executor may be different after a resume.
    It also is more efficient, because this means that the 3 values don't have to preserved in the context over a suspension point.

commit 863dcec142c8309ed51954619f38308624b7bbc2
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Wed Nov 25 18:52:35 2020 +0100

    [concurrency] stdlib: add a _runAsyncHandler compiler intrinsic.
    
    It just calls Task.runDetatched.
    It's more efficient to have a non-generic compiler intrinsic than to let the compiler call the generic Task.runDetatched.
    The _runAsyncHandler doesn't have to be generic because the return value of the run function is defined to be Void.

commit f58c14335518e611aa8b8a29aafe9ea73848f919
Merge: dcc42bc0f38 16dd929959f
Author: Ben Langmuir <blangmuir@apple.com>
Date:   Wed Nov 11 11:18:19 2020 -0800

    Merge pull request #34672 from benlangmuir/sourcekit-inproc-and-xpc-together
    
    [sourcekit] Make it possible to install both sourcekitdInProc and sourcekitd efficiently

commit 0287a3d820a847099a8218dc73cd5a4ff7be9515
Author: Andrew Trick <atrick@apple.com>
Date:   Tue Sep 4 16:46:12 2018 -0700

    Verify no SIL critical edges.
    
    There are multiple reasons this is needed.
    
    1. Most passes do not perform CFG transformations. However, we often
    need to split critical edges and remember to invalidate all SIL
    analyses at the end of virtually every pass. This is very innefficient
    and highly bug prone.
    
    2. Many SIL analysis algorithms needs to reason about CFG
    edges. Avoiding critical edges leads to far simpler and more efficient
    designs when edges can be identified by blocks.
    
    3. Handling block arguments on conditional branches create complexity
    at the lowest level of the SIL interface. This complexity is difficult
    to abstract over and bleeds until any algorithm that needs to reason
    about phi operands. It's far easier to work with phis if we can easily
    recover the phi operand with only a reference to the predecessor
    block.
    
    4. Attempting to preserve critical edges in high and mid level IR
    blocks optimizations that otherwise have no business optimizing
    branches. Branch optimization should always be defered to machine
    level IR where the most relevant heuristics are employed to remove
    unconditional branches. If code didn't need to be placed on a critical
    edges, then a branch optimization can easily remove that code from the
    critical edge.

commit acc3398fed87ae8fe1f7242648d54fcca35be1da
Author: Andrew Trick <atrick@apple.com>
Date:   Tue Sep 4 16:46:12 2018 -0700

    Add -allow-critical-edges flag.
    
    Provide a mechanism to gradually migrate unit tests away from allowing
    critical edges via -allow-critical-edges=false.
    
    This will be the default in OSSA very soon, and will hopefully become
    the default eventually for all SIL stages.
    
    Note that not all required optimization pass changes have been
    committed yet. I have pending changes in:
    - SimplifyCFG
    - SILCloner subclasses
    - EagerSpecializer
    - ArraySpecialization
    - LoopUtils
    - LoopRotate
    
    There are multiple reasons we need to disallow critical edges:
    
    1. Most passes do not perform CFG transformations. However, we often
    need to split critical edges and remember to invalidate all SIL
    analyses at the end of virtually every pass. This is very innefficient
    and highly bug prone.
    
    2. Many SIL analysis algorithms needs to reason about CFG
    edges. Avoiding critical edges leads to far simpler and more efficient
    designs when edges can be identified by blocks.
    
    3. Handling block arguments on conditional branches create complexity
    at the lowest level of the SIL interface. This complexity is difficult
    to abstract over and bleeds until any algorithm that needs to reason
    about phi operands. It's far easier to work with phis if we can easily
    recover the phi operand with only a reference to the predecessor
    block.
    
    4. Attempting to preserve critical edges in high and mid level IR
    blocks optimizations that otherwise have no business optimizing
    branches. Branch optimization should always be defered to machine
    level IR where the most relevant heuristics are employed to remove
    unconditional branches. If code didn't need to be placed on a critical
    edges, then a branch optimization can easily remove that code from the
    critical edge.

commit 8e3fb44f2d7981fa443c45386342eeb8cdefb440
Author: Andrew Trick <atrick@apple.com>
Date:   Tue Sep 1 13:03:37 2020 -0700

    Rewrite LoadBorrowImmutabilityChecker using AccessPath.
    
    The verification will now be as complete as it can be within the
    capability of our SIL utilities. It is much more aggressive with
    respect to boxes, references, and pointers. It's more efficient in
    that it only considers "overlapping" uses.
    
    It is also now wholly consistent with the utilities that it uses, so
    can be reenabled.
    
    We could probably go even further and remove the switch statement
    entirely, relying on AccessPath to recognize any operations that
    propagate addresses, boxes, or pointers. But I didn't want to
    potentially weaken enforcement without more careful consideration.

commit b272dc5e1a8357c7d882027e6bf4a7136de265ad
Author: Andrew Trick <atrick@apple.com>
Date:   Fri Sep 25 23:43:13 2020 -0700

    Cache 'isLet' within AccessedStorage.
    
    Compute 'isLet' from the VarDecl that is available when constructing
    AccessedStorage so we don't need to recover the VarDecl for the base
    later.
    
    This generally makes more sense and is more efficient, but it will be
    necessary when we look past class casts when finding the reference root.

commit 5c3814e4dd0f3d0cd0a4776dbaebd3751bc8e985
Author: Josh Learn <joshua_learn@apple.com>
Date:   Tue Oct 6 11:43:13 2020 -0700

    [OSSignpost] Update apinotes to allow usage of os_signpost ABI entrypoint
    
    Currently, the `_os_signpost_emit_with_name_impl` function is not available to be called from Swift. This is the main ABI entrypoint for making os_signpost calls. In order to facilitate more efficient calls to os_signpost in future iterations of the Swift os_signpost API, we need to allow calling this function from Swift.
    
    rdar://70015938

commit 7985896949e6e672e2d3861230366c5df332e0e8
Author: Stephen Canon <scanon@apple.com>
Date:   Tue Jun 30 20:59:31 2020 -0400

    Provide an implementation of init?<T:BinaryInteger>(exactly:T) on each stdlib FP type. (#32632)
    
    Previously these always went through the FloatingPoint-provided default implementation, which is not particularly efficient. Also try removing inlinable from the generic _convert hooks, since we probably never want to actually inline them.

commit 0f7eb025594c62604269e4849dcccd4a988a7f07
Author: Martin Boehme <mboehme@google.com>
Date:   Fri Jun 19 14:27:39 2020 +0200

    Pass PrintOptions by const reference instead of by value.
    
    `PrintOptions` likely started as a small type that made sense to pass by
    value, but it's become big enough that passing by const reference is
    more efficient now.

commit f2fb53967c7f9a622288db4eaadfc1a720d4c670
Author: Mike Ash <mikeash@apple.com>
Date:   Tue Jun 2 11:22:32 2020 -0400

    [Runtime] Unify debug variable parsing from the environment and avoid getenv when possible.
    
    There are a few environment variables used to enable debugging options in the
    runtime, and we'll likely add more over time. These are implemented with
    scattered getenv() calls at the point of use. This is inefficient, as most/all
    OSes have to do a linear scan of the environment for each call. It's also not
    discoverable, since the only way to find these variables is to inspect the
    source.
    
    This commit places all of these variables in a central location.
    stdlib/public/runtime/EnvironmentVariables.def defines all of the debug
    variables including their name, type, default value, and a help string. On OSes
    which make an `environ` array available, the entire array is scanned in a single
    pass the first time any debug variable is requested. By quickly rejecting
    variables that do not start with `SWIFT_`, we optimize for the common case where
    no debug variables are set. We also have a fallback to repeated `getenv()` calls
    when a full scan is not possible.
    
    Setting `SWIFT_HELP=YES` will print out all available debug variables along with
    a brief description of what they do.

commit 564c1a5eec3d50d9529ba8017c5f3c67ef475dc0
Author: Joe Groff <jgroff@apple.com>
Date:   Mon Jun 1 12:24:18 2020 -0700

    Add a [nonoverridden] kind for SILVTable entries.
    
    This will let us track class methods that must exist for pass ordering, interface, or ABI reasons, but which can
    be given more efficient runtime representation because they have no overrides.

commit cd7f205ade20915e16c70a22687af5c3c0ea8923
Author: Martin Boehme <mboehme@google.com>
Date:   Fri Apr 24 17:19:19 2020 +0200

    [IRGen] Check as early as possible for Clang decls we've seen before.
    
    Previously, we were only doing this after the fast-path code that
    handles decls without any executable code. This meant, however, that we
    were potentially processing these decls multiple times. This is
    definitely inefficient; it may even be a correctness issue, depending on
    what amount of checking `HandleTopLevelDecl` does to see if it has
    processed a particular decl before (which I'm not sure about either
    way).

commit 9e28e0a8c44c54174f8e6e967f2e3c55023fd526
Author: Dan Zheng <danielzheng@google.com>
Date:   Sun Apr 5 19:19:38 2020 -0700

    [AutoDiff upstream] Add AdjointValue.
    
    Add `AdjointValue`: a symbolic representation for adjoint values enabling
    efficient differentiation by avoiding zero materialization.

commit 8e447a4d143451b331e3e49c1b038c4c5a0ec12f
Merge: 291f432069b 00d45769772
Author: Michael Gottesman <gottesmm@users.noreply.github.com>
Date:   Tue Mar 31 18:04:29 2020 -0700

    Merge pull request #30665 from gottesmm/pr-148512f9e52b5c768ddd3312717c2fffcd71a96e
    
    [multimapcache] Add an efficient CRTP based write-once multimap cache  that can be small.

commit 00d45769772f63095a3761e1ab2c482e3b21954b
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Wed Mar 25 15:08:59 2020 -0700

    [multimapcache] Add an efficient CRTP based write-once multimap cache that can be small.
    
    The properties of this multimap cache are:
    
    1. Values are stored (inline if Small) in a Vector and our map internally maps
       keys to (start, length) of slices of the Vector. This is done instead of
       storing arrays refs to ensure that if our array goes from small -> large, we
       do not have stale pointers.
    
    2. Values are only allowed to be inserted all at once. This is ok, since this is
       a cache.
    
    3. One is not storing individual small vectors in a map (or state storing
       SmallVectors). This can inadvertantly add up to using a lot of memory and is
       not needed for homogenous data.

commit 57d228b39ebf00c45b3761972bfc792e01eacb05
Author: Richard Wei <rxrwei@gmail.com>
Date:   Sat Mar 28 23:09:31 2020 -0700

    [AutoDiff upstream] Add differential operators and some utilities.
    
    * Add all [differential operators](https://github.com/apple/swift/blob/master/docs/DifferentiableProgramming.md#list-of-differential-operators).
    * Add `withoutDerivative(at:)`, used for efficiently stopping the derivative propagation at a value and causing the derivative at the value to be zero.
    * Add utility `differentiableFunction(from:)`, used for creating a `@differentiable` function from an original function and a derivative function.
    
    Mostly work done by @marcrasi and @dan-zheng.
    Partially resolves TF-843.
    
    TODO:
    * Add `AnyDerivative`.
    * Add `Array.differentiableMap(_:)` and `differentiableReduce(_:_:)`.

commit 3508dfb0ea089308dfc8e074463e99703991323f
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Feb 11 10:14:56 2020 -0800

    [Constraint system] Use hash table lookup appropriately.
    
    Itâ€™s pointlessly inefficient to do an O(n) scan through a hash table to match
    keys!

commit 3e932c075d82e16542522f8229d95051cb2838ec
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Tue Nov 26 18:19:32 2019 +0100

    stdlib: Add custom .first to Array
    
    This makes Array.first much small and more efficient.
    Without this, Array.first compiled down to RandomAccessCollection.first, which ended up in pretty unefficient code.
    
    rdar://problem/46291397

commit 87298c07c8374fe112cc67dc3849502df3cebc9c
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Tue Nov 26 18:17:30 2019 +0100

    stdlib: make the non-native Array.count smaller more efficient.
    
    Use _CocoaArrayWrapper.endIndex which returns the NSArray.count.
    In the old version, "count" translated to RandomAccessCollection.count, which ended up in multiple calls to endIndex.

commit f06b5688e24bec19a3ed3eebaffa0727b83a2ce3
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Nov 12 17:12:40 2019 -0800

    [IDE] Drop an unnecessarily inefficient use of depth maps

commit 2c7e34881441071d9e33db86007ae9d09b47ffc4
Author: Andrew Trick <atrick@apple.com>
Date:   Thu Nov 7 14:03:41 2019 -0800

    EscapeAnalysis: rework graph update and merge algorithms
    
    The two major take aways from this patch are:
    
    (1) Impose graph structure and reduce superfluous nodes and edges.
    
    Incrementally make the connection graph and the APIs used to construct
    it more structured.
    
    _This allows node properties based on the SILValue to be reliably added to nodes_
    
    Although that was the initial motiviation, there are other
    benefits. Non-content nodes now have verifiable SILValues. Content
    nodes now have meaningful SILValues even though they can't be
    guaranteed due to merging. As a result it is *much* easier to debug
    the graph and correlate it with the SIL. Rather than a web of
    connection graph nodes with no identity and edges that don't
    correspond to anything in SIL, the graph nodes now have value number
    that correspond to the instruction used to dereference the node. The
    edges also exhibit structure now. A pointsTo edge now (in practice)
    always corresponds to a real pointer deference in the SIL. Doing this
    required more than just adding some helpers, it was also necessary to
    rewrite the graph merge and update algorithms.
    
    (2) Split up underlying functionality into more explicit steps
    
    Breaks apart the most complex parts of the graph algorithms into small
    self-contained, self-checked steps. The purpose of each step is clear
    and it's possible to reason about correctness from basic
    invariants. Each merge step can now run full graph verification.
    
    This was also done to move toward an invariant that the graph is never
    mutated during a query. But to finish that goal, we need to add a
    use-point query. With that, there will be no node creation, use point
    propagation, new defer edges, etc. after graph building. At the very
    least, this will make it sane to debug the output of the analysis.
    
    ---
    Here is a change-by-change description in diff order:
    
    Replace `updatePointsTo` with `initializePointsTo` and
    `mergePointsTo`. Merging is very simple on its own. Initialization
    requires some extra consideration for graph invariants. This
    separation makes it possible to write stong asserts and to
    independently reason about the correctness of each step based on
    static invariants.
    
    Replace `getContentNode` with `createContentNode`, and add two higher
    level APIs `createMergedContent`, and `getFieldContent`. This makes
    explicit the important cases of merging nodes and creating a special
    nodes for class fields. This slightly simplifies adding properties to
    content nodes and helps understand the structure of the graph.
    
    Factor out an `escapeContentsOf` helper for use elsewhere...
    
    Add a `getValueContent` helper. This is where we can tie the
    properties of content nodes to the address values that are used to
    address that content. This now also ensures that a Value node's
    value field is consistent with all SILValues that map to it.
    
    Add -escapes-internal-verify to check that the graph is in a valid
    state after every merge or update step. This verification drove the
    partial rewrite of mergeAllScheduledNodes.
    
    ConnectionGraph::defer implementation: explictly handle the three
    possible cases of pointsTo initialization or pointsTo merging at the
    top level, so that those underlying implementations do not need to
    dynamically handle weirdly different scenarios.
    
    ConnectionGraph::initializePointsTo implementation: this simplified
    implementation is possible by relying on invariants that can be
    checked at each merge/update step. The major functional difference is
    that it avoids creating unnecessary pointsTo edges. The previous
    implementation often created pointsTo edges when adding defer edges
    just to be conservative. Fixing this saved my sanity during debugging
    because the pointsTo edges now always correspond to a SIL operations
    that dereference the pointer. I'm also arguing without evidence that
    this should be much more efficient.
    
    ConnectionGraph::mergeAllScheduledNodes implementation: Add
    verification to each step so that we can prove the other utilities
    that are used while merging aren't making incorrect assumptions about
    the graph state. Remove checks for merged nodes now that the graph is
    consistently valid. Also remove a loop at the end that didn't seem to
    do anything. The diff is impossible to review, but the idea is
    basically the same. As long as it's still possible to scan through the
    steps in the new code without getting totally lost, then the goal was
    achieved.
    
    ConnectionGraph::mergePointsTo: This is extremely simple now. In all
    the places where we used to call updatePointsTo, and now call
    mergePointsTo, it's a lot easier for someone debugging the code to
    reason about what could possibly happen at that point.
    
    `createMergedContent` is a placeholder for transferring node properties.
    
    The `getFieldContent` helper may seem silly, but I find it helpful to
    see all the important ways that content can be created in one place
    next to the createContentNode, and I like the way that the creation of
    the special "field content" node is more explicit in the source.
    
    ConnectionGraph::mergeFrom implementation: this is only a minor
    cleanup to remove some control flow nesting and use the CGNodeWorklist
    abstraction.
    
    In AnalyzeInstruction, add EscapeAnalysis::getValueContent helper. It
    eliminates an extra step of going through the value node to get at its
    content node. This is where we can derive content node properties from
    the SILValue that dereferences the content. We can update the content
    node's associated value 'V' if it's useful. It's also a place to put
    assertions specific to the first level of content.
    
    In AnalyzeInstruction, Array semantic calls: add support for
    getValueContent so we can derive node properties. This is also nice
    because it's explicit about which nodes are value content vs. field
    content.
    
    In AnalyzeInstruction, cleanup Release handling: use the explicit
    APIs: getValueContent, getFieldContent, and escapeContentsOf.
    
    In AnalyzeInstruction, assert that load-like things can't produce addresses.
    
    In AnalyzeInstruction, add comments to clarify object projection handling.
    
    In AnalyzeInstruction, add comments to explain store handling.
    
    In AnalyzeInstruction, drop the assumption that all partial applies hold pointers.
    
    In AnalyzeInstruction, handle aggregates differently so that Value
    nodes are always consistent with their SILValue and can be
    verified. Aggregates nodes are still coalesced if they only have a
    single pointer-type subelement. If we arbitrarily coalesced an
    aggregate with just one of its subelements then there would be no
    consistent way to identify the value that corresponds to a connection
    graph node.

commit fd3a8804eb4606a5210134d1b23071d5e3af0998
Author: Richard Wei <rxrwei@gmail.com>
Date:   Sat Oct 5 12:03:11 2019 -0700

    [AutoDiff upstream] [AST] Add 'AutoDiffIndexSubset' data structure.
    
    `AutoDiffIndexSubset` is a fixed-size bit vector that is used for efficiently representing a subset of indices in automatic differentiation, specifically for representing a subset of parameters and results of a function to differentiate with respect to. It is uniqued in `ASTContext`.
    
    This patch adds definition and unit tests for `AutoDiffIndexSubset` along with new files `AutoDiff.h` and `AutoDiff.cpp` into the 'AST' target, with no changes to the compiler's behavior. More data structures used for AutoDiff will be added to these files.
    
    ----------------------------
    
    This is part of the ongoing effort to merge the experimental [differentiable programming feature](https://forums.swift.org/t/differentiable-programming-mega-proposal/28547) (informally referred to as "AutoDiff") to the 'master' branch for code reviews and better maintenance.
    
    Upstreaming task: [TF-879](https://bugs.swift.org/browse/TF-879)

commit 9b6ff03f9eff89ee93831080a27e47b16c784f99
Author: Adrian Prantl <aprantl@apple.com>
Date:   Fri Sep 27 13:35:42 2019 -0700

    On Linux build LLVM and subprojects with -gsplit-dwarf which is more space/time
    efficient than -g on that platform. This improves time spent to link products
    built with debug info quite a bit.

commit c79214c3c6f7d2a9880c76b93cae1ec3659dbc8f
Merge: 2c5c4935f21 b4e25f9274d
Author: Michael Gottesman <gottesmm@users.noreply.github.com>
Date:   Tue Sep 10 22:52:11 2019 -0700

    Merge pull request #27030 from gottesmm/pr-691278faf35b36748a5434a7bcfd3feb58044d07
    
    [benchmark] Add versions of prims that in a non-efficient way uses vaâ€¦

commit b4e25f9274d1307e120ad78e070461a481da5d98
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Wed Sep 4 13:05:58 2019 -0700

    [benchmark] Add versions of prims that in a non-efficient way uses various forms of non-strong references.
    
    Specifically, I add some benchmarks for weak, unowned, unsafe (unowned), and
    unmanaged. The reason for the split in between unsafe (unowned) and unmanaged is
    that one is testing the raw compiler features and the other is validating stdlib
    performance.

commit 9e1b2069844664049e346d123e54eaeb379aab54
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Fri Aug 23 13:48:59 2019 -0700

    [Serialization] Collapse indirection in DeclContextID
    
    ...by making it a tagged union of either a DeclID or a
    LocalDeclContextID. This should lead to smaller module files and be
    slightly more efficient to deserialize, and also means that every
    AST entity kind is serialized in exactly one way, which allows for
    the following commit's refactoring.

commit da267bf7cab73038d7af9eaabe8b5cdba509a631
Author: Doug Gregor <dgregor@apple.com>
Date:   Fri Aug 16 10:45:47 2019 -0700

    [Constraint system] Switch TypeVariables to a SetVector.
    
    There were a few places where we wanted fast testing to see whether a
    particular type variable is currently of interest. Instead of building
    local hash tables in those places, keep type variables in a SetVector
    for efficient testing.

commit 05baaa86cc15540bbcae054598fa494ff6be0e7b
Author: Slava Pestov <spestov@apple.com>
Date:   Mon Aug 5 15:23:20 2019 -0400

    AST: Slightly more efficient requiresNewVTableEntry()

commit f0e5e1911d975564642fdb8da6a224c58e20b813
Author: Joe Groff <jgroff@apple.com>
Date:   Wed Jul 31 19:17:22 2019 -0700

    IRGen: Access concrete type metadata by mangled name.
    
    When we generate code that asks for complete metadata for a fully concrete specific type that
    doesn't have trivial metadata access, like `(Int, String)` or `[String: [Any]]`,
    generate a cache variable that points to a mangled name, and use a common accessor function
    that turns that cache variable into a pointer to the instantiated metadata. This saves a bunch
    of code size, and should have minimal runtime impact, since the demangling of any string only
    has to happen once.
    
    This mostly just works, though it exposed a couple of issues:
    
    - Mangling a type ref including objc protocols didn't cause the objc protocol record to get
      instantiated. Fixed as part of this patch.
    - The runtime type demangler doesn't correctly handle retroactive conformances. If there are
      multiple retroactive conformances in a process at runtime, then even though the mangled string
      refers to a specific conformance, the runtime still just picks one without listening to the
      mangler. This is left to fix later, rdar://problem/53828345.
    
    There is some more follow-up work that we can do to further improve the gains:
    
    - We could improve the runtime-provided entry points, adding versions that don't require size
      to be cached, and which can handle arbitrary metadata requests. This would allow for mangled
      names to also be used for incomplete metadata accesses and improve code size of some generic
      type accessors. However, we'd only be able to take advantage of the new entry points in
      OSes that ship a new runtime.
    - We could choose to always symbolic reference all type references, which would generally reduce
      the size of mangled strings, as well as make runtime demangling more efficient, since it wouldn't
      need to hit the runtime caches. This would however require that we be able to handle symbolic
      references across files in the MetadataReader in order to avoid regressing remote mirror
      functionality.

commit 015b7c52660e4d0513e4679dc272ddbe9a1f319f
Author: Doug Gregor <dgregor@apple.com>
Date:   Thu Jul 11 10:55:41 2019 -0700

    [AST] Use AbstractFunctionDecl::getBodySourceRange() more frequently.
    
    A number of callers to AbstractFunctionDecl::getBody() were only
    extracting the source range of the body... which can be retrieved more
    efficiently with getBodySourceRange().

commit 2df36527d3d73c69966c3118be8e13cae544d8c4
Author: Stephen Canon <scanon@apple.com>
Date:   Wed Jun 12 01:02:48 2019 -0400

    Provide a default implementation of multipliedFullWidth (#25346)
    
    * Provide a default implementation of multipliedFullWidth
    
    Previously, [U]Int64 fatalErrored on 32b platforms, which is obviously undesirable. This PR provides a default implementation on FixedWidthInteger, which is not ideally efficient for all types, but is correct, and gives the optimizer all the information that it needs to generate good code in the important case of Int64 arithmetic on 32b platforms. There's still some minor room for improvement, but we'll call that an optimizer bug now.
    
    * Clarify comments somewhat, remove `merge` nested function
    
    I was only using `merge` in one place, so making it a function seems unnecessary. Also got rid of some trucatingIfNeeded inits where the compiler is able to reason that no checks are needed anyway.
    
    * Add some basic test coverage specifically for multipliedFullWidth
    
    * Fix typo, further clarify bounds comments.
    
    * Make new defaulted implementation @_aEIC so we don't need availability.

commit 6befb10d35f5f621df4058812245106df800ba42
Author: Andrew Trick <atrick@apple.com>
Date:   Wed May 8 19:51:33 2019 -0700

    Cache struct/class field offsets in SIL.
    
    The field's ordinal value is used by the Projection abstraction, which is
    the basis of efficiently comparing and sorting access paths in SIL. It must
    be cached before it is used by any SIL passes, including the verifier, or it
    causes widespread quadratic complexity.
    
    Fixes <rdar://problem/50353228> Swift compile time regression with optimizations enabled
    
    In production code, a file that was taking 40 minutes to compile now
    takes 1 minute, with more than half of the time in LLVM.
    
    Here's a short script that reproduces the problem. It used to take 30s
    and now takes 0.06s:
    
    // swift genlazyinit.swift > lazyinit.sil
    // sil-opt ./lazyinit.sil --access-enforcement-opts
    
    var NumProperties = 300
    
    print("""
          sil_stage canonical
    
          import Builtin
          import Swift
          import SwiftShims
    
          public class LazyProperties {
          """)
    
    for i in 0..<NumProperties {
      print("""
              //  public lazy var i\(i): Int { get set }
              @_hasStorage @_hasInitialValue final var __lazy_storage__i\(i): Int? { get set }
            """)
    }
    
    print("""
          }
    
         // LazyProperties.init()
         sil @$s4lazy14LazyPropertiesCACycfc : $@convention(method) (@owned LazyProperties) -> @owned LazyProperties {
         bb0(%0 : $LazyProperties):
           %enum = enum $Optional<Int>, #Optional.none!enumelt
         """)
    
    for i in 0..<NumProperties {
      let adr = (i*4) + 2
      let access = adr + 1
      print("""
              %\(adr) = ref_element_addr %0 : $LazyProperties, #LazyProperties.__lazy_storage__i\(i)
              %\(access) = begin_access [modify] [dynamic] %\(adr) : $*Optional<Int>
              store %enum to %\(access) : $*Optional<Int>
              end_access %\(access) : $*Optional<Int>
            """)
    }
    
    print("""
            return %0 : $LazyProperties
          } // end sil function '$s4lazy14LazyPropertiesCACycfc'
          """)

commit d22b3a7b0b49f565a831df66a9fc4b9b9093e5fe
Author: Slava Pestov <spestov@apple.com>
Date:   Thu May 9 15:30:45 2019 -0400

    Sema: Move the Optional-typed nil peephole to SILGen
    
    When applying a solution to a nil literal of Optional type, we would
    build a direct reference to Optional<T>.none instead of leaving the
    NilLiteralExpr in place, because this would generate more efficient
    SIL that avoided the call to the Optional(nilLiteral: ()) witness.
    
    However, a few places in the type checker build type-checked AST, and
    they build NilLiteralExpr directly. Moving the peephole to SILGen's
    lowering of NilLiteralExpr allows us to simplify generated SIL even
    further by eliding an unnecessary metatype value. Furthermore, it
    allows SILGen to accept NilLiteralExprs that do not have a
    ConcreteDeclRef set, which makes type-checked AST easier to build.

commit f4c7d4611f091673579605eab4827abc82d5dd47
Author: Andrew Trick <atrick@apple.com>
Date:   Tue Mar 5 11:31:02 2019 -0800

    Change the algorithm for the AccessEnforcementDom pass.
    
    This adds a mostly flow-insensitive analysis that runs before the
    dominator-based transformations. The analysis is simple and efficient
    because it only needs to track data flow of currently in-scope
    accesses. The original dominator tree walk remains simple, but it now
    checks the flow insensitive analysis information to determine general
    correctness. This is now correct in the presence of all kinds of nested
    static and dynamic nested accesses, call sites, coroutines, etc.
    
    This is a better compromise than:
    
    (a) disabling the pass and taking a major performance loss.
    
    (b) converting the pass itself to full-fledged data flow driven
    optimization, which would be more optimal because it could remove
    accesses when nesting is involved, but would be much more expensive
    and complicated, and there's no indication that it's useful.
    
    The new approach is also simpler than adding more complexity to
    independently handle to each of many issues:
    
    - Nested reads followed by a modify without a false conflict.
    - Reads nested within a function call without a false conflict.
    - Conflicts nested within a function call without dropping enforcement.
    - Accesses within a generalized accessor.
    - Conservative treatment of invalid storage locations.
    - Conservative treatment of unknown apply callee.
    - General analysis invalidation.
    
    Some of these issues also needed to be considered in the
    LoopDominatingAccess sub-pass. Rather than fix that sub-pass, I just
    integrated it into the main pass. This is a simplification, is more
    efficient, and also handles nested loops without creating more
    redundant accesses. It is also generalized to:
    - hoist non-uniquely identified accesses.
    - Avoid unnecessarily promoting accesses inside the loop.
    
    With this approach we can remove the scary warnings and caveats in the
    comments.
    
    While doing this I also took the opportunity to eliminate quadratic
    behavior, make the domtree walk non-recursive, and eliminate cutoff
    thresholds.
    
    Note that simple nested dynamic reads to identical storage could very
    easily be removed via separate logic, but it does not fit with the
    dominator-based algorithm. For example, during the analysis phase, we
    could simply mark the "fully nested" read scopes, then convert them to
    [static] right after the analysis, removing them from the result
    map. I didn't do this because I don't know if it happens in practice.

commit c037886c2b2adeba2e098f036410b58cb8c7ceeb
Author: David Zarzycki <dave@znu.io>
Date:   Sun Feb 17 12:49:21 2019 -0500

    [Sema] NFC: refactor coerceToType() to use switch statments
    
    The current series of "unrelated" `if` statements makes understanding
    and updating this function harder than necessary. By using two `switch`
    statements, we can avoid these problems and as a bonus, generate more
    efficient code gen.

commit 415cc8fb0c89683ee45ae50cb107b7001eb25cb3
Author: Michael Ilseman <milseman@apple.com>
Date:   Fri Jan 25 16:39:06 2019 -0800

    [String.Index] Deprecate encodedOffset var/init
    
    String.Index has an encodedOffset-based initializer and computed
    property that exists for serialization purposes. It was documented as
    UTF-16 in the SE proposal introducing it, which was String's
    underlying encoding at the time, but the dream of String even then was
    to abstract away whatever encoding happend to be used.
    
    Serialization needs an explicit encoding for serialized indices to
    make sense: the offsets need to align with the view. With String
    utilizing UTF-8 encoding for native contents in Swift 5, serialization
    isn't necessarily the most efficient in UTF-16.
    
    Furthermore, the majority of usage of encodedOffset in the wild is
    buggy and operates under the assumption that a UTF-16 code unit was a
    Swift Character, which isn't even valid if the String is known to be
    all-ASCII (because CR-LF).
    
    This change introduces a pair of semantics-preserving alternatives to
    encodedOffset that explicitly call out the UTF-16 assumption. These
    serve as a gentle off-ramp for current mis-uses of encodedOffset.

commit a310f23b8a6378cbdabc2e5919cddd91d41db30e
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Sun Feb 10 23:26:56 2019 -0800

    [ownership] Add support for load_borrow in predictable mem opt.
    
    This reduces the diff in between -Onone output when stripping before/after
    serialization.
    
    We support load_borrow by translating it to the load [copy] case. Specifically,
    for +1, we normally perform the following transform.
    
      store %1 to [init] %0
      ...
      %2 = load [copy] %0
      ...
      use(%2)
      ...
      destroy_value %2
    
    =>
    
      %1a = copy_value %1
      store %1 to [init] %0
      ...
      use(%1a)
      ...
      destroy_value %1a
    
    We analogously can optimize load_borrow by replacing the load with a
    begin_borrow:
    
      store %1 to [init] %0
      ...
      %2 = load_borrow %0
      ...
      use(%2)
      ...
      end_borrow %2
    
    =>
    
      %1a = copy_value %1
      store %1 to [init] %0
      ...
      %2 = begin_borrow %1a
      ...
      use(%2)
      ...
      end_borrow %2
      destroy_value %1a
    
    The store from outside a loop being used by a load_borrow inside a loop is a
    similar transformation as the +0 version except that we use a begin_borrow
    inside the loop instead of a copy_value (making it even more efficient).

commit c7ac859310448b26c67dd8ac2aa383eecbf9255a
Author: Argyrios Kyrtzidis <kyrtzidis@apple.com>
Date:   Sun Dec 30 15:19:48 2018 -0800

    [Parse] Optimize syntax parsing: Speed-up Lexer::lexTrivia()
    
    Introduce ParsedTrivia which is a more efficient structure to use during lexing than syntax::Trivia.

commit 2ba7090fe8807b1aa5e9e55a6a9fc967fd3a60cb
Author: John McCall <rjmccall@apple.com>
Date:   Mon Dec 10 15:34:06 2018 -0500

    Remove the extra-inhabitant value witness functions.
    
    This is essentially a long-belated follow-up to Arnold's #12606.
    The key observation here is that the enum-tag-single-payload witnesses
    are strictly more powerful than the XI witnesses: you can simulate
    the XI witnesses by using an extra case count that's <= the XI count.
    Of course the result is less efficient than the XI witnesses, but
    that's less important than overall code size, and we can work on
    fast-paths for that.
    
    The extra inhabitant count is stored in a 32-bit field (always present)
    following the ValueWitnessFlags, which now occupy a fixed 32 bits.
    This inflates non-XI VWTs on 32-bit targets by a word, but the net effect
    on XI VWTs is to shrink them by two words, which is likely to be the
    more important change.  Also, being able to access the XI count directly
    should be a nice win.

commit 50b5044abb2d0150cc6f94a2d2160fa47e7a4080
Author: Doug Gregor <dgregor@apple.com>
Date:   Thu Dec 6 16:48:59 2018 -0800

    [IRGen] Don't emit relative references to Objective-C class references.
    
    Objective-C class references (which show up in the __objc_classrefs
    section) are always coalesced by the linker. When we relatively
    address them (which occurs in protocol conformance records), the
    linker may compute the relative offset *before* coalescing, leading to
    an incorrect result. The net effect is a protocol conformance record
    that applies to the wrong Objective-C class, causing all sorts of
    runtime mayhem.
    
    Switch relatively-addressed Objective-C classes over to using the
    Objective-C runtime name of the class. It's a less efficient encoding
    (since we need to go through objc_lookUpClass), but it avoids the
    linker bug.
    
    Fixes rdar://problem/46428085 by working around the linker bug.

commit c0519c7185bbbb3a26f98de7eec09c16ca6e394f
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon Dec 3 17:29:33 2018 -0800

    [ABI] Put mangled inherited protocol witnesses into the resilient witness table
    
    Emit mangled inherited protocol witnesses into the resilient witness table,
    and realize them when we instantiate the resilient witness table. Donâ€™t
    put mangled inherited protocol witnesses into non-resilient witness tables:
    there is no efficient way to make sure they are realized, so keep the
    previous instantiation-function approach.
    
    Implements the rest of rdar://problem/46282080.

commit c3c6fdc77f9e8e7b1c43786fdafb6ee88eddba1a
Author: Michael Ilseman <michael.ilseman@gmail.com>
Date:   Thu Nov 29 18:19:32 2018 -0800

    [String] ASCII fast-path for UTF16View (#20848)
    
    Add an isASCII fast-path for many UTF16View operations. These are
    heavily utilized in random-access scenarios, allowing us to both be
    more efficient and skip generating breadcrumbs for ASCII strings.

commit 702981f7757623ddf1e796f7566af3d29637e91f
Author: Andrew Trick <atrick@apple.com>
Date:   Fri Nov 16 12:56:58 2018 -0800

    Fix MandatoryInlining to not be quadratic.
    
    Inlining has always been quadratic for no good reason. There was a
    special hack for single-block callees that allowed linear inlining.
    
    Instead, the now iterates over blocks and instructions in reverse,
    splitting blocks as it inlines. There no longer needs to be special
    case for single block callees, and the inliner is linear for all kinds
    of callees.
    
    This further simplifies and cleans up the code. There are just a few
    basic invariants that the common inliner needs to provide about how
    blocks are split and laid out. We can do this if we don't add hacks
    for special cases within the inliner. Those invariants allow the
    inliner clients to be much simpler and more efficient.
    
    PerformanceInliner still needs to be fixed.
    
    Fixes SR-9223: Inliner exhibits slow compilation time with a large
    static array.

commit 06807c2ea6080cdd38f8195fd1fcd04be082718f
Author: Doug Gregor <dgregor@apple.com>
Date:   Sun Nov 25 22:10:04 2018 -0800

    [Runtime] Only scan the type descriptor once for metadata cache entries.
    
    Rather than scanning the type descriptor each time we perform a comparison
    or hash of a metadata cache entry, do so only once to establish the number
    of key parameters and the number of witness tables. Use those values to
    more efficiently compare keys.

commit 3e5165d1ab31e4c2b3374bb26c48ff024afaa205
Author: John McCall <rjmccall@apple.com>
Date:   Sat Nov 10 01:52:01 2018 -0500

    Change the compiler ABI of keypaths.
    
    Previously, the stdlib provided:
    
    - getters for AnyKeyPath and PartialKeyPath, which have remained;
    
    - a getter for KeyPath, which still exists alongside a new read
      coroutine; and
    
    - a pair of owned mutable addressors that provided modify-like behavior
      for WritableKeyPath and ReferenceWritableKeyPath, which have been
      replaced with modify coroutines and augmented with dedicated setters.
    
    SILGen then uses the most efficient accessor available for the access
    it's been asked to do: for example, if it's been asked to produce a
    borrowed r-value, it uses the read accessor.
    
    Providing a broad spectrum of accessor functions here seems acceptable
    because the code-size hit is fixed-size: we don't need to generate
    extra code per storage declaration to support more alternatives for
    key paths.
    
    Note that this is just the compiler ABI; the implementation is still
    basically what it was.  That means the implementation of the setters
    and the read accessor is pretty far from optimal.  But we can improve
    the implementation later; we can't improve the ABI.
    
    The coroutine accessors have to be implemented in C++ and used via
    hand-rolled declarations in SILGen because it's not currently possible
    to declare independent coroutine accessors in Swift.

commit abe101c5b9257728fd4edc39a8570b0ca0ebc7a0
Author: Michael Ilseman <milseman@apple.com>
Date:   Thu Nov 8 11:42:28 2018 -0800

    [String] Custom iterator for UnicodeScalarView
    
    Provide a custom iterator rather than relying a the IndexingIterator,
    as an indexing model is less efficient for stateful processing of
    strings. Provides around a 30% speedup.

commit c04dcf3b38f3e4e43aa7bd66a651871cf425cf89
Author: Michael Ilseman <milseman@apple.com>
Date:   Mon Oct 29 15:39:42 2018 -0700

    [String] More efficient breadcrumb-scanning code.
    
    Rather than rely on the UTF16View, scan between breadcrumbs by hand
    for a decent 20% speedup. This code will also make it more obvious how
    to slot in a vectorized solution later.

commit 2e368a3f6a25b5e84c0f682861ea0a5c9b3b26af
Author: Michael Ilseman <milseman@apple.com>
Date:   Sun Oct 28 14:42:42 2018 -0700

    [String] Introduce StringBreadcrumbs
    
    Breadcrumbs provide us amortized O(1) access to the UTF-16 view, which
    is vital for efficient Cocoa interoperability.

commit fe7c3ce2e4524983c3b3297b64b500f26f0a53fa
Author: Michael Ilseman <milseman@apple.com>
Date:   Sun Oct 28 14:40:37 2018 -0700

    [String] Refactorings and cleanup
    
    * Refactor out RRC implementation into dedicated file.
    
    * Change our `_invariantCheck` pattern to generate efficient code in
      asserts builds and make the optimizer job's easier.
    
    * Drop a few Bidi shims we no longer need.
    
    * Restore View decls to String, workaround no longer needed
    
    * Cleaner unicode helper facilities

commit 8395e355e5c8280a1f5f09950494eef603707e8d
Author: Saleem Abdulrasool <compnerd@compnerd.org>
Date:   Tue Oct 16 16:34:56 2018 -0700

    SourceKit: micro-optimization of CMake (NFC)
    
    Set all the target properties in a single shot.  This avoids the multiple string
    parsing within CMake.  Although this makes no noticable differnce, it is
    slightly more efficient and also colocates all the target properties.

commit ce770cdf4e837c4d37007c7bf800295b3a9b156c
Author: Slava Pestov <spestov@apple.com>
Date:   Thu Sep 27 16:36:51 2018 -0700

    AST: Introduce GenericSignature::forEachParam()
    
    This replaces the inefficient pattern:
    
      for (auto param : sig->getGenericParams()) {
        if (sig->isCanonicalTypeInContext(param)) {
          ...
        } else {
          ...
        }
      }

commit 098759f070a9b0346e66ed3bcfb4a1624b7af124
Author: Andrew Trick <atrick@apple.com>
Date:   Tue Aug 21 10:24:42 2018 -0700

    CopyPropagation for SILValues with ownership.
    
    This is a simple "utility" pass that canonicalizes SSA SILValues with
    respect to copies and destroys. It is a self-contained, provably
    complete pass that eliminates spurious copy_value instructions from
    scalar SSA SILValues. It fundamentally depends on ownership SIL, but
    otherwise can be run efficiently after any other pass. It separates
    the pure problem of handling scalar SSA values from the more important
    and complex problems:
    
    - Promoting variables to SSA form (PredictableMemOps and Mem2Reg
      partially do this).
    
    - Optimizing copies within "SIL borrow" scopes (another mandatory pass
      will be introduced to do this).
    
    - Composing and decomposing aggregates (SROA handles some of this).
    
    - Coalescing phis (A BlockArgumentOptimizer will be introduced as part
      of AddressLowering).
    
    - Removing unnecessary retain/release when nothing within its scope
      may release the same object (ARC Code Motion does some of this).
    
    Note that removing SSA copies was more obviously necessary before the
    migration to +0 argument convention.

commit 01ecfdaaf0500a99a0f081760340801799fc0ec0
Author: Xi Ge <xi_ge@apple.com>
Date:   Tue Aug 28 18:19:41 2018 -0700

    IDE: use more efficient API to collect default implementations for protocol members. NFC (#19032)

commit ff684523018e3dae6445898962581ac8dfc4971c
Author: Alex Hoppen <ahoppen@apple.com>
Date:   Wed Jul 18 15:41:59 2018 -0700

    [swiftBasic] Introduce an exponentially growing appending binary stream
    
    It is more efficient than llvm::AppendingBinaryByteStream if a lot of
    small data gets appended to it because it doesn't need to resize its
    buffer on each write.

commit 8294c0003af1301ae9e80044735cafef0e111823
Author: Michael Ilseman <milseman@apple.com>
Date:   Thu Aug 2 15:49:58 2018 -0700

    [string] Drop _StringGuts subscript; NFC
    
    _StringGuts shouldn't expose a subscript, implying efficient
    access. Switch to the explicit code unit fetch method. Update tests
    accordingly, and switch off of deprecated typealiases.

commit b48f7407126fecc22bda7d43e92a0a09d17ff6b3
Author: Alex Hoppen <ahoppen@apple.com>
Date:   Thu May 31 14:35:48 2018 -0700

    [SourceKit] Serialize the syntax tree in the EditorConsumer
    
    This will allow us to switch to a more efficient serialization format in
    the future.

commit 7a4aeed5706604141fd25275a7cc2e6e88924a95
Author: John McCall <rjmccall@apple.com>
Date:   Sun Jul 22 02:28:59 2018 -0400

    Implement generalized accessors using yield-once coroutines.
    
    For now, the accessors have been underscored as `_read` and `_modify`.
    I'll prepare an evolution proposal for this feature which should allow
    us to remove the underscores or, y'know, rename them to `purple` and
    `lettuce`.
    
    `_read` accessors do not make any effort yet to avoid copying the
    value being yielded.  I'll work on it in follow-up patches.
    
    Opaque accesses to properties and subscripts defined with `_modify`
    accessors will use an inefficient `materializeForSet` pattern that
    materializes the value to a temporary instead of accessing it in-place.
    That will be fixed by migrating to `modify` over `materializeForSet`,
    which is next up after the `read` optimizations.
    
    SIL ownership verification doesn't pass yet for the test cases here
    because of a general fault in SILGen where borrows can outlive their
    borrowed value due to being cleaned up on the general cleanup stack
    when the borrowed value is cleaned up on the formal-access stack.
    Michael, Andy, and I discussed various ways to fix this, but it seems
    clear to me that it's not in any way specific to coroutine accesses.
    
    rdar://35399664

commit 1b63b688bf2c6e5a3349a9038bfd24c1c6371ed4
Author: Doug Gregor <dgregor@apple.com>
Date:   Wed Jul 18 16:56:43 2018 -0700

    [AST] Use TinyPtrVector for the list of overridden declarations.
    
    TinyPtrVector is a more-space-efficient SmallVector<_, 1>. Use it.

commit 1954a1a6670f5ca6430dfaab14c4cbc7ec71cb89
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Jul 17 11:13:14 2018 -0700

    [AST] Try to resolve generic parameter depths lazily.
    
    If we need the depth of a generic parameter declaration for
    canonicalization, but it hasnâ€™t been computed yet, resolve the
    signature of its enclosing declaration. This is a heavy hammer for
    a property that should be efficiently computable, but itâ€™s along an
    existing crashing path and addresses a compiler-crasher regression
    due to us validating ahead of time with the request-evaluator.

commit 1ebe33e03c8b0f0c1ecbcc836ffe7a2b267c891d
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Wed Jul 11 12:42:10 2018 -0700

    GenericSpecializer: Allow simple function specialization cycles.
    
    So far we immediately bailed once we detect a cycle in specializations. But it turned out that this prevented efficient code generation for some stdlib functions like compactMap.
    With this change we allow specialization of cycles up to a depth of 1 (= still very limited to prevent code size explosion in some corner cases).
    
    The effect of this optimization is tested with the existing benchmark FatCompactMap.
    
    SR-7952, rdar://problem/41005326

commit 7938272c426186ae2c16c79f1dc2f346293c0691
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Fri Jun 15 08:23:11 2018 -0700

    [DebugInfo] Don't record that the standard library imports itself (#17226)
    
    This causes problems for cross-compilation -parse-stdlib tests that
    emit debug info. At the moment we have zero of those, but we're
    trying to add one.
    
    Also, don't try to load new modules when recording imports. (This
    isn't harmful, just inefficient.)

commit 8409d29ca3aaef309a57f6cc5498dc7f2da06a6d
Author: Saleem Abdulrasool <compnerd@compnerd.org>
Date:   Fri Jun 8 09:23:42 2018 -0700

    stdlib: unify AnyObject downcast
    
    Now that bridging is enabled, unify the ObjC and non-ObjC paths.  Expand
    the comment with how to efficiently grab the object reference and when
    it can be enabled.

commit a5b33396ce62ae0348dd2e67562e6647c765f3ba
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Tue Jun 5 17:46:26 2018 -0700

    Allow global variables which are string literals to be generated in the data section.
    
    So there is no need to initialize global string variables dynamically (with dispatch_once) anymore.
    This is much more efficient, both in terms of code size and performance

commit 4ec98bb25ca45fe3d4ba027d6c5459cd4b251af1
Author: Michael Ilseman <milseman@apple.com>
Date:   Thu May 24 14:18:03 2018 -0700

    [String] Plumb more Substring.*View APIs through Slice
    
    Rather than rely on (potentially inefficient) generic specializations,
    plumb all Slice APIs explicitly from Substring.*View to
    Slice<String.*View>.

commit 459833725ea41436069985da2d1511295931b774
Author: Michael Ilseman <milseman@apple.com>
Date:   Fri May 11 12:26:43 2018 -0700

    [String] Streamline more String creation logic.
    
    Streamline and de-genericize non-inlinable internal functions to
    create a String from UTF-8 efficiently.

commit 17b0ed47089b91f337c4b19e444f7c3a8b1b8c93
Author: Michael Ilseman <milseman@apple.com>
Date:   Sun Apr 29 14:48:20 2018 -0700

    [String] Define _copyContents for UTF8View
    
    Define _copyContents on String.UTF8View, which allows it to
    efficiently memcpy bytes when the String is already in UTF-8 (or
    ASCII).

commit ab56fa3e9ca27422913d355a3cb0139cea200be4
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon Apr 30 16:21:58 2018 -0700

    [Serialization] Support (de-)serialization of SubstitutionMaps.
    
    Allow substitution maps to be serialized directly (via an ID), writing out
    the replacement types and conformances as appropriate. This is a more
    efficient form of serialization than the current SubstitutionList approach,
    because it maintains uniqueness of substitution maps within a module file,
    and is a step toward eliminating SubstitutionList entirely.

commit cdfeb88cfed7781bbdef7c9b07fb265347a87c6e
Author: Michael Ilseman <milseman@apple.com>
Date:   Sun Mar 25 15:07:58 2018 -0700

    [string] Simplify creation logic, especially for C strings.
    
    Streamline internal String creation. Previously, everything funneled
    into a single generic function, however, every single call of the
    generic funnel had relevant specific information that could be used
    for a more efficient algorithm.
    
    In preparation for efficiently forming small strings, refactor this
    logic into a handful of more specialized subroutines to preserve more
    specific information from the callers.

commit 977a0e0d865b68153b5d75c9fc079ca3ba9e1d59
Author: Michael Ilseman <michael.ilseman@gmail.com>
Date:   Sat Mar 24 16:22:27 2018 -0700

    [docs] Add productivity hacks to stdlib progman (#15403)
    
    [docs] Add productivity hacks to stdlib prog man
    
    Small section about using ninja to more efficiently work on the standard library.

commit dac06898e94b24b5a6b83533d2e0bae356a64529
Author: Robert Widmann <devteam.codafi@gmail.com>
Date:   Mon Oct 3 01:28:28 2016 -0400

    [SE-0194] Deriving Collections of Enum Cases
    
    Implements the minimum specified by the SE-proposal.
    
    * Add the CaseIterable protocol with AllCases associatedtype and
    allCases requirement
    * Automatic synthesis occurs for "simple" enums
        - Caveat: Availability attributes suppress synthesis.  This can be
                  lifted in the future
        - Caveat: Conformance must be stated on the original type
                  declaration (just like synthesizing Equatable/Hashable)
        - Caveat: Synthesis generates an [T].  A more efficient collection
                  - possibly even a lazy one - should be put here.

commit f2136713279a025e3b62d44352cb84b892f9e094
Author: Joe Groff <jgroff@apple.com>
Date:   Thu Mar 1 19:23:31 2018 -0800

    Runtime: Handle `getDescription` correctly in +0 mode.
    
    This can eventually be made more efficient by avoiding copies in all the
    callees, but this is the minimal fix. Remove an unnecessary bit of
    reverse-dependency on the Foundation overlay while we're here.
    
    rdar://34222540

commit 953dddd5d315e31bafe2669ff2a38f0ef6de7187
Author: Joe Groff <jgroff@apple.com>
Date:   Thu Feb 1 17:40:31 2018 -0800

    IRGen/Runtime: Allow mangled type refs to embed "symbolic references" to type context descriptors.
    
    This makes resolving mangled names to nominal types in the same module more efficient, and for eventual secrecy improvements, also allows types in the same module to be referenced from mangled typerefs without encoding any source-level name information about them.

commit 29065095ff4bb5c3ca0388890231f5b803388694
Author: Andrew Trick <atrick@apple.com>
Date:   Thu Feb 1 17:36:11 2018 -0800

    Make DiagnoseUnreachable and NoReturnFolding function passes.
    
    We want as few module passes as possible.
    
    Function passes allow the PassManager to do its job.
    e.g. it can filter certain functions that should not be applied to the
    current pipeline. This will result in less work in the pass itself and
    fewer pass manager related bugs.
    
    Function passes are easier to understand and debug in the context of the
    pipeline. Things like PrettyStackTrace are handled automatically.
    Bisecting functionality is builtin.
    
    Function passes are more compatible in general with inter-procedural
    analysis.
    
    Function passes are more efficient.
    
    A single module pass in the middle of the pipeline destroys the benefit
    of the rest of the pipeline uses function passes.

commit a7a3b175978bd84b866554e666a2647b3142a26c
Author: Joe Groff <jgroff@apple.com>
Date:   Tue Dec 12 09:57:36 2017 -0800

    Replace nominal type descriptors with a hierarchy of context descriptors.
    
    This new format more efficiently represents existing information, while
    more accurately encoding important information about nested generic
    contexts with same-type and layout constraints that need to be evaluated
    at runtime. It's also designed with an eye to forward- and
    backward-compatible expansion for ABI stability with future Swift
    versions.

commit cc0e64dec47645920ec8df510817b35c10c8f655
Author: Xiaodi Wu <xiaodi.wu@gmail.com>
Date:   Sun Jan 28 00:29:20 2018 -0600

    Implement efficient DoubleWidth division and fix division-related bugs

commit 6118d8603236c434544eac502f3e77ba8b87c42c
Author: Doug Gregor <dgregor@apple.com>
Date:   Thu Jan 4 11:47:50 2018 -0800

    [Runtime] Introduce equality operation for nominal type descriptors.
    
    Nominal type descriptors are not always unique, so testing them via pointer
    equality is not correct. Introduce an "isEqual()" operation for
    nominal type descriptors that performs the appropriate equality check,
    using pointer equality when possible, and falling back to string
    comparisons of the mangled type name when it is not possible.
    
    Introduce a "nonunique" flag into nominal type descriptors to describe
    when they are, in fact, not unique. The only nonunique nominal type
    descriptors currently come from Clang-imported types; all
    Swift-defined types have unique nominal type descriptors. Use this
    flag to make the aforementioned operation efficient in the "unique"
    case.
    
    Use the new isEqual() operation for protocol conformance lookup, and
    make sure we're caching results based on the known-canonical nominal
    type descriptor.

commit 3910c0d2118e96fc6445e5f35f03056b84bfdaf1
Author: David Zarzycki <dave@znu.io>
Date:   Wed Dec 27 11:15:04 2017 -0500

    [SIL] NFC: Remove TailAllocatedOperandList template
    
    Please use InstructionBaseWithTrailingOperands template now. It is more type safe and more space efficient.

commit 4f9cb35f6afead17cbe2ba643feecff5140efe2e
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Nov 21 22:19:00 2017 -0800

    [Type checker] Fix ranking of _OptionalNilComparisonType overloads of ==/!=.
    
    Always prefer them to Optionals ==/!=, because they are @transparent and
    more efficient.

commit 28c8fb8b0f0fda871b3b342faf35ddde2f855e02
Merge: 2ec6eb51cda 3f51dbc3b16
Author: Slava Pestov <sviatoslav.pestov@gmail.com>
Date:   Wed Nov 15 22:50:38 2017 -0800

    Merge pull request #12855 from slavapestov/fix-extension-binding
    
    More efficient extension binding

commit 95d251051b869c2cf7ea27ebb4764e412a6ddd0b
Author: Joe Groff <jgroff@apple.com>
Date:   Thu Nov 2 15:53:58 2017 -0700

    Runtime: Put ObjC class wrapper unwrapping behind a runtime call.
    
    This is a small code size win, and also gives us some abstraction so that future cooperative ObjC compilers/runtimes might be able to interoperate ObjC class objects with Swift type metadata efficiently than they currently are in the fragile Swift runtime.
    
    While I'm here, I also noticed that swift_getObjCClassMetadata was unnecessarily getting exposed in non-ObjC-interop runtime builds, so I fixed that as well.

commit 5b8c914582c5a8e454c7b691b369caf92fb0c0d8
Author: Doug Gregor <dgregor@apple.com>
Date:   Fri Oct 27 21:46:45 2017 -0700

    [GSB] Reimplement equivalence class "anchor" logic.
    
    Replace the pair of PotentialArchetype's getArchetypeAnchor() and
    getNestedArchetypeAnchor() with a straightforward, more-efficient
    computation based on equivalence classes. This reduces the number of
    times we query the archetype anchor cache by 88% when building the
    standard library, as well as eliminating some
    PotentialArchetype-specific APIs.

commit 086c12114dfdff1b7b7179a6052f43a8d73557ed
Author: Saleem Abdulrasool <compnerd@compnerd.org>
Date:   Thu Sep 28 21:29:57 2017 -0700

    IRGen: switch to absolute pointers for nominal type descriptors
    
    Alter the value metadata layout to use an absolute pointer for the
    nominal type descriptor rather than a relative offset relative to the
    complete type metadata.  Although this is slightly less efficient in
    terms of load times, this is more portable across different
    environments.  For example, PE/COFF does not provide a cross-section
    relative offset relocation.  Other platform ports are unable to provide
    a 64-bit relative offset encoding.
    
    Given that the value witness table reference in the value metadata is
    currently an absolute pointer, this page is most likely going to be
    dirtied by the loader.

commit fb253b182af71aa8a4843d0b50b3af61a1ce073b
Author: Doug Gregor <dgregor@apple.com>
Date:   Thu Sep 14 10:21:47 2017 -0700

    Use a more efficient SubSequence type for lazy map and filter.
    
    Rather than using the default slice type when slicing the collection produced
    by a lazy map or filter, slice the base collection and form a new
    lazy map/filter collection from it. This allows any optimizations provided by
    the collection SubSequence type to kick in, as well as ensuring that slicing
    a lazy collection provides the same type as producing a lazy collection of a
    slice.
    
    This is technically source-breaking, because someone could have spelled out
    the types of slicing a lazy filter or mapâ€¦ but it seems unlikely to matter
    in practice and the benefits could be significant.
    
    Fixes ABI FIXMEâ€™s #28 and #46.

commit 8b73de60a9ec5043c44ae2bc210846fbe3445a11
Author: Slava Pestov <spestov@apple.com>
Date:   Thu Sep 21 18:37:43 2017 -0700

    IRGen: Remove GenericTypeRequirements::ParentType
    
    Finally, remove the parent type metadata argument from type
    constructors.
    
    Now that type constructors don't take a parent metadata pointer,
    we can hit some asserts concerning type constructors that do not
    have any parameters. This happens when you define a concrete type
    in a fully-constrained extension of a generic type.
    
    A more efficient ABI would use concrete type metadata for these
    cases, but that would be a bigger change that we can do later, so
    for now just relax these assertions.
    
    This resolves a runtime crasher since a circular metadata case is
    no longer circular. I renamed the crasher to reference the more
    specific radar since the more general issue of circular metadata
    is still unresolved.

commit fb8d5c3d52a6729be9803900149a3274ad11daa7
Merge: da22d25b8c2 d6f67742147
Author: eeckstein <eeckstein@apple.com>
Date:   Mon Sep 18 19:35:09 2017 -0700

    Merge pull request #11936 from eeckstein/string-enums
    
    Produce more efficient code for the init(rawValue: String) constructor of string enums

commit ba1a5f9caea64673e98f466acd2fb055aa7af6ea
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Thu Sep 14 15:49:55 2017 -0700

    Produce more efficient code for the init(rawValue: String) constructor of string enums, part 2.
    
    Use a dictionary for string lookup, which is initialized the first time the constructor is called.
    This is more efficient than just iterating of the string table.
    
    Unfortunately it's still not as fast as the original version (where all the string comparisons are inlined into the constructor) for enums with < 100 strings.
    But this will improve once we can pass the string and string table as borrowed parameters and we can reduce the ARC overhead.

commit 0bdd91a0397bf131859112ecca27b6b983261acd
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Thu Sep 14 15:19:14 2017 -0700

    Produce more efficient code for the init(rawValue: String) constructor of string enums.
    
    Instead of inlining a series of string comparisons, we call a library function which does the string lookup on a table of static strings.
    This reduces the code size of those initializers dramatically.
    Performance wise it's slower than before, because the string comparisons are not inlined anymore.

commit b1debfc401bd80cfe519761155d6219a23dd83ff
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Fri Aug 11 14:05:00 2017 -0700

    [epilogue-arc-analysis] Be more efficient with memory usage.
    
    This patch fixes a number of issues:
    
    The analysis was using EpilogueARCContext as a temporary when computing. This is
    an performance problem since EpilogueARCContext contains all of the memory used
    in the analysis. So essentially, we were mallocing tons of memory every time we
    missed the analyses cache. This patch changes the pass to instead have 1
    EpilogueARCContext whose internal state is cleared in between invocations. Since
    the data structures (see below) used after this patch do not shrink memory after
    being cleared, this should cause us to have far less memory churn.
    
    The analysis was managing its block state data structure by allocating the
    individual block state structs using a BumpPtrAllocator/DenseMap stored in
    EpilogueARCContext. The individual state structures were allocated from the
    BumpPtrAllocator and the DenseMap then mapped a specific SILBasicBlock to its
    State data structure. Ignoring that we were mallocing this memory every time we
    computed rather than reusing global state, this pessimizes performance on small
    functions significantly. This is because the BumpPtrAllocator by default heap
    allocates initially a page and DenseMap initially mallocs a 64 entry hash
    table. Thus for a 1 block function, we would be allocating a large amount of
    memory that is just unneeded.
    
    Instead this patch changes the analysis to use a std::vector in combination with
    PostOrderFunctionInfo to manage the per block state. The way this works is that
    PostOrderFunctionInfo already contains a map from a SILBasicBlock to its post
    order number. So, when we are allocating memory for each block, we visit the CFG
    in post order. Thus we know that each block's state will be stored in the vector
    at vector[post order number].
    
    This has a number of nice effects:
    
    1. By eliminating the need for the DenseMap, in large test cases, we are
    signficiantly reducing the memory overhead (by 24 bytes per basic block assuming
    8 byte ptrs).
    2. We will use far less memory when applying this analysis to small functions.
    
    rdar://33841629

commit c042058b82080c68a9a17057d504d3f450646ae9
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Tue Aug 1 17:04:36 2017 -0700

    Modify the optimization pipeline to better support custom array iterators
    
    This patch is supposed to recover the performance regressions that would be introduced by yet to be merged PR #9145, which introduces custom, more efficient array iterators.
    
    The crucial part of this patch is running loop unrolling also during the mid-level optimizations phase, because it may catch more loops with constant trip counts. To make trip counts constant, an additional run of constant propagation is helpful.

commit 4f5e14845bcbff85dafcba5e010e6748f63f0f90
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Mon May 22 15:59:43 2017 -0700

    [stdlib] Eliminate optionals from specialization code
    
    Apparently this makes the optimizer happier.  Still waiting on #9792 for truly
    efficient specialization.
    
    https://github.com/apple/swift/pull/9792

commit fd2ac31c6e8a6c18da0b40bfe1c93407b076e463
Author: Max Moiseev <moiseev@apple.com>
Date:   Thu May 18 12:24:13 2017 -0700

    [stdlib] Adding RangeReplaceable.filter returning Self
    
    This overload allows `String.filter` to return a `String`, and not
    `[Character]`.
    
    In the other hand, introduction of this overload makes `[123].filter`
    somewhat ambiguous in a sence, that the compiler will now prefer an
    implementatin from a more concrete protocol, which is less efficient for
    arrays, therefore extra work is needed to make sure Array types fallback
    to the `Sequence.filter`.
    
    Implements: <rdar://problem/32209927>

commit a797b29c1ca6b0516fd05669da2225fa8e9cd323
Author: Max Moiseev <moiseev@apple.com>
Date:   Thu May 4 14:53:52 2017 -0700

    [stdlib] A more efficient implementation of signum for integer types

commit 0290c2d5d8f5b491fb5b313c30c9682f4327a438
Author: Slava Pestov <spestov@apple.com>
Date:   Mon Apr 24 01:24:25 2017 -0700

    AST: Make GenericSignature and GenericEnvironment SubstitutionMaps interchangable
    
    SubstitutionMap::lookupConformance() would map archetypes out
    of context to compute a conformance path. Do the same thing
    in SubstitutionMap::lookupSubstitution().
    
    The DenseMap of replacement types in a SubstitutionMap now
    always has GenericTypeParamTypes as keys.
    
    This simplifies some code and brings us one step closer to
    a more efficient representation of SubstitutionMaps.

commit 2396e7d3cce0d4cd225e46a11ac4d22e8f225b07
Author: Andrew Trick <atrick@apple.com>
Date:   Fri Mar 31 16:43:19 2017 -0700

    [SILOpt][NFC] Print projections readably and efficiently.
    
    Begin to make the RLE pass debuggable.
    Overhaul the ProjectionPath printing feature and fixup tests.

commit 98e6cafdc8ebb75e4454f7e53cce2107957b9395
Author: Maxim Moiseev <moiseev@users.noreply.github.com>
Date:   Mon Mar 6 10:16:42 2017 -0800

    [stdlib] Adding a deprecated version of flatMap to warn misuses. (#7823)
    
    Due to implicit promotion to optional it is possible to call flatMap
    with a closure, that does not return an optional. This way the code
    works, but is unnecessary inefficient. Such uses of flatMap can and
    should be replaced with map.

commit a04a29af4fa472840c18403133d00eae1c805e02
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Thu Mar 2 16:09:13 2017 -0800

    mangling: efficient mangling of repeated substitutions
    
    Instead of appending a character for each substitution, we now prefix the substitution with the repeat count, e.g.
    AbbbbB -> A5B
    
    The same is done for known-type substitutions, e.g.
    SiSiSi -> S3i
    
    This significantly shrinks mangled names which contain large lists of the same type, like
      func foo(_ x: (Int, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int, Int))
    
    rdar://problem/30707433

commit c2da97172b92085dc5cccd67f1a4293fd1348d36
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon Feb 27 23:10:17 2017 -0800

    [GSB] Store the root potential archetype in RequirementSource.
    
    Use TrailingObjects to help us efficiently store the root potential
    archetype within requirement sources, so we can reconstruct the
    complete path from the point where a requirement was created to the
    potential archetype it affects.
    
    The test changes are because we are now dumping the root potential
    archetype as part of -debug-generic-signatures.

commit c5dfb5238a1d707c7bd6ff46c143364b3628273c
Author: Slava Pestov <spestov@apple.com>
Date:   Sun Feb 12 01:32:56 2017 -0800

    AST: Completely hide internal representation of SubstitutionMap
    
    Make the addSubstitution() and addConformance() methods private,
    and declare GenericEnvironment and GenericSignature as friends of
    SubstitutionMap.
    
    At some point in the future, we can switch to a more efficient
    representation of SubstitutionMap, where instead of storing
    multiple hashtables, we store arrays; the keys are pre-determined.

commit a21d1827a94c3c16df4ca2967a23eb5669eecced
Author: Michael Ilseman <milseman@apple.com>
Date:   Mon Dec 12 13:08:34 2016 -0800

    [Clang Importer] Unified ForeignErrorConvention
    
    Expose a ForeignErrorConvention::Info struct, so that the
    ClangImporter can also utilize this space-efficient
    storage. Eliminates the ClangImporter's ad-hoc representation, and
    shaves off a pointer or so off of the ImportedName size.
    
    While we're out it, make some of the bools bitfields to shave off
    another pointer in size. Total ImportedName size is now 6.

commit f751d0139e27d9ada16fa13655af3b5c06a62235
Author: Erik Eckstein <eeckstein@apple.com>
Date:   Fri Dec 2 13:56:59 2016 -0800

    docs, Mangling: specification of the new mangling scheme.
    
    These are the main changes:
    
    *) Change the order of the mangling to a post-fix like structure.
    This is the biggest change.
    It will help to get more common prefixes in the mangled names to optimize the trie in the mach-o object files.
    The length of the mangled names will mostly stay the same but the order of 'operands' inside the mangling is more or less reversed.
    This change also required to use different 'operator' characters in some cases.
    
    *) Word-substitutions
    Similar to the S-substitutions, but finer grained. See section 'Identifiers'.
    Reduces the size of mangled names in general.
    
    *) Combined substitutions
    A more efficient way to mangle multiple A-substitutions (which were S-substitutions in the old scheme)
    Reduces the size of mangled names with lots of substitutions, e.g. specialized functions.
    
    *) Change the '_T' prefix to '_S'
    Because it's basically a completely new mangling scheme.

commit f150d239188d36fdae02711c9b36cbe2cd47da8d
Author: Doug Gregor <dgregor@apple.com>
Date:   Wed Nov 30 23:58:26 2016 -0800

    [AST] Tail-allocate context types in GenericEnvironment.
    
    Rather than storing a heavyweight DenseMap for the mapping from
    interface types (which are always generic type parameters) to their
    corresponding context types, tail-allocate the array of context types
    as an array parallel to the generic type parameters array. Use
    GenericParamKey's lookup facilities and the new
    type-substitution-function-based version of Type::subst() to handle
    queries efficiently.

commit 9e465fa03de2c079fe10f4cb5dc6eeebb1052f39
Author: Doug Gregor <dgregor@apple.com>
Date:   Wed Nov 30 23:12:07 2016 -0800

    [AST] Add a form of Type::subst() that takes an arbitrary substitution function
    
    The "core" data structure used to record the substitutions to be
    performed is a TypeSubstitutionMap, which is a DenseMap. This is a
    fairly heavyweight, static data structure for something where
    
    * We occasionally want a more dynamic, lazily-populated data structure, and
    * We can usually provide more efficient storage than a DenseMap.
    
    So, introduce a Type::subst() variant that takes a TypeSubstitutionFn,
    which is just a function that maps a SubstitutableType * to a Type (or
    nothing). Use this as the core variant of subst(). with an adapter for
    existing TypeSubstitutionMaps. Over time, TypeSubstitutionMap should
    go away.

commit ffc8a3781160fee80f4787181cd0d831bc98ba22
Author: Max Moiseev <moiseev@apple.com>
Date:   Wed Nov 16 12:20:05 2016 -0800

    Strideable for _Pointer is not necessary in Swift 4
    
    All the necessary methods and operators are already implemented for
    concrete pointer types more efficiently.

commit 4d946f4918fe1fe02fff5a447b506de06658f8fe
Author: Xi Ge <xi_ge@apple.com>
Date:   Mon Oct 10 14:42:40 2016 -0700

    [SyntaxModel] Improve the performance of searching URLs in comments (#5214)
    
    [SyntaxModel] When searching URLs in doc comments, reduce the number of protocol name comparisons by looking ahead more characters, NFC. rdar://28298506
    
    Searching URL in doc comments can be expensive. We used to look for
    every colon as an indicator of potential URLs. However, this is not
    efficient enough. Suggested by Ben, we further divide protocols into
    categories so that most protocols can use "://" as an indicator of its
    existence.
    
    Not sure whether this is enough to close the radar, but I believe it is
    a valuable performance improvement anyway.

commit bf77f75aa7d25fe58776cf450f6cec07461bd2d2
Author: Michael Ilseman <milseman@apple.com>
Date:   Fri Sep 30 16:14:29 2016 -0700

    [Clang Importer] Make EnumInfoCache be per-Clang-instance based.
    
    Delay initialization of the EnumInfoCache until a Clang instance is
    ready, simplifying its interface and allowing us to finally make this
    per-Clang-instance. This will allow us to further de-couple ImportName
    from the importer imply, as well as allow us to use a more efficient
    and simpler caching mechanism. It is now owned by the NameImporter.
    
    NFC.

commit 92c180031fc613de82c5a4fbb58773146cd45c2e
Author: Dmitri Gribenko <gribozavr@gmail.com>
Date:   Fri Sep 9 09:41:12 2016 -0700

    stdlib: make 'Sequence.first(where:)' a pure protocol extension (no dynamic dispatch)
    
    SE-0032 did not propose a protocol entry point, only a protocol
    extension.
    
    Using a pure protocol extension is the right choice here because a
    concrete sequence can't provide a more efficient implementation of this
    method than the default one.

commit 64ce6698ebef4387ad3113b532b7b592c3168141
Author: Slava Pestov <spestov@apple.com>
Date:   Sat Aug 20 02:44:29 2016 -0700

    Sema: Refactor ConstraintSystem::computeSubstitutions() to use GenericSignature::getSubstitutions(), NFC
    
    Note that there was some non-obvious dead code here:
    
    - We already drop conformance requirements on dependent types that
      are the subject of same-type constraints, so we no longer have to
      skip dependent types that are mapped to concrete types explicitly.
    
      In fact, if this were not the case, other code that iterates over
      the requirements of a GenericSignature would be wrong. The original
      hack was added in 2014, I guess we fixed the ArchetypeBuilder
      since then.
    
    - We never end up here where the original type to substitute is a
      recursive archetype. Recursive archetypes are not really a thing
      that is implemented properly, and even in the compiler_crashers
      collection this wasn't triggered.
    
    - With the above two changes, the mapTypeIntoContext() call is not
      necessary at all, which is nice because this is somewhat inefficient;
      mapTypeIntoContext() walks all outer generic parameter lists to
      find the archetype for the given dependent type.

commit fc6d6d7c51ee177d90954bbb7a400787f375bb5a
Author: John McCall <rjmccall@apple.com>
Date:   Thu Aug 18 15:40:22 2016 -0700

    Perform collection force-casts by force-casting the elements
    instead of forcing conditional casts of the elements.
    
    This should produce better and more compact code, allow more
    efficient runtime behavior, and generate much better runtime
    diagnostics if the cast fails.

commit a7a954ae8ca732ee04b07c7be0e76a54f8a5af3e
Author: John McCall <rjmccall@apple.com>
Date:   Thu Aug 18 15:40:22 2016 -0700

    Perform collection force-casts by force-casting the elements
    instead of forcing conditional casts of the elements.
    
    This should produce better and more compact code, allow more
    efficient runtime behavior, and generate much better runtime
    diagnostics if the cast fails.

commit d06d4e5ad7813553edc70f1c6f126666d77db9b2
Author: Manav Gabhawala <manav1907@gmail.com>
Date:   Sun Apr 10 01:00:06 2016 -0400

    [AST][Sema] Fixes the IterativeTypeChecker and better manages circular protocol inheritance
    
    The IterativeTypeChecker now use loops instead of recursion to help keep the stack size low
    We diagnose circular dependencies for protocols in a more efficient manner and also prevent the possibility of infinite loops

commit b4d305905163f4c0f09658dc4041b8646cc68ba5
Author: Adrian Prantl <aprantl@apple.com>
Date:   Fri Jul 1 11:38:14 2016 -0700

    Debug Info: Unique forward declarations generated for scopes and types.
    This is mostly a cleanup and results slightly more efficient debug info.
    
    rdar://problem/25965038

commit be793d26eb0280ddd22813e6138971b117a1ba78
Author: Xin Tong <xin_tong@apple.com>
Date:   Wed Jun 8 08:49:09 2016 -0700

    Remove last bit of retain release code motion in SILCodeMotion. All these code are
    replaced by retain release code motion. This code has been disabled for sometime now.
    
    This should bring the retain release code motion into a close. The retain release
    code motion pipeline looks like this. There could be some minor cleanups after this though.
    
    1. We perform a global data flow for retain release code motion in RRCM (RetainReleaseCodeMotion)
    2. We perform a local form of retain release code motion in SILCodeMotion. This is more
    for cases which can not be handled in RRCM. e.g. sinking into a switch is more efficiently
    done in a local form, the retain is not needed on the None block. Release on SILArgument needs
    to be split to incoming values, this can not be done in RRCM and other cases.
    3. We do not perform code motion in ASO, only elimination which are very important.
    
    Some modifications to test cases, they look different, but functionally the same.
    RRCM has this canonicalization effect, i.e. it uses the rc root, instead of
    the SSA value the retain/release is currently using. As a result some test cases need
    to be modified.
    
    I also removed some test cases that do not make sense anymore and lot of duplicate test
    cases between earlycodemotion.sil and latecodemotion.sil. These tests cases only have retains
    and should be used to test early code motion.

commit ea8ab58f18def08d402b143a95beb0425f418a79
Author: Joe Groff <jgroff@apple.com>
Date:   Fri May 27 11:13:51 2016 -0700

    Extend Accessors doc with tentative getForMutation design.
    
    The intent here is to expose an internal mechanism to the standard library in order to allow it to take advantage of the runtime's "pinning" mechanism to enable efficient divide-and-conquer mutation using slices.

commit f54926926b4387f624f8f02e1652800f65df09ed
Author: Joe Groff <jgroff@apple.com>
Date:   Thu May 12 14:44:23 2016 -0700

    IRGen: Use clang::Sema::LookupName instead of TranslationUnit::lookup to find NSInteger.
    
    Sema's lookup is much more efficient.

commit 8d809993bec5187167f1d11253969a7113e4d7dc
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Sun May 1 19:42:40 2016 -0700

    [stdlib] Use CountableRange for Indices more
    
    This will be much more efficient than the default, when it applies,
    because it doesn't need to carry a reference to underlying collection
    storage.

commit bfc9683b49da7dcfb92ec229f45c6434b61d9f79
Author: Xin Tong <xin_tong@apple.com>
Date:   Mon Apr 18 08:58:35 2016 -0700

    Use a SmallPtrSet instead of a DenseSet. More memory efficient

commit fd353df19eebdc55a7613c98948c95f8be0c787a
Author: Xin Tong <xin_tong@apple.com>
Date:   Thu Mar 17 21:10:27 2016 -0700

    Remove some of unneeded functionality in CallerAnalysis
    
    We really only need the analysis to tell whether a function has caller
    inside the module or not. We do not need to know the callsites.
    
    Remove them for now to make the analysis more memory efficient.
    
    Add a note to indicate it can be extended.

commit c59de00b8528e20a5ec394fde4fca711f0bdbfa9
Author: Dmitri Gribenko <gribozavr@gmail.com>
Date:   Tue Feb 16 00:22:12 2016 -0800

    CollectionsMoveIndices: wording improvements, added an analysis section
    
    One important advantage is that the proposed model does not prevent
    existing viable collection designs from being implemented and trivially
    ported, but it allows more efficient designs.

commit 56dff87c74f2b71823ebc9500a2c2bcbfc81a242
Author: Michael Ilseman <milseman@apple.com>
Date:   Tue Feb 9 10:10:39 2016 -0800

    [SILGen] Use the more efficient lookupConformance

commit 70c5755adb22a1605e78dceff5160a9b5b96b087
Author: Luke Howard <lukeh@padl.com>
Date:   Wed Dec 30 22:43:15 2015 -0800

    [SR-381]: runtime resolution of type metadata from a name
    
    replace ProtocolConformanceTypeKind with TypeMetadataRecordKind
    
    metadata reference does not need to be indirectable
    
    more efficient check for protocol conformances
    
    remove swift_getMangledTypeName(), not needed yet
    
    kill off Remangle.cpp for non-ObjC builds
    
    cleanup
    
    cleanup
    
    cleanup comments

commit f1682cd9a8e010eb477f2f90bac8f162c76ceadc
Author: John McCall <rjmccall@apple.com>
Date:   Wed Jan 13 17:42:41 2016 -0800

    Use real types instead of "Self" for the IR value names of local type data.
    
    Since that's somewhat expensive, allow the generation of meaningful
    IR value names to be efficiently controlled in IRGen.  By default,
    enable meaningful value names only when generating .ll output.
    
    I considered giving protocol witness tables the name T:Protocol
    instead of T.Protocol, but decided that I didn't want to update that
    many test cases.

commit 1e2f1ff5f2cb72816bc67692431571aae1938d9e
Author: Nadav Rotem <nrotem@apple.com>
Date:   Fri Jan 1 22:38:30 2016 -0800

    [Compression] Accenerate the encoding of variable length strings.
    
    Before this commit we allocated a large bitstream and we kept shifting
    and adding new bits every iteration.
    
    With this commit we'll keep growing the APInt right before we insert new bits.
    This makes the first iterations very efficient because the bitstream is small,
    and the last iterations slightly better then the previous implementation
    (because our initial estimate for bitstream size is conservative).

commit 77080e4ca22594a7cf130f211653380799226fd7
Author: Nadav Rotem <nrotem@apple.com>
Date:   Tue Dec 29 13:51:49 2015 -0800

    [Mangler] Add methods for encoding already-mangled symbols.
    
    This commit is related to the work of encoding mangled names more efficiently by
    compressing them. This commit adds two new methods to the mangler that allows it
    to identify requests to mangle strings that are already mangled.  Right now the
    mangler does not do anything with this information.
    
    This API is needed in all of the places in the compiler where we compose mangled
    names. For example, when the optimizer is cloning functions it adds a prefix to
    an existing name.
    
    I verified that this change is correct by adding a compress/decompress methods
    that add a prefix to the mangled names with assertions to catch compression of
    already compressed symbols or decompression of non-compressed named.  I plan to
    commit the verification code together with the compression implementation later
    on.

commit cc1dda478e6dbf85df46d9a93686830f79bbe5f1
Author: Xin Tong <xin_tong@apple.com>
Date:   Mon Dec 28 15:04:54 2015 -0800

    Move to a genset and killset for the dataflow in redundant load elimination.
    
    Previously we process every instruction every time the data flow re-iterates.
    This is very inefficient.
    
    In addition to moving to genset and killset, we also group function into
    OneIterationFunction which we know that the data flow would converge in 1 iteration
    and functions that requre the iterative data flow, mostly due to backedges in loops.
    we process them differently.
    
    I observed that there are ~93% of the functions that require just a single iteration
    to perform the RLE.
    
    But the other 7% accounts for 2321 (out of 6318) of the redundant loads we eliminated.
    
    This change reduces RLE compilation from 4.1% to 2.7% of the entire compilation time
    (frontend+OPT+LLVM) on stdlib with -O. This represents 6.9% of the time spent
    in SILOptimizations (38.8%).
    
    ~2 weeks ago, RLE was taking 1.9% of the entire compilation time. It rose to 4.1%
    mostly due to that we are now eliminating many more redundant loads (mostly thank
    to Erik's integragtion of escape analysis in alias analysis). i.e. 3945 redundant
    loads elimnated before Erik's change to 6318 redundant loads eliminated now.

commit 99fcb2dfe1d0836f5a00fc3b9ebde19da313d167
Author: Chris Lattner <clattner@apple.com>
Date:   Wed Dec 16 22:38:37 2015 -0800

    Change all uses of x = x.successor() to use x._successorInPlace()
    
    ...because it is apparently more efficient in some cases.  Technically
    we don't do this in ALL places, because it would be unfortunate if
    the implementation of _successorInPlace() were self recursive :-)

commit 9d0c912c70461fa47a64e4ca6a47eaf4d8d51f3a
Author: Maxim Moiseev <moiseev@apple.com>
Date:   Mon Nov 9 13:52:01 2015 -0800

    Merging `UnsafePointer.deinitializePointee`
    
    An optimization should be added in order for the new one to be
    efficient, i.e. if the `count` value is equal to `1`, the underlying
    `Builtin.destroy` should be called, instead of
    `Builtin.destroyArray`.

commit 95f22eddcb585c54efa7ed578c5b633316b836e9
Author: Brian Gesiak <modocache@gmail.com>
Date:   Mon Dec 14 21:36:41 2015 -0500

    [cmpcodesize] Use Python indices for separator
    
    Because of the way Python uses references for lists, the logic around
    `oldFileArgs`, `newFileArgs`, and `curFiles` is difficult to follow.
    Use a less efficient algorithm to find and split the elements based on
    the '--' separator.
    
    Although its performance is negligibly worse O(2n) as opposed to O(n), it's
    easier to understand and uses one less imperative loop. Also, it's not as if
    we'll ever encounter input that makes the performance difference
    matter, so :shipit:!

commit e9a2e1e1281cdb54d660ac1f0f04fbff83d0f807
Author: Chris Lattner <clattner@apple.com>
Date:   Tue Dec 15 23:18:55 2015 -0800

    Eliminate all of the uses of ++/-- from stdlib/public/core.
    
    At DaveA's suggestion, I took a mostly mechanical approach to this:
    pointers and numeric types start using += 1, and indexes use
    i = i.successor().  The index model is likely to be revised in
    Swift 3 anyway, so micro-optimizing this code syntactically isn't
    super important.
    
    There is some performance concern of this patch, since some
    in-place succesor operations are more efficient than
    i = i.successor().  The one that seems particularly at issue is the
    instance in the implementation of partition(), which I changed to
    use i._successorInPlace().  If other instances lead to a perf issue,
    they can be changed to use that as well.

commit 65a5a03f2682b973f6c9eb7e8c2c720a929b8d6b
Author: Slava Pestov <spestov@apple.com>
Date:   Tue Dec 8 14:49:16 2015 -0800

    IRGen: Add a new destructiveInjectEnumTag value witness function
    
    This value witness function takes an address of an enum value where the
    payload has already been initialized, together with a case index, and
    forms the enum value.
    
    The formal behavior can be thought of as satisfying an identity in
    relation to the existing two enum value witnesses. For any enum
    value, the following is to leave the value unchanged:
    
      tag = getEnumTag(value)
      destructiveProjectEnumData(value)
      destructiveInjectEnumData(value, tag)
    
    This is the last missing piece for the inject_enum_addr SIL instruction
    to handle resilient enums, allowing the implementation of an enum to be
    decoupled from its uses. Also, it should be useful for dynamically
    constructing enum cases with write reflection, once we get around to
    doing such a thing.
    
    The body of the value witness is emitted by a new emitStoreTag() method
    on EnumImplStrategy. This is similar to the existing storeTag(), except
    the case index is a value instead of a contant.
    
    This is implemented as follows for the different enum strategies:
    
    1) For enums consisting of a single case, this is trivial.
    
    2) For enums where all cases are empty, stores the case index into the
       payload area.
    
    3) For enums with a single payload case, emits a call to a runtime
       function. Note that for non-generic single payload enums, this could
       be open-coded more efficiently, but the function still has the
       correct behavior since it supports extra inhabitants and so on.
       A follow-up patch will make this more efficient.
    
    4) For multi-payload enums, there are two cases:
    
       a) If one of the payloads is generic or resilient, the enum is
          dynamically-sized, and a call to a runtime function is emitted.
    
       b) If the entire enum is fixed-size, the value witness checks if
          the case is empty or not.
    
          If the case has a payload, the case index is swizzled into
          spare bits of the payload, if any, with remaining bits going
          into the extra tag area.
    
          If the case is empty, the case index is swizzled into the
          spare bits of the payload, the remaining bits of the payload,
          and the extra tag area.
    
    The implementations of emitStoreTag() duplicate existing logic in the
    enum strategies, in particular case 4)b) is rather complicated.
    
    Code cleanups are welcome here!

commit 92b56cb96d811e43a04c6217b55382f95bbd9ac6
Author: Nadav Rotem <nrotem@apple.com>
Date:   Mon Dec 7 09:16:32 2015 -0800

    [AliasAnalysis] Reintroduce the cache to AliasAnalysis.
    
    This commit reintroduces the cache to AliasAnalysis. This cache solves the
    problem of dead pointers that stay in the cache maps after instructions are
    being deleted. It's inefficient to scan the whole cache every time we want to
    invalidate a single pointer (pointers can be a part of a key). We solve this
    problem by mapping pointers to internal indices. With this map in place it's
    very easy to invalidate real pointers (by removing them from the ptr-to-index
    map), and there is no harm in keeping stale indices in the map. We limit the
    size of the map to keep the memory usage down.
    
    The hit rate of the cache on stdlib is about ~85%.

commit a7a9e162986058b6746184d00d92311abe85d556
Author: Slava Pestov <spestov@apple.com>
Date:   Sat Dec 5 00:39:43 2015 -0800

    SILGen: Open code calls of enum case constructors
    
    Sema models enum case constructors as ApplyExprs. Formerly SILGen
    would emit a case constructor function for each enum case,
    constructing the enum value in the constructor body. ApplyExprs
    of case constructors were lowered like any other call.
    
    This is nice and straightforward but has several downsides:
    
    1) Case constructor functions are very repetitive and trivial,
       in particular for no-payload cases. They were declared
       @_transparent and so were inlined at call sites, but for
       public enums they still had to be emitted into the final
       object file.
    
    2) If the enum is generic, the substituted type may be loadable
       even if the unsubstituted type is not, but since the case
       constructor is polymorphic we had to allocate stack buffers
       anyway, to pass the payload and result at the right abstration
       level. This meant that for example Optional.Some(foo)
       generated less-efficient SIL than the equivalent implicit
       conversion.
    
    3) We were missing out on peephole optimization opportunities when
       the payload of an indirect case or address-only enum could be
       emitted directly into the destination buffer, avoiding a copy.
       One example would be when an enum payload is the result of
       calling a function that returns an address-only value indirectly.
       It appears we had unnecessary copies and takes even with -O.
       Again, optional implicit conversions special-cased this.
    
    This patch implements a new approach where a fully-formed call to
    a element constructor is handled via a special code path where
    the 'enum' or 'init_enum_data_addr' / 'inject_enum_addr'
    instructions are emitted directly. These always work on the
    substituted type, avoiding stack allocations unless needed.
    An additional optimization is that the ArgumentSource abstraction
    is used to delay evaluation of the payload argument until the
    indirect box or address-only payload was set up.
    
    If a element constructor is partially applied, we still emit a
    reference to the constant as before.
    
    It may seem like case constructor functions are at least useful
    for resilience, but case constructors are transparent, so making
    them resilient would require a new "transparent but only in this
    module, and don't serialize the SIL body" declaration.
    @inline(always) is almost what we need here, but this affect
    mandatory inlining, only the optimizer, so it would be a
    regression for non-resilient enums, or usages of resilient enums
    in the current module.
    
    A better approach is to construct resilient enums with a new
    destructiveInjectEnumTag value witness function, which is
    coming soon, and the general improvement from that approach
    is what prompted this patch.

commit 8ab1e2dd502fc7f37a67572699165e03f268c377
Author: Adrian Prantl <aprantl@apple.com>
Date:   Thu Nov 19 09:23:38 2015 -0800

    Unify debug scope and location handling in SILInstruction and SILBuilder.
    
    The drivers for this change are providing a simpler API to SIL pass
    authors, having a more efficient of the in-memory representation,
    and ruling out an entire class of common bugs that usually result
    in hard-to-debug backend crashes.
    
    Summary
    -------
    
    SILInstruction
    
    Old                   New
    +---------------+     +------------------+    +-----------------+
    |SILInstruction |     |SILInstruction    |    |SILDebugLocation |
    +---------------+     +------------------+    +-----------------+
    | ...           |     | ...              |    | ...             |
    |SILLocation    |     |SILDebugLocation *| -> |SILLocation      |
    |SILDebugScope *|     +------------------+    |SILDebugScope *  |
    +---------------+                             +-----------------+
    
    Weâ€™re introducing a new class SILDebugLocation which represents the
    combination of a SILLocation and a SILDebugScope.
    Instead of storing an inline SILLocation and a SILDebugScope pointer,
    SILInstruction now only has one SILDebugLocation pointer. The APIs of
    SILBuilder and SILDebugLocation guarantees that every SILInstruction
    has a nonempty SILDebugScope.
    
    Developer-visible changes include:
    
    SILBuilder
    ----------
    
    In the old design SILBuilder populated the InsertedInstrs list to
    allow setting the debug scopes of all built instructions in bulk
    at the very end (as the responsibility of the user). In the new design,
    SILBuilder now carries a "current debug scope" state and immediately
    sets the debug scope when an instruction is inserted.
    This fixes a use-after-free issue with with SIL passes that delete
    instructions before destroying the SILBuilder that created them.
    
    Because of this, SILBuilderWithScopes no longer needs to be a template,
    which simplifies its call sites.
    
    SILInstruction
    --------------
    
    It is neither possible or necessary to manually call setDebugScope()
    on a SILInstruction any more. The function still exists as a private
    method, but is only used when splicing instructions from one function
    to another.
    
    Efficiency
    ----------
    
    In addition to dropping 20 bytes from each SILInstruction,
    SILDebugLocations are now allocated in the SILModule's bump pointer
    allocator and are uniqued by SILBuilder. Unfortunately repeat compiles
    of the standard library already vary by about 5% so I couldnâ€™t yet
    produce reliable numbers for how much this saves overall.
    
    rdar://problem/22017421

commit 7a5fe1dc0f58edcc3e5e9918fc4fd878db2c49d4
Author: Nadav Rotem <nrotem@apple.com>
Date:   Wed Nov 18 11:19:14 2015 -0800

    Change the analysis invalidation message from "preserve" to "invalidate".
    
    This commit changes the way passes invalidate analysis. Passes now report the
    list of traits that they invalidate.  We went back and forth on this a few times
    and we are now going back to the invalidation mode. In a few places in the
    optimizer passes had to record the fact that they deleted a call or a branch and
    had to construct the enum that will contain the preserve list. Now passes can
    either use the new enum states that are the intersection of traits or even send
    multiple invalidation message.
    
    We are making this change now because Mark added a new kind of invalidation
    trait ("function"). Adopting this new invalidation trait required that we
    inspect all of the invalidation sites anyway. This commit includes a more
    efficient use of the 'function' attribute, and our function passes don't
    invalidate the 'function' attribute anymore.

commit 9c02b2c6d575067bfcd1b19a57ae7732b42b3a2e
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Sat Oct 24 05:31:25 2015 +0000

    [loop-arc] Instead of using a lambda, use a callback data structure to process ARC results.
    
    This is necessary since previously we would:
    
    1. Perform data flow over an entire function.
    2. Process all of the gathered data, inserting instructions to be deleted into a
    list to avoid dangling pointer issues.
    3. Delete instructions after processing the data to avoid the dangling pointer
    issues.
    4. If we saw any nested retain/releases and deleted any instructions, run ARC
    again on the entire function.
    
    For loop arc, we want to:
    
    1. Visit the loop nest in post order.
    2. For each loop:
       a. Perform data flow over the loop.
       b. Process all of the gathered data, inserting instructions to be deleted
          into a list to avoid dangling pointer issues.
       c. Delete instructions after loop processing has finished and potentially
          summarize the loop.
       d. If we saw any nested retain/releases and deleted any instructions, run ARC
          again on the loop.
    
    This is more efficient in terms of the number of times that we perform dataflow
    and allows us to summarize as we go.
    
    The main disadvantage is that Block ARC steps (3,4) could occur in
    GlobalARCOpts.cpp. Loop ARC on the other hand needs (2.c.) and (2.d.) to occur
    while processing.
    
    This means I need a real callback context and an extra callback call to say when
    it is ok for a user of the analysis to remove instructions.
    
    rdar://22238658
    
    * The dangling pointer issue is that a retain/release could act as a last
    use/first decrement for a different retain/release. If process the different
    retain/release later, we will be touching a dangling pointer.
    
    Swift SVN r32867

commit 8c9be9be123a0650e9981b761e5daf0e4d281b5f
Author: Doug Gregor <dgregor@apple.com>
Date:   Fri Oct 9 17:18:49 2015 +0000

    Iterative type checker: simple circular reference detection.
    
    Put in some rudimentary logic for finding circular references within
    the iterative type checker and diagnosing those cycles. The
    "rudimentary" part is because we're performing linear searches within
    a stack rather than keeping a proper dependency graph, which is
    inefficient and could display longer cycles than are actually
    present. Additionally, the diagnostic is not specialized to the actual
    query, so we get a generic "circular reference" diagnostic. OTOH, we
    show all of the declarations involved in the cycle, which at least
    lets the user figure out where the cycle occurred.
    
    Enable the iterative type checker for resolving the type of a global
    typealiases.
    
    Swift SVN r32572

commit cc7938ad2179696ee06e3baece9bf029f195afcb
Author: John McCall <rjmccall@apple.com>
Date:   Tue Oct 6 01:14:30 2015 +0000

    Implement a trie data structure.  Specifically, implement
    a ternary tree with a fixed-length per-node inline key buffer.
    
    I plan to use this for metadata path caches, where it's useful to
    be able to quickly find the most-derived point along a path that
    you've already cached, but it should be useful for other things
    in the compiler as well, like function-with-argument-label
    lookups and possibly code completion.
    
    This is quite a bit more space-efficient (and somewhat faster)
    than doing scans after a lower_bound on a std::map<std::string, T>.
    
    I haven't implemented balancing yet, and I don't need delete at
    all for metadata paths, so I don't plan to work on that.
    
    Swift SVN r32453

commit d5661b8ff4e556e1cc78c488d55cebfc8c0a76c9
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Sat Sep 12 03:51:28 2015 +0000

    [stdlib] Integers Prototype: make InPlace the intrinsics
    
    This will allow us to implement bigInt operations more efficiently.
    
    Swift SVN r31912

commit 8759b79d792992c5abc00fe6b65079e7e3471cf5
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Wed Aug 12 05:31:19 2015 +0000

    [arc] When checking if a BB is an ARC Inert Trap BB, first check if the end is unreachable before you do anything further.
    
    This inefficient call took up 8-9% of the compile time of Global ARC Opts when
    compiling a release no-assert stdlib with a release compiler with assertions.
    
    NFC.
    
    <rdar://problem/22244924>
    
    Swift SVN r31167

commit 313a4c93c683735dbc953bdf5a4b2a2444e92606
Author: David Farler <dfarler@apple.com>
Date:   Tue Aug 4 03:13:14 2015 +0000

    Review: Index protocol extensions
    
    - Add Strict/Defaulted Index types to StdlibUnittest
    - Test whether a random access index calls its more efficient
      customization by tracking successor calls.
    - Fix the RandomAccessIndex.advancedBy(n, limit:) API by de-underscoring
      the limit parameter
    - Inline some internal transparent default implementations to their only
      call site
    - Attach _RandomAccessAmbiguity type to RandomAccessIndex
    
    rdar://problem/22085119
    
    Swift SVN r30979

commit eec59477aeffb1a9e1a493c9fb7830fe4c6f07d3
Author: Joe Groff <jgroff@apple.com>
Date:   Thu Jul 30 05:28:30 2015 +0000

    stdlib: Factor _copy*ToNativeArrayBuffer into a general-purpose "builder" for ContiguousArrayBuffer.
    
    This makes the code for efficiently initializing array buffers in-place more accessible to the rest of the standard library, and should also provide a performance boost for _copySequenceToNativeArrayBuffer, which had been implemented as a naive append loop, by handling reallocating the buffer when necessary when initializing from a sequence that underestimates its count.
    
    Swift SVN r30793

commit 0b1283b1c92b117b26c9beb9e7315c62a926840b
Author: Joe Groff <jgroff@apple.com>
Date:   Sat Jul 25 21:28:06 2015 +0000

    Have 'defer' statements cons up func decls instead of closure literals.
    
    The defer body func is only ever fully applied, so SILGen can avoid allocating a closure for it if it's declared as a 'func', making it slightly more efficient at -Onone.
    
    Swift SVN r30638

commit 19a3821a56b264e953d1458db822600af22f7d06
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Fri Jul 17 06:52:07 2015 +0000

    Implementation of the pre-specialization for the most popular stdlib generic types.
    
    This patch implements the pre-specialization for the most popular generic types from the standard library. If there are invocations of generic functions from the standard library in the user-code and the compiler can find the specialized, optimized versions of these functions, then calls of generic functions are simply replaced by the calls of the specialized functions.
    
    This feature is supposed to be used with -Onone to produce much faster (e.g. 5x-10x faster) executables in debug builds without impacting the compile time. In fact, the compile-time is even improved, because IRGen has less work to do. The feature can be considered a light-weight version of the -Odebug, because pre-specialization is limited in scope, but does not have a potentially negative compile-time impact compared to -Odebug. It is planned to enable it by default in the future.
    
    This feature is disabled by default for the time being. It can be enabled by using a hidden flag: -Xllvm -use-prespecialized.
    
    The implementation consists of two logical steps:
    - When the standard library is being built, we force a creation of specializations for the most popular generic types from the stdlib, e.g. Arrays of integer and floating point types, Range<Int>, etc. The list of specializations is not fixed and can be easily altered by editing the Prespecialized.swift file, which is responsible for forcing the specialization of generic types (this is simple solution for now, until we have a proper annotation to indicate which specializations of a given generic type or function we want to generate by means of the pre-specialization). These specializations are then optimized and preserved in the stdlib dylib and in the Swift SIL module. The size increase of the stdlib due to creation of pre-specializations is currently about 3%-7%.
    
    - When a user-code is being compiled with -Onone, the compiler would run a generic specializer over the user-code. If there are calls of generic functions from the standard library, the specializer would check if there is an existing specialization matching these invocations. If such a specialization is found, the original call is replaced by the call of this more efficient specialized version.
    
    Swift SVN r30309

commit ec61fa4c5a68cc65b2327ea76b93ec81dbc27cae
Author: Joe Groff <jgroff@apple.com>
Date:   Thu Jul 16 15:38:17 2015 +0000

    IRGen/Runtime: Use only the 'layout' subset of the vwtable to perform value type layout.
    
    Full type metadata isn't necessary to calculate the runtime layout of a dependent struct or enum; we only need the non-function data from the value witness table (size, alignment, extra inhabitant count, and POD/BT/etc. flags). This can be generated more efficiently than the type metadata for many types--if we know a specific instantiation is fixed-layout, we can regenerate the layout information, or if we know the type has the same layout as another well-known type, we can get the layout from a common value witness table. This breaks a deadlock in most (but not all) cases where a value type is recursive using classes or fixed-layout indirected structs like UnsafePointer. rdar://problem/19898165
    
    This time, factor out the ObjC-dependent parts of the tests so they only run with ObjC interop.
    
    Swift SVN r30266

commit 2641d566ac86b821a4dfe33f4594e6907af58a28
Author: Joe Groff <jgroff@apple.com>
Date:   Thu Jul 16 01:28:42 2015 +0000

    IRGen/Runtime: Use only the 'layout' subset of the vwtable to perform value type layout.
    
    Full type metadata isn't necessary to calculate the runtime layout of a dependent struct or enum; we only need the non-function data from the value witness table (size, alignment, extra inhabitant count, and POD/BT/etc. flags). This can be generated more efficiently than the type metadata for many types--if we know a specific instantiation is fixed-layout, we can regenerate the layout information, or if we know the type has the same layout as another well-known type, we can get the layout from a common value witness table. This breaks a deadlock in most (but not all) cases where a value type is recursive using classes or fixed-layout indirected structs like UnsafePointer. rdar://problem/19898165
    
    Swift SVN r30243

commit 729e599ffcf2b1238752515a00df404e2d41060d
Author: Joe Groff <jgroff@apple.com>
Date:   Tue Jun 30 23:10:38 2015 +0000

    SILGen: Project boxed payloads when switching indirect enums.
    
    And fix some bugs in the switch implementation I ran into on the way:
    
    - Make getManagedSubobject(CopyOnSuccess) really produce a CopyOnSuccess ConsumableManagedValue;
    - Avoid invalidating address-only enums when they can't be unconditionally taken by copying the enum before projecting it. Ideally there'd be a copy_enum_data_addr instruction to do this more efficiently.
    
    Swift SVN r29817

commit a065ae99c8dbdffc206aa9aab1658931711d1a43
Author: John McCall <rjmccall@apple.com>
Date:   Thu Jun 18 07:13:15 2015 +0000

    Add a data structure for efficiently storing a byte-encoded
    sequence which can be read with a forward iterator.
    
    This will be useful for storing access paths to metadata or
    protocol conformance values, which are typically very short.
    
    Now with a fix to directly include <climits> for CHAR_BIT.
    This was being transitively included on Darwin, but that's
    not portable.
    
    Swift SVN r29485

commit 4036074b56d16e85978e3007a73e83a5f98c671b
Author: Ted Kremenek <kremenek@apple.com>
Date:   Thu Jun 18 04:39:30 2015 +0000

    Revert "Add a data structure for efficiently storing a byte-encoded"
    
    This broke the Linux bot.
    
    Swift SVN r29476

commit c099ec8321dc76b23d0b2e0bd4885709bd780410
Author: John McCall <rjmccall@apple.com>
Date:   Wed Jun 17 21:34:00 2015 +0000

    Add a data structure for efficiently storing a byte-encoded
    sequence which can be read with a forward iterator.
    
    This will be useful for storing access paths to metadata or
    protocol conformance values, which are typically very short.
    
    Swift SVN r29458

commit f1243f5f65c1c91b57fdd62a35cb5dc00f33f26d
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Sun May 24 22:43:40 2015 +0000

    [stdlib] Pattern match Ranges efficiently again.
    
    Fixes <rdar://21091371>.  When SequenceType acquired a O(N) 'contains'
    method, the code for pattern matching a Range started using that method
    instead of implicitly deducing a HalfOpenInterval.
    
    We still have a problem though:
    
      (1_000_000..<1_000_000_000).contains(1)
    
    will compile and appear to hang at runtime.  I'll bring this up on the
    mailing list.
    
    Swift SVN r28998

commit 6f207716dda7905ccd7bdc37935e390e86cdb4ea
Author: Andrew Trick <atrick@apple.com>
Date:   Tue May 19 20:54:06 2015 +0000

    Add MayBindDynamicSelf to CallGraphAnalysis.
    
    Why did I choose to pollute the call graph? Because it's convenient,
    efficient, and more robust than any current alternative.  This is a
    property of the parent function that depends on all the call sites. We
    need to cache this information somehow and update it whenever edges
    are added to the call graph. The call graph does this perfectly.
    
    The thing I don't like is that it is conservative and may not return a
    precise answer after updates, which is generally undesirable. You
    normally want to get the same answer whether you update or recompute
    an analysis. Although this is generally bad, it is a good tradeoff
    now. The need for this call graph property is a consequence of the
    current SIL representation which should change soon. It only comes up
    in bizarre corner cases, and the imprecision will never show up in
    practice.
    
    Swift SVN r28784

commit 0b339b9a4649533335ebb3a7f91be5fe328ff981
Author: Joe Groff <jgroff@apple.com>
Date:   Mon Apr 27 00:35:04 2015 +0000

    IRGen: Populate nominal type descriptor metadata for enums.
    
    Store the number of payload and no-payload cases, the case names, and a lazy case type accessor function for enums, like we do for stored properties of structs and classes. This will be useful for multi-payload runtime support, and should also be enough info to hack together a reflection implementation for enums.
    
    For dynamic multi-payload enums to not be ridiculously inefficient, we'll need to track the size of the payload area in the enum, like we do the field offsets of generic structs and classes, so hack off a byte in the payload case count to track the offset of that field in metadata records. 16 million payloads ought to be enough for anyone, right? (and 256 words between the enum metadata's address point and the payload size offset)
    
    Swift SVN r27789

commit 4fc21092dc514011304b5d0e8407db29b553de7c
Author: John McCall <rjmccall@apple.com>
Date:   Wed Apr 22 00:04:14 2015 +0000

    Check for a couple of special-cased selectors more
    efficiently than re-uniquing them constantly.
    
    Swift SVN r27540

commit 02d254047b8d6671080865a13fe82cee574ee1d0
Author: Dmitri Hrybenko <dgribenko@apple.com>
Date:   Fri Apr 17 05:03:28 2015 +0000

    stdlib: add a hook for dynamic dispatch in CollectionType.find()
    
    This hook allows Set.find() to be equally efficient in static and
    generic contexts.
    
    This time, with correct tests.
    
    Swift SVN r27404

commit fe53f87dfc5f79621de8549b9bcf1c25a69866bf
Author: Dmitri Hrybenko <dgribenko@apple.com>
Date:   Fri Apr 17 04:19:49 2015 +0000

    stdlib: add a hook for dynamic dispatch in CollectionType.find()
    
    This allows Set.find() to be equally efficient in static and generic
    contexts.
    
    Swift SVN r27402

commit aa07bc3d908762f48fb6cb112f329113cd2e84ef
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Thu Apr 16 20:28:15 2015 +0000

    Lower bridged casts always, when the outcome is not provably failing.
    
    Even when we don't know for sure if a bridged cast would succeed, we still want to lower it to produce a more efficient code that does not performs conformance checks at run-time.
    
    This is useful when performing casts optimizations as a guaranteed optimization.
    
    Swift SVN r27377

commit 2dd38eee0ede4a51f4e7d1bf9272ae91b3a8a1ff
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Tue Apr 7 22:53:57 2015 +0000

    [sil-combine] Teach the optimizer how to optimize bridged casts.
    
    If a conformance to _BridgedToObjectiveC is statically known, generate a more efficient code by using the newly introduced library functions for bridging casts.
    This covers the casts resulting from SIL optimizations.
    
    Tests are included. I tried to cover most typical casts from ObjC types into Swift types and vice versa and to check that we always generate something more efficient than a checked_cast or unconditional_checked_cast. But probably even more tests should be written or generated by means of gyb files to make sure that nothing important is missing.
    
    The plan is to make the bridged casts SIL optimization a guaranteed optimization. Once it is done, there is no need to lower the bridged casts in a special way inside Sema, because they all can be handled by the optimizer in a uniform way. This would apply to bridging of Error types too.
    
    With this change, no run-time conformance checks are performed at run-time if conformances are statically known at compile-time.
    As a result, the performance of rdar://19081345 is improved by about 15%. In the past, conformance checks in this test took 50% of its execution time, then after some improvements 15% and now it is 0%, as it should be.
    
    Swift SVN r27102

commit a762fb2b942b9dd49900c780e221ce10707b8f64
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Tue Apr 7 22:53:56 2015 +0000

    Teach bridgeFromObjectiveC to make use of the knownConditionallyBridgeFromObjectiveC/knownForceBridgeFromObjectiveC
    
    If a conformance to _BridgedToObjectiveC is statically known, generate a more efficient code by using the newly introduced library functions for bridging casts.
    This covers the cases arising from the source code, but does not cover any casts resulting from SIL optimizations. Those will be covered by the subsequent commit.
    
    This change is to stay in CSApply for now. But the plan is to make the bridged casts optimization a guaranteed optimization. Once it is done, there is no need to lower the bridged casts in a special way inside Sema, because they all can be handled by the optimizer in a uniform way. This would apply to bridging of Error types too.
    
    Swift SVN r27101

commit 3effd6fcf75593f03e224f523770ca3308a75570
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Tue Apr 7 22:53:55 2015 +0000

    Introduce two new compiler-known library functions for performing bridging casts when conformances are known statically.
    
    We define two new library functions _knownForceBridgeFromObjectiveC/_knownConditionallyBridgeFromObjectiveC, similar to _forceBridgeFromObjectiveC/_conditionallyBridgeFromObjectiveC. The main difference is that they require their arguments to conform to _BridgedToObjectiveC and _BridgedToObjectiveC. _ObjectiveCType accordingly. With this change, it is now possible to invoke the _BridgedToObjectiveC._forceBridgeFromObjectiveC witness directly, without going via the inefficient swift_bridgeNonVerbatimFromObjectiveC.
    
    So now, for a cast O -> S, if it can be statically proven that an ObjC type O is bridgeable to a Swift type S implementing the _BridgedToObjectiveC protocol (i.e. O is the class (or its subclass) defined by the S._ObjectiveCType alias), we can generate a code to invoke the newly defined library function _knownForceBridgeFromObjectiveC/_knownConditionallyBridgeFromObjectiveC instead of _forceBridgeFromObjectiveC/_conditionallyBridgeFromObjectiveC.
    
    After inlining, this will end-up invoking  S._forceBridgeFromObjectiveC directly instead of invoking a more general, but less effective swift_bridgeNonVerbatimFromObjectiveC, which always performs conformance checks at runtime, even if conformances are known statically. As a result, no conformance checks are performed at run-time if conformances are known statically.
    
    The client code making use of these new APIs and the tests are coming in the subsequent commits.
    
    The naming of the two new helper library functions was discussed with Dmitri.
    
    This is part of the bridging casts optimization effort. And it is specifically useful for e.g. rdar://19081345.
    
    Swift SVN r27100

commit b2cb75ad30afebeae922893ebbfcf1b29d3d0201
Author: Joe Groff <jgroff@apple.com>
Date:   Fri Apr 3 22:16:52 2015 +0000

    Runtime: Add a 'swift_becomeNSError' entry point to coerce an ErrorType box to an NSError instance.
    
    If the NSError part of the box hasn't been initialized yet, fill it in with the domain and code of the contained value. This will allow us to efficiently turn ErrorType values into NSErrors, either for bridging or for coercion purposes.
    
    Swift SVN r26958

commit 520df977876198fca1a673d0aed4deba854762c4
Author: Xi Ge <xi_ge@apple.com>
Date:   Sat Mar 14 08:30:16 2015 +0000

    [CodeCompletion] Adopt a more efficient algorithm
    to find the nearest AST parent that meets a certain condition
    
    Swift SVN r26134

commit 51c69ae75e4bc7359fde0610209e139e1859176d
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Fri Mar 13 21:13:36 2015 +0000

    [sil-verify] Remove the requirement that "from" and "to" types of a cast should not be the same.
    
    We discussed it with Joe and this requirement does not make sense, especially if casts are produced by one pass, but optimized by another one. Moreover, having equal from and to types is eventually not very efficient, but is not semantically wrong. Such casts can often be produced by the inliner or specialised.
    
    Swift SVN r26110

commit 61d7f0a0a4ddd6189f40d8128beec29e2434dfcc
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Sat Feb 21 19:12:45 2015 +0000

    [stdlib] Downcasted arrays stay native
    
    We used to handle deferred type-checking by treating down-casted native
    array buffers as NSArrays (which they are, but we know more).  Instead,
    we now save a bit that indicates deferred type-checking is needed and
    remember that the buffer is native, which saves dispatching through
    objc_MsgSend.
    
    Fixes <rdar://problem/19302286> down-casted Arrays are inefficient
    
    Swift SVN r25472

commit 6ff94330aebf3cadeeee172fd87c185a6f4cdbd9
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Fri Feb 13 06:42:37 2015 +0000

    Add SwiftRuntime dtrace provider with probes for retain, release, allocateObject, deallocateObject.
    
    Now if you want to get these dynamic metrics from the runtime all you
    need to do is:
    
    1. Configure Swift with -DSWIFT_RUNTIME_ENABLE_DTRACE=YES
    2. Run your routine with the command:
    
    sudo dtrace -s ./utils/runtime_statistics.d -c "$MY_COMMAND_LINE"
    
    After your app finishes running, it will dump out the counts. This is a
    much more efficient and low maintenance way to get such statistics than
    custom instrumenting the code.
    
    Nothing is changed if -DSWIFT_RUNTIME_ENABLE_DTRACE is not set.
    
    Swift SVN r25264

commit f7000ce3cb1c3ac448272cf89e7d55bf442ea329
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Thu Feb 5 19:18:30 2015 +0000

    [stdlib] Add unsafeUnwrap(x)
    
    Also the internal version, _unsafeUnwrap, for more-efficiently
    unwrapping non-empty optionals.
    
    Swift SVN r25008

commit 2ff77a9cd11b23dfac6515aba6416feb2b1d3a6e
Author: John McCall <rjmccall@apple.com>
Date:   Fri Jan 30 18:39:07 2015 +0000

    Only import *Ref typedefs as CF types if they have a bridging
    attribute or appear in a whitelist.
    
    The initial whitelist is based on an audit I performed of our current
    public SDKs.  If there are CF types which appear only in our internal
    SDKs, and somebody urgently needs to use them from Swift, they can
    adopt the bridging attributes.  The goal is to eventually eliminate
    the whitelist and rely solely on bridging attributes anyway.
    
    Sadly, CoreCooling was not included in my SDK audit and must be
    explicitly annotated. :(
    
    I've left the main database organized by framework, but I wanted
    a quasi-lexicographically sorted version to permit efficient lookup.
    We generate that copy automatically with gyb.  I ended up having
    to tweak handle_gyb_sources to allow it to drop the result in
    CMAKE_CURRENT_BINARY_DIR instead of CMAKE_CURRENT_BINARY_DIR/{4,8}
    if an architecture is not provided.  I think this is abstractly
    reasonable for generated includes, which have independent ability
    to detect the target word size.  But just between you and me,
    I did it because I couldn't figure out how to add
    "-I${CMAKE_CURRENT_BINARY_DIR/{4,8}" as a compile flag;
    the obvious thing didn't work.  Anyway, I'd appreciate it if
    someone double-checked my cmake hackery here.
    
    Swift SVN r24850

commit 2969aab1c1051fdea9850e090cbb0eaef752b9de
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Fri Jan 30 03:54:07 2015 +0000

    Stop giving local types private linkage; the debugger may still access them.
    
    Instead, just fall through to the normal public/internal/private switch
    added in the previous commit. Local declarations are always private.
    
    Make sure we emit all local declarations by using the list in the SourceFile,
    rather than walking the AST (which missed a few cases and was less efficient
    anyway).
    
    As an exception, declarations without accessibility at all still get private
    linkage. These are things like local variables that don't get accessed by
    symbol, even when using the debugger.
    
    rdar://problem/19623016
    
    Swift SVN r24839

commit 80a46cdae23b0c95ced9c0340577bcea6da89577
Author: Arnold Schwaighofer <aschwaighofer@apple.com>
Date:   Tue Jan 20 17:00:12 2015 +0000

    RefCount: Use rotate right in isUniquelyReferencedOrPinned
    
    This generates more efficient code.
    
    Swift SVN r24555

commit 06ff115af92b82e0146a3ef72d11573e34291635
Author: Chris Lattner <clattner@apple.com>
Date:   Tue Jan 13 00:05:39 2015 +0000

    destroy_addr on a known-nil optional is always a noop, *never* emit it.
    This cleans up codegen for if-let a LOT and should make it more efficient
    even in release builds.
    
    
    Swift SVN r24380

commit 2ea05c3adbc2981ca95b0917a62e3f5303ccbd9a
Author: Chris Lattner <clattner@apple.com>
Date:   Sun Jan 11 07:36:53 2015 +0000

    Add new entry points to the enum implementation strategy for testing an enum against a
    specific case and returning an i1 result.  This can be done a lot more efficiently (in
    terms of generated LLVM IR) than doing a general switch over the cases.  This will be used
    to implement rdar://19404937, but there is a miscompilation here that I'm tracking down.
    
    Until it is working, I'm not committing the code to use these entrypoints, which wires it
    into IRGen of SelectEnumInst and SelectEnumAddrInst.  Since that part isn't included, this
    is NFC.
    
    
    Swift SVN r24362

commit 169e4fe31994757e86b4f389e0437e3a6d8dda0e
Author: John McCall <rjmccall@apple.com>
Date:   Sat Dec 13 01:27:12 2014 +0000

    Add Builtin.UnsafeValueBuffer, which provides opaque
    storage for arbitrary values.
    
    A buffer doesn't provide any way to identify the type of
    value it stores, and so it cannot be copied, moved, or
    destroyed independently; thus it's not available as a
    first-class type in Swift, which is why I've labelled
    it Unsafe.  But it does allow an efficient means of
    opaquely preserving information between two cooperating
    functions.  This will be useful for the adjustments I
    need to make to materializeForSet to support safe
    addressors.
    
    I considered making this a SIL type category instead,
    like $@value_buffer T.  This is an attractive idea because
    it's generally better-typed.  The disadvantages are that:
    - it would need its own address_to_pointer equivalents and
    - alloc_stack doesn't know what type will be stored in
      any particular buffer, so there still needs to be
      something opaque.
    
    This representation is a bit gross, but it'll do.
    
    Swift SVN r23903

commit 3f46b30ca407b3d78cc820a1ff9333996630c3f3
Author: John McCall <rjmccall@apple.com>
Date:   Sat Dec 6 09:46:01 2014 +0000

    Add runtime functions to "pin" a native Swift object.
    
    Pinning an object prevents it from being deallocated,
    just like retaining it, but only one client can own the
    pin at once.  Sensible "sharing" of the pin can occur
    if attempts are perfectly nested.  It is efficient to
    simultaneously query the pin state of an object in
    conjunction with its strong reference count.
    
    This combination of traits makes pinning suitable for
    use in tracking whether a data structure backed by
    an object is undergoing a non-structural modification:
    
    - A structural change would require unique ownership
      of the object, but two non-structural changes (to
      different parts of the object) can occur at once
      without harm.  So a non-structural change can check
      for either uniqueness or a pin and then, if necessary,
      assert the pin for the duration of the change.
      Meanwhile, this act of asserting the pin prevents
      simultaneous structural changes.
    
    - A very simple code-generation discipline leads to
      changes being perfectly nested as long as they're
      all performed by a single thread (or synchronously).
      Asynchrony can introduce imperfect nesting, but it's
      easy to write that off as a race condition and hence
      undefined behavior.
    
    See Accessors.rst for more on both of these points.
    
    Swift SVN r23761

commit fde23a435d5d8a30a1812f1dc853b41b5cd95ead
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Thu Dec 4 00:35:06 2014 +0000

    [Sema] Look up the NSCopying protocol slightly more efficiently.
    
    Don't search all imports; just jump right to Foundation.
    
    Adjusts the test to actually import Foundation (from the mock SDK).
    
    Swift SVN r23668

commit f5e78f78feba6eb5ddcda5367a52442a1d01b576
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Thu Dec 4 00:34:55 2014 +0000

    TypeChecker: Look up "Bool" more efficiently.
    
    This is not quite the same as ASTContext::getBoolDecl because it handles
    -parse-stdlib source files that don't import the standard library, but it
    still doesn't need to do a general lookup.
    
    No functionality change.
    
    Swift SVN r23667

commit c8d180e660525ef63ec31bc3d4dd91607c579049
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Fri Oct 31 22:55:56 2014 +0000

    Generalize the switch_int instruction into switch_value instruction, which may switch on arguments of builtin integer types or function types. The later is required for implementing a more efficient speculative devirtualizaiton implementation. Implement lowering of switch_value into LLVM code.  In case of integer operands, it reuses LLVM's switch optimizations. Support for switching on function types is not yet bullet-proof and will be refined in the subsequent patches.
    
    rdar://18508812
    
    Swift SVN r23042

commit f016754ef93b04d3f303dc276a216fae558ed457
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Fri Oct 31 20:44:11 2014 +0000

    Add a new select_value instruction. This instruction should be the equivalent of select_enum, just for builtin int types. Such an instruction is needed e.g. to efficiently implement conversions of Int raw values to C-like enums.
    
    rdar://18812325
    
    Swift SVN r23036

commit 02808fb73331bcaf20a9d06bf0e0860d7c598cc9
Author: Doug Gregor <dgregor@apple.com>
Date:   Thu Oct 9 22:43:55 2014 +0000

    Remove resolvePotentialArchetypeToType() in favor of PotentialArchetype::getDependentType()
    
    The latter uses information that is maintained by the potential
    archetype, which makes it more efficient and simpler. NFC
    
    Swift SVN r22645

commit f844ab43310c55899e69293fcfa5e65c5aa50f6d
Author: Doug Gregor <dgregor@apple.com>
Date:   Thu Oct 9 21:52:32 2014 +0000

    Store the generic type parameter type for non-associated potential archetypes.
    
    This lets us map from potential archetypes back to dependent types
    more efficiently, eliminating a linear search.
    
    Swift SVN r22640

commit f20225a34bb0a9b3aaab7f57013ccaf76564d92e
Author: John McCall <rjmccall@apple.com>
Date:   Fri Sep 26 06:35:51 2014 +0000

    Access properties and subscripts in the most efficient
    semantically valid way.
    
    Previously, this decision algorithm was repeated in a
    bunch of different places, and it was usually expressed
    in terms of whether the decl declared any accessor
    functions.  There are, however, multiple reasons why a
    decl might provide accessor functions that don't require
    it to be accessed through them; for example, we
    generate trivial accessors for a stored property that
    satisfies a protocol requirement, but non-protocol
    uses of the property do not need to use them.
    
    As part of this, and in preparation for allowing
    get/mutableAddressor combinations, I've gone ahead and
    made l-value emission use-sensitive.  This happens to
    also optimize loads from observing properties backed
    by storage.
    
    rdar://18465527
    
    Swift SVN r22298

commit 167d2f4bbb5efcc15b142f23983d85b47eb47a16
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Fri Sep 12 02:54:50 2014 +0000

    Include extensions in ClangModuleUnit::getTopLevelDecls.
    
    We don't do this very efficiently, but it does work. And now that it's working,
    drop some special cases in module interface printing -- just always print
    Clang decls in Clang source order.
    
    Swift SVN r21901

commit 0085b945099faf32051929ab28aa5e83217a2bbd
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Wed Aug 27 00:04:25 2014 +0000

    [stdlib] AnyObject wrapper for bridging purposes
    
    As part of the evolution toward a one-word array layout, create a type
    that can be used to efficiently store Cocoa or Native class instances
    and discriminate between them.
    
    Swift SVN r21469

commit 00668527e3f19fedfd442037b05dda420f3d36ea
Author: Andrew Trick <atrick@apple.com>
Date:   Fri Aug 8 00:50:29 2014 +0000

    EnumSimplification was designed to avoid generating a hashmap from BB to RPO, which is well-intentioned, but I think it's simpler and safer just to use an RPO map here. This patch changes the code to:
    
    - Simplify the code for reading/debugging.
    
    - Skip the sorting step.
    
    - Allow removing this land mine: C++ copy ctors need to really copy.
    
    - Skip the searching step in each loop iteration.
    
    - Be as space efficient. No redundant BB pointers in an array (and removed the AA pointer).
    
    Swift SVN r21102

commit 5465993f81857f4a9b34d8cfae0c01390796480a
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Fri Aug 1 01:46:09 2014 +0000

    [stdlib] UnsafePointer : RandomAccessIndexType
    
    It can be useful to form Ranges of these things, and there's no reason
    it shouldn't be efficient to measure them, for example.
    
    Swift SVN r20875

commit 4d03ef63f75c5d7a3bbca52fbac21fcd83ba7646
Author: Chris Lattner <clattner@apple.com>
Date:   Mon Jul 28 23:55:14 2014 +0000

    Rip out my previous work that produced perplexing "inout writeback to
    computed property" errors when SILGen could determine that there was
    an inout writeback alias, and have the code instead perform CSE of the
    writebacks directly.
    
    This means that we produce more efficient code, that a lot of things
    now "just work" the way users would expect, and that the still erroneous
    cases now get diagnosed with the "inout arguments are not allowed to
    alias each other" error, which people have a hope of understanding.
    
    There is still more to do here in terms of detecting identical cases,
    but that was true of the previous diagnostic as well.
    
    
    
    
    Swift SVN r20658

commit 4f6b85c32cf2aec8e629c153c9227de59fded885
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Sat Jul 26 22:43:51 2014 +0000

    Add a new OptionalEnum type to encapsulate the "enum-value-or-nothing" pattern.
    
    This implements the logic of storing "absent" values as 0 and "present"
    values as the enumerator's underlying value plus 1. For enums whose raw
    values are arbitrary or small, this is a safe, space-efficient way to add
    the "absent" value.
    
    The type has been designed to be used with PointerIntPair, so it converts
    (explicitly) to intptr_t, and (explicitly) from any integral type.
    
    Adopt this for use with the Accessibility bits in a ValueDecl.
    
    No intended functionality change.
    
    Swift SVN r20597

commit 10d2e7e2c5f3b44255dac6db964a03e2aabeede1
Author: Adrian Prantl <aprantl@apple.com>
Date:   Sat Jul 26 00:18:16 2014 +0000

    Debug info: Emit a more efficient representation for -O0 shadow copies for
    explosions by copying them into a single, aggregate alloca.
    This also happens to hide <rdar://problem/17815972> for llvm-600.
    
    Swift SVN r20582

commit 312c55a30828c084b7e0ab6d3ba8d3b238ff1b48
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Thu Jul 3 00:59:48 2014 +0000

    [stdlib] Range WIP 1: create RandomAccessRange
    
    Using ..< and ... on RandomAccessIndex endpoints now produces a distinct
    RandomAccessRange type that can have zero-based indexing, validation at
    creation, efficient strides (the other Range should probably lose its
    stride capability), and all kinds of other goodness.  This is the first
    step in solving our cluster of outstanding Range-related issues.
    
    Swift SVN r19497

commit 4814e00fdad29fe95ab4b9d63f9a529607c61c0b
Author: Dmitri Hrybenko <dgribenko@apple.com>
Date:   Mon Jun 30 14:38:53 2014 +0000

    stdlib/String: implement Unicode extended grapheme cluster segmentation
    algorithm
    
    The implementation uses a specialized trie that has not been tuned to the table
    data.  I tried guessing parameter values that should work well, but did not do
    any performance measurements.
    
    There is no efficient way to initialize arrays with static data in Swift.  The
    required tables are being generated as C++ code in the runtime library.
    
    rdar://16013860
    
    
    Swift SVN r19340

commit 044ff2f4e426121a465cb2dc82a84655d2ffc5a7
Author: Doug Gregor <dgregor@apple.com>
Date:   Sat Jun 21 09:24:43 2014 +0000

    Eliminate forced downcasting's dependency on conditional downcasting.
    
    Previously, a forced downcast was implemented as a conditional
    downcast following by an implicit unwrap, and relied on peephole
    optimizations (in both constraint application and SILGen) to turn them
    into a forced downcast. However, these didn't kick in for AnyObject ->
    T[] downcasts, bypassing the more-efficient deferred checking our
    native collections can do. Finishes <rdar://problem/17319154>.
    
    
    
    
    
    Swift SVN r19064

commit 960347249036958a2cfa8756780d8c762a235f48
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Mon Jun 16 14:04:53 2014 +0000

    [stdlib] Add Array.withMutableStorage
    
    This is our backdoor for efficient bulk in-place mutation of arrays
    until uniqueness-check hoisting is available.
    
    Swift SVN r18925

commit 7acecc5457c81c26da191e9ee4715055cd9a789c
Author: Doug Gregor <dgregor@apple.com>
Date:   Sat Jun 14 16:55:53 2014 +0000

    Add a library entry point for Dictionary downcasting for bridged keys & values.
    
    This is an inefficient, copying implementation of
    _dictionaryBridgeToObjectiveC to aid progress on wiring up dictionary
    downcasting <rdar://problem/16847470>. Making this implementation
    efficient is tracked by <rdar://problem/16852016>.
    
    
    
    Swift SVN r18897

commit 2d4148796ba3de3f9d2fd4d8da5a5502a218e25e
Author: Doug Gregor <dgregor@apple.com>
Date:   Sat Jun 14 16:25:08 2014 +0000

    Add a library entry point for Dictionary upcasting for object keys & values.
    
    This is an inefficient, copying implementation of
    _dictionaryCheckedDownCast to aid progress on wiring up dictionary
    downcasting <rdar://problem/16847470>.  Making this implementation
    efficient is tracked by <rdar://problem/16852016>.
    
    
    Swift SVN r18896

commit 5c532d61113876d63c555b727aa1ffca88c61766
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon May 19 14:18:39 2014 +0000

    Introduce a fast path for Array.bridgeFromObjectiveC()
    
    We were using the bridged non-verbatim path
    (_arrayBridgeFromObjectiveC) for bridged-verbatim types. While that
    path can do the right thing (and does when the standard library's
    internal checking is turned off), it's unnecessarily inefficient.
    
    Swift SVN r18418

commit 9ccd1ffcc9533b65c392cff6b88cf09d3aa5a115
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon May 19 05:44:53 2014 +0000

    Make "x is T[]" for array downcasting go through checked casting.
    
    We can't actually make the "is" check any more efficient for arrays
    anyway, and re-using the checked casting code makes "is" work properly
    for arrays. More of <rdar://problem/16952771>.
    
    
    Swift SVN r18395

commit 17f7684a20d23b52e63a0f5d551d504702d0b502
Author: Argyrios Kyrtzidis <kyrtzidis@apple.com>
Date:   Sun May 18 20:01:29 2014 +0000

    [IDE/CodeCompletion] Make collecting and caching the global completion results more efficient.
    
    We already don't try to detect and filter out shadowed declarations due to performance reasons, so cache the visible declarations contained
    in a specific ImportedModule and collect all the results by recursing via calling Module::forAllVisibleModules().
    
    This is more efficient because we avoid doing the work to collect all the global results for both Cocoa and AppKit, and we save on memory usage for the cache.
    This also fixes a correctness issue where if you imported a module that was transitively imported by a previous import (e.g. Cocoa and AppKit) you would get duplicate results.
    
    For this test case:
    
    ----------
    import Cocoa
    import AppKit
    import Foundation
    
    func foo() {
      <ESC>
    }
    ----------
    
    We go from
      - 7.05 secs to 1.43 secs to collect the results
      - 9.0 secs to 2.4 to ultimately show them in Xcode for first-time invocation.
    
    Swift SVN r18344

commit 47eea01ab83044ae27dab53d4efd94ce8ca35a26
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Thu May 8 23:02:08 2014 +0000

    [stdlib] Eager non-verbatim bridging
    
    Trying to lazily compute bridge results and cache them isn't going to
    work, because there's no place to efficiently invalidate the cache in
    cases like this:
    
      func f(a: NSArray) {
        for i in 0...a.count {
          println(a.objectAtIndex(i)) // Fills up the cache
        }
      }
    
      var message = ["hello", "world"]
      f(message)
      message[0] = "goodbye, cruel" // need a cache invalidation or else
      f(message)                    // ...this prints "hello\nworld\n"
    
    Since we need C performance for subscript assignment, we just can't
    afford to do anything extra there.
    
    Instead, when the element type isn't "Bridged Verbatim," just eagerly
    convert it to an NSArray.
    
    Swift SVN r17722

commit dea14570b9837a90bede64f329aa755e019e363f
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Tue May 6 07:52:58 2014 +0000

    [stdlib] Array doc adjustment
    
    We were wishfully thinking that we could convert all NSArrays lazily.
    However, since non-class/existential types are supposed to have a
    statically-knowable efficient representation we need an eager conversion
    in those cases.
    
    Swift SVN r17538

commit b3f470ad16a4d72a1ef9f6a363e69c45c27163bf
Author: Argyrios Kyrtzidis <kyrtzidis@apple.com>
Date:   Mon Apr 21 07:18:50 2014 +0000

    [ClangImporter] Make getting the ClangNode from a swift Decl more efficient by
    allocating extra memory and storing it directly before the swift AST object.
    
    Reduces code-completion time for Cocoa by -25%.
    
    Swift SVN r16615

commit cd4ca76b6a7bc5aa65a3bc9f211fad94bc8865be
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon Apr 14 20:05:35 2014 +0000

    Introduce the ObjCSelector class to store an Objective-C selector.
    
    We have to work with selectors quite often, so provide an efficient
    representation for them. Switch ObjCAttr over to this representation,
    which has the nice property that it efficiently represents implicit
    @objc attributes with names and allows us to overwrite the Objective-C
    name without losing all source information. Addresses
    <rdar://problem/16478678>, and sets us up for dealing with selectors
    better.
    
    Swift SVN r16327

commit 3d0cf8bbd9b04e4c9b5aacde3e290b27c961ccc5
Author: Greg Parker <gparker@apple.com>
Date:   Wed Mar 26 08:44:39 2014 +0000

    [build] Fix iOS bugs, again.
    
    In retrospect testing this directly on the buildbot was not the most efficient approach.
    
    
    Swift SVN r15497

commit ec52d16f57505ac2340fa097b6d3606d5a583529
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Thu Feb 20 22:22:20 2014 +0000

    Teach SerializedSILLoader how to load all SILFunctions. Also add SerializedSILLoader::getAll().
    
    getAll deserializes all SIL (except for globals). This enables us to iterate
    over the SILModules once instead of once for first SILFunctions, then VTables,
    then WitnessTables which is just inefficient.
    
    Swift SVN r14176

commit 20349edbad3389da94812a4742231198ff7000d6
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Thu Feb 20 21:19:53 2014 +0000

    Change SILDeserializer to use StringRef instead of Identifier as its internal key.
    
    This enables us to lookup a function from a key by avoiding the need to create
    an identifier inside the OnDiskHashTable structure. Doing so would require an
    ASTContext, something that is not available therein.
    
    As a side effect this also makes OnDiskHashTable more efficient by just using a
    StringRef reference instead of creating a uniqued identifier in the AST for
    every deserialized node in the table.
    
    Swift SVN r14169

commit 90bdf1f8418b0d5b60efcd26d9d62750cf335536
Author: Andrew Trick <atrick@apple.com>
Date:   Sat Feb 8 08:20:46 2014 +0000

    Run a sequence of SILFunctionTransforms on the same function.
    
    It is much easier to follow the progreses of optimizations. And should
    be more efficient.
    
    Swift SVN r13678

commit b22321d89b80faf919e397c419ce9d1035320f33
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon Feb 3 15:27:55 2014 +0000

    Allow invocation of a DynamicSelf-returning method on an existential.
    
    As part of this, rewrite the check to use interface types. It's
    cleaner and more efficient.
    
    Swift SVN r13353

commit 9ccaf820b84a67460b03946c1f02c7bd793b6e4a
Author: Doug Gregor <dgregor@apple.com>
Date:   Wed Jan 29 11:08:56 2014 +0000

    Start making the method overriding rules a bit more sane.
    
    When we type check the signature of a method, determine which method
    it overrides (if any) at that time. This ensures that we always have
    this information for name lookup (per <rdar://problem/15932845>).
    
    As part of this, start to make the overriding rules a little more
    sane. John has worked out more of the model here, but this patch:
      - Considers an Objective-C method an override of another Objective-C
      method if the selectors match and the type vs. instance-ness of the
      methods match. The method types are checked for consistency
      (subtyping is okay).
      - Diagnoses when a method overrides more than one method from a
      superclass, and
      - Eliminates the "cannot overload method from a superclass"
      diagnostic, which is overly pedantic and oddly limiting.
    
    Note that we lose some amount of checking here. Specifically, we don't
    have a good place to check that one has not provided two different
    methods that override the same superclass method. The older code did
    that (somewhat), and it's not a trivial problem to solve efficiently.
    
    This fixes the part of <rdar://problem/15597226> that is needed for
    <rdar://problem/15932845>. It still doesn't handle properties,
    subscripts, and undoubtedly other issues.
    
    
    
    Swift SVN r13108

commit 84e20a06201d0f8b1036e555b8336af898de38e9
Author: Argyrios Kyrtzidis <kyrtzidis@apple.com>
Date:   Sat Jan 18 20:19:09 2014 +0000

    [AST] Break down IdentTypeRepr to different subtypes.
    
    This makes memory allocation for it more efficient and it's more convenient to handle.
    
    Swift SVN r12541

commit 36b765eac9fc5b05d26bca73b236c5765d0d9533
Author: Chris Lattner <clattner@apple.com>
Date:   Mon Jan 13 17:02:42 2014 +0000

    Switch the rest of the ArgumentInitVisitor cases over to using
    RValue::forwardInto.  While this isn't directly related to my
    current problem, it is obvious goodness.  This avoids materializing
    "as a single value" the argument values, which means that we don't
    create a temporary holding the value when we have tuple elements,
    allowing us to generate more efficient code (particularly for
    address-only tuple).
    
    
    
    Swift SVN r12233

commit 651f858dbbaf69483b7bda991a0596c95348602a
Author: Doug Gregor <dgregor@apple.com>
Date:   Fri Dec 6 01:23:39 2013 +0000

    Represent all type variables within the constraint graph without simplification.
    
    Previously, the constraint graph only represented type variables that
    were both unbound and were the representatives within their respective
    equivalence classes. To achieve this, each constraint was fully
    simplified when it was added to the graph, which is a fairly expensive
    process. This representation made certain operations---merging two type
    variables, replacing a type variable with a fixed type, etc---both
    hard to implement and hard to reverse, forcing us to rebuild the
    constraint graph each time.
    
    Now, add all type variables to the graph (including those with fixed
    type bindings and non-representatives) and add constraints without
    simplification. Separately track the equivalence classes of each type
    variable (in the representative's node) and adjacencies due to type
    variables showing up in the fixed type bindings of other type
    variables. Although not yet implemented, the merging and type variable
    replacement operations are far easier to implement (and more
    efficient) with this representation, and are also easier to undo,
    making this a step toward creating and updating a single consistent,
    global constraint graph rather than re-creating a constraint graph
    during each solver step.
    
    Performance-wise, this is a 4% regression when type-checking the
    standard library. I expect to make that up easily once we switch to a
    single constraint graph.
    
    
    
    
    Swift SVN r10897

commit e28c425a643018e780c81e23ccdfcb3eab6971a8
Author: Doug Gregor <dgregor@apple.com>
Date:   Fri Nov 22 19:21:43 2013 +0000

    Compute connected components for the constraint graph.
    
    This implements an offline algorithm for connected components. We
    could use an online algorithm, which would be slightly more efficient
    in the case where we always require the connected components, but such
    algorithms don't cope with edge removals very well.
    
    Still just a debugging tool!
    
    
    
    Swift SVN r10663

commit 08171453dae2e1afe0bc32dc5ad6a7263c416055
Author: John McCall <rjmccall@apple.com>
Date:   Thu Nov 14 05:25:06 2013 +0000

    Make it a bit easier to propogate expressions around instead
    of having to lower to an RValue.
    
    This is valuable because we can often emit an expression to a
    desired abstraction level more efficiently than just emitting
    it to minimal abstraction and then generalizing.
    
    Swift SVN r10455

commit f1c977889ce1be47688c1d8f67bf8dbd781fae9c
Author: Andrew Trick <atrick@apple.com>
Date:   Sat Oct 26 09:03:36 2013 +0000

    Added a ReachingBlockSet and ReachingBlockMatrix for efficiency.
    
    Eventually, we may decide not to compute global reachability, but this
    should be reasonably efficient in the meantime.
    
    Swift SVN r9691

commit 6c265ef3164e4b4ecd58f0db56cd3ba062a9e80a
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Oct 15 20:09:45 2013 +0000

    Use subtyping constraints rather than conversion constraints for dot access.
    
    Subtyping constraints are more efficient to check, and we only need
    conversion constraints when we're dealing with protocol members.
    
    
    Swift SVN r9367

commit c10340433cf3b13565cd66b7afd1bfcc2a8b9c8b
Author: Joe Groff <jgroff@apple.com>
Date:   Thu Sep 19 23:16:53 2013 +0000

    IRGen: Forward extra inhabitants through single-payload unions.
    
    If a single-payload union doesn't use up all of its payload's extra inhabitants, it can claim the remaining ones as its own. While we're here, specialize the extra inhabitants implementation for no-payload unions to generously (and more efficiently) give out all integer values within its storage type above the largest discriminator as extra inhabitants, instead of relying entirely on the spare bits mask.
    
    Swift SVN r8470

commit 795496079770782d60658c3b2bcf2a6cad272f4c
Author: John McCall <rjmccall@apple.com>
Date:   Thu Sep 19 22:14:55 2013 +0000

    Add 'copy_value' and 'destroy_value' operations to destroy
    entire aggregates at once.
    
    This has three worth effects:
      - It significantly decreases the amount of SIL required
        for these operations.
      - It makes it far easier for IR-gen to choose efficient
        patterns of destruction, e.g. calling a single entrypoint
        or recognizing that it can just use the runtime 'release'
        entrypoints.
      - It makes it easier to recognize and optimize aggregate
        copy/destroy operations.
    It does make SROA-like tasks a bit more challenging.  The
    intent is to give TypeLowering a way to expand these into
    their primitive behavior.
    
    Swift SVN r8465

commit 7a0ce11980db7b3ed182a2b65a76588144e1dec6
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Sep 3 15:15:13 2013 +0000

    Replace ProtocolConformanceWitness with ConcreteDeclRef.
    
    The latter is more efficient and should eventually be more common.
    
    
    Swift SVN r7840

commit 33589550426da58467a6f7bd6e2b9e3051daf6ef
Author: Joe Groff <jgroff@apple.com>
Date:   Fri Aug 30 23:28:38 2013 +0000

    SILGenPattern: Destroy destructured subject rvalue before entering cases.
    
    Before, we left the switch subject rvalue to be cleaned up at the close of the switch's scope. This is a bit wasteful because it keeps the rvalue live long after it's actually needed, and destroying the subject potentially requires replicating a bunch of destructuring we already do in the course of the pattern match. This patch changes things so that, just prior to entering a case, we clean up the destructured occurrence vector we have at the point of entering the case, which should be equivalent to but potentially more efficient than destroying the original subject rvalue.
    
    Swift SVN r7814

commit 9b9e669bf78d008c60243161a1093ae72b9762b1
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Mon Aug 26 23:07:54 2013 +0000

    Add support for :print_module on Clang modules.
    
    This isn't very efficient: it scans every decl in the Clang TU (forcing
    deserialization) and filters based on the decl's enclosing module.
    Moreover, since getClangModuleForDecl() currently only handles top-level
    modules, all submodules get implicitly added to the top-level module...
    and will /not/ match an explicit submodule request.
    
    (This is probably close to the behavior we actually want: include decls that
    are from modules that are (a) submodules and (b) re-exported by the top-level
    module. We do want that extra check, though, and we would want to find things
    specifically by submodule.)
    
    Swift SVN r7602

commit c74dc796109f02a4de8cfa7facd3d54390e8d795
Author: Joe Groff <jgroff@apple.com>
Date:   Mon Aug 26 20:50:31 2013 +0000

    SIL: Add instructions to implement address-only unions.
    
    Because union layout may interleave tag bits with payload data, we need to be able to efficiently inject and remove tag bits from an address-only union in-place. To do this, we'll model address-only union initialization by projecting out the data address (union_data_addr) and storing to it, then overlaying the tag bits (inject_union_addr). To dispatch and project out the data, we'll use a destructive_switch_union_addr instruction that clears any tag bits in-place necessary to give a valid data address.
    
    Swift SVN r7589

commit 4cefdf263ca648966e9dc4afbefc1323fb79c784
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Mon Aug 19 22:45:14 2013 +0000

    [ClangImporter] Don't unique selector names when doing id-style lookup.
    
    Dmitri pointed out that we're just bloating the AST context's identifier
    table and not actually saving any string comparison. StringRef's == is
    pretty efficient anyway.
    
    Swift SVN r7339

commit 6400fdc814bda14dc8f244e729c385105af9af0e
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon Aug 5 18:16:54 2013 +0000

    [Constraint solver] Track resolved overload sets more efficiently.
    
    Rather than maintain a big DenseMap that was only ever used for
    iteration, use a constraint-system--allocated linked list with a
    shared head. Good for 1.5% compile time on the standard library, and a
    necessary refactor for tracking more information for diagnostics.
    
    
    Swift SVN r6907

commit d1d584a1ef56ea4a2473275966daca6eebc6e504
Author: Bill Wendling <wendling@apple.com>
Date:   Fri Jul 26 22:44:35 2013 +0000

    Using an attribute builder to is more efficient than adding attributes one by one.
    
    Swift SVN r6659

commit 63ff23147fc6e5e36f7670d88a1a13652ab60887
Author: Doug Gregor <dgregor@apple.com>
Date:   Wed Jul 10 01:15:15 2013 +0000

    Implement another new closure syntax.
    
    In this syntax, the closure signature (when present) is placed within
    the braces and the 'in' keyword separates it from the body of the
    closure, e.g.,
    
          magic(42, { (x : Int, y : Int) -> Bool in
            print("Comparing \(x) to \(y).\n")
            return y < x
          })
    
    When types are omitted from the parameter list, one can also drop the
    parentheses, e.g.,
    
          magic(42, { x, y -> Bool in
            print("Comparing \(x) to \(y).\n")
            return y < x
          })
    
    The parsing is inefficient and recovers poorly (in part because 'in'
    is a contextual keyword rather than a real keyword), but it should
    handle the full grammar. A number of tests, along with the whitepaper
    and related rational documents, still need to be updated. Still, this
    is the core of <rdar://problem/14004323>.
    
    
    
    Swift SVN r6105

commit 4554961979d1dd2b8169d4c28c714877ac43f22b
Author: Doug Gregor <dgregor@apple.com>
Date:   Fri Jun 28 18:43:41 2013 +0000

    [Name lookup] Introduce a lookup table into each nominal type declaration.
    
    The lookup table for a nominal type declaration provides efficient
    (O(1)) access to all of the declarations with a given name in a
    nominal type and its extensions. This is architecturally different
    from Clang's handling of Objective-C classes and
    categories/extensions, where each category/extension has its own
    lookup table, and is meant to reduce the number of hash table lookups
    required, especially once these hash tables are stored in the module.
    
    The lookup table is built and updated lazily as extensions and members
    are introduced, similarly to Clang's lookup tables. However, the
    simpler name lookup rules in Swift (vs. C/C++/Objective-C) make this
    approach actually semantically correct.
    
    
    
    Swift SVN r5874

commit 5ffcdaea985035cb4a4525633c46d00713813fa4
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Thu Jun 27 20:47:21 2013 +0000

    [stdlib] Replace String storage with a reference-counted buffer with capacity
    
    Allows us to use a copy-on-write strategy to efficiently grow Strings.
    This should obviate the need for a StringBuffer class anytime soon and
    make comfortable formatting efficient.
    
    If we backslide and the COW/capacity functionality of String is no
    longer doing its job, the test added here will appear to hang and
    hopefully someone will be annoyed enough to fix it.  We can't do
    better right now because performance measurements vary based on the
    test platform.
    
    Swift SVN r5845

commit 965035dc2adaa3fd3906dc355bfe262cfb2c624b
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Sat May 25 01:34:52 2013 +0000

    [serialization] Load the decl and type offset arrays from the module.
    
    ...but don't do anything with them yet. This does check that they're being
    correctly serialized, though.
    
    This introduces a new ADT, PointerIntUnion, which like PointerUnion is an
    efficient variant type using the lowest bit of data as a discriminator.
    By default, the union can store any pointer-bits-minus-one-sized integer,
    but both the integer type and the underlying storage type can be
    customized.
    
    Swift SVN r5321

commit 1641477826531a66fa729c94f4e65425b36b0cb5
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon May 20 18:26:07 2013 +0000

    Eliminate lookupExtensions() and the extension cache.
    
    This infrastructure has been replaced by the extension list on nominal
    declaration, which is simpler and more efficient.
    
    
    Swift SVN r5225

commit 8114ce16f808db6b1d87ecc2ee25071f22a83da0
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon May 20 18:06:51 2013 +0000

    Use the list of extensions of a nominal type for name lookup into that type.
    
    This replaces the obscure, inefficient lookup into extensions with
    something more straightforward: walk all of the known extensions
    (available as a simple list), then eliminate any declarations that
    have been shadowed by other declarations. The shadowing rules still
    need to consider the module re-export DAG, but we'll leave that for
    later.
    
    As part of this, keep track of the last time we loaded extensions for
    a given nominal type. If the list of extensions is out-of-date with
    respect to the global generation count (which tracks resolved module
    imports), ask the modules to load any additional extensions. Only the
    Clang module importer can currently load extensions in this manner.
    
    
    Swift SVN r5223

commit 65db19395d32dbfe2fbb6fab6c70d64070acbb64
Author: John McCall <rjmccall@apple.com>
Date:   Fri Nov 30 02:47:01 2012 +0000

    Make metatype layout compatible with struct objc_class.
    
    The principal difficulty here is that we need accessing the
    value witness table for a type to be an efficient operation,
    but there (obviously) isn't a VWT field for ObjC classes.
    Placing this field after the metatype would tend to bloat
    metatypes by quite a bit.  Placing it before is best, but
    it introduces an unfortunate difference between the address
    point of a metatype and the address of the global symbol.
    That, however, can be fixed with appropriate linker support.
    Still, for now this is rather unfortunately over-subtle.
    
    Swift SVN r3307

commit 11e06f41253da0c2cdb2e94faa2f2ea2bbb46035
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Oct 9 00:07:15 2012 +0000

    When we run out of supertypes in our fallback, allow a fallback to the
    default literal type. We need on-line type constraints for this to be
    efficient.
    
    
    Swift SVN r2954

commit 9e8633ac41af488553512a5079545e94125aab7a
Author: Doug Gregor <dgregor@apple.com>
Date:   Thu Jul 26 17:52:04 2012 +0000

    Encode and pass all of the archetypes, including derived archetypes,
    in SpecializeExpr, so that we have complete substitution and
    protocol-conformance information. On the IR generation side, pass
    witness tables for all of the archetypes (again, including derived
    archetypes) into generic functions, so that we have witness tables for
    all of the associated types.
    
    There are at least two major issues:
    
      (1) This is a terribly inefficient way to pass witness tables for
      associated types. The witness tables for associated types should be
      accessible via the witness tables of their parent. However, we need
      more information in the ASTs here, because there may be additional
      witness tables that will need to be passed for requirements that are
      placed on the associated type by the generic function itself.
    
      (2) Something about my test triggers a void/non-void verification failure
      in the witness build for an instance function whose abstracted form
      returns an associated type archetype and whose concrete form returns
      an empty struct. See the FIXME in the test.
    
    
    
    Swift SVN r2464

commit bac547088105cc45364d67b2384dca703c90d2f6
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Jul 3 21:46:52 2012 +0000

    Collate the OneOfType*/ClassType*/StructType*/ProtocolType* members of
    the various NominalDecl subclasses into a single NominalType* member
    in NominalDecl. Use it to make TypeDecl::getDeclaredType() more
    efficient/simpler, and simplify the ProtocolDecl/ProtocolType
    interaction along the way.
    
    No functionality change.
    
    
    Swift SVN r2298

commit ff6f88236283eb023169e8b64db2d1f25546e728
Author: Eli Friedman <eli_friedman@apple.com>
Date:   Mon Jul 2 23:00:29 2012 +0000

    Change the convertFromStringLiteral convention to be a bit more efficient:
    add an alternate entry-point called convertFromASCIIStringLiteral which is called for
    string literals which contain only ASCII codepoints, and add an optional byteLength
    argument for the conversion.  <rdar://problem/11764780>.
    
    I've also tentatively changed swift.String to use the new convention; Dave,
    please review.
    
    
    
    Swift SVN r2290

commit 3dd77943dfc3911a7654b156a1c6117aa97d482f
Author: John McCall <rjmccall@apple.com>
Date:   Tue Jun 5 04:50:42 2012 +0000

    Change the size of a prototype type to 3 pointers rather than
    a fixed size of 16 bytes.  3 pointers is the magic value in
    swift:  many, many things are better if we can handle three
    pointers efficiently.
    
    Swift SVN r2147

commit 26159b59ee74911c38e7b51de1eef80567e15755
Author: Eli Friedman <eli_friedman@apple.com>
Date:   Sat Jun 2 01:26:58 2012 +0000

    First draft of some code to make the general unqualified lookup able to perform lookup into local scopes.  Normally, name-binding doesn't need this because the parser can resolve references to locals, but we need this sort of lookup to handle cases involving local types.  It's also likely this will be generally useful for tools consuming the AST, typo-correction, etc.
    
    We probably need to add some sort of data structure to represent this information, but as a proof of concept the current code appears to work.  I'm still working out how to make sure the parser doesn't prematurely bind names and how to make name-binding use it where appropriate (and avoid it when we don't need it, because no matter how efficient we make it, it will still be relatively expensive).
    
    
    
    Swift SVN r2112

commit 3fbd9682a103ba225addd2b9f1db5544ad7fe007
Author: Doug Gregor <dgregor@apple.com>
Date:   Thu May 24 23:27:43 2012 +0000

    Update the printf() implementation to match the May demo slides. Major
    changes include:
    
      - Each '%' in the format string is followed by some number of
        non-alpha characters (the "layout") and then an alpha character
        (the "kind"). The string and character are passed along to each
        FormattedPrintable's printFormatted() so they can do custom
        formatting
      - The Format struct is now used for the 'default' layout, which all
        of the standard libaries use
      - Added 'x' and 'o' custom formatting for formatting numbers in hex
        and octal, respectively. 'u' and 'l' can be used to print
        characters in uppercase or lowercase (String versions are yet to
        come).
      - The code for splitFormat()/printf() match what will be on the
        slides. They use the split operations to keep the whole thing
        linear, rather than performing inefficient slices.
    
    There are some ugly hacks here due to <rdar://problem/11529601>.
    
    
    Swift SVN r1980

commit 06992eec7abed18bad2c2b20c727400207b00f56
Author: John McCall <rjmccall@apple.com>
Date:   Sat May 19 03:16:08 2012 +0000

    Introduce helper templates to make it easier to define
    type infos that can be efficiently scalarized.
    
    Swift SVN r1904

commit d699e00390aad072abf77a62e42e2b5b420d6032
Author: John McCall <rjmccall@apple.com>
Date:   Mon Mar 19 08:59:35 2012 +0000

    Teach IR-gen how to efficiently emit a broad range of
    expressions when no value is required.  Teach IR-gen
    to emit DotSyntaxBaseIgnoredExprs as known function
    references with possibly non-trivial semantics.
    Undo the change to getSemanticsProvidingExpr().
    
    Swift SVN r1238

commit 9038a38ce8d1600d13eb0c6e29c09b6a5efc8b3a
Author: John McCall <rjmccall@apple.com>
Date:   Thu Feb 16 06:11:30 2012 +0000

    We will need a DiverseStack<T> for cleanups, but a stack is really
    the wrong structure for an l-value.  Use a list instead (that is,
    something which allows efficient iteration forward through the
    collection).
    
    Swift SVN r1117

commit 5f1bcd7b7e8fae5596fbde670e7776856b04e44c
Author: John McCall <rjmccall@apple.com>
Date:   Thu Feb 2 01:13:33 2012 +0000

    Move all the side-allocated members of ASTContext to a single
    side-allocation.  This is both easier to work with and extend
    and slightly more efficient.
    
    
    
    Swift SVN r1106

commit c21b11eb28e0b007080949772026578567faee7b
Author: John McCall <rjmccall@apple.com>
Date:   Thu Nov 17 09:32:06 2011 +0000

    Add a method for conditionally trying to evaluate an expression
    as its underlying l-value.  Often more efficient than evaluating
    it fully into an r-value and only then transforming it.
    
    
    
    Swift SVN r875
commit ab0601d97af90d92f7354c7e0c25929d80a5e017
Author: Andrew Trick <atrick@apple.com>
Date:   Thu Sep 30 21:16:08 2021 -0700

    Add PrunedLiveness::computeSSALiveness
    
    Straighforward API to build pruned liveness from a single SSA value.
    
    This 3-line function allows PrunedLiveness to be used anywhere
    ValueLifetimeAnalysis was used in OSSA form. Aside from simplicity and
    efficiency, the difference is that this composes with other utilities
    that only care about PrunedLiveBlocks, PrunedLiveness, or
    PrunesLivenessBounary independent of how those data structures were generated.

diff --git a/include/swift/SIL/PrunedLiveness.h b/include/swift/SIL/PrunedLiveness.h
index ffed5b69842..077175fc16b 100644
--- a/include/swift/SIL/PrunedLiveness.h
+++ b/include/swift/SIL/PrunedLiveness.h
@@ -284,6 +284,9 @@ public:
 
   bool areUsesWithinBoundary(ArrayRef<Operand *> uses,
                              DeadEndBlocks &deadEndBlocks) const;
+
+  /// Compute liveness for a single SSA definition.
+  void computeSSALiveness(SILValue def);
 };
 
 /// Record the last use points and CFG edges that form the boundary of
diff --git a/lib/SIL/Utils/PrunedLiveness.cpp b/lib/SIL/Utils/PrunedLiveness.cpp
index da2aeec1935..d0b14638e2c 100644
--- a/lib/SIL/Utils/PrunedLiveness.cpp
+++ b/lib/SIL/Utils/PrunedLiveness.cpp
@@ -161,6 +161,17 @@ bool PrunedLiveness::areUsesWithinBoundary(ArrayRef<Operand *> uses,
   return true;
 }
 
+// An SSA def meets all the criteria for pruned liveness--def dominates all uses
+// with no holes in the liverange. The lifetime-ending uses are also
+// recorded--destroy_value or end_borrow. However destroy_values may not
+// jointly-post dominate if dead-end blocks are present.
+void PrunedLiveness::computeSSALiveness(SILValue def) {
+  initializeDefBlock(def->getParentBlock());
+  for (Operand *use : def->getUses()) {
+    updateForUse(use->getUser(), use->isLifetimeEnding());
+  }
+}
+
 void PrunedLivenessBoundary::visitInsertionPoints(
     llvm::function_ref<void(SILBasicBlock::iterator insertPt)> visitor,
     DeadEndBlocks *deBlocks) {

commit cc0aa2f8b8740221420ac35e86d7e67cbcd8529b
Author: Andrew Trick <atrick@apple.com>
Date:   Thu Aug 27 17:48:16 2020 -0700

    Add an AccessPath abstraction and formalize memory access
    
    Things that have come up recently but are somewhat blocked on this:
    
    - Moving AccessMarkerElimination down in the pipeline
    - SemanticARCOpts correctness and improvements
    - AliasAnalysis improvements
    - LICM performance regressions
    - RLE/DSE improvements
    
    Begin to formalize the model for valid memory access in SIL. Ignoring
    ownership, every access is a def-use chain in three parts:
    
    object root -> formal access base -> memory operation address
    
    AccessPath abstracts over this path and standardizes the identity of a
    memory access throughout the optimizer. This abstraction is the basis
    for a new AccessPathVerification.
    
    With that verification, we now have all the properties we need for the
    type of analysis requires for exclusivity enforcement, but now
    generalized for any memory analysis. This is suitable for an extremely
    lightweight analysis with no side data structures. We currently have a
    massive amount of ad-hoc memory analysis throughout SIL, which is
    incredibly unmaintainable, bug-prone, and not performance-robust. We
    can begin taking advantage of this verifably complete model to solve
    that problem.
    
    The properties this gives us are:
    
    Access analysis must be complete over memory operations: every memory
    operation needs a recognizable valid access. An access can be
    unidentified only to the extent that it is rooted in some non-address
    type and we can prove that it is at least *not* part of an access to a
    nominal class or global property. Pointer provenance is also required
    for future IRGen-level bitfield optimizations.
    
    Access analysis must be complete over address users: for an identified
    object root all memory accesses including subobjects must be
    discoverable.
    
    Access analysis must be symmetric: use-def and def-use analysis must
    be consistent.
    
    AccessPath is merely a wrapper around the existing accessed-storage
    utilities and IndexTrieNode. Existing passes already very succesfully
    use this approach, but in an ad-hoc way. With a general utility we
    can:
    
    - update passes to use this approach to identify memory access,
      reducing the space and time complexity of those algorithms.
    
    - implement an inexpensive on-the-fly, debug mode address lifetime analysis
    
    - implement a lightweight debug mode alias analysis
    
    - ultimately improve the power, efficiency, and maintainability of
      full alias analysis
    
    - make our type-based alias analysis sensistive to the access path

diff --git a/include/swift/SIL/MemAccessUtils.h b/include/swift/SIL/MemAccessUtils.h
index dd198db2b68..a65effe5254 100644
--- a/include/swift/SIL/MemAccessUtils.h
+++ b/include/swift/SIL/MemAccessUtils.h
@@ -2,7 +2,7 @@
 //
 // This source file is part of the Swift.org open source project
 //
-// Copyright (c) 2014 - 2018 Apple Inc. and the Swift project authors
+// Copyright (c) 2014 - 2020 Apple Inc. and the Swift project authors
 // Licensed under Apache License v2.0 with Runtime Library Exception
 //
 // See https://swift.org/LICENSE.txt for license information
@@ -22,20 +22,40 @@
 /// To verify access markers, SIL checks that all memory operations either have
 /// an address that originates in begin_access, or originates from a pattern
 /// that is recognized as a non-formal-access. This implies that every SIL
-/// memory operation has a recognizable address source.
+/// memory operation has a recognizable address source. Given the address of a
+/// memory operation, there are three levels of APIs that inspect the origin of
+/// that address:
 ///
-/// If the memory operation is part of a formal access, then getAddressAccess()
-/// returns the begin_access marker.
+/// 1. getAccessAddress(): Find the originating address as close as possible to
+/// the address of the formal access *without* looking past any storage
+/// casts. This is useful when the type of the returned access address must be
+/// consistent with the memory operation's type (the same type or a parent
+/// type). For a formal access, this typically returns the begin_access, but it
+/// is not guaranteed to because some accesses contain storage casts. For
+/// non-formal access, it returns a best-effort address corresponding to the
+/// base of an access.
 ///
-/// AccessedStorage identifies the storage location of a memory access.
+/// 2. getAccessBegin(): If the memory operation is part of a formal access,
+/// then this is guaranteed to return the begin_access marker. Otherwise, it
+/// returns the best-effort address or pointer corresponding to the base of an
+/// access. Useful to find the scope of a formal access.
 ///
-/// identifyFormalAccess() returns the formally accessed storage of a
-/// begin_access instruction. This must return a valid AccessedStorage value
-/// unless the access has "Unsafe" enforcement. The formal access location may
-/// be nested within an outer begin_access. For the purpose of exclusivity,
-/// nested accesses are considered distinct formal accesses so they return
-/// distinct AccessedStorage values even though they may access the same
-/// memory.
+/// 3. getAccessBase(): Find the ultimate base of any address corresponding to
+/// the accessed object, regardless of whether the address is nested within
+/// access scopes, and regardless of any storage casts. This returns either an
+/// address type, pointer type, or box type, but never a reference type.
+/// Each object's property or its tail storage is separately accessed.
+///
+/// For better identification an access base, use findAccessedStorage(). It
+/// returns an AccessedStorage value that identifies the storage location of a
+/// memory access. It provides APIs for inspecting type of accessed storage and
+/// allows for disambiguation between different types of storage and different
+/// properties within a class.
+///
+/// findAccessedStorage() follows the same logic as getAccessBase(), but if the
+/// base is not recognized as a valid access, it returns invalid
+/// AccessedStorage. It also performs further analysis to determine the root
+/// reference of an object access.
 ///
 /// findAccessedStorage() returns the outermost AccessedStorage for any memory
 /// address. It can be called on the address of a memory operation, the address
@@ -50,19 +70,28 @@
 ///    formal access.
 ///
 /// The AccessEnforcementWMO pass is an example of an optimistic optimization
-/// that relies on the above requirements for correctness. If
-/// findAccessedStorage() simply bailed out on an unrecognized memory address by
-/// returning an invalid AccessedStorage, then the optimization could make
-/// incorrect assumptions about the absence of access to globals or class
-/// properties.
+/// that relies on this requirement for correctness. If findAccessedStorage()
+/// simply bailed out on an unrecognized memory address by returning an invalid
+/// AccessedStorage, then the optimization could make incorrect assumptions
+/// about the absence of access to globals or class properties.
+///
+/// identifyFormalAccess() is similar to findAccessedStorage(), but returns the
+/// formally accessed storage of a begin_access instruction. This must return a
+/// valid AccessedStorage value unless the access has "Unsafe" enforcement. The
+/// formal access location may be nested within an outer begin_access. For the
+/// purpose of exclusivity, nested accesses are considered distinct formal
+/// accesses so they return distinct AccessedStorage values even though they may
+/// access the same memory.
 ///
 //===----------------------------------------------------------------------===//
 
 #ifndef SWIFT_SIL_MEMACCESSUTILS_H
 #define SWIFT_SIL_MEMACCESSUTILS_H
 
+#include "swift/Basic/IndexTrie.h"
 #include "swift/SIL/ApplySite.h"
 #include "swift/SIL/InstructionUtils.h"
+#include "swift/SIL/Projection.h"
 #include "swift/SIL/SILArgument.h"
 #include "swift/SIL/SILBasicBlock.h"
 #include "swift/SIL/SILGlobalVariable.h"
@@ -75,7 +104,7 @@
 
 namespace swift {
 
-/// Get the base address of a formal access by stripping access markers.
+/// Get the source address of a formal access by stripping access markers.
 ///
 /// Postcondition: If \p v is an address, then the returned value is also an
 /// address (pointer-to-address is not stripped).
@@ -86,65 +115,49 @@ inline SILValue stripAccessMarkers(SILValue v) {
   return v;
 }
 
-/// An address projection that may be inside of a formal access, such as
-/// (begin_borrow, struct_element_addr, tuple_element_addr).
-struct AccessProjection {
-  SingleValueInstruction *projectionInst = nullptr;
-
-  /// If \p v is not a recognized access projection the result is invalid.
-  AccessProjection(SILValue v) {
-    switch (v->getKind()) {
-    default:
-      break;
-
-    case ValueKind::StructElementAddrInst:
-    case ValueKind::TupleElementAddrInst:
-    case ValueKind::UncheckedTakeEnumDataAddrInst:
-    case ValueKind::TailAddrInst:
-    case ValueKind::IndexAddrInst:
-      projectionInst = cast<SingleValueInstruction>(v);
-    };
-  }
-
-  operator bool() const { return projectionInst != nullptr; }
-
-  SILValue baseAddress() const { return projectionInst->getOperand(0); }
-};
-
-/// Return the base address after stripping access projections. If \p v is an
-/// access projection, return the enclosing begin_access. Otherwise, return a
-/// "best effort" base address.
+/// Return the source address after stripping as many access projections as
+/// possible without losing the address type.
 ///
-/// Precondition: \p v must be an address.
+/// For formal accesses, this typically returns the begin_access, but may fail
+/// for accesses that call into an addressor, which performs pointer
+/// conversion.
 ///
-/// To get the base address of the formal access behind the access marker,
-/// either call stripAccessMarkers() on the returned value, or call
-/// getAccessedAddress() on \p v.
+/// If there is no access marker, then this returns the "best-effort" address
+/// corresponding to the accessed variable. This never looks through
+/// pointer_to_address or other conversions that may change the address type
+/// other than via type-safe (TBAA-compatible) projection.
+SILValue getAccessAddress(SILValue address);
+
+/// Return the source address or pointer after stripping all access projections
+/// and storage casts.
 ///
-/// To identify the underlying storage object of the access, call
-/// findAccessedStorage() either on \p v or on the returned address.
-SILValue getAddressAccess(SILValue v);
-
-/// Convenience for stripAccessMarkers(getAddressAccess(v)).
-SILValue getAccessedAddress(SILValue v);
+/// If this is a formal access, then it is guaranteed to return the immediately
+/// enclosing begin_access and may "see through" storage casts to do so.
+///
+/// If there is no access marker, then it returns a "best effort" address
+/// corresponding to the accessed variable. In this case, the returned value
+/// could be a non-address pointer type.
+SILValue getAccessBegin(SILValue address);
 
-/// Return true if \p accessedAddress points to a let-variable.
+/// Return the source address or pointer after stripping access projections,
+/// access markers, and storage casts.
 ///
-/// Precondition: \p accessedAddress must be an address-type value representing
-/// the base of a formal access (not a projection within the access).
+/// The returned base address is guaranteed to match the unique AccessedStorage
+/// value for the same \p address. That is, if two calls to getAccessBase()
+/// return the same base address, then they must also have the same storage.
+SILValue getAccessBase(SILValue address);
+
+/// Return true if \p address points to a let-variable.
 ///
 /// let-variables are only written during let-variable initialization, which is
-/// assumed to store directly to the same, unaliased accessedAddress.
+/// assumed to store directly to the same, unaliased access base.
 ///
 /// The address of a let-variable must be the base of a formal access, not an
 /// access projection. A 'let' member of a struct is *not* a let-variable,
 /// because it's memory may be written when formally modifying the outer
 /// struct. A let-variable is either an entire local variable, global variable,
 /// or class property (these are all formal access base addresses).
-///
-/// The caller should derive the accessed address using
-/// stripAccessMarkers(getAccessedAddress(ptr)).
-bool isLetAddress(SILValue accessedAddress);
+bool isLetAddress(SILValue address);
 
 /// Return true if two accesses to the same storage may conflict given the kind
 /// of each access.
@@ -231,8 +244,8 @@ public:
 
   static const char *getKindName(Kind k);
 
-  // Give object tail storage a fake property index for convenience.
-  static constexpr unsigned TailIndex = ~0U;
+  // Give object tail storage a fake large property index for convenience.
+  static constexpr unsigned TailIndex = std::numeric_limits<int>::max();
 
   /// Directly create an AccessedStorage for class or tail property access.
   static AccessedStorage forClass(SILValue object, unsigned propertyIndex) {
@@ -363,8 +376,10 @@ public:
     return global;
   }
 
+  bool isReference() const { return getKind() == Class || getKind() == Tail; }
+
   SILValue getObject() const {
-    assert(getKind() == Class || getKind() == Tail);
+    assert(isReference());
     return value;
   }
   unsigned getPropertyIndex() const {
@@ -372,6 +387,27 @@ public:
     return getElementIndex();
   }
 
+  /// Return the address or reference root that the storage was based
+  /// on. Returns an invalid SILValue for globals or invalid storage.
+  SILValue getRoot() const {
+    switch (getKind()) {
+    case AccessedStorage::Box:
+    case AccessedStorage::Stack:
+    case AccessedStorage::Argument:
+    case AccessedStorage::Yield:
+    case AccessedStorage::Unidentified:
+      return getValue(); // Can be invalid for Unidentified storage.
+    case AccessedStorage::Global:
+      return SILValue();
+    case AccessedStorage::Class:
+    case AccessedStorage::Tail:
+      return getObject();
+    case AccessedStorage::Nested:
+      assert(false && "AccessPath cannot identify nested access");
+      return SILValue();
+    }
+  }
+
   /// Return true if the given storage objects have identical storage locations.
   ///
   /// This compares only the AccessedStorage base class bits, ignoring the
@@ -427,18 +463,17 @@ public:
 
   /// If this is a uniquely identified formal access, then it cannot
   /// alias with any other uniquely identified access to different storage.
-  ///
-  /// This determines whether access markers may conflict, so it cannot assume
-  /// that exclusivity is enforced.
   bool isUniquelyIdentified() const {
     switch (getKind()) {
     case Box:
     case Stack:
     case Global:
       return true;
+    case Argument:
+      return
+        getArgument()->getArgumentConvention().isExclusiveIndirectParameter();
     case Class:
     case Tail:
-    case Argument:
     case Yield:
     case Nested:
     case Unidentified:
@@ -447,16 +482,10 @@ public:
     llvm_unreachable("unhandled kind");
   }
 
-  /// Return true if this a uniquely identified formal access location assuming
-  /// exclusivity enforcement. Do not use this to optimize access markers.
-  bool isUniquelyIdentifiedAfterEnforcement() const {
-    if (isUniquelyIdentified())
-      return true;
-
-    return getKind() == Argument
-           && getArgument()
-                  ->getArgumentConvention()
-                  .isExclusiveIndirectParameter();
+  /// Return true if this storage is guaranteed not to overlap with \p other's
+  /// storage.
+  bool isDistinctFrom(const AccessedStorage &other) const {
+    return isDistinctFrom<&AccessedStorage::isUniquelyIdentified>(other);
   }
 
   /// Return true if this identifies the base of a formal access location.
@@ -470,11 +499,27 @@ public:
     return getKind() == Class;
   }
 
-  // Return true if this storage is guaranteed not to overlap with \p other's
-  // storage.
+  /// Returns the ValueDecl for the underlying storage, if it can be
+  /// determined. Otherwise returns null.
+  ///
+  /// WARNING: This is not a constant-time operation. It is for diagnostics and
+  /// checking via the ValueDecl if we are processing a `let` variable.
+  const ValueDecl *getDecl() const;
+
+  void print(raw_ostream &os) const;
+  void dump() const;
+
+private:
+  // Disable direct comparison because we allow subclassing with bitfields.
+  // Currently, we use DenseMapInfo to unique storage, which defines key
+  // equalilty only in terms of the base AccessedStorage class bits.
+  bool operator==(const AccessedStorage &) const = delete;
+  bool operator!=(const AccessedStorage &) const = delete;
+
+  template <bool (AccessedStorage::*IsUniqueFn)() const>
   bool isDistinctFrom(const AccessedStorage &other) const {
-    if (isUniquelyIdentified()) {
-      if (other.isUniquelyIdentified() && !hasIdenticalBase(other))
+    if ((this->*IsUniqueFn)()) {
+      if ((other.*IsUniqueFn)() && !hasIdenticalBase(other))
         return true;
 
       if (other.isObjectAccess())
@@ -484,8 +529,8 @@ public:
       // Box/Stack storage.
       return false;
     }
-    if (other.isUniquelyIdentified())
-      return other.isDistinctFrom(*this);
+    if ((other.*IsUniqueFn)())
+      return other.isDistinctFrom<IsUniqueFn>(*this);
 
     // Neither storage is uniquely identified.
     if (isObjectAccess()) {
@@ -508,7 +553,7 @@ public:
       return false;
     }
     if (other.isObjectAccess())
-      return other.isDistinctFrom(*this);
+      return other.isDistinctFrom<IsUniqueFn>(*this);
 
     // Neither storage is from a class or tail.
     //
@@ -516,23 +561,6 @@ public:
     // nested/argument access.
     return false;
   }
-
-  /// Returns the ValueDecl for the underlying storage, if it can be
-  /// determined. Otherwise returns null.
-  ///
-  /// WARNING: This is not a constant-time operation. It is for diagnostics and
-  /// checking via the ValueDecl if we are processing a `let` variable.
-  const ValueDecl *getDecl() const;
-
-  void print(raw_ostream &os) const;
-  void dump() const;
-
-private:
-  // Disable direct comparison because we allow subclassing with bitfields.
-  // Currently, we use DenseMapInfo to unique storage, which defines key
-  // equalilty only in terms of the base AccessedStorage class bits.
-  bool operator==(const AccessedStorage &) const = delete;
-  bool operator!=(const AccessedStorage &) const = delete;
 };
 
 } // end namespace swift
@@ -634,6 +662,314 @@ inline AccessedStorage identifyCapturedStorage(SILValue capturedAddress) {
 
 } // end namespace swift
 
+//===----------------------------------------------------------------------===//
+//                               AccessPath
+//===----------------------------------------------------------------------===//
+
+namespace swift {
+
+/// Identify an addressable location based the AccessedStorage and projection
+/// path.
+///
+/// Each unique path from a base address implies a unique memory location within
+/// that object. A path prefix identifies memory that contains all paths with
+/// the same prefix. The AccessPath returned by AccessPath::compute(address)
+/// identifies the object seen by any memory operation that *directly* operates
+/// on 'address'. The computed path is a prefix of the paths of any contained
+/// subobjects.
+///
+/// Path indices, encoded by AccessPath::Index, may be either subobject
+/// projections or offset indices. We print subobject indices as '#n' and offset
+/// indices as '@n'.
+///
+/// Example Def->Use: (Path indices)
+///   struct_element_addr #1: (#1)
+///   ref_tail_addr -> struct_element_addr #2: (#2)
+///   ref_tail_addr -> index_addr #1 -> struct_element_addr #2: (@1, #2)
+///   pointer_to_address -> struct_element_addr #2: (#2)
+///   pointer_to_address -> index_addr #1 -> struct_element_addr #2: (@1, #2)
+///
+/// The index of ref_element_addr is part of the storage identity and does
+/// not contribute to the access path indices.
+///
+/// A well-formed path has at most one offset component at the begining of the
+/// path (chained index_addrs are merged into one offset). In other words,
+/// taking an offset from a subobject projection is not well-formed access
+/// path. However, it is possible (however undesirable) for programmers to
+/// convert a subobject address into a pointer (for example, via implicit
+/// conversion), then advance that pointer. Since we can't absolutely prevent
+/// this, we instead consider it an invalid AccessPath. This is the only case in
+/// which AccessPath::storage can differ from findAccessedStorage().
+///
+/// Storing an AccessPath ammortizes to constant space. To cache identification
+/// of address locations, AccessPath should be used rather than the
+/// ProjectionPath which requires quadratic space in the number of address
+/// values and quadratic time when comparing addresses.
+///
+/// Type-cast operations such as address_to_pointer may appear on the access
+/// path. It is illegal to use these operations to cast to a non-layout
+/// compatible type. TODO: add enforcement for this rule.
+class AccessPath {
+public:
+  /// Create the AccessPath for any memory operation on the given address.
+  static AccessPath compute(SILValue address);
+
+  // Encode a dynamic index_addr as an UnknownOffset.
+  static constexpr int UnknownOffset = std::numeric_limits<int>::min() >> 1;
+
+  struct PathNode;
+
+  // An access path index.
+  //
+  // Note:
+  // - IndexTrieNode::RootIndex   = INT_MIN      = 0x80000000
+  // - AccessedStorage::TailIndex = INT_MAX      = 0x7FFFFFFF
+  // - AccessPath::UnknownOffset  = (INT_MIN>>1) = 0xC0000000
+  // - An offset index is never zero
+  class Index {
+  public:
+    friend struct PathNode;
+
+    // Use the sign bit to identify offset indices. Subobject projections are
+    // always positive.
+    constexpr static unsigned IndexFlag = unsigned(1) << 31;
+    static int encodeOffset(int indexValue) {
+      assert(indexValue != 0 && "an offset index cannot be zero");
+      // Must be able to sign-extended the 31-bit value.
+      assert(((indexValue << 1) >> 1) == indexValue);
+      return indexValue | IndexFlag;
+    }
+
+    // Encode a positive field index, property index, or TailIndex.
+    static Index forSubObjectProjection(unsigned projIdx) {
+      assert(Index(projIdx).isSubObjectProjection());
+      return Index(projIdx);
+    }
+
+    static Index forOffset(unsigned projIdx) {
+      return Index(encodeOffset(projIdx));
+    }
+
+  private:
+    int indexEncoding;
+    Index(int indexEncoding) : indexEncoding(indexEncoding) {}
+
+  public:
+    bool isSubObjectProjection() const { return indexEncoding >= 0; }
+
+    int getSubObjectIndex() const {
+      assert(isSubObjectProjection());
+      return indexEncoding;
+    }
+
+    // Sign-extend the 31-bit value.
+    int getOffset() const {
+      assert(!isSubObjectProjection());
+      return ((indexEncoding << 1) >> 1);
+    }
+
+    bool isUnknownOffset() const {
+      return indexEncoding == AccessPath::UnknownOffset;
+    }
+
+    int getEncoding() const { return indexEncoding; }
+    
+    void print(raw_ostream &os) const;
+    
+    void dump() const;
+  };
+
+  // A component of the AccessPath.
+  //
+  // Transient wrapper around the underlying IndexTrieNode that encodes either a
+  // subobject projection or an offset index.
+  struct PathNode {
+    IndexTrieNode *node = nullptr;
+
+    constexpr PathNode() = default;
+
+    PathNode(IndexTrieNode *node) : node(node) {}
+
+    bool isValid() const { return node != nullptr; }
+
+    bool isRoot() const { return node->isRoot(); }
+
+    bool isLeaf() const { return node->isLeaf(); }
+
+    Index getIndex() const { return Index(node->getIndex()); }
+
+    PathNode getParent() const { return node->getParent(); }
+
+    // Return the PathNode from \p subNode's path one level deeper than \p
+    // prefixNode.
+    //
+    // Precondition: this != subNode
+    PathNode findPrefix(PathNode subNode) const;
+
+    bool operator==(PathNode other) const { return node == other.node; }
+    bool operator!=(PathNode other) const { return node != other.node; }
+  };
+
+private:
+  AccessedStorage storage;
+  PathNode pathNode;
+  // store the single offset index independent from the PathNode to simplify
+  // checking for path overlap.
+  int offset = 0;
+
+public:
+  // AccessPaths are built by AccessPath::compute(address).
+  //
+  // AccessedStorage is only used to identify the storage location; AccessPath
+  // ignores its subclass bits.
+  AccessPath(AccessedStorage storage, PathNode pathNode, int offset)
+      : storage(storage), pathNode(pathNode), offset(offset) {
+    assert(storage.getKind() != AccessedStorage::Nested);
+    assert(pathNode.isValid() || !storage && "Access path requires a pathNode");
+  }
+
+  AccessPath() = default;
+
+  bool operator==(AccessPath other) const {
+    return
+      storage.hasIdenticalBase(other.storage) && pathNode == other.pathNode;
+  }
+  bool operator!=(AccessPath other) const { return !(*this == other); }
+
+  bool isValid() const { return pathNode.isValid(); }
+
+  AccessedStorage getStorage() const { return storage; }
+
+  PathNode getPathNode() const { return pathNode; }
+
+  int getOffset() const { return offset; }
+
+  /// Return true if this path contains \p subPath.
+  bool contains(AccessPath subPath) const;
+
+  /// Return true if this path may overlap with \p otherPath.
+  bool mayOverlap(AccessPath otherPath) const;
+
+  /// Return the address root that the access path was based on. Returns
+  /// an invalid SILValue for globals or invalid storage.
+  SILValue getRoot() const { return storage.getRoot(); }
+
+  /// Get all uses of all address values that have a common AccessPath. Return
+  /// true if all uses were found before reaching the limit.
+  ///
+  /// This should find all uses for which calling AccessPath::compute() would
+  /// yield an identical AccessPath.
+  ///
+  /// This fails on global variables which have no root. To collect all uses,
+  /// including global variable uses, use AccessPathWithBase::collectUses.
+  bool
+  collectUses(SmallVectorImpl<Operand *> &uses, bool collectOverlappingUses,
+              unsigned useLimit = std::numeric_limits<unsigned>::max()) const;
+
+  void printPath(raw_ostream &os) const;
+  void print(raw_ostream &os) const;
+  void dump() const;
+};
+
+// Encapsulate the result of computing an AccessPath. AccessPath does not store
+// the base address of the formal access because it does not always uniquely
+// indentify the access, but AccessPath users may use the base address to to
+// recover the def-use chain.
+//
+// AccessPathWithBase::collectUses is guaranteed to be complete for all storage
+// types, while AccessPath::collectUses cannot handle globals.
+struct AccessPathWithBase {
+  AccessPath accessPath;
+  // The address-type value that is the base of the formal access. For
+  // class storage, it is the ref_element_addr. For global storage it is the
+  // global_addr or initializer apply. For other storage, it is the same as
+  // accessPath.getRoot().
+  //
+  // base may be invalid for global_addr -> address_to_pointer -> phi patterns.
+  // FIXME: add a structural requirement to SIL so base is always valid in OSSA.
+  SILValue base;
+
+  /// \p address identifies the object seen by any memory operation that
+  /// directly operates on the address. For indexable addresses, this implies an
+  /// operation at index zero.
+  static AccessPathWithBase compute(SILValue address);
+
+  AccessPathWithBase(AccessPath accessPath, SILValue base)
+      : accessPath(accessPath), base(base) {}
+
+  bool operator==(AccessPathWithBase other) const {
+    return accessPath == other.accessPath && base == other.base;
+  }
+  bool operator!=(AccessPathWithBase other) const { return !(*this == other); }
+
+  /// Get all uses of all address values that have a common AccessPath. Return
+  /// true if all uses were found before reaching the limit.
+  ///
+  /// This should find all uses for which calling AccessPath::compute() would
+  /// yield an identical AccessPath and, for global variables, have the same
+  /// access base (e.g. from the same global_addr instruction).
+  bool collectUses(SmallVectorImpl<Operand *> &uses,
+                   bool collectOverlappingUses,
+                   unsigned useLimit = std::numeric_limits<int>::max()) const;
+
+  void print(raw_ostream &os) const;
+  void dump() const;
+};
+
+inline AccessPath AccessPath::compute(SILValue address) {
+  return AccessPathWithBase::compute(address).accessPath;
+}
+
+} // end namespace swift
+
+namespace llvm {
+/// Allow AccessPath to be used in DenseMap.
+template <> struct DenseMapInfo<swift::AccessPath> {
+  static inline swift::AccessPath getEmptyKey() {
+    return swift::AccessPath(
+        DenseMapInfo<swift::AccessedStorage>::getEmptyKey(),
+        swift::AccessPath::PathNode(
+          DenseMapInfo<swift::IndexTrieNode *>::getEmptyKey()), 0);
+  }
+  static inline swift::AccessPath getTombstoneKey() {
+    return swift::AccessPath(
+        DenseMapInfo<swift::AccessedStorage>::getTombstoneKey(),
+        swift::AccessPath::PathNode(
+          DenseMapInfo<swift::IndexTrieNode *>::getTombstoneKey()), 0);
+  }
+  static inline unsigned getHashValue(const swift::AccessPath &val) {
+    return llvm::hash_combine(
+        DenseMapInfo<swift::AccessedStorage>::getHashValue(val.getStorage()),
+        val.getPathNode().node);
+  }
+  static bool isEqual(const swift::AccessPath &lhs,
+                      const swift::AccessPath &rhs) {
+    return lhs == rhs;
+  }
+};
+template <> struct DenseMapInfo<swift::AccessPathWithBase> {
+  static inline swift::AccessPathWithBase getEmptyKey() {
+    return swift::AccessPathWithBase(
+        DenseMapInfo<swift::AccessPath>::getEmptyKey(),
+        DenseMapInfo<swift::SILValue>::getEmptyKey());
+  }
+  static inline swift::AccessPathWithBase getTombstoneKey() {
+    return swift::AccessPathWithBase(
+        DenseMapInfo<swift::AccessPath>::getTombstoneKey(),
+        DenseMapInfo<swift::SILValue>::getTombstoneKey());
+  }
+  static inline unsigned getHashValue(const swift::AccessPathWithBase &val) {
+    return llvm::hash_combine(
+        DenseMapInfo<swift::AccessPath>::getHashValue(val.accessPath),
+        DenseMapInfo<swift::SILValue>::getHashValue(val.base));
+  }
+  static bool isEqual(const swift::AccessPathWithBase &lhs,
+                      const swift::AccessPathWithBase &rhs) {
+    return lhs == rhs;
+  }
+};
+} // end namespace llvm
+
 //===----------------------------------------------------------------------===//
 //             MARK: Helper API for specific formal access patterns
 //===----------------------------------------------------------------------===//
@@ -709,7 +1045,110 @@ SILBasicBlock::iterator removeBeginAccess(BeginAccessInst *beginAccess);
 
 namespace swift {
 
-/// Abstract CRTP class for a visitor passed to \c visitAccessUseDefChain.
+/// If \p svi is an access projection, return an address-type operand for the
+/// incoming address.
+///
+/// An access projection is on the inside of a formal access. It includes
+/// struct_element_addr and tuple_element_addr, but not ref_element_addr.
+///
+/// The returned address may point to any compatible type, which may alias with
+/// the projected address. Arbitrary address casts are not allowed.
+inline Operand *getAccessProjectionOperand(SingleValueInstruction *svi) {
+  switch (svi->getKind()) {
+  default:
+    return nullptr;
+
+  case SILInstructionKind::StructElementAddrInst:
+  case SILInstructionKind::TupleElementAddrInst:
+  case SILInstructionKind::IndexAddrInst:
+  case SILInstructionKind::TailAddrInst:
+  // open_existential_addr and unchecked_take_enum_data_addr are problematic
+  // because they both modify memory and are access projections. Ideally, they
+  // would not be casts, but will likely be eliminated with opaque values.
+  case SILInstructionKind::OpenExistentialAddrInst:
+  case SILInstructionKind::UncheckedTakeEnumDataAddrInst:
+    return &svi->getAllOperands()[0];
+
+  // Special-case this indirect enum pattern:
+  //   unchecked_take_enum_data_addr -> load -> project_box
+  // (the individual load and project_box are not access projections)
+  //
+  // FIXME: Make sure this case goes away with OSSA and opaque values.  If not,
+  // then create a special instruction for this pattern. That way we have an
+  // invariant that all access projections are single-value address-to-address
+  // conversions. Then reuse this helper for both use-def an def-use traversals.
+  //
+  // Check getAccessProjectionOperand() before isAccessedStorageCast() because
+  // it will consider any project_box to be a storage cast.
+  case SILInstructionKind::ProjectBoxInst:
+    if (auto *load = dyn_cast<LoadInst>(svi->getOperand(0)))
+      return &load->getOperandRef();
+
+    return nullptr;
+  };
+}
+
+/// An address, pointer, or box cast that occurs outside of the formal
+/// access. These convert the base of accessed storage without affecting the
+/// AccessPath. Useful for both use-def and def-use traversal. The source
+/// address must be at operand(0).
+///
+/// Some of these casts, such as address_to_pointer, may also occur inside of a
+/// formal access. TODO: Add stricter structural guarantee such that these never
+/// occur within an access. It's important to be able to get the accessed
+/// address without looking though type casts or pointer_to_address [strict],
+/// which we can't do if those operations are behind access projections.
+inline bool isAccessedStorageCast(SingleValueInstruction *svi) {
+  switch (svi->getKind()) {
+  default:
+    return false;
+
+  // Simply pass-thru the incoming address.
+  case SILInstructionKind::MarkUninitializedInst:
+  case SILInstructionKind::UncheckedAddrCastInst:
+  case SILInstructionKind::MarkDependenceInst:
+  // Look through a project_box to identify the underlying alloc_box as the
+  // accesed object. It must be possible to reach either the alloc_box or the
+  // containing enum in this loop, only looking through simple value
+  // propagation such as copy_value and begin_borrow.
+  case SILInstructionKind::ProjectBoxInst:
+  case SILInstructionKind::ProjectBlockStorageInst:
+  case SILInstructionKind::CopyValueInst:
+  case SILInstructionKind::BeginBorrowInst:
+  // Casting to RawPointer does not affect the AccessPath. When converting
+  // between address types, they must be layout compatible (with truncation).
+  case SILInstructionKind::AddressToPointerInst:
+  // Access to a Builtin.RawPointer. It may be important to continue looking
+  // through this because some RawPointers originate from identified
+  // locations. See the special case for global addressors, which return
+  // RawPointer, above.
+  //
+  // If the inductive search does not find a valid addressor, it will
+  // eventually reach the default case that returns in invalid location. This
+  // is correct for RawPointer because, although accessing a RawPointer is
+  // legal SIL, there is no way to guarantee that it doesn't access class or
+  // global storage, so returning a valid unidentified storage object would be
+  // incorrect. It is the caller's responsibility to know that formal access
+  // to such a location can be safely ignored.
+  //
+  // For example:
+  //
+  // - KeyPath Builtins access RawPointer. However, the caller can check
+  // that the access `isFromBuilin` and ignore the storage.
+  //
+  // - lldb generates RawPointer access for debugger variables, but SILGen
+  // marks debug VarDecl access as 'Unsafe' and SIL passes don't need the
+  // AccessedStorage for 'Unsafe' access.
+  case SILInstructionKind::PointerToAddressInst:
+    return true;
+  }
+}
+
+/// Abstract CRTP class for a visiting instructions that are part of the use-def
+/// chain from an accessed address up to the storage base.
+///
+/// Given the address of a memory operation begin visiting at
+/// getAccessedAddress(address).
 template <typename Impl, typename Result = void>
 class AccessUseDefChainVisitor {
 protected:
@@ -751,32 +1190,40 @@ public:
   // Result visitBase(SILValue base, AccessedStorage::Kind kind);
   // Result visitNonAccess(SILValue base);
   // Result visitPhi(SILPhiArgument *phi);
-  // Result visitCast(SingleValueInstruction *cast, Operand *parentAddr);
-  // Result visitPathComponent(SingleValueInstruction *projectedAddr,
-  //                           Operand *parentAddr);
+  // Result visitStorageCast(SingleValueInstruction *cast, Operand *sourceOper);
+  // Result visitAccessProjection(SingleValueInstruction *cast,
+  //                              Operand *sourceOper);
 
   Result visit(SILValue sourceAddr);
 };
 
 template<typename Impl, typename Result>
 Result AccessUseDefChainVisitor<Impl, Result>::visit(SILValue sourceAddr) {
+  if (auto *svi = dyn_cast<SingleValueInstruction>(sourceAddr)) {
+    if (auto *projOper = getAccessProjectionOperand(svi))
+      return asImpl().visitAccessProjection(svi, projOper);
+
+    if (isAccessedStorageCast(svi))
+      return asImpl().visitStorageCast(svi, &svi->getAllOperands()[0]);
+  }
   switch (sourceAddr->getKind()) {
   default:
-    if (isAddressForLocalInitOnly(sourceAddr))
-      return asImpl().visitUnidentified(sourceAddr);
-    return asImpl().visitNonAccess(sourceAddr);
+    break;
 
   // MARK: Handle immediately-identifiable instructions.
 
   // An AllocBox is a fully identified memory location.
   case ValueKind::AllocBoxInst:
     return asImpl().visitBoxAccess(cast<AllocBoxInst>(sourceAddr));
+
   // An AllocStack is a fully identified memory location, which may occur
   // after inlining code already subjected to stack promotion.
   case ValueKind::AllocStackInst:
     return asImpl().visitStackAccess(cast<AllocStackInst>(sourceAddr));
+
   case ValueKind::GlobalAddrInst:
     return asImpl().visitGlobalAccess(sourceAddr);
+
   case ValueKind::ApplyInst: {
     FullApplySite apply(cast<ApplyInst>(sourceAddr));
     if (auto *funcRef = apply.getReferencedFunctionOrNull()) {
@@ -792,25 +1239,37 @@ Result AccessUseDefChainVisitor<Impl, Result>::visit(SILValue sourceAddr) {
   }
   case ValueKind::RefElementAddrInst:
     return asImpl().visitClassAccess(cast<RefElementAddrInst>(sourceAddr));
+
+  // ref_tail_addr project an address from a reference.
+  // This is a valid address producer for nested @inout argument
+  // access, but it is never used for formal access of identified objects.
+  case ValueKind::RefTailAddrInst:
+    return asImpl().visitTailAccess(cast<RefTailAddrInst>(sourceAddr));
+
   // A yield is effectively a nested access, enforced independently in
   // the caller and callee.
   case ValueKind::BeginApplyResult:
     return asImpl().visitYieldAccess(cast<BeginApplyResult>(sourceAddr));
+
   // A function argument is effectively a nested access, enforced
   // independently in the caller and callee.
   case ValueKind::SILFunctionArgument:
-    return asImpl().visitArgumentAccess(cast<SILFunctionArgument>(sourceAddr));
+    return asImpl().visitArgumentAccess(
+      cast<SILFunctionArgument>(sourceAddr));
 
   // View the outer begin_access as a separate location because nested
   // accesses do not conflict with each other.
   case ValueKind::BeginAccessInst:
     return asImpl().visitNestedAccess(cast<BeginAccessInst>(sourceAddr));
 
+  // Static index_addr is handled by getAccessProjectionOperand. Dynamic
+  // index_addr is currently unidentified because we can't form an AccessPath
+  // including them.
   case ValueKind::SILUndef:
     return asImpl().visitUnidentified(sourceAddr);
 
-    // MARK: The sourceAddr producer cannot immediately be classified,
-    // follow the use-def chain.
+  // MARK: The sourceAddr producer cannot immediately be classified,
+  // follow the use-def chain.
 
   case ValueKind::StructExtractInst:
     // Handle nested access to a KeyPath projection. The projection itself
@@ -834,98 +1293,11 @@ Result AccessUseDefChainVisitor<Impl, Result>::visit(SILValue sourceAddr) {
     checkSwitchEnumBlockArg(cast<SILPhiArgument>(sourceAddr));
     return asImpl().visitUnidentified(sourceAddr);
   }
-  // Load a box from an indirect payload of an opaque enum.
-  // We must have peeked past the project_box earlier in this loop.
-  // (the indirectness makes it a box, the load is for address-only).
-  //
-  // %payload_adr = unchecked_take_enum_data_addr %enum : $*Enum, #Enum.case
-  // %box = load [take] %payload_adr : $*{ var Enum }
-  //
-  // FIXME: this case should go away with opaque values.
-  //
-  // Otherwise return invalid AccessedStorage.
-  case ValueKind::LoadInst:
-    if (sourceAddr->getType().is<SILBoxType>()) {
-      Operand *addrOper = &cast<LoadInst>(sourceAddr)->getOperandRef();
-      assert(isa<UncheckedTakeEnumDataAddrInst>(addrOper->get()));
-      return asImpl().visitCast(cast<SingleValueInstruction>(sourceAddr),
-                                addrOper);
-    }
-    return asImpl().visitNonAccess(sourceAddr);
-
-  // ref_tail_addr project an address from a reference.
-  // This is a valid address producer for nested @inout argument
-  // access, but it is never used for formal access of identified objects.
-  case ValueKind::RefTailAddrInst:
-    return asImpl().visitTailAccess(cast<RefTailAddrInst>(sourceAddr));
-
-  // Inductive single-operand cases:
-  // Look through address casts to find the source address.
-  case ValueKind::MarkUninitializedInst:
-  case ValueKind::OpenExistentialAddrInst:
-  case ValueKind::UncheckedAddrCastInst:
-  // Inductive cases that apply to any type.
-  case ValueKind::CopyValueInst:
-  case ValueKind::MarkDependenceInst:
-  // Look through a project_box to identify the underlying alloc_box as the
-  // accesed object. It must be possible to reach either the alloc_box or the
-  // containing enum in this loop, only looking through simple value
-  // propagation such as copy_value.
-  case ValueKind::ProjectBoxInst:
-  // Handle project_block_storage just like project_box.
-  case ValueKind::ProjectBlockStorageInst:
-  // Look through begin_borrow in case a local box is borrowed.
-  case ValueKind::BeginBorrowInst:
-  // Casting to RawPointer does not affect the AccessPath. When converting
-  // between address types, they must be layout compatible (with truncation).
-  case ValueKind::AddressToPointerInst:
-  // A tail_addr is a projection that does not affect the access path because it
-  // must always originate from a ref_tail_addr. Any projection within the
-  // object's tail storage effectively has the same access path.
-  case ValueKind::TailAddrInst:
-    return asImpl().visitCast(
-        cast<SingleValueInstruction>(sourceAddr),
-        &cast<SingleValueInstruction>(sourceAddr)->getAllOperands()[0]);
+  } // end switch
+  if (isAddressForLocalInitOnly(sourceAddr))
+    return asImpl().visitUnidentified(sourceAddr);
 
-  // Access to a Builtin.RawPointer. It may be important to continue looking
-  // through this because some RawPointers originate from identified
-  // locations. See the special case for global addressors, which return
-  // RawPointer, above.
-  //
-  // If the inductive search does not find a valid addressor, it will
-  // eventually reach the default case that returns in invalid location. This
-  // is correct for RawPointer because, although accessing a RawPointer is
-  // legal SIL, there is no way to guarantee that it doesn't access class or
-  // global storage, so returning a valid unidentified storage object would be
-  // incorrect. It is the caller's responsibility to know that formal access
-  // to such a location can be safely ignored.
-  //
-  // For example:
-  //
-  // - KeyPath Builtins access RawPointer. However, the caller can check
-  // that the access `isFromBuilin` and ignore the storage.
-  //
-  // - lldb generates RawPointer access for debugger variables, but SILGen
-  // marks debug VarDecl access as 'Unsafe' and SIL passes don't need the
-  // AccessedStorage for 'Unsafe' access.
-  //
-  // This is always considered a path component because an IndexAddr may
-  // project from it.
-  case ValueKind::PointerToAddressInst:
-    return asImpl().visitPathComponent(
-        cast<SingleValueInstruction>(sourceAddr),
-        &cast<SingleValueInstruction>(sourceAddr)->getAllOperands()[0]);
-
-  // Address-to-address subobject projections. Projection::isAddressProjection
-  // returns true for these.
-  case ValueKind::StructElementAddrInst:
-  case ValueKind::TupleElementAddrInst:
-  case ValueKind::UncheckedTakeEnumDataAddrInst:
-  case ValueKind::IndexAddrInst:
-    return asImpl().visitPathComponent(
-        cast<SingleValueInstruction>(sourceAddr),
-        &cast<SingleValueInstruction>(sourceAddr)->getAllOperands()[0]);
-  }
+  return asImpl().visitNonAccess(sourceAddr);
 }
 
 } // end namespace swift
diff --git a/include/swift/SIL/PatternMatch.h b/include/swift/SIL/PatternMatch.h
index 9a7ab469ac9..07b7449e1d9 100644
--- a/include/swift/SIL/PatternMatch.h
+++ b/include/swift/SIL/PatternMatch.h
@@ -668,6 +668,16 @@ using BuiltinApplyTy = typename Apply_match<BuiltinValueKind, Tys...>::Ty;
 // Define matchers for most of builtin instructions.
 #include "swift/AST/Builtins.def"
 
+#undef BUILTIN_UNARY_OP_MATCH_WITH_ARG_MATCHER
+#undef BUILTIN_BINARY_OP_MATCH_WITH_ARG_MATCHER
+#undef BUILTIN_VARARGS_OP_MATCH_WITH_ARG_MATCHER
+#undef BUILTIN_CAST_OPERATION
+#undef BUILTIN_CAST_OR_BITCAST_OPERATION
+#undef BUILTIN_BINARY_OPERATION_ALL
+#undef BUILTIN_BINARY_PREDICATE
+#undef BUILTIN_MISC_OPERATION
+#undef BUILTIN
+
 //===
 // Convenience compound builtin instructions matchers that succeed
 // if any of the sub-matchers succeed.
diff --git a/include/swift/SILOptimizer/Analysis/ValueTracking.h b/include/swift/SILOptimizer/Analysis/ValueTracking.h
index 32569f2f25b..be2ca665ae6 100644
--- a/include/swift/SILOptimizer/Analysis/ValueTracking.h
+++ b/include/swift/SILOptimizer/Analysis/ValueTracking.h
@@ -39,8 +39,8 @@ bool pointsToLocalObject(SILValue V);
 /// Returns true if \p V is a uniquely identified address or reference. Two
 /// uniquely identified pointers with distinct roots cannot alias. However, a
 /// uniquely identified pointer may alias with unidentified pointers. For
-/// example, the uniquely identified pointer may escape to a call that returns an
-/// alias of that pointer.
+/// example, the uniquely identified pointer may escape to a call that returns
+/// an alias of that pointer.
 ///
 /// It may be any of:
 ///
@@ -53,6 +53,9 @@ bool pointsToLocalObject(SILValue V);
 ///
 /// - an address projection based on an exclusive argument with no levels of
 /// indirection (e.g. ref_element_addr, project_box, etc.).
+///
+/// TODO: Fold this into the AccessedStorage API. pointsToLocalObject should be
+/// performed by AccessedStorage::isUniquelyIdentified.
 inline bool isUniquelyIdentified(SILValue V) {
   SILValue objectRef = V;
   if (V->getType().isAddress()) {
@@ -60,7 +63,7 @@ inline bool isUniquelyIdentified(SILValue V) {
     if (!storage)
       return false;
 
-    if (storage.isUniquelyIdentifiedAfterEnforcement())
+    if (storage.isUniquelyIdentified())
       return true;
 
     if (!storage.isObjectAccess())
diff --git a/lib/IRGen/IRGenSIL.cpp b/lib/IRGen/IRGenSIL.cpp
index 9661d638d83..690708643af 100644
--- a/lib/IRGen/IRGenSIL.cpp
+++ b/lib/IRGen/IRGenSIL.cpp
@@ -4125,7 +4125,7 @@ void IRGenSILFunction::visitRefTailAddrInst(RefTailAddrInst *i) {
 }
 
 static bool isInvariantAddress(SILValue v) {
-  SILValue accessedAddress = getAccessedAddress(v);
+  SILValue accessedAddress = getAccessAddress(v);
   if (auto *ptrRoot = dyn_cast<PointerToAddressInst>(accessedAddress)) {
     return ptrRoot->isInvariant();
   }
diff --git a/lib/SIL/Utils/MemAccessUtils.cpp b/lib/SIL/Utils/MemAccessUtils.cpp
index 6377fd2643c..c34ae23d3fc 100644
--- a/lib/SIL/Utils/MemAccessUtils.cpp
+++ b/lib/SIL/Utils/MemAccessUtils.cpp
@@ -13,11 +13,9 @@
 #define DEBUG_TYPE "sil-access-utils"
 
 #include "swift/SIL/MemAccessUtils.h"
-#include "swift/SIL/ApplySite.h"
-#include "swift/SIL/Projection.h"
-#include "swift/SIL/SILGlobalVariable.h"
 #include "swift/SIL/SILModule.h"
 #include "swift/SIL/SILUndef.h"
+#include "llvm/Support/Debug.h"
 
 using namespace swift;
 
@@ -25,48 +23,417 @@ using namespace swift;
 //                            MARK: General Helpers
 //===----------------------------------------------------------------------===//
 
-// TODO: When the optimizer stops stripping begin_access markers, then we should
-// be able to assert that the result is a BeginAccessInst and the default case
-// is unreachable.
-SILValue swift::getAddressAccess(SILValue v) {
-  while (true) {
-    assert(v->getType().isAddress());
-    auto projection = AccessProjection(v);
-    if (!projection)
-      return v;
+namespace {
+
+enum StorageCastTy { StopAtStorageCast, IgnoreStorageCast };
 
-    v = projection.baseAddress();
+// Handle a single phi-web within an access use-def chain.
+//
+// Recursively calls the useDefVisitor on any operations that aren't recognized
+// as storage casts or projections. If the useDefVisitor finds a consistent
+// result for all operands, then it's result will remain valid. If the
+// useDefVisitor has an invalid result after processing the phi web, then it's
+// original result is restored, then the phi reported to the useDefVisitor as a
+// NonAccess.
+//
+// Phi-web's are only allowed to contain casts and projections that do not
+// affect the access path. If AccessPhiVisitor reaches an unhandled projection,
+// it remembers that as the commonDefinition. If after processing the entire
+// web, the commonDefinition is unique, then it calls the original useDefVisitor
+// to update its result. Note that visitAccessProjection and setDefinition are
+// only used by visitors that process access projections; once the accessed
+// address is reached, they are no longer relevant.
+template <typename UseDefVisitor>
+class AccessPhiVisitor
+    : public AccessUseDefChainVisitor<AccessPhiVisitor<UseDefVisitor>> {
+
+  UseDefVisitor &useDefVisitor;
+  StorageCastTy storageCastTy;
+
+  Optional<SILValue> commonDefinition;
+  SmallVector<SILValue, 8> pointerWorklist;
+  SmallPtrSet<SILPhiArgument *, 4> nestedPhis;
+
+public:
+  AccessPhiVisitor(UseDefVisitor &useDefVisitor, StorageCastTy storageCastTy)
+    : useDefVisitor(useDefVisitor), storageCastTy(storageCastTy) {}
+
+  // Main entry point.
+  void findPhiAccess(SILPhiArgument *phiArg) && {
+    auto savedResult = useDefVisitor.saveResult();
+    visitPhi(phiArg);
+    while (!pointerWorklist.empty()) {
+      this->visit(pointerWorklist.pop_back_val());
+    }
+    // If a common path component was found, recursively look for the result.
+    if (commonDefinition) {
+      if (commonDefinition.getValue()) {
+        useDefVisitor.reenterUseDef(commonDefinition.getValue());
+      } else {
+        // Divergent paths were found; invalidate any previously discovered
+        // storage.
+        useDefVisitor.invalidateResult();
+      }
+    }
+    // If the result is now invalid, reset it and process the current phi as an
+    // unrecgonized access instead.
+    if (!useDefVisitor.isResultValid()) {
+      useDefVisitor.restoreResult(savedResult);
+      visitNonAccess(phiArg);
+    }
+  }
+
+  // Visitor helper.
+  void setDefinition(SILValue def) {
+    if (!commonDefinition) {
+      commonDefinition = def;
+      return;
+    }
+    if (commonDefinition.getValue() != def)
+      commonDefinition = SILValue();
+  }
+
+  void checkVisitorResult(SILValue result) {
+    assert(!result && "must override any visitor that returns a result");
+  }
+
+  // MARK: Visitor implementation.
+
+  // Recursively call the original storageVisitor for each base. We can't simply
+  // look for a common definition on all phi inputs, because the base may be
+  // cloned on each path. For example, two global_addr instructions may refer to
+  // the same global storage. Those global_addr instructions may each be
+  // converted to a RawPointer before being passed into the non-address phi.
+  void visitBase(SILValue base, AccessedStorage::Kind kind) {
+    checkVisitorResult(useDefVisitor.visitBase(base, kind));
+  }
+
+  void visitNonAccess(SILValue value) {
+    checkVisitorResult(useDefVisitor.visitNonAccess(value));
+  }
+
+  void visitNestedAccess(BeginAccessInst *access) {
+    checkVisitorResult(useDefVisitor.visitNestedAccess(access));
+  }
+
+  void visitPhi(SILPhiArgument *phiArg) {
+    if (nestedPhis.insert(phiArg).second)
+      phiArg->getIncomingPhiValues(pointerWorklist);
+  }
+
+  void visitStorageCast(SingleValueInstruction *projectedAddr,
+                        Operand *sourceOper) {
+    // Allow conversions to/from pointers and addresses on disjoint phi paths
+    // only if the underlying useDefVisitor allows it.
+    if (storageCastTy == IgnoreStorageCast)
+      pointerWorklist.push_back(sourceOper->get());
+    else
+      visitNonAccess(projectedAddr);
+  }
+
+  void visitAccessProjection(SingleValueInstruction *projectedAddr,
+                             Operand *sourceOper) {
+    // An offset index on a phi path is always conservatively considered an
+    // unknown offset.
+    if (isa<IndexAddrInst>(projectedAddr) || isa<TailAddrInst>(projectedAddr)) {
+      useDefVisitor.addUnknownOffset();
+      pointerWorklist.push_back(sourceOper->get());
+      return;
+    }
+    // No other access projections are expected to occur on disjoint phi
+    // paths. Stop searching at this projection.
+    setDefinition(projectedAddr);
+  }
+};
+
+enum NestedAccessTy { StopAtAccessBegin, IgnoreAccessBegin };
+
+// Find the origin of an access while skipping projections and casts and
+// handling phis.
+template <typename Impl>
+class FindAccessVisitorImpl : public AccessUseDefChainVisitor<Impl, SILValue> {
+  using SuperTy = AccessUseDefChainVisitor<Impl, SILValue>;
+
+protected:
+  NestedAccessTy nestedAccessTy;
+  StorageCastTy storageCastTy;
+
+  SmallPtrSet<SILPhiArgument *, 4> visitedPhis;
+  bool hasUnknownOffset = false;
+
+public:
+  FindAccessVisitorImpl(NestedAccessTy nestedAccessTy,
+                        StorageCastTy storageCastTy)
+      : nestedAccessTy(nestedAccessTy), storageCastTy(storageCastTy) {}
+
+  // MARK: AccessPhiVisitor::UseDefVisitor implementation.
+  //
+  // Subclasses must implement:
+  //   isResultValid()
+  //   invalidateResult()
+  //   saveResult()
+  //   restoreResult(Result)
+  //   addUnknownOffset()
+
+  void reenterUseDef(SILValue sourceAddr) {
+    SILValue nextAddr = this->visit(sourceAddr);
+    while (nextAddr) {
+      checkNextAddressType(nextAddr, sourceAddr);
+      nextAddr = this->visit(nextAddr);
+    }
+  }
+
+  // MARK: visitor implementation.
+
+  // Override AccessUseDefChainVisitor to ignore access markers and find the
+  // outer access base.
+  SILValue visitNestedAccess(BeginAccessInst *access) {
+    if (nestedAccessTy == IgnoreAccessBegin)
+      return access->getSource();
+
+    return SuperTy::visitNestedAccess(access);
+  }
+
+  SILValue visitPhi(SILPhiArgument *phiArg) {
+    // Cycles involving phis are only handled within AccessPhiVisitor.
+    // Path components are not allowed in phi cycles.
+    if (visitedPhis.insert(phiArg).second) {
+      AccessPhiVisitor<Impl>(this->asImpl(), storageCastTy)
+          .findPhiAccess(phiArg);
+      // Each phi operand was now reentrantly processed. Stop visiting.
+      return SILValue();
+    }
+    // Cannot treat unresolved phis as "unidentified" because they may alias
+    // with global or class access.
+    return this->asImpl().visitNonAccess(phiArg);
+  }
+
+  SILValue visitStorageCast(SingleValueInstruction *projectedAddr,
+                            Operand *sourceAddr) {
+    assert(storageCastTy == IgnoreStorageCast);
+    return sourceAddr->get();
+  }
+
+  SILValue visitAccessProjection(SingleValueInstruction *projectedAddr,
+                                 Operand *sourceAddr) {
+    if (auto *indexAddr = dyn_cast<IndexAddrInst>(projectedAddr)) {
+      if (!Projection(indexAddr).isValid())
+        this->asImpl().addUnknownOffset();
+    } else if (isa<TailAddrInst>(projectedAddr)) {
+      this->asImpl().addUnknownOffset();
+    }
+    return sourceAddr->get();
+  }
+
+protected:
+  // Helper for reenterUseDef
+  void checkNextAddressType(SILValue nextAddr, SILValue sourceAddr) {
+#ifdef NDEBUG
+    return;
+#endif
+    SILType type = nextAddr->getType();
+    // FIXME: This relatively expensive pointer getAnyPointerElementType check
+    // is only needed because keypath generation incorrectly produces
+    // pointer_to_address directly from stdlib Pointer types without a
+    // struct_extract (as is correctly done in emitAddressorAccessor), and
+    // the PointerToAddressInst operand type is never verified.
+    if (type.getASTType()->getAnyPointerElementType())
+      return;
+
+    if (type.isAddress() || isa<SILBoxType>(type.getASTType())
+        || isa<BuiltinRawPointerType>(type.getASTType())) {
+      return;
+    }
+    llvm::errs() << "Visiting ";
+    sourceAddr->dump();
+    llvm::errs() << "  not an address ";
+    nextAddr->dump();
+    nextAddr->getFunction()->dump();
+    assert(false);
+  }
+};
+
+// Implement getAccessAddress, getAccessBegin, and getAccessBase.
+class FindAccessBaseVisitor
+    : public FindAccessVisitorImpl<FindAccessBaseVisitor> {
+  using SuperTy = FindAccessVisitorImpl<FindAccessBaseVisitor>;
+
+protected:
+  Optional<SILValue> base;
+
+public:
+  FindAccessBaseVisitor(NestedAccessTy nestedAccessTy,
+                        StorageCastTy storageCastTy)
+      : FindAccessVisitorImpl(nestedAccessTy, storageCastTy) {}
+
+  // Returns the accessed address or an invalid SILValue.
+  SILValue findBase(SILValue sourceAddr) && {
+    reenterUseDef(sourceAddr);
+    return base.getValueOr(SILValue());
+  }
+
+  void setResult(SILValue foundBase) {
+    if (!base)
+      base = foundBase;
+    else if (base.getValue() != foundBase)
+      base = SILValue();
+  }
+
+  // MARK: AccessPhiVisitor::UseDefVisitor implementation.
+
+  bool isResultValid() const { return base && bool(base.getValue()); }
+
+  void invalidateResult() { base = SILValue(); }
+
+  Optional<SILValue> saveResult() const { return base; }
+
+  void restoreResult(Optional<SILValue> result) { base = result; }
+
+  void addUnknownOffset() { return; }
+
+  // MARK: visitor implementation.
+
+  SILValue visitBase(SILValue base, AccessedStorage::Kind kind) {
+    setResult(base);
+    return SILValue();
   }
-}
 
-SILValue swift::getAccessedAddress(SILValue v) {
-  while (true) {
-    SILValue v2 = stripAccessMarkers(getAddressAccess(v));
-    if (v2 == v)
-      return v;
-    v = v2;
+  SILValue visitNonAccess(SILValue value) {
+    setResult(value);
+    return SILValue();
   }
+
+  // Override visitStorageCast to avoid seeing through arbitrary address casts.
+  SILValue visitStorageCast(SingleValueInstruction *projectedAddr,
+                            Operand *sourceAddr) {
+    if (storageCastTy == StopAtStorageCast)
+      return visitNonAccess(projectedAddr);
+
+    return SuperTy::visitStorageCast(projectedAddr, sourceAddr);
+  }
+};
+
+} // end anonymous namespace
+
+SILValue swift::getAccessAddress(SILValue address) {
+  assert(address->getType().isAddress());
+  SILValue accessAddress =
+      FindAccessBaseVisitor(StopAtAccessBegin, StopAtStorageCast)
+          .findBase(address);
+  assert(accessAddress->getType().isAddress());
+  return accessAddress;
+}
+
+// TODO: When the optimizer stops stripping begin_access markers and SILGen
+// protects all memory operations with at least an "unsafe" access scope, then
+// we should be able to assert that this returns a BeginAccessInst.
+SILValue swift::getAccessBegin(SILValue address) {
+  assert(address->getType().isAddress());
+  return FindAccessBaseVisitor(StopAtAccessBegin, IgnoreStorageCast)
+      .findBase(address);
 }
 
-bool swift::isLetAddress(SILValue accessedAddress) {
-  assert(accessedAddress == getAccessedAddress(accessedAddress)
-         && "caller must find the address root");
+// This is allowed to be called on a non-address pointer type.
+SILValue swift::getAccessBase(SILValue address) {
+  return FindAccessBaseVisitor(IgnoreAccessBegin, IgnoreStorageCast)
+      .findBase(address);
+}
+
+bool swift::isLetAddress(SILValue address) {
+  SILValue base = getAccessBase(address);
+  if (!base)
+    return false;
+
   // Is this an address of a "let" class member?
-  if (auto *rea = dyn_cast<RefElementAddrInst>(accessedAddress))
+  if (auto *rea = dyn_cast<RefElementAddrInst>(base))
     return rea->getField()->isLet();
 
   // Is this an address of a global "let"?
-  if (auto *gai = dyn_cast<GlobalAddrInst>(accessedAddress)) {
+  if (auto *gai = dyn_cast<GlobalAddrInst>(base)) {
     auto *globalDecl = gai->getReferencedGlobal()->getDecl();
     return globalDecl && globalDecl->isLet();
   }
   return false;
 }
 
+//===----------------------------------------------------------------------===//
+//                            MARK: FindReferenceRoot
+//===----------------------------------------------------------------------===//
+
+namespace {
+
+// Essentially RC identity where the starting point is already a reference.
+//
+// FIXME: We cannot currently see through type casts for true RC identity,
+// because property indices are not unique within a class hierarchy. Either fix
+// RefElementAddr::getFieldNo so it returns a unique index, or figure out a
+// different way to encode the property's VarDecl. (Note that the lack of a
+// unique property index could be the source of bugs elsewhere).
+class FindReferenceRoot {
+  SmallPtrSet<SILPhiArgument *, 4> visitedPhis;
+
+public:
+  SILValue findRoot(SILValue ref) && {
+    SILValue root = recursiveFindRoot(ref);
+    assert(root && "all phi inputs must be reachable");
+    return root;
+  }
+
+protected:
+  // Return an invalid value for a phi with no resolved inputs.
+  //
+  // FIXME: We should be able to see through RC identity like this:
+  //   nextRoot = stripRCIdentityCasts(nextRoot);
+  SILValue recursiveFindRoot(SILValue ref) {
+    while (true) {
+      SILValue nextRoot = ref;
+      nextRoot = stripOwnershipInsts(nextRoot);
+      if (nextRoot == ref)
+        break;
+      ref = nextRoot;
+    }
+
+    auto *phi = dyn_cast<SILPhiArgument>(ref);
+    if (!phi || !phi->isPhiArgument()) {
+      return ref;
+    }
+    // Handle phis...
+    if (!visitedPhis.insert(phi).second) {
+      return SILValue();
+    }
+    SILValue commonInput;
+    phi->visitIncomingPhiOperands([&](Operand *operand) {
+      SILValue input = recursiveFindRoot(operand->get());
+      // Ignore "back/cross edges" to previously visited phis.
+      if (!input)
+        return true;
+
+      if (!commonInput) {
+        commonInput = input;
+        return true;
+      }
+      if (commonInput == input)
+        return true;
+
+      commonInput = phi;
+      return false;
+    });
+    return commonInput;
+  }
+};
+
+} // end anonymous namespace
+
+static SILValue findReferenceRoot(SILValue ref) {
+  return FindReferenceRoot().findRoot(ref);
+}
+
 //===----------------------------------------------------------------------===//
 //                            MARK: AccessedStorage
 //===----------------------------------------------------------------------===//
 
+constexpr unsigned AccessedStorage::TailIndex;
+
 AccessedStorage::AccessedStorage(SILValue base, Kind kind) {
   assert(base && "invalid storage base");
   initKind(kind);
@@ -113,17 +480,17 @@ AccessedStorage::AccessedStorage(SILValue base, Kind kind) {
   case Class: {
     // Do a best-effort to find the identity of the object being projected
     // from. It is OK to be unsound here (i.e. miss when two ref_element_addrs
-    // actually refer the same address) because these addresses will be
-    // dynamically checked, and static analysis will be sufficiently
-    // conservative given that classes are not "uniquely identified".
+    // actually refer the same address) because, when the effort fails, static
+    // analysis will be sufficiently conservative given that classes are not
+    // "uniquely identified", and these addresses will be dynamically checked.
     auto *REA = cast<RefElementAddrInst>(base);
-    value = stripBorrow(REA->getOperand());
+    value = findReferenceRoot(REA->getOperand());
     setElementIndex(REA->getFieldIndex());
     break;
   }
   case Tail: {
     auto *RTA = cast<RefTailAddrInst>(base);
-    value = stripBorrow(RTA->getOperand());
+    value = findReferenceRoot(RTA->getOperand());
     break;
   }
   }
@@ -228,200 +595,748 @@ void AccessedStorage::print(raw_ostream &os) const {
     break;
   case Tail:
     os << getObject();
-    os << "  Tail\n";
   }
 }
 
-void AccessedStorage::dump() const { print(llvm::dbgs()); }
+LLVM_ATTRIBUTE_USED void AccessedStorage::dump() const { print(llvm::dbgs()); }
 
 namespace {
-// Find common AccessedStorage that leads to all arguments of a given
-// pointer phi use. Return an invalid SILValue on failure.
-//
-// Also guarantees that all phi inputs follow the same access path. If any phi
-// inputs have different access path components, then the phi is considered an
-// invalid access. This is ok because path components always have an address
-// type, and we are phasing out all address-type phis. Pointer-phis will
-// continue to be allowed but they cannot affect the access path.
-template <typename StorageVisitor>
-class FindPhiStorageVisitor
-    : public AccessUseDefChainVisitor<FindPhiStorageVisitor<StorageVisitor>> {
-  StorageVisitor &storageVisitor;
-  Optional<SILValue> commonDefinition;
-  SmallVector<SILValue, 8> pointerWorklist;
-  SmallPtrSet<SILPhiArgument *, 4> nestedPhis;
+
+// Implementation of AccessUseDefChainVisitor that looks for a single common
+// AccessedStorage object for all projection paths.
+class FindAccessedStorageVisitor
+    : public FindAccessVisitorImpl<FindAccessedStorageVisitor> {
+
+public:
+  struct Result {
+    Optional<AccessedStorage> storage;
+    SILValue base;
+  };
+
+private:
+  Result result;
+
+  void setResult(AccessedStorage foundStorage, SILValue foundBase) {
+    if (!result.storage) {
+      result.storage = foundStorage;
+      assert(!result.base);
+      result.base = foundBase;
+    } else {
+      // `storage` may still be invalid. If both `storage` and `foundStorage`
+      // are invalid, this check passes, but we return an invalid storage
+      // below.
+      if (!result.storage->hasIdenticalBase(foundStorage))
+        result.storage = AccessedStorage();
+      if (result.base != foundBase)
+        result.base = SILValue();
+    }
+  }
+
+public:
+  FindAccessedStorageVisitor(NestedAccessTy nestedAccessTy)
+      : FindAccessVisitorImpl(nestedAccessTy, IgnoreStorageCast) {}
+
+  // Main entry point
+  void findStorage(SILValue sourceAddr) { this->reenterUseDef(sourceAddr); }
+
+  AccessedStorage getStorage() const {
+    return result.storage.getValueOr(AccessedStorage());
+  }
+  // getBase may return an invalid value for valid Global storage because there
+  // may be multiple global_addr bases for identical storage.
+  SILValue getBase() const { return result.base; }
+
+  // MARK: AccessPhiVisitor::UseDefVisitor implementation.
+
+  // A valid result requires valid storage, but not a valid base.
+  bool isResultValid() const {
+    return result.storage && bool(result.storage.getValue());
+  }
+
+  void invalidateResult() { setResult(AccessedStorage(), SILValue()); }
+
+  Result saveResult() const { return result; }
+
+  void restoreResult(Result savedResult) { result = savedResult; }
+
+  void addUnknownOffset() { return; }
+
+  // MARK: visitor implementation.
+
+  SILValue visitBase(SILValue base, AccessedStorage::Kind kind) {
+    setResult(AccessedStorage(base, kind), base);
+    return SILValue();
+  }
+
+  SILValue visitNonAccess(SILValue value) {
+    invalidateResult();
+    return SILValue();
+  }
+};
+
+} // end anonymous namespace
+
+AccessedStorage swift::findAccessedStorage(SILValue sourceAddr) {
+  FindAccessedStorageVisitor visitor(IgnoreAccessBegin);
+  visitor.findStorage(sourceAddr);
+  return visitor.getStorage();
+}
+
+AccessedStorage swift::identifyAccessedStorageImpl(SILValue sourceAddr) {
+  FindAccessedStorageVisitor visitor(StopAtAccessBegin);
+  visitor.findStorage(sourceAddr);
+  return visitor.getStorage();
+}
+
+//===----------------------------------------------------------------------===//
+//                               AccessPath
+//===----------------------------------------------------------------------===//
+
+bool AccessPath::contains(AccessPath subPath) const {
+  assert(isValid() && subPath.isValid());
+
+  if (!storage.hasIdenticalBase(subPath.storage))
+    return false;
+
+  // Does the offset index match?
+  if (offset != subPath.offset || offset == UnknownOffset)
+    return false;
+
+  return pathNode.node->isPrefixOf(subPath.pathNode.node);
+}
+
+bool AccessPath::mayOverlap(AccessPath otherPath) const {
+  assert(isValid() && otherPath.isValid());
+
+  if (storage.isDistinctFrom(otherPath.storage))
+    return false;
+
+  // If subpaths are disjoint, they do not overlap regardless of offset.
+  if (!pathNode.node->isPrefixOf(otherPath.pathNode.node)
+      && !otherPath.pathNode.node->isPrefixOf(pathNode.node)) {
+    return true;
+  }
+  return offset == otherPath.offset || offset == UnknownOffset
+         || otherPath.offset == UnknownOffset;
+}
+
+namespace {
+
+// Implementation of AccessUseDefChainVisitor that builds an AccessPath.
+class AccessPathVisitor : public FindAccessVisitorImpl<AccessPathVisitor> {
+  using SuperTy = FindAccessVisitorImpl<AccessPathVisitor>;
+
+  SILModule *module;
+
+  // This nested visitor holds the AccessedStorage and base results.
+  FindAccessedStorageVisitor storageVisitor;
+
+  // Save just enough information for to checkpoint before processing phis. Phis
+  // can add path components and add an unknown offset.
+  struct Result {
+    FindAccessedStorageVisitor::Result storageResult;
+    int savedOffset;
+    unsigned pathLength;
+
+    Result(FindAccessedStorageVisitor::Result storageResult, int offset,
+           unsigned pathLength)
+        : storageResult(storageResult), savedOffset(offset),
+          pathLength(pathLength) {}
+  };
+
+  // Only access projections affect this path. Since they are are not allowed
+  // beyond phis, this path is not part of AccessPathVisitor::Result.
+  llvm::SmallVector<AccessPath::Index, 8> reversePath;
+  // Holds a non-zero value if an index_addr has been processed without yet
+  // creating a path index for it.
+  int pendingOffset = 0;
 
 public:
-  FindPhiStorageVisitor(StorageVisitor &storageVisitor)
-      : storageVisitor(storageVisitor) {}
+  AccessPathVisitor(SILModule *module)
+      : FindAccessVisitorImpl(IgnoreAccessBegin, IgnoreStorageCast),
+        module(module), storageVisitor(IgnoreAccessBegin) {}
 
   // Main entry point.
-  void findPhiStorage(SILPhiArgument *phiArg) {
-    // Visiting a phi will call storageVisitor to set the storage result
-    // whenever it finds a base.
-    visitPhi(phiArg);
-    while (!pointerWorklist.empty()) {
-      this->visit(pointerWorklist.pop_back_val());
-    }
-    // If a common path component was found, recursively look for the storage.
-    if (commonDefinition) {
-      if (commonDefinition.getValue()) {
-        auto storage = storageVisitor.findStorage(commonDefinition.getValue());
-        (void)storage; // The same storageVisitor called us. It has already
-                       // recorded the storage that it found.
-      } else {
-        // If divergent paths were found, invalidate any previously discovered
-        // storage.
-        storageVisitor.setStorage(AccessedStorage());
-      }
+  AccessPathWithBase findAccessPath(SILValue sourceAddr) && {
+    this->reenterUseDef(sourceAddr);
+    if (auto storage = storageVisitor.getStorage()) {
+      return AccessPathWithBase(
+          AccessPath(storage, computeForwardPath(), pendingOffset),
+          storageVisitor.getBase());
     }
+    return AccessPathWithBase(AccessPath(), SILValue());
   }
 
-  // Visitor helper.
-  void setDefinition(SILValue def) {
-    if (!commonDefinition) {
-      commonDefinition = def;
+protected:
+  void addPathOffset(int offset) {
+    if (pendingOffset == AccessPath::UnknownOffset)
+      return;
+
+    if (offset == AccessPath::UnknownOffset) {
+      pendingOffset = offset;
       return;
     }
-    if (commonDefinition.getValue() != def)
-      commonDefinition = SILValue();
+    // Accumulate static offsets
+    pendingOffset = pendingOffset + offset;
   }
 
-  // MARK: Visitor implementation.
-
-  void checkResult(SILValue result) {
-    assert(!result && "must override any visitor that returns a result");
+  // Return the trie node corresponding to the current state of reversePath.
+  AccessPath::PathNode computeForwardPath() {
+    IndexTrieNode *forwardPath = module->getIndexTrieRoot();
+    for (AccessPath::Index nextIndex : llvm::reverse(reversePath)) {
+      forwardPath = forwardPath->getChild(nextIndex.getEncoding());
+    }
+    return AccessPath::PathNode(forwardPath);
   }
 
-  // Recursively call the original storageVisitor for each base. We can't simply
-  // look for a common definition on all phi inputs, because the base may be
-  // cloned on each path. For example, two global_addr instructions may refer to
-  // the same global storage. Those global_addr instructions may each be
-  // converted to a RawPointer before being passed into the non-address phi.
-  void visitBase(SILValue base, AccessedStorage::Kind kind) {
-    checkResult(storageVisitor.visitBase(base, kind));
+public:
+  // MARK: AccessPhiVisitor::UseDefVisitor implementation.
+
+  bool isResultValid() const { return storageVisitor.isResultValid(); }
+
+  void invalidateResult() {
+    storageVisitor.invalidateResult();
+    // Don't clear reversePath. We my call restoreResult later.
+    pendingOffset = 0;
   }
 
-  void visitNonAccess(SILValue value) {
-    checkResult(storageVisitor.visitNonAccess(value));
+  Result saveResult() const {
+    return Result(storageVisitor.saveResult(), pendingOffset,
+                  reversePath.size());
   }
 
-  void visitNestedAccess(BeginAccessInst *access) {
-    checkResult(storageVisitor.visitNestedAccess(access));
+  void restoreResult(Result result) {
+    storageVisitor.restoreResult(result.storageResult);
+    pendingOffset = result.savedOffset;
+    assert(result.pathLength <= reversePath.size()
+           && "a phi should only add to the path");
+    reversePath.erase(reversePath.begin() + result.pathLength,
+                      reversePath.end());
   }
 
-  void visitPhi(SILPhiArgument *phiArg) {
-    if (nestedPhis.insert(phiArg).second)
-      phiArg->getIncomingPhiValues(pointerWorklist);
+  void addUnknownOffset() { pendingOffset = AccessPath::UnknownOffset; }
+
+  // MARK: visitor implementation. Return the address source as the next use-def
+  // value to process. An invalid SILValue stops def-use traversal.
+
+  SILValue visitBase(SILValue base, AccessedStorage::Kind kind) {
+    return storageVisitor.visitBase(base, kind);
   }
 
-  void visitCast(SingleValueInstruction *projectedAddr, Operand *parentAddr) {
-    // Allow conversions to/from pointers and addresses on disjoint phi paths.
-    this->pointerWorklist.push_back(parentAddr->get());
+  SILValue visitNonAccess(SILValue value) {
+    invalidateResult();
+    return SILValue();
   }
 
-  void visitPathComponent(SingleValueInstruction *projectedAddr,
-                          Operand *parentAddr) {
-    // Path components are not expected to occur on disjoint phi paths. Stop
-    // searching at this projection.
-    setDefinition(projectedAddr);
+  // Override FindAccessVisitorImpl to record path components.
+  SILValue visitAccessProjection(SingleValueInstruction *projectedAddr,
+                                 Operand *sourceAddr) {
+    auto projIdx = ProjectionIndex(projectedAddr);
+    if (auto *indexAddr = dyn_cast<IndexAddrInst>(projectedAddr)) {
+      addPathOffset(projIdx.isValid() ? projIdx.Index
+                                      : AccessPath::UnknownOffset);
+    } else if (isa<TailAddrInst>(projectedAddr)) {
+      addPathOffset(AccessPath::UnknownOffset);
+    } else if (projIdx.isValid()) {
+      if (pendingOffset) {
+        LLVM_DEBUG(llvm::dbgs() << "Subobject projection with offset index: "
+                                << *projectedAddr);
+        // Return an invalid result even though findAccessedStorage() may be
+        // able to find valid storage, because an offset from a subobject is an
+        // invalid access path.
+        return visitNonAccess(projectedAddr);
+      }
+      reversePath.push_back(
+          AccessPath::Index::forSubObjectProjection(projIdx.Index));
+    } else {
+      // Ignore everything in getAccessProjectionOperand that is an access
+      // projection with no affect on the access path.
+      assert(isa<OpenExistentialAddrInst>(projectedAddr)
+             || isa<UncheckedTakeEnumDataAddrInst>(projectedAddr)
+             || isa<ProjectBoxInst>(projectedAddr));
+    }
+    return sourceAddr->get();
   }
 };
-} // namespace
+
+} // end anonymous namespace
+
+AccessPathWithBase AccessPathWithBase::compute(SILValue address) {
+  return AccessPathVisitor(address->getModule()).findAccessPath(address);
+}
 
 namespace {
-// Implementation of AccessUseDefChainVisitor that looks for a single common
-// AccessedStorage object for all projection paths.
-template <typename Impl>
-class FindAccessedStorageVisitorBase
-    : public AccessUseDefChainVisitor<Impl, SILValue> {
-protected:
-  Optional<AccessedStorage> storage;
-  SmallPtrSet<SILPhiArgument *, 4> visitedPhis;
+
+// Perform def-use DFS traversal along a given AccessPath. DFS terminates at
+// each discovered use.
+//
+// If \p collectOverlappingUses is false, then the collected uses all have the
+// same AccessPath. Uses that exactly match the AccessPath may either be exact
+// uses, or may be subobject projections within that access path, including
+// struct_element_addr and tuple_element_addr. The transitive uses of those
+// subobject projections are not included.
+//
+// If \p collectOverlappingUses is true, then the collected uses also include
+// uses that access an object that contains the given AccessPath. As before,
+// overlapping uses do not include transitive uses of subobject projections
+// contained by the current path; the def-use traversal stops at those
+// projections regardless of collectOverlappingUses. However, overlapping uses
+// may be at an unknown offset relative to the current path, so they don't
+// necessarily contain the current path.
+//
+// Example: path = "(#2)"
+//   %base = ...                            // access base
+//   load %base                             // containing use
+//   %elt1 = struct_element_addr %base, #1  // non-use (ignored)
+//   load %elt1                             // non-use (unseen)
+//   %elt2 = struct_element_addr %base, #2  // chained use (ignored)
+//   load %elt2                             // exact use
+//   %sub = struct_element_addr %elt2,  #i  // projection use
+//   load %sub                              // interior use (ignored)
+//
+// A use may be a BranchInst if the corresponding phi does not have common
+// AccessedStorage.
+//
+// For class storage, the def-use traversal starts at the reference
+// root. Eventually, traversal reach the base address of the formal access:
+//
+//   %ref = ...                        // reference root
+//   %base = ref_element_addr %refRoot // formal access address
+//   load %base                        // use
+class CollectAccessPathUses {
+  // Origin of the def-use traversal
+  AccessedStorage storage;
+
+  // Result: Exact uses, projection uses, and containing uses.
+  SmallVectorImpl<Operand *> &uses;
+
+  bool collectOverlappingUses;
+  unsigned useLimit;
+
+  // Access path indices from storage to exact uses
+  SmallVector<AccessPath::Index, 4> pathIndices; // in use-def order
+
+  // A point in the def-use traversal. isRef() is true only for object access
+  // prior to reaching the base address.
+  struct DFSEntry {
+    // Next potential use to visit and flag indicating whether traversal has
+    // reachaed the access base yet.
+    llvm::PointerIntPair<Operand *, 1, bool> useAndIsRef;
+    unsigned pathCursor; // position within pathIndices
+    int offset;          // index_addr offsets seen prior to this use
+
+    DFSEntry(Operand *use, bool isRef, unsigned pathCursor, int offset)
+        : useAndIsRef(use, isRef), pathCursor(pathCursor), offset(offset) {}
+
+    Operand *getUse() const { return useAndIsRef.getPointer(); }
+    // Is this pointer a reference?
+    bool isRef() const { return useAndIsRef.getInt(); }
+  };
+  SmallVector<DFSEntry, 16> dfsStack;
+
+  SmallPtrSet<const SILPhiArgument *, 4> visitedPhis;
 
 public:
-  // Main entry point. May be called reentrantly by the phi visitor.
-  AccessedStorage findStorage(SILValue sourceAddr) {
-    SILValue nextAddr = this->visit(sourceAddr);
-    while (nextAddr) {
-      assert(nextAddr->getType().isAddress()
-             || isa<SILBoxType>(nextAddr->getType().getASTType())
-             || isa<BuiltinRawPointerType>(nextAddr->getType().getASTType()));
-      nextAddr = this->visit(nextAddr);
+  CollectAccessPathUses(AccessPath accessPath, SmallVectorImpl<Operand *> &uses,
+                        bool collectOverlappingUses, unsigned useLimit)
+      : storage(accessPath.getStorage()), uses(uses),
+        collectOverlappingUses(collectOverlappingUses), useLimit(useLimit) {
+    assert(accessPath.isValid());
+    for (AccessPath::PathNode currentNode = accessPath.getPathNode();
+         !currentNode.isRoot(); currentNode = currentNode.getParent()) {
+      assert(currentNode.getIndex().isSubObjectProjection() &&
+             "a valid AccessPath does not contain any intermediate offsets");
+      pathIndices.push_back(currentNode.getIndex());
+    }
+    if (int offset = accessPath.getOffset())
+      pathIndices.push_back(AccessPath::Index::forOffset(offset));
+
+    // The search will start from the object root, not the formal access base,
+    // so add the class index to the front.
+    auto storage = accessPath.getStorage();
+    if (storage.getKind() == AccessedStorage::Class) {
+      pathIndices.push_back(AccessPath::Index::forSubObjectProjection(
+          storage.getPropertyIndex()));
+    }
+    if (storage.getKind() == AccessedStorage::Tail) {
+      pathIndices.push_back(AccessPath::Index::forSubObjectProjection(
+          ProjectionIndex::TailIndex));
     }
-    return storage.getValueOr(AccessedStorage());
   }
 
-  void setStorage(AccessedStorage foundStorage) {
-    if (!storage) {
-      storage = foundStorage;
-    } else {
-      // `storage` may still be invalid. If both `storage` and `foundStorage`
-      // are invalid, this check passes, but we return an invalid storage
-      // below.
-      if (!storage->hasIdenticalBase(foundStorage))
-        storage = AccessedStorage();
+  // Return true if all uses were collected. This is always true as long as the
+  // access has a single root, or globalBase is provided, and there is no
+  // useLimit.
+  //
+  // For Global storage \p globalBase must be provided as the head of the
+  // def-use search.
+  bool collectUses(SILValue globalBase = SILValue()) && {
+    SILValue root = storage.getRoot();
+    if (!root) {
+      assert(storage.getKind() == AccessedStorage::Global);
+      if (!globalBase)
+        return false;
+
+      root = globalBase;
     }
+    // If the expected path has an unknown offset, then none of the uses are
+    // exact.
+    if (!collectOverlappingUses && !pathIndices.empty()
+        && pathIndices.back().isUnknownOffset()) {
+      return true;
+    }
+    pushUsers(root,
+              DFSEntry(nullptr, storage.isReference(), pathIndices.size(), 0));
+    while (!dfsStack.empty()) {
+      if (!visitUser(dfsStack.pop_back_val()))
+        return false;
+    }
+    return true;
   }
 
-  // MARK: visitor implementation.
+protected:
+  void pushUsers(SILValue def, const DFSEntry &dfs) {
+    for (auto *use : def->getUses())
+      pushUser(DFSEntry(use, dfs.isRef(), dfs.pathCursor, dfs.offset));
+  }
 
-  SILValue visitBase(SILValue base, AccessedStorage::Kind kind) {
-    setStorage(AccessedStorage(base, kind));
-    return SILValue();
+  void pushUser(DFSEntry dfs) {
+    Operand *use = dfs.getUse();
+    if (auto *bi = dyn_cast<BranchInst>(use->getUser())) {
+      if (pushPhiUses(bi->getArgForOperand(use), dfs))
+        return;
+    }
+    // If we didn't find and process a phi, continue DFS.
+    dfsStack.emplace_back(dfs);
   }
 
-  SILValue visitNonAccess(SILValue value) {
-    setStorage(AccessedStorage());
-    return SILValue();
+  // Return true if this phi has been processed and does not need to be
+  // considered as a separate use.
+  bool pushPhiUses(const SILPhiArgument *phi, DFSEntry dfs) {
+    if (!visitedPhis.insert(phi).second)
+      return true;
+
+    // If this phi has a common base, continue to follow the access path. This
+    // check is different for reference types vs pointer types.
+    if (dfs.isRef()) {
+      assert(!dfs.offset && "index_addr not allowed on reference roots");
+      // When isRef is true, the address access hasn't been seen yet and
+      // we're still following the reference root's users. Check if all phi
+      // inputs have the same reference root before looking through it.
+      if (findReferenceRoot(phi) == storage.getObject()) {
+        pushUsers(phi, dfs);
+        return true;
+      }
+      // The branch will be pushed onto the normal user list.
+      return false;
+    }
+    // Check if all phi inputs have the same accessed storage before
+    // looking through it. If the phi input differ the its storage is invalid.
+    auto phiPath = AccessPath::compute(phi);
+    if (phiPath.isValid()) {
+      assert(phiPath.getStorage().hasIdenticalBase(storage)
+             && "inconsistent phi storage");
+      // If the phi paths have different offsets, its path has unknown offset.
+      if (phiPath.getOffset() == AccessPath::UnknownOffset) {
+        if (!collectOverlappingUses)
+          return true;
+        dfs.offset = AccessPath::UnknownOffset;
+      }
+      pushUsers(phi, dfs);
+      return true;
+    }
+    // The branch will be pushed onto the normal user list.
+    return false;
   }
 
-  SILValue visitPhi(SILPhiArgument *phiArg) {
-    // Cycles involving phis are only handled within FindPhiStorageVisitor.
-    // Path components are not allowed in phi cycles.
-    if (visitedPhis.insert(phiArg).second) {
-      FindPhiStorageVisitor<Impl>(this->asImpl()).findPhiStorage(phiArg);
-      return SILValue();
+  // Return the offset at the current DFS path cursor, or zero.
+  int getPathOffset(const DFSEntry &dfs) const {
+    if (dfs.pathCursor == 0
+        || pathIndices[dfs.pathCursor - 1].isSubObjectProjection()) {
+      return 0;
     }
-    // Cannot treat unresolved phis as "unidentified" because they may alias
-    // with global or class access.
-    return visitNonAccess(phiArg);
+    return pathIndices[dfs.pathCursor - 1].getOffset();
   }
 
-  SILValue visitCast(SingleValueInstruction *projectedAddr,
-                     Operand *parentAddr) {
-    return parentAddr->get();
+  // Returns true as long as the useLimit is not reached.
+  bool visitUser(DFSEntry dfs) {
+    Operand *use = dfs.getUse();
+    assert(!(dfs.isRef() && use->get()->getType().isAddress()));
+    if (auto *svi = dyn_cast<SingleValueInstruction>(use->getUser())) {
+      if (use->getOperandNumber() == 0
+          && visitSingleValueUser(svi, dfs) == IgnoredUse) {
+        return true;
+      }
+    }
+    // We weren't able to "see through" any more address conversions; so
+    // record this as a use.
+
+    // Do the path offsets match?
+    if (!checkAndUpdateOffset(dfs))
+      return true;
+
+    // Is this a full or partial path match?
+    if (!collectOverlappingUses && dfs.pathCursor > 0)
+      return true;
+
+    // Record the use if we haven't reached the limit.
+    if (uses.size() == useLimit)
+      return false;
+
+    uses.push_back(use);
+    return true;
   }
 
-  SILValue visitPathComponent(SingleValueInstruction *projectedAddr,
-                              Operand *parentAddr) {
-    return parentAddr->get();
+  // Return true if the accumulated offset matches the current path index.
+  // Update the DFSEntry and pathCursor to skip remaining offsets.
+  bool checkAndUpdateOffset(DFSEntry &dfs) {
+    int pathOffset = getPathOffset(dfs);
+    if (pathOffset == 0) {
+      // No offset is on the expected path.
+      if (collectOverlappingUses && dfs.offset == AccessPath::UnknownOffset) {
+        dfs.offset = 0;
+      }
+      return dfs.offset == 0;
+    }
+    // pop the offset from the expected path; there should only be one.
+    --dfs.pathCursor;
+    assert(getPathOffset(dfs) == 0 && "only one offset index allowed");
+
+    int useOffset = dfs.offset;
+    dfs.offset = 0;
+
+    // Ignore all uses on this path unless we're collecting containing uses.
+    // UnknownOffset appears to overlap with all offsets and subobject uses.
+    if (pathOffset == AccessPath::UnknownOffset
+        || useOffset == AccessPath::UnknownOffset) {
+      return collectOverlappingUses;
+    }
+    // A known offset must match regardless of collectOverlappingUses.
+    return pathOffset == useOffset;
   }
-};
 
-struct FindAccessedStorageVisitor
-    : public FindAccessedStorageVisitorBase<FindAccessedStorageVisitor> {
+  enum UseKind { LeafUse, IgnoredUse };
+  UseKind visitSingleValueUser(SingleValueInstruction *svi, DFSEntry dfs);
 
-  SILValue visitNestedAccess(BeginAccessInst *access) {
-    return access->getSource();
+  // Handle non-index_addr projections.
+  UseKind followProjection(SingleValueInstruction *svi, DFSEntry dfs) {
+    if (!checkAndUpdateOffset(dfs))
+      return IgnoredUse;
+
+    if (dfs.pathCursor == 0)
+      return LeafUse;
+
+    AccessPath::Index pathIndex = pathIndices[dfs.pathCursor - 1];
+    auto projIdx = ProjectionIndex(svi);
+    assert(projIdx.isValid());
+    // Only subobjects indices are expected because offsets are handled above.
+    if (projIdx.Index == pathIndex.getSubObjectIndex()) {
+      --dfs.pathCursor;
+      pushUsers(svi, dfs);
+    }
+    return IgnoredUse;
   }
 };
 
-struct IdentifyAccessedStorageVisitor
-    : public FindAccessedStorageVisitorBase<IdentifyAccessedStorageVisitor> {};
+} // end anonymous namespace
 
-} // namespace
+// During the def-use traversal, visit a single-value instruction in which the
+// used address is at operand zero.
+//
+// This must handle the def-use side of all operations that
+// AccessUseDefChainVisitor::visit can handle.
+//
+// Return IgnoredUse if the def-use traversal either continues past \p
+// svi or ignores this use.
+//
+// FIXME: Reuse getAccessProjectionOperand() instead of using special cases once
+// the unchecked_take_enum_data_addr -> load -> project_box pattern is fixed.
+CollectAccessPathUses::UseKind
+CollectAccessPathUses::visitSingleValueUser(SingleValueInstruction *svi,
+                                            DFSEntry dfs) {
+  if (isAccessedStorageCast(svi)) {
+    pushUsers(svi, dfs);
+    return IgnoredUse;
+  }
+  switch (svi->getKind()) {
+  default:
+    return LeafUse;
 
-AccessedStorage swift::findAccessedStorage(SILValue sourceAddr) {
-  return FindAccessedStorageVisitor().findStorage(sourceAddr);
+  case SILInstructionKind::BeginAccessInst:
+    pushUsers(svi, dfs);
+    return IgnoredUse;
+
+  // Handle ref_element_addr since we start at the object root instead of
+  // the access base.
+  case SILInstructionKind::RefElementAddrInst:
+    assert(dfs.isRef());
+    assert(dfs.pathCursor > 0 && "ref_element_addr cannot occur within access");
+    dfs.useAndIsRef.setInt(false);
+    return followProjection(svi, dfs);
+
+  case SILInstructionKind::RefTailAddrInst: {
+    assert(dfs.isRef());
+    assert(dfs.pathCursor > 0 && "ref_tail_addr cannot occur within an access");
+    dfs.useAndIsRef.setInt(false);
+    --dfs.pathCursor;
+    AccessPath::Index pathIndex = pathIndices[dfs.pathCursor];
+    assert(pathIndex.isSubObjectProjection());
+    if (pathIndex.getSubObjectIndex() == AccessedStorage::TailIndex)
+      pushUsers(svi, dfs);
+
+    return IgnoredUse;
+  }
+
+  // MARK: Access projections
+
+  case SILInstructionKind::StructElementAddrInst:
+  case SILInstructionKind::TupleElementAddrInst:
+    return followProjection(svi, dfs);
+
+  case SILInstructionKind::IndexAddrInst:
+  case SILInstructionKind::TailAddrInst: {
+    auto projIdx = ProjectionIndex(svi);
+    if (projIdx.isValid()) {
+      if (dfs.offset != AccessPath::UnknownOffset)
+        dfs.offset += projIdx.Index;
+      else
+        assert(collectOverlappingUses);
+    } else if (collectOverlappingUses) {
+      dfs.offset = AccessPath::UnknownOffset;
+    } else {
+      return IgnoredUse;
+    }
+    pushUsers(svi, dfs);
+    return IgnoredUse;
+  }
+
+  // open_existential_addr and unchecked_take_enum_data_addr are classified as
+  // access projections, but they also modify memory. Both see through them and
+  // also report them as uses.
+  case SILInstructionKind::OpenExistentialAddrInst:
+  case SILInstructionKind::UncheckedTakeEnumDataAddrInst:
+    pushUsers(svi, dfs);
+    return LeafUse;
+
+  case SILInstructionKind::StructExtractInst:
+    // Handle nested access to a KeyPath projection. The projection itself
+    // uses a Builtin. However, the returned UnsafeMutablePointer may be
+    // converted to an address and accessed via an inout argument.
+    if (isUnsafePointerExtraction(cast<StructExtractInst>(svi))) {
+      pushUsers(svi, dfs);
+      return IgnoredUse;
+    }
+    return LeafUse;
+
+  case SILInstructionKind::LoadInst:
+    // Load a box from an indirect payload of an opaque enum. See comments
+    // in AccessUseDefChainVisitor::visit. Record this load as a leaf-use even
+    // when we look through its project_box because anyone inspecting the load
+    // itself will see the same AccessPath.
+    // FIXME: if this doesn't go away with opaque values, add a new instruction
+    // for load+project_box.
+    if (svi->getType().is<SILBoxType>()) {
+      Operand *addrOper = &cast<LoadInst>(svi)->getOperandRef();
+      assert(isa<UncheckedTakeEnumDataAddrInst>(addrOper->get()));
+      // Push the project_box uses
+      for (auto *use : svi->getUses()) {
+        if (isa<ProjectBoxInst>(use->getUser()))
+          pushUser(DFSEntry(use, dfs.isRef(), dfs.pathCursor, dfs.offset));
+      }
+    }
+    return LeafUse;
+  }
 }
 
-AccessedStorage swift::identifyAccessedStorageImpl(SILValue sourceAddr) {
-  return IdentifyAccessedStorageVisitor().findStorage(sourceAddr);
+bool AccessPath::collectUses(SmallVectorImpl<Operand *> &uses,
+                             bool collectOverlappingUses,
+                             unsigned useLimit) const {
+  return CollectAccessPathUses(*this, uses, collectOverlappingUses, useLimit)
+      .collectUses();
+}
+
+bool AccessPathWithBase::collectUses(SmallVectorImpl<Operand *> &uses,
+                                     bool collectOverlappingUses,
+                                     unsigned useLimit) const {
+  CollectAccessPathUses collector(accessPath, uses, collectOverlappingUses,
+                                  useLimit);
+  if (accessPath.getRoot())
+    return std::move(collector).collectUses();
+
+  if (!base)
+    return false;
+
+  return std::move(collector).collectUses(base);
+}
+
+void AccessPath::Index::print(raw_ostream &os) const {
+  if (isSubObjectProjection())
+    os << '#' << getSubObjectIndex();
+  else {
+    os << '@';
+    if (getOffset() == AccessPath::UnknownOffset)
+      os << "Unknown";
+    else
+      os << getOffset();
+  }
+}
+
+LLVM_ATTRIBUTE_USED void AccessPath::Index::dump() const {
+  print(llvm::dbgs());
+}
+
+static void recursivelyPrintPath(AccessPath::PathNode node, raw_ostream &os) {
+  AccessPath::PathNode parent = node.getParent();
+  if (!parent.isRoot()) {
+    recursivelyPrintPath(parent, os);
+    os << ",";
+  }
+  node.getIndex().print(os);
+}
+
+void AccessPath::printPath(raw_ostream &os) const {
+  os << "Path: ";
+  if (!isValid()) {
+    os << "INVALID\n";
+    return;
+  }
+  os << "(";
+  PathNode node = getPathNode();
+  if (offset != 0) {
+    Index::forOffset(offset).print(os);
+    if (!node.isRoot())
+      os << ",";
+  }
+  if (!node.isRoot())
+    recursivelyPrintPath(node, os);
+  os << ")\n";
+}
+
+void AccessPath::print(raw_ostream &os) const {
+  if (!isValid()) {
+    os << "INVALID\n";
+    return;
+  }
+  os << "Storage: ";
+  getStorage().print(os);
+  printPath(os);
+}
+
+LLVM_ATTRIBUTE_USED void AccessPath::dump() const { print(llvm::dbgs()); }
+
+void AccessPathWithBase::print(raw_ostream &os) const {
+  if (base)
+    os << "Base: " << base;
+
+  accessPath.print(os);
+}
+
+LLVM_ATTRIBUTE_USED void AccessPathWithBase::dump() const {
+  print(llvm::dbgs());
 }
 
 //===----------------------------------------------------------------------===//
-//                               MARK: Helper API
+//             MARK: Helper API for specific formal access patterns
 //===----------------------------------------------------------------------===//
 
 static bool isScratchBuffer(SILValue value) {
@@ -556,7 +1471,9 @@ bool swift::isExternalGlobalAddressor(ApplyInst *AI) {
 // Return true if the given StructExtractInst extracts the RawPointer from
 // Unsafe[Mutable]Pointer.
 bool swift::isUnsafePointerExtraction(StructExtractInst *SEI) {
-  assert(isa<BuiltinRawPointerType>(SEI->getType().getASTType()));
+  if (!isa<BuiltinRawPointerType>(SEI->getType().getASTType()))
+    return false;
+  
   auto &C = SEI->getModule().getASTContext();
   auto *decl = SEI->getStructDecl();
   return decl == C.getUnsafeMutablePointerDecl()
@@ -652,9 +1569,9 @@ SILBasicBlock::iterator swift::removeBeginAccess(BeginAccessInst *beginAccess) {
 //                            Verification
 //===----------------------------------------------------------------------===//
 
-/// Helper for visitApplyAccesses that visits address-type call arguments,
-/// including arguments to @noescape functions that are passed as closures to
-/// the current call.
+// Helper for visitApplyAccesses that visits address-type call arguments,
+// including arguments to @noescape functions that are passed as closures to
+// the current call.
 static void visitApplyAccesses(ApplySite apply,
                                llvm::function_ref<void(Operand *)> visitor) {
   for (Operand &oper : apply.getArgumentOperands()) {
@@ -686,14 +1603,27 @@ static void visitBuiltinAddress(BuiltinInst *builtin,
       builtin->dump();
       llvm_unreachable("unexpected builtin memory access.");
 
-      // WillThrow exists for the debugger, does nothing.
+    // Handle builtin "generic_add"<V>($*V, $*V, $*V) and the like.
+#define BUILTIN(Id, Name, Attrs)
+#define BUILTIN_BINARY_OPERATION_POLYMORPHIC(Id, Name)        \
+    case BuiltinValueKind::Id:
+
+#include "swift/AST/Builtins.def"
+
+      visitor(&builtin->getAllOperands()[1]);
+      visitor(&builtin->getAllOperands()[2]);
+      return;
+
+    // WillThrow exists for the debugger, does nothing.
     case BuiltinValueKind::WillThrow:
       return;
 
-      // Buitins that affect memory but can't be formal accesses.
+    // Buitins that affect memory but can't be formal accesses.
+    case BuiltinValueKind::AssumeTrue:
     case BuiltinValueKind::UnexpectedError:
     case BuiltinValueKind::ErrorInMain:
     case BuiltinValueKind::IsOptionalType:
+    case BuiltinValueKind::CondFailMessage:
     case BuiltinValueKind::AllocRaw:
     case BuiltinValueKind::DeallocRaw:
     case BuiltinValueKind::Fence:
@@ -703,14 +1633,15 @@ static void visitBuiltinAddress(BuiltinInst *builtin,
     case BuiltinValueKind::Unreachable:
     case BuiltinValueKind::CondUnreachable:
     case BuiltinValueKind::DestroyArray:
-    case BuiltinValueKind::COWBufferForReading:
     case BuiltinValueKind::UnsafeGuaranteed:
     case BuiltinValueKind::UnsafeGuaranteedEnd:
     case BuiltinValueKind::Swift3ImplicitObjCEntrypoint:
+    case BuiltinValueKind::PoundAssert:
+    case BuiltinValueKind::IntInstrprofIncrement:
     case BuiltinValueKind::TSanInoutAccess:
       return;
 
-      // General memory access to a pointer in first operand position.
+    // General memory access to a pointer in first operand position.
     case BuiltinValueKind::CmpXChg:
     case BuiltinValueKind::AtomicLoad:
     case BuiltinValueKind::AtomicStore:
@@ -720,8 +1651,8 @@ static void visitBuiltinAddress(BuiltinInst *builtin,
       // visitor(&builtin->getAllOperands()[0]);
       return;
 
-      // Arrays: (T.Type, Builtin.RawPointer, Builtin.RawPointer,
-      // Builtin.Word)
+    // Arrays: (T.Type, Builtin.RawPointer, Builtin.RawPointer,
+    // Builtin.Word)
     case BuiltinValueKind::CopyArray:
     case BuiltinValueKind::TakeArrayNoAlias:
     case BuiltinValueKind::TakeArrayFrontToBack:
@@ -811,11 +1742,11 @@ void swift::visitAccessedAddress(SILInstruction *I,
     visitor(&I->getAllOperands()[0]);
     return;
 
+  case SILInstructionKind::InitExistentialAddrInst:
+  case SILInstructionKind::InjectEnumAddrInst:
 #define NEVER_OR_SOMETIMES_LOADABLE_CHECKED_REF_STORAGE(Name, ...) \
   case SILInstructionKind::Load##Name##Inst:
 #include "swift/AST/ReferenceStorage.def"
-  case SILInstructionKind::InitExistentialAddrInst:
-  case SILInstructionKind::InjectEnumAddrInst:
   case SILInstructionKind::LoadInst:
   case SILInstructionKind::LoadBorrowInst:
   case SILInstructionKind::OpenExistentialAddrInst:
@@ -845,6 +1776,8 @@ void swift::visitAccessedAddress(SILInstruction *I,
   case SILInstructionKind::BeginAccessInst:
   case SILInstructionKind::BeginApplyInst:
   case SILInstructionKind::BeginBorrowInst:
+  case SILInstructionKind::BeginCOWMutationInst:
+  case SILInstructionKind::EndCOWMutationInst:
   case SILInstructionKind::BeginUnpairedAccessInst:
   case SILInstructionKind::BindMemoryInst:
   case SILInstructionKind::CheckedCastValueBranchInst:
@@ -863,6 +1796,7 @@ void swift::visitAccessedAddress(SILInstruction *I,
   case SILInstructionKind::EndLifetimeInst:
   case SILInstructionKind::ExistentialMetatypeInst:
   case SILInstructionKind::FixLifetimeInst:
+  case SILInstructionKind::GlobalAddrInst:
   case SILInstructionKind::InitExistentialValueInst:
   case SILInstructionKind::IsUniqueInst:
   case SILInstructionKind::IsEscapingClosureInst:
diff --git a/lib/SIL/Verifier/LoadBorrowInvalidationChecker.cpp b/lib/SIL/Verifier/LoadBorrowInvalidationChecker.cpp
index 1f5e84803d5..b5aea5103bc 100644
--- a/lib/SIL/Verifier/LoadBorrowInvalidationChecker.cpp
+++ b/lib/SIL/Verifier/LoadBorrowInvalidationChecker.cpp
@@ -453,12 +453,20 @@ bool LoadBorrowNeverInvalidatedAnalysis::
 
 bool LoadBorrowNeverInvalidatedAnalysis::isNeverInvalidated(
     LoadBorrowInst *lbi) {
-  SILValue address = getAddressAccess(lbi->getOperand());
+
+  SILValue address = getAccessBegin(lbi->getOperand());
   if (!address)
     return false;
 
+  auto storage = findAccessedStorage(address);
+  // If we couldn't find an access storage, return that we are assumed to write.
+  if (!storage) {
+    llvm::errs() << "Couldn't compute access storage?!\n";
+    return false;
+  }
+
   // If we have a let address, then we are already done.
-  if (isLetAddress(stripAccessMarkers(address)))
+  if (storage.isLetAccess(lbi->getFunction()))
     return true;
 
   // At this point, we know that we /may/ have writes. Now we go through various
@@ -478,24 +486,34 @@ bool LoadBorrowNeverInvalidatedAnalysis::isNeverInvalidated(
 
     // Otherwise, validate that any writes to our begin_access is not when the
     // load_borrow's result is live.
+    //
+    // FIXME: do we verify that the load_borrow scope is always nested within
+    // the begin_access scope (to ensure no aliasing access)?
     return doesAddressHaveWriteThatInvalidatesLoadBorrow(lbi, endBorrowUses,
                                                          bai);
   }
 
+  // FIXME: the subsequent checks appear to assume that 'address' is not aliased
+  // within the scope of the load_borrow. This can only be assumed when either
+  // the load_borrow is nested within an access scope or when the
+  // storage.isUniquelyIdentified() and all uses of storage.getRoot() have been
+  // analyzed. The later can be done with AccessPath::collectUses().
+
   // Check if our unidentified access storage is a project_box. In such a case,
   // validate that all uses of the project_box are not writes that overlap with
   // our load_borrow's result. These are things that may not be a formal access
   // base.
   //
-  // FIXME: we don't seem to verify anywhere that a pointer_to_address cannot
-  // itself be derived from another address that is accessible in the same
-  // function, either because it was returned from a call or directly
-  // address_to_pointer'd.
+  // FIXME: Remove this PointerToAddress check. It appears to be incorrect. we
+  // don't verify anywhere that a pointer_to_address cannot itself be derived
+  // from another address that is accessible in the same function, either
+  // because it was returned from a call or directly address_to_pointer'd.
   if (isa<PointerToAddressInst>(address)) {
     return doesAddressHaveWriteThatInvalidatesLoadBorrow(lbi, endBorrowUses,
                                                          address);
   }
 
+  // FIXME: This ProjectBoxInst
   // If we have a project_box, we need to see if our base, modulo begin_borrow,
   // copy_value have any other project_box that we need to analyze.
   if (auto *pbi = dyn_cast<ProjectBoxInst>(address)) {
@@ -503,13 +521,6 @@ bool LoadBorrowNeverInvalidatedAnalysis::isNeverInvalidated(
                                                      pbi->getOperand());
   }
 
-  auto storage = findAccessedStorage(address);
-
-  // If we couldn't find an access storage, return that we are assumed to write.
-  if (!storage) {
-    llvm::errs() << "Couldn't compute access storage?!\n";
-    return false;
-  }
 
   switch (storage.getKind()) {
   case AccessedStorage::Stack: {
@@ -562,7 +573,8 @@ bool LoadBorrowNeverInvalidatedAnalysis::isNeverInvalidated(
   }
   case AccessedStorage::Unidentified: {
     // Otherwise, we didn't understand this, so bail.
-    llvm::errs() << "Unidentified access storage: " << storage;
+    llvm::errs() << "Unidentified access storage: ";
+    storage.dump();
     return false;
   }
   case AccessedStorage::Nested: {
diff --git a/lib/SILOptimizer/Analysis/AliasAnalysis.cpp b/lib/SILOptimizer/Analysis/AliasAnalysis.cpp
index f9b28518454..44c40297e4b 100644
--- a/lib/SILOptimizer/Analysis/AliasAnalysis.cpp
+++ b/lib/SILOptimizer/Analysis/AliasAnalysis.cpp
@@ -334,7 +334,7 @@ static bool isAccessedAddressTBAASafe(SILValue V) {
   if (!V->getType().isAddress())
     return false;
 
-  SILValue accessedAddress = getAccessedAddress(V);
+  SILValue accessedAddress = getAccessAddress(V);
   if (isa<SILFunctionArgument>(accessedAddress))
     return true;
 
diff --git a/lib/SILOptimizer/Analysis/MemoryBehavior.cpp b/lib/SILOptimizer/Analysis/MemoryBehavior.cpp
index b867c18b630..376c9a22031 100644
--- a/lib/SILOptimizer/Analysis/MemoryBehavior.cpp
+++ b/lib/SILOptimizer/Analysis/MemoryBehavior.cpp
@@ -78,11 +78,13 @@ public:
   }
 
   /// If 'V' is an address projection within a formal access, return the
-  /// canonical address of the formal access. Otherwise, return 'V' itself,
-  /// which is either a reference or unknown pointer or address.
+  /// canonical address of the formal access if possible without looking past
+  /// any storage casts. Otherwise, a "best-effort" address
+  ///
+  /// If 'V' is an address, then the returned value is also an address.
   SILValue getValueAddress() {
     if (!cachedValueAddress) {
-      cachedValueAddress = V->getType().isAddress() ? getAccessedAddress(V) : V;
+      cachedValueAddress = V->getType().isAddress() ? getAccessAddress(V) : V;
     }
     return cachedValueAddress;
   }
@@ -147,7 +149,7 @@ public:
 
     case SILAccessKind::Modify:
       if (isLetValue()) {
-        assert(stripAccessMarkers(beginAccess) != getValueAddress()
+        assert(getAccessBase(beginAccess) != getValueAddress()
                && "let modification not allowed");
         return MemBehavior::None;
       }
@@ -251,8 +253,7 @@ MemBehavior MemoryBehaviorVisitor::visitLoadInst(LoadInst *LI) {
 MemBehavior MemoryBehaviorVisitor::visitStoreInst(StoreInst *SI) {
   // No store besides the initialization of a "let"-variable
   // can have any effect on the value of this "let" variable.
-  if (isLetValue()
-      && (getAccessedAddress(SI->getDest()) != getValueAddress())) {
+  if (isLetValue() && (getAccessBase(SI->getDest()) != getValueAddress())) {
     return MemBehavior::None;
   }
   // If the store dest cannot alias the pointer in question, then the
diff --git a/lib/SILOptimizer/LoopTransforms/LICM.cpp b/lib/SILOptimizer/LoopTransforms/LICM.cpp
index 39c4628324e..6eb5e30a7f9 100644
--- a/lib/SILOptimizer/LoopTransforms/LICM.cpp
+++ b/lib/SILOptimizer/LoopTransforms/LICM.cpp
@@ -711,7 +711,7 @@ static bool analyzeBeginAccess(BeginAccessInst *BI,
       return true;
     }
     return BIAccessedStorageNonNested.isDistinctFrom(
-        findAccessedStorage(OtherBI));
+      findAccessedStorage(OtherBI));
   };
 
   if (!std::all_of(BeginAccesses.begin(), BeginAccesses.end(), safeBeginPred))
diff --git a/lib/SILOptimizer/Mandatory/DiagnoseStaticExclusivity.cpp b/lib/SILOptimizer/Mandatory/DiagnoseStaticExclusivity.cpp
index eace1f0eac6..67403697316 100644
--- a/lib/SILOptimizer/Mandatory/DiagnoseStaticExclusivity.cpp
+++ b/lib/SILOptimizer/Mandatory/DiagnoseStaticExclusivity.cpp
@@ -964,20 +964,20 @@ static void checkStaticExclusivity(SILFunction &Fn, PostOrderFunctionInfo *PO,
 // Check that the given address-type operand is guarded by begin/end access
 // markers.
 static void checkAccessedAddress(Operand *memOper, StorageMap &Accesses) {
-  SILValue address = getAddressAccess(memOper->get());
+  SILValue accessBegin = getAccessBegin(memOper->get());
   SILInstruction *memInst = memOper->getUser();
 
-  auto error = [address, memInst]() {
+  auto error = [accessBegin, memInst]() {
     llvm::dbgs() << "Memory access not protected by begin_access:\n";
     memInst->printInContext(llvm::dbgs());
-    llvm::dbgs() << "Accessing: " << address;
+    llvm::dbgs() << "Accessing: " << accessBegin;
     llvm::dbgs() << "In function:\n";
     memInst->getFunction()->print(llvm::dbgs());
     abort();
   };
 
   // Check if this address is guarded by an access.
-  if (auto *BAI = dyn_cast<BeginAccessInst>(address)) {
+  if (auto *BAI = dyn_cast<BeginAccessInst>(accessBegin)) {
     if (BAI->getEnforcement() == SILAccessEnforcement::Unsafe)
       return;
 
@@ -1017,7 +1017,7 @@ static void checkAccessedAddress(Operand *memOper, StorageMap &Accesses) {
       return;
   }
 
-  const AccessedStorage &storage = findAccessedStorage(address);
+  const AccessedStorage &storage = findAccessedStorage(accessBegin);
   // findAccessedStorage may return an invalid storage object if the address
   // producer is not recognized by its allowlist. For the purpose of
   // verification, we assume that this can only happen for local

commit 69b513afaf073de6246a25156df87bc0e29d4dcb
Author: Owen Voorhees <owenvoorhees@gmail.com>
Date:   Tue Mar 3 08:48:32 2020 -0800

    [Diagnostics] Updated (experimental) diagnostic printing style (#30027)
    
    * [Diagnostics] Experimental diagnostic printing updates
    
    This new style directly annotates small snippets of code with
    error messages, highlights and fix-its. It also uses color more
    effectively to highlight important segments.
    
    * [Diagnostics] Stage educational notes and experimental formatting behind separate frontend flags
    
    educational notes -> -enable-educational-notes
    formatting -> -enable-experimental-diagnostic-formatting
    
    * [Diagnostics] Refactor expensive line lookups in diag formatting
    
    * [Diagnostics] Refactor some PrintingDiagnosticConsumer code into a flush method
    
    * [Diag-Experimental-Formatting] Custom formatting for Xcode editor placeholders
    
    * [Diag-Experimental-Formatting] Better and more consistent textual description of fix its
    
    * [Diags-Experimental-Formatting] Handle lines with tab characters correctly when rendering highlights and messages
    
    Tabs are converted to 2 spaces for display purposes.
    
    * [Diag-Experimental-Formatting] Refactor byte-to-column mapping for efficiency
    
    * [Diag-Experimental-Formatting] Fix line number indent calculation
    
    * [Diag-Experimental-Formatting] Include indicators of insertions and deletions in the highlight line
    
    Inserts are underlined by green '+' chars, deletions by red '-' chars.
    
    * [Diag-Experimental-Formatting] Change color of indicator arrow for non-ASCII anchored messages
    * [Diag-experimental-formatting] Make tests less sensitive to line numbering
    
    * [Diag-Experimental-Formatting] Update tests to allow windows path separators
    
    * [Diag-Experimental-Formatting] Bug fixes for the integrated REPL

diff --git a/include/swift/AST/DiagnosticConsumer.h b/include/swift/AST/DiagnosticConsumer.h
index 1d77b16952b..0b582a2faef 100644
--- a/include/swift/AST/DiagnosticConsumer.h
+++ b/include/swift/AST/DiagnosticConsumer.h
@@ -126,6 +126,9 @@ public:
   /// \returns true if an error occurred while finishing-up.
   virtual bool finishProcessing() { return false; }
 
+  /// Flush any in-flight diagnostics.
+  virtual void flush() {}
+
   /// In batch mode, any error causes failure for all primary files, but
   /// anyone consulting .dia files will only see an error for a particular
   /// primary in that primary's serialized diagnostics file. For other
diff --git a/include/swift/AST/DiagnosticEngine.h b/include/swift/AST/DiagnosticEngine.h
index ab691bb708e..7ec80268f87 100644
--- a/include/swift/AST/DiagnosticEngine.h
+++ b/include/swift/AST/DiagnosticEngine.h
@@ -676,8 +676,8 @@ namespace swift {
     /// Print diagnostic names after their messages
     bool printDiagnosticNames = false;
 
-    /// Use descriptive diagnostic style when available.
-    bool useDescriptiveDiagnostics = false;
+    /// Use educational notes when available.
+    bool useEducationalNotes = false;
 
     /// Path to diagnostic documentation directory.
     std::string diagnosticDocumentationPath = "";
@@ -705,6 +705,11 @@ namespace swift {
       return state.getShowDiagnosticsAfterFatalError();
     }
 
+    void flushConsumers() {
+      for (auto consumer : Consumers)
+        consumer->flush();
+    }
+
     /// Whether to skip emitting warnings
     void setSuppressWarnings(bool val) { state.setSuppressWarnings(val); }
     bool getSuppressWarnings() const {
@@ -725,12 +730,8 @@ namespace swift {
       return printDiagnosticNames;
     }
 
-    void setUseDescriptiveDiagnostics(bool val) {
-       useDescriptiveDiagnostics = val;
-    }
-    bool getUseDescriptiveDiagnostics() const {
-      return useDescriptiveDiagnostics;
-    }
+    void setUseEducationalNotes(bool val) { useEducationalNotes = val; }
+    bool getUseEducationalNotes() const { return useEducationalNotes; }
 
     void setDiagnosticDocumentationPath(std::string path) {
       diagnosticDocumentationPath = path;
diff --git a/include/swift/Basic/DiagnosticOptions.h b/include/swift/Basic/DiagnosticOptions.h
index 12a39435550..d1552f06f80 100644
--- a/include/swift/Basic/DiagnosticOptions.h
+++ b/include/swift/Basic/DiagnosticOptions.h
@@ -55,9 +55,13 @@ public:
   // When printing diagnostics, include the diagnostic name at the end
   bool PrintDiagnosticNames = false;
 
-  /// If set to true, produce more descriptive diagnostic output if available.
-  /// Descriptive diagnostic output is not intended to be machine-readable.
-  bool EnableDescriptiveDiagnostics = false;
+  /// If set to true, display educational note content to the user if available.
+  /// Educational notes are documentation which supplement diagnostics.
+  bool EnableEducationalNotes = false;
+
+  // If set to true, use the more descriptive experimental formatting style for
+  // diagnostics.
+  bool EnableExperimentalFormatting = false;
 
   std::string DiagnosticDocumentationPath = "";
 
diff --git a/include/swift/Basic/SourceManager.h b/include/swift/Basic/SourceManager.h
index 878c72a67b2..d05e8c9e66f 100644
--- a/include/swift/Basic/SourceManager.h
+++ b/include/swift/Basic/SourceManager.h
@@ -256,6 +256,8 @@ public:
                                SourceLoc();
   }
 
+  std::string getLineString(unsigned BufferID, unsigned LineNumber);
+
   SourceLoc getLocFromExternalSource(StringRef Path, unsigned Line, unsigned Col);
 private:
   const VirtualFile *getVirtualFile(SourceLoc Loc) const;
diff --git a/include/swift/Frontend/PrintingDiagnosticConsumer.h b/include/swift/Frontend/PrintingDiagnosticConsumer.h
index f0645839a75..988a21768b5 100644
--- a/include/swift/Frontend/PrintingDiagnosticConsumer.h
+++ b/include/swift/Frontend/PrintingDiagnosticConsumer.h
@@ -2,7 +2,7 @@
 //
 // This source file is part of the Swift.org open source project
 //
-// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors
+// Copyright (c) 2014 - 2020 Apple Inc. and the Swift project authors
 // Licensed under Apache License v2.0 with Runtime Library Exception
 //
 // See https://swift.org/LICENSE.txt for license information
@@ -25,24 +25,39 @@
 #include "llvm/Support/Process.h"
 
 namespace swift {
+class AnnotatedSourceSnippet;
 
 /// Diagnostic consumer that displays diagnostics to standard error.
 class PrintingDiagnosticConsumer : public DiagnosticConsumer {
   llvm::raw_ostream &Stream;
   bool ForceColors = false;
   bool DidErrorOccur = false;
+  bool ExperimentalFormattingEnabled = false;
+  // The current snippet used to display an error/warning/remark and the notes
+  // implicitly associated with it. Uses `std::unique_ptr` so that
+  // `AnnotatedSourceSnippet` can be forward declared.
+  std::unique_ptr<AnnotatedSourceSnippet> currentSnippet;
+
 public:
-  PrintingDiagnosticConsumer(llvm::raw_ostream &stream = llvm::errs()) :
-    Stream(stream) { }
+  PrintingDiagnosticConsumer(llvm::raw_ostream &stream = llvm::errs());
+  ~PrintingDiagnosticConsumer();
 
   virtual void handleDiagnostic(SourceManager &SM,
                                 const DiagnosticInfo &Info) override;
 
+  virtual bool finishProcessing() override;
+
+  void flush(bool includeTrailingBreak);
+
+  virtual void flush() override { flush(false); }
+
   void forceColors() {
     ForceColors = true;
     llvm::sys::Process::UseANSIEscapeCodes(true);
   }
 
+  void enableExperimentalFormatting() { ExperimentalFormattingEnabled = true; }
+
   bool didErrorOccur() {
     return DidErrorOccur;
   }
diff --git a/include/swift/Option/FrontendOptions.td b/include/swift/Option/FrontendOptions.td
index a3bbd2e6a09..b18dfb561dd 100644
--- a/include/swift/Option/FrontendOptions.td
+++ b/include/swift/Option/FrontendOptions.td
@@ -131,8 +131,12 @@ def enable_cross_import_overlays : Flag<["-"], "enable-cross-import-overlays">,
 def disable_cross_import_overlays : Flag<["-"], "disable-cross-import-overlays">,
   HelpText<"Do not automatically import declared cross-import overlays.">;
 
-def enable_descriptive_diagnostics : Flag<["-"], "enable-descriptive-diagnostics">,
-  HelpText<"Show descriptive diagnostic information, if available.">;
+def enable_educational_notes : Flag<["-"], "enable-educational-notes">,
+  HelpText<"Show educational notes, if available.">;
+  
+def enable_experimental_diagnostic_formatting :
+  Flag<["-"], "enable-experimental-diagnostic-formatting">,
+  HelpText<"Enable experimental diagnostic formatting features.">;
   
 def diagnostic_documentation_path
   : Separate<["-"], "diagnostic-documentation-path">, MetaVarName<"<path>">,
diff --git a/lib/AST/DiagnosticEngine.cpp b/lib/AST/DiagnosticEngine.cpp
index 56aa0879096..31dac150ac5 100644
--- a/lib/AST/DiagnosticEngine.cpp
+++ b/lib/AST/DiagnosticEngine.cpp
@@ -984,7 +984,7 @@ void DiagnosticEngine::emitDiagnostic(const Diagnostic &diagnostic) {
     info->ChildDiagnosticInfo = childInfoPtrs;
     
     SmallVector<std::string, 1> educationalNotePaths;
-    if (useDescriptiveDiagnostics) {
+    if (useEducationalNotes) {
       auto associatedNotes = educationalNotes[(uint32_t)diagnostic.getID()];
       while (associatedNotes && *associatedNotes) {
         SmallString<128> notePath(getDiagnosticDocumentationPath());
diff --git a/lib/Frontend/CompilerInvocation.cpp b/lib/Frontend/CompilerInvocation.cpp
index 5210ccc0688..9dfbbdb131a 100644
--- a/lib/Frontend/CompilerInvocation.cpp
+++ b/lib/Frontend/CompilerInvocation.cpp
@@ -819,8 +819,9 @@ static bool ParseDiagnosticArgs(DiagnosticOptions &Opts, ArgList &Args,
   Opts.SuppressWarnings |= Args.hasArg(OPT_suppress_warnings);
   Opts.WarningsAsErrors |= Args.hasArg(OPT_warnings_as_errors);
   Opts.PrintDiagnosticNames |= Args.hasArg(OPT_debug_diagnostic_names);
-  Opts.EnableDescriptiveDiagnostics |=
-      Args.hasArg(OPT_enable_descriptive_diagnostics);
+  Opts.EnableEducationalNotes |= Args.hasArg(OPT_enable_educational_notes);
+  Opts.EnableExperimentalFormatting |=
+      Args.hasArg(OPT_enable_experimental_diagnostic_formatting);
   if (Arg *A = Args.getLastArg(OPT_diagnostic_documentation_path)) {
     Opts.DiagnosticDocumentationPath = A->getValue();
   }
diff --git a/lib/Frontend/Frontend.cpp b/lib/Frontend/Frontend.cpp
index 92bc0c8fa60..79771cd0be9 100644
--- a/lib/Frontend/Frontend.cpp
+++ b/lib/Frontend/Frontend.cpp
@@ -406,8 +406,8 @@ void CompilerInstance::setUpDiagnosticOptions() {
   if (Invocation.getDiagnosticOptions().PrintDiagnosticNames) {
     Diagnostics.setPrintDiagnosticNames(true);
   }
-  if (Invocation.getDiagnosticOptions().EnableDescriptiveDiagnostics) {
-    Diagnostics.setUseDescriptiveDiagnostics(true);
+  if (Invocation.getDiagnosticOptions().EnableEducationalNotes) {
+    Diagnostics.setUseEducationalNotes(true);
   }
   Diagnostics.setDiagnosticDocumentationPath(
       Invocation.getDiagnosticOptions().DiagnosticDocumentationPath);
diff --git a/lib/Frontend/PrintingDiagnosticConsumer.cpp b/lib/Frontend/PrintingDiagnosticConsumer.cpp
index 87736c550b2..929b5b13d16 100644
--- a/lib/Frontend/PrintingDiagnosticConsumer.cpp
+++ b/lib/Frontend/PrintingDiagnosticConsumer.cpp
@@ -2,7 +2,7 @@
 //
 // This source file is part of the Swift.org open source project
 //
-// Copyright (c) 2014 - 2019 Apple Inc. and the Swift project authors
+// Copyright (c) 2014 - 2020 Apple Inc. and the Swift project authors
 // Licensed under Apache License v2.0 with Runtime Library Exception
 //
 // See https://swift.org/LICENSE.txt for license information
@@ -15,14 +15,17 @@
 //===----------------------------------------------------------------------===//
 
 #include "swift/Frontend/PrintingDiagnosticConsumer.h"
+#include "swift/AST/DiagnosticEngine.h"
 #include "swift/Basic/LLVM.h"
 #include "swift/Basic/SourceManager.h"
-#include "swift/AST/DiagnosticEngine.h"
 #include "llvm/ADT/SmallString.h"
 #include "llvm/ADT/StringRef.h"
 #include "llvm/ADT/Twine.h"
+#include "llvm/Support/FormatAdapters.h"
+#include "llvm/Support/FormatVariadic.h"
 #include "llvm/Support/MemoryBuffer.h"
 #include "llvm/Support/raw_ostream.h"
+#include <algorithm>
 
 using namespace swift;
 
@@ -61,24 +64,651 @@ namespace {
       return 0;
     }
   };
+
+  /// A stream which drops all color settings.
+  class NoColorStream : public raw_ostream {
+    raw_ostream &Underlying;
+
+  public:
+    explicit NoColorStream(raw_ostream &underlying) : Underlying(underlying) {}
+    ~NoColorStream() override { flush(); }
+
+    bool has_colors() const override { return false; }
+
+    void write_impl(const char *ptr, size_t size) override {
+      Underlying.write(ptr, size);
+    }
+    uint64_t current_pos() const override {
+      return Underlying.tell() - GetNumBytesInBuffer();
+    }
+
+    size_t preferred_buffer_size() const override { return 0; }
+  };
+
+  // MARK: Experimental diagnostic printing.
+
+  static void printDiagnosticKind(DiagnosticKind kind, raw_ostream &out) {
+    switch (kind) {
+    case DiagnosticKind::Error:
+      out.changeColor(ColoredStream::Colors::RED, true);
+      out << "error:";
+      break;
+    case DiagnosticKind::Warning:
+      out.changeColor(ColoredStream::Colors::YELLOW, true);
+      out << "warning:";
+      break;
+    case DiagnosticKind::Note:
+      out.changeColor(ColoredStream::Colors::CYAN, true);
+      out << "note:";
+      break;
+    case DiagnosticKind::Remark:
+      out.changeColor(ColoredStream::Colors::CYAN, true);
+      out << "remark:";
+      break;
+    }
+    out.resetColor();
+  }
+
+  static void printNumberedGutter(unsigned LineNumber,
+                                  unsigned LineNumberIndent, raw_ostream &Out) {
+    Out.changeColor(ColoredStream::Colors::BLUE, true);
+    Out << llvm::formatv(
+        "{0} | ",
+        llvm::fmt_align(LineNumber, llvm::AlignStyle::Right, LineNumberIndent));
+    Out.resetColor();
+  }
+
+  static void printEmptyGutter(unsigned LineNumberIndent, raw_ostream &Out) {
+    Out.changeColor(ColoredStream::Colors::BLUE, true);
+    Out << std::string(LineNumberIndent + 1, ' ') << "| ";
+    Out.resetColor();
+  }
+
+  // Describe a fix-it out-of-line.
+  static void describeFixIt(SourceManager &SM, DiagnosticInfo::FixIt fixIt,
+                            raw_ostream &Out) {
+    if (fixIt.getRange().getByteLength() == 0) {
+      Out << "insert '" << fixIt.getText() << "'";
+    } else if (fixIt.getText().empty()) {
+      Out << "remove '" << SM.extractText(fixIt.getRange()) << "'";
+    } else {
+      Out << "replace '" << SM.extractText(fixIt.getRange()) << "' with '"
+          << fixIt.getText() << "'";
+    }
+  }
+
+  static void describeFixIts(SourceManager &SM,
+                             ArrayRef<DiagnosticInfo::FixIt> fixIts,
+                             raw_ostream &Out) {
+    Out << "[";
+    for (unsigned i = 0; i < fixIts.size(); ++i) {
+      if (fixIts.size() > 2 && i + 1 == fixIts.size()) {
+        Out << ", and ";
+      } else if (fixIts.size() > 2 && i > 0) {
+        Out << ", ";
+      } else if (fixIts.size() == 2 && i == 1) {
+        Out << " and ";
+      }
+      describeFixIt(SM, fixIts[i], Out);
+    }
+    Out << "]";
+  }
+
+  /// Represents a single line of source code annotated with optional messages,
+  /// highlights, and fix-its.
+  class AnnotatedLine {
+    friend class AnnotatedFileExcerpt;
+
+    // A diagnostic message located at a specific byte in the line.
+    struct Message {
+      unsigned Byte;
+      DiagnosticKind Kind;
+      std::string Text;
+    };
+
+    // A half-open byte range which should be highlighted.
+    struct Highlight {
+      unsigned StartByte;
+      unsigned EndByte;
+    };
+
+    // A half-open byte range which should be replaced with the given text.
+    struct FixIt {
+      unsigned StartByte;
+      unsigned EndByte;
+      std::string Text;
+    };
+
+    unsigned LineNumber;
+    std::string LineText;
+    SmallVector<Message, 1> Messages;
+    SmallVector<Highlight, 1> Highlights;
+    SmallVector<FixIt, 1> FixIts;
+
+    // Adjust output color as needed if this byte is part of a fix-it deletion.
+    void applyStyleForLineByte(unsigned Byte, raw_ostream &Out, bool &Deleted) {
+      bool shouldDelete = false;
+
+      for (auto fixIt : FixIts) {
+        if (Byte >= fixIt.StartByte && Byte < fixIt.EndByte)
+          shouldDelete = true;
+      }
+
+      // Only modify deletions when we reach the start or end of
+      // a fix-it. This ensures that so long as the original
+      // SourceLocs pointed to the first byte of a grapheme cluster, we won't
+      // output an ANSI escape sequence in the middle of one.
+      if (shouldDelete != Deleted) {
+        Out.resetColor();
+        if (shouldDelete) {
+          Out.changeColor(ColoredStream::Colors::RED);
+        }
+      }
+      Deleted = shouldDelete;
+    }
+
+    // Insert fix-it replacement text at the appropriate point in the line.
+    bool maybePrintInsertionAfter(int Byte, bool isLineASCII,
+                                  raw_ostream &Out) {
+      // Don't print insertions inline for non-ASCII lines, because we can't
+      // print an underline beneath them.
+      if (!isLineASCII)
+        return false;
+
+      for (auto fixIt : FixIts) {
+        if ((int)fixIt.EndByte - 1 == Byte) {
+          Out.changeColor(ColoredStream::Colors::GREEN, /*bold*/ true);
+          for (unsigned i = 0; i < fixIt.Text.size(); ++i) {
+            // Invert text colors for editor placeholders.
+            if (i + 1 < fixIt.Text.size() && fixIt.Text.substr(i, 2) == "<#") {
+              Out.changeColor(ColoredStream::Colors::GREEN, /*bold*/ true,
+                              /*background*/ true);
+              ++i;
+            } else if (i + 1 < fixIt.Text.size() &&
+                       fixIt.Text.substr(i, 2) == "#>") {
+              Out.changeColor(ColoredStream::Colors::GREEN, /*bold*/ true,
+                              /*background*/ false);
+              ++i;
+            } else {
+              Out << fixIt.Text[i];
+            }
+          }
+          Out.resetColor();
+          return true;
+        }
+      }
+      return false;
+    }
+
+    unsigned lineByteOffsetForLoc(SourceManager &SM, SourceLoc Loc) {
+      SourceLoc lineStart = SM.getLocForLineCol(SM.findBufferContainingLoc(Loc),
+                                                getLineNumber(), 1);
+      return SM.getByteDistance(lineStart, Loc);
+    }
+
+  public:
+    AnnotatedLine(unsigned LineNumber, StringRef LineText)
+        : LineNumber(LineNumber), LineText(LineText) {}
+
+    unsigned getLineNumber() { return LineNumber; }
+
+    void addMessage(SourceManager &SM, SourceLoc Loc, DiagnosticKind Kind,
+                    StringRef Message) {
+      Messages.push_back({lineByteOffsetForLoc(SM, Loc), Kind, Message});
+    }
+
+    void addHighlight(SourceManager &SM, CharSourceRange Range) {
+      Highlights.push_back({lineByteOffsetForLoc(SM, Range.getStart()),
+                            lineByteOffsetForLoc(SM, Range.getEnd())});
+    }
+
+    void addFixIt(SourceManager &SM, CharSourceRange Range, StringRef Text) {
+      FixIts.push_back({lineByteOffsetForLoc(SM, Range.getStart()),
+                        lineByteOffsetForLoc(SM, Range.getEnd()), Text});
+    }
+
+    void render(unsigned LineNumberIndent, raw_ostream &Out) {
+      printNumberedGutter(LineNumber, LineNumberIndent, Out);
+
+      // Determine if the line is all-ASCII. This will determine a number of
+      // later formatting decisions.
+      bool isASCII = true;
+      for (unsigned i = 0; i < LineText.size(); ++i)
+        isASCII = isASCII && static_cast<unsigned char>(LineText[i]) <= 127;
+
+      // Map a byte in the original source line to a column in the annotated
+      // line.
+      unsigned *byteToColumnMap = new unsigned[LineText.size() + 1];
+      unsigned extraColumns = 0;
+      // We count one past the end of LineText here to handle trailing fix-it
+      // insertions.
+      for (unsigned i = 0; i < LineText.size() + 1; ++i) {
+        if (isASCII) {
+          for (auto fixIt : FixIts) {
+            if (fixIt.EndByte == i) {
+              // We don't print editor placeholder indicators, so make sure we
+              // don't count them here.
+              extraColumns += fixIt.Text.size() -
+                              StringRef(fixIt.Text).count("<#") * 2 -
+                              StringRef(fixIt.Text).count("#>") * 2;
+            }
+          }
+        }
+        // Tabs are mapped to 2 spaces so they have a known column width.
+        if (i < LineText.size() && LineText[i] == '\t')
+          extraColumns += 1;
+
+        byteToColumnMap[i] = i + extraColumns;
+      }
+
+      // Print the source line byte-by-byte, emitting ANSI escape sequences as
+      // needed to style fix-its, and checking for non-ASCII characters.
+      bool deleted = false;
+      maybePrintInsertionAfter(-1, isASCII, Out);
+      for (unsigned i = 0; i < LineText.size(); ++i) {
+        applyStyleForLineByte(i, Out, deleted);
+        if (LineText[i] == '\t')
+          Out << "  ";
+        else
+          Out << LineText[i];
+        if (maybePrintInsertionAfter(i, isASCII, Out)) {
+          deleted = false;
+        }
+      }
+      maybePrintInsertionAfter(LineText.size(), isASCII, Out);
+      Out.resetColor();
+      Out << "\n";
+
+      // If the entire line is composed of ASCII characters, we can position '~'
+      // characters in the appropriate columns on the following line to
+      // represent highlights.
+      if (isASCII) {
+        auto highlightLine = std::string(byteToColumnMap[LineText.size()], ' ');
+        for (auto highlight : Highlights) {
+          for (unsigned i = highlight.StartByte; i < highlight.EndByte; ++i)
+            highlightLine[byteToColumnMap[i]] = '~';
+        }
+
+        for (auto fixIt : FixIts) {
+          // Mark deletions.
+          for (unsigned i = fixIt.StartByte; i < fixIt.EndByte; ++i)
+            highlightLine[byteToColumnMap[i]] = '-';
+
+          // Mark insertions.
+          for (unsigned i = byteToColumnMap[fixIt.StartByte - 1] + 1;
+               i < byteToColumnMap[fixIt.StartByte]; ++i)
+            highlightLine[i] = '+';
+        }
+
+        // Print the highlight line with the appropriate colors.
+        if (!(Highlights.empty() && FixIts.empty())) {
+          printEmptyGutter(LineNumberIndent, Out);
+          auto currentColor = ColoredStream::Colors::WHITE;
+          for (unsigned i = 0; i < highlightLine.size(); ++i) {
+            llvm::raw_ostream::Colors charColor;
+            switch (highlightLine[i]) {
+            case '+':
+              charColor = ColoredStream::Colors::GREEN;
+              break;
+            case '-':
+              charColor = ColoredStream::Colors::RED;
+              break;
+            case '~':
+              charColor = ColoredStream::Colors::BLUE;
+              break;
+            default:
+              charColor = ColoredStream::Colors::WHITE;
+              break;
+            }
+            if (currentColor != charColor) {
+              currentColor = charColor;
+              Out.changeColor(charColor, /*bold*/ true);
+            }
+            Out << highlightLine[i];
+          }
+          Out.resetColor();
+          Out << "\n";
+        }
+      }
+
+      // Print each message on its own line below the source line. If the source
+      // line is ASCII, we can insert a caret pointing directly to the message
+      // location. If not, use a more generic "-->" indicator.
+      // FIXME: Improve Unicode support so every message can include a direct
+      // location indicator.
+      for (auto msg : Messages) {
+        printEmptyGutter(LineNumberIndent, Out);
+        if (isASCII) {
+          Out << std::string(byteToColumnMap[msg.Byte], ' ') << "^ ";
+          printDiagnosticKind(msg.Kind, Out);
+          Out << " " << msg.Text << "\n";
+        } else {
+          Out.changeColor(ColoredStream::Colors::BLUE, /*bold*/true);
+          Out << "--> ";
+          Out.resetColor();
+          printDiagnosticKind(msg.Kind, Out);
+          Out << " " << msg.Text << "\n";
+        }
+      }
+      delete[] byteToColumnMap;
+    }
+  };
+
+  /// Represents an excerpt of a source file which contains one or more
+  /// annotated source lines.
+  class AnnotatedFileExcerpt {
+    SourceManager &SM;
+    unsigned BufferID;
+    /// The primary location of the parent error/warning/remark for this
+    /// diagnostic message. This is printed alongside the file path so it can be
+    /// parsed by editors and other tooling.
+    SourceLoc PrimaryLoc;
+    std::vector<AnnotatedLine> AnnotatedLines;
+
+    /// Return the AnnotatedLine for a given SourceLoc, creating it if it
+    /// doesn't already exist.
+    AnnotatedLine &lineForLoc(SourceLoc Loc) {
+      // FIXME: This call to `getLineAndColumn` is expensive.
+      unsigned lineNo = SM.getLineAndColumn(Loc).first;
+      AnnotatedLine newLine(lineNo, "");
+      auto iter =
+          std::lower_bound(AnnotatedLines.begin(), AnnotatedLines.end(),
+                           newLine, [](AnnotatedLine l1, AnnotatedLine l2) {
+                             return l1.getLineNumber() < l2.getLineNumber();
+                           });
+      if (iter == AnnotatedLines.end() || iter->getLineNumber() != lineNo) {
+        newLine.LineText = SM.getLineString(BufferID, lineNo);
+        return *AnnotatedLines.insert(iter, newLine);
+      } else {
+        return *iter;
+      }
+    }
+
+    unsigned getLineNumberIndent() {
+      // The lines are already in sorted ascending order, and we render one line
+      // after the last one for context. Use the last line number plus one to
+      // determine the indent.
+      return floor(1 + log10(AnnotatedLines.back().getLineNumber() + 1));
+    }
+
+    void printNumberedLine(SourceManager &SM, unsigned BufferID,
+                           unsigned LineNumber, unsigned LineNumberIndent,
+                           raw_ostream &Out) {
+      printNumberedGutter(LineNumber, LineNumberIndent, Out);
+      Out << SM.getLineString(BufferID, LineNumber) << "\n";
+    }
+
+    void lineRangesForRange(CharSourceRange Range,
+                            SmallVectorImpl<CharSourceRange> &LineRanges) {
+      // FIXME: The calls to `getLineAndColumn` and `getLocForLineCol` are
+      // expensive.
+      unsigned startLineNo = SM.getLineAndColumn(Range.getStart()).first;
+      unsigned endLineNo = SM.getLineAndColumn(Range.getEnd()).first;
+
+      if (startLineNo == endLineNo) {
+        LineRanges.push_back(Range);
+        return;
+      }
+
+      // Split the range by line.
+      SourceLoc lineEnd = SM.getLocForOffset(
+          BufferID, *SM.resolveOffsetForEndOfLine(BufferID, startLineNo));
+      LineRanges.push_back(CharSourceRange(SM, Range.getStart(), lineEnd));
+
+      for (unsigned intermediateLine = startLineNo + 1;
+           intermediateLine < endLineNo; ++intermediateLine) {
+        SourceLoc lineStart =
+            SM.getLocForLineCol(BufferID, intermediateLine, 1);
+        SourceLoc lineEnd = SM.getLocForOffset(
+            BufferID,
+            *SM.resolveOffsetForEndOfLine(BufferID, intermediateLine));
+        LineRanges.push_back(CharSourceRange(SM, lineStart, lineEnd));
+      }
+
+      SourceLoc lastLineStart = SM.getLocForLineCol(BufferID, endLineNo, 1);
+      LineRanges.push_back(CharSourceRange(SM, lastLineStart, Range.getEnd()));
+    }
+
+  public:
+    AnnotatedFileExcerpt(SourceManager &SM, unsigned BufferID,
+                         SourceLoc PrimaryLoc)
+        : SM(SM), BufferID(BufferID), PrimaryLoc(PrimaryLoc) {}
+
+    void addMessage(SourceLoc Loc, DiagnosticKind Kind, StringRef Message) {
+      lineForLoc(Loc).addMessage(SM, Loc, Kind, Message);
+    }
+
+    void addHighlight(CharSourceRange Range) {
+      SmallVector<CharSourceRange, 1> ranges;
+      lineRangesForRange(Range, ranges);
+      for (auto lineRange : ranges)
+        lineForLoc(lineRange.getStart()).addHighlight(SM, lineRange);
+    }
+
+    void addFixIt(CharSourceRange Range, StringRef Text) {
+      SmallVector<CharSourceRange, 1> ranges;
+      lineRangesForRange(Range, ranges);
+      // The removals are broken down line-by-line, so only add any insertions
+      // to the last replacement.
+      auto last = ranges.pop_back_val();
+      lineForLoc(last.getStart()).addFixIt(SM, last, Text);
+      for (auto lineRange : ranges)
+        lineForLoc(lineRange.getStart()).addFixIt(SM, lineRange, "");
+    }
+
+    void render(raw_ostream &Out) {
+      // Tha maximum number of intermediate lines without annotations to render
+      // between annotated lines before using an ellipsis.
+      static const unsigned maxIntermediateLines = 3;
+
+      assert(!AnnotatedLines.empty() && "File excerpt has no lines");
+      unsigned lineNumberIndent = getLineNumberIndent();
+
+      // Print the file name at the top of each excerpt.
+      auto primaryLineAndColumn = SM.getLineAndColumn(PrimaryLoc);
+      Out.changeColor(ColoredStream::Colors::MAGENTA, /*bold*/ true);
+      Out << SM.getIdentifierForBuffer(BufferID) << ":"
+          << primaryLineAndColumn.first << ":" << primaryLineAndColumn.second
+          << "\n";
+      Out.resetColor();
+
+      // Print one extra line at the top for context.
+      if (AnnotatedLines.front().getLineNumber() > 1)
+        printNumberedLine(SM, BufferID,
+                          AnnotatedLines.front().getLineNumber() - 1,
+                          lineNumberIndent, Out);
+
+      // Render the first annotated line.
+      AnnotatedLines.front().render(lineNumberIndent, Out);
+      unsigned lastLineNumber = AnnotatedLines.front().getLineNumber();
+
+      // Render intermediate lines/ellipsis, followed by the next annotated
+      // line until they have all been output.
+      for (auto line = AnnotatedLines.begin() + 1; line != AnnotatedLines.end();
+           ++line) {
+        unsigned lineNumber = line->getLineNumber();
+        if (lineNumber - lastLineNumber > maxIntermediateLines) {
+          // Use an ellipsis to denote an ommitted part of the file.
+          printNumberedLine(SM, BufferID, lastLineNumber + 1, lineNumberIndent,
+                            Out);
+          Out.changeColor(ColoredStream::Colors::BLUE, true);
+          Out << llvm::formatv("{0}...\n",
+                               llvm::fmt_repeat(" ", lineNumberIndent));
+          Out.resetColor();
+          printNumberedLine(SM, BufferID, lineNumber - 1, lineNumberIndent,
+                            Out);
+        } else {
+          // Print all the intermediate lines.
+          for (unsigned l = lastLineNumber + 1; l < lineNumber; ++l) {
+            printNumberedLine(SM, BufferID, l, lineNumberIndent, Out);
+          }
+        }
+        // Print the annotated line.
+        line->render(lineNumberIndent, Out);
+        lastLineNumber = lineNumber;
+      }
+      // Print one extra line at the bottom for context.
+      printNumberedLine(
+          SM, BufferID,
+          AnnotatedLines[AnnotatedLines.size() - 1].getLineNumber() + 1,
+          lineNumberIndent, Out);
+    }
+  };
 } // end anonymous namespace
 
+namespace swift {
+/// Represents one or more annotated file snippets which together form a
+/// complete diagnostic message.
+class AnnotatedSourceSnippet {
+  SourceManager &SM;
+  std::map<unsigned, AnnotatedFileExcerpt> FileExcerpts;
+  SmallVector<std::pair<DiagnosticKind, std::string>, 1>
+      UnknownLocationMessages;
+
+  AnnotatedFileExcerpt &excerptForLoc(SourceLoc Loc) {
+    unsigned bufID = SM.findBufferContainingLoc(Loc);
+    FileExcerpts.emplace(bufID, AnnotatedFileExcerpt(SM, bufID, Loc));
+    return FileExcerpts.find(bufID)->second;
+  }
+
+public:
+  AnnotatedSourceSnippet(SourceManager &SM) : SM(SM){};
+
+  void addMessage(SourceLoc Loc, DiagnosticKind Kind, StringRef Message) {
+    if (Loc.isInvalid()) {
+      UnknownLocationMessages.push_back({Kind, Message.str()});
+      return;
+    }
+    excerptForLoc(Loc).addMessage(Loc, Kind, Message);
+  }
+
+  void addHighlight(CharSourceRange Range) {
+    if (Range.isInvalid())
+      return;
+    excerptForLoc(Range.getStart()).addHighlight(Range);
+  }
+
+  void addFixIt(CharSourceRange Range, StringRef Text) {
+    if (Range.isInvalid())
+      return;
+    excerptForLoc(Range.getStart()).addFixIt(Range, Text);
+  }
+
+  void render(raw_ostream &Out) {
+    // Print the excerpt for each file.
+    for (auto excerpt : FileExcerpts)
+      excerpt.second.render(Out);
+
+    // Handle messages with invalid locations.
+    if (!UnknownLocationMessages.empty()) {
+      Out.changeColor(ColoredStream::Colors::MAGENTA, /*bold*/ true);
+      Out << "Unknown Location\n";
+      Out.resetColor();
+    }
+    for (auto unknownMessage : UnknownLocationMessages) {
+      printEmptyGutter(2, Out);
+      printDiagnosticKind(unknownMessage.first, Out);
+      Out << " " << unknownMessage.second << "\n";
+    }
+  }
+};
+} // namespace swift
+
+static void annotateSnippetWithInfo(SourceManager &SM,
+                                    const DiagnosticInfo &Info,
+                                    AnnotatedSourceSnippet &Snippet) {
+  llvm::SmallString<256> Text;
+  {
+    llvm::raw_svector_ostream Out(Text);
+    DiagnosticEngine::formatDiagnosticText(Out, Info.FormatString,
+                                           Info.FormatArgs);
+    // Show associated fix-its as part of the message. This is a
+    // better experience when notes offer a choice of fix-its. It's redundant
+    // for fix-its which are also displayed inline, but helps improve
+    // readability in some situations.
+    if (!Info.FixIts.empty()) {
+      Out << " ";
+      describeFixIts(SM, Info.FixIts, Out);
+    }
+  }
+
+  Snippet.addMessage(Info.Loc, Info.Kind, Text);
+  for (auto range : Info.Ranges) {
+    Snippet.addHighlight(range);
+  }
+
+  // Don't print inline fix-its for notes.
+  if (Info.Kind != DiagnosticKind::Note) {
+    for (auto fixIt : Info.FixIts) {
+      Snippet.addFixIt(fixIt.getRange(), fixIt.getText());
+    }
+  }
+  // Add any explicitly grouped notes to the snippet.
+  for (auto ChildInfo : Info.ChildDiagnosticInfo) {
+    annotateSnippetWithInfo(SM, *ChildInfo, Snippet);
+  }
+}
+
+// MARK: Main DiagnosticConsumer entrypoint.
 void PrintingDiagnosticConsumer::handleDiagnostic(SourceManager &SM,
                                                   const DiagnosticInfo &Info) {
+  if (Info.Kind == DiagnosticKind::Error) {
+    DidErrorOccur = true;
+  }
+
   if (Info.IsChildNote)
     return;
 
-  printDiagnostic(SM, Info);
-  for (auto path : Info.EducationalNotePaths) {
-    if (auto buffer = SM.getFileSystem()->getBufferForFile(path))
-      Stream << buffer->get()->getBuffer() << "\n";
+  if (ExperimentalFormattingEnabled) {
+    if (Info.Kind == DiagnosticKind::Note && currentSnippet) {
+      // If this is a note and we have an in-flight message, add it to that
+      // instead of emitting it separately.
+      annotateSnippetWithInfo(SM, Info, *currentSnippet);
+    } else {
+      // If we encounter a new error/warning/remark, flush any in-flight
+      // snippets.
+      flush(/*includeTrailingBreak*/ true);
+      currentSnippet = std::make_unique<AnnotatedSourceSnippet>(SM);
+      annotateSnippetWithInfo(SM, Info, *currentSnippet);
+    }
+  } else {
+    printDiagnostic(SM, Info);
+
+    for (auto path : Info.EducationalNotePaths) {
+      if (auto buffer = SM.getFileSystem()->getBufferForFile(path))
+        Stream << buffer->get()->getBuffer() << "\n";
+    }
+
+    for (auto ChildInfo : Info.ChildDiagnosticInfo) {
+      printDiagnostic(SM, *ChildInfo);
+    }
   }
+}
 
-  for (auto ChildInfo : Info.ChildDiagnosticInfo) {
-    printDiagnostic(SM, *ChildInfo);
+void PrintingDiagnosticConsumer::flush(bool includeTrailingBreak) {
+  if (currentSnippet) {
+    if (ForceColors) {
+      ColoredStream colorStream{Stream};
+      currentSnippet->render(colorStream);
+      if (includeTrailingBreak)
+        colorStream << "\n\n";
+    } else {
+      NoColorStream noColorStream{Stream};
+      currentSnippet->render(noColorStream);
+      if (includeTrailingBreak)
+        noColorStream << "\n\n";
+    }
+    currentSnippet.reset();
   }
 }
 
+bool PrintingDiagnosticConsumer::finishProcessing() {
+  // If there's an in-flight snippet, flush it.
+  flush(false);
+  return false;
+}
+
+// MARK: LLVM style diagnostic printing
 void PrintingDiagnosticConsumer::printDiagnostic(SourceManager &SM,
                                                  const DiagnosticInfo &Info) {
 
@@ -101,10 +731,6 @@ void PrintingDiagnosticConsumer::printDiagnostic(SourceManager &SM,
     break;
   }
 
-  if (Info.Kind == DiagnosticKind::Error) {
-    DidErrorOccur = true;
-  }
-
   // Translate ranges.
   SmallVector<llvm::SMRange, 2> Ranges;
   for (auto R : Info.Ranges)
@@ -194,3 +820,31 @@ SourceManager::GetMessage(SourceLoc Loc, llvm::SourceMgr::DiagKind Kind,
                             LineStr, ColRanges, FixIts);
 }
 
+// These must come after the declaration of AnnotatedSourceSnippet due to the
+// `currentSnippet` member.
+PrintingDiagnosticConsumer::PrintingDiagnosticConsumer(
+    llvm::raw_ostream &stream)
+    : Stream(stream) {}
+PrintingDiagnosticConsumer::~PrintingDiagnosticConsumer() = default;
+
+// FIXME: This implementation is inefficient.
+std::string SourceManager::getLineString(unsigned BufferID,
+                                         unsigned LineNumber) {
+  SourceLoc Loc = getLocForLineCol(BufferID, LineNumber, 1);
+  if (Loc.isInvalid())
+    return "";
+
+  auto CurMB = LLVMSourceMgr.getMemoryBuffer(findBufferContainingLoc(Loc));
+  const char *LineStart = Loc.Value.getPointer();
+  const char *BufStart = CurMB->getBufferStart();
+  while (LineStart != BufStart && LineStart[-1] != '\n' &&
+         LineStart[-1] != '\r')
+    --LineStart;
+
+  // Get the end of the line.
+  const char *LineEnd = Loc.Value.getPointer();
+  const char *BufEnd = CurMB->getBufferEnd();
+  while (LineEnd != BufEnd && LineEnd[0] != '\n' && LineEnd[0] != '\r')
+    ++LineEnd;
+  return std::string(LineStart, LineEnd);
+}
diff --git a/lib/FrontendTool/FrontendTool.cpp b/lib/FrontendTool/FrontendTool.cpp
index ce845d43016..e5451e36a5e 100644
--- a/lib/FrontendTool/FrontendTool.cpp
+++ b/lib/FrontendTool/FrontendTool.cpp
@@ -2145,6 +2145,11 @@ int swift::performFrontend(ArrayRef<const char *> Args,
   if (Invocation.getDiagnosticOptions().UseColor)
     PDC.forceColors();
 
+  // Temporarily stage the new diagnostic formatting style behind
+  // -enable-descriptive-diagnostics
+  if (Invocation.getDiagnosticOptions().EnableExperimentalFormatting)
+    PDC.enableExperimentalFormatting();
+
   if (Invocation.getFrontendOptions().DebugTimeCompilation)
     SharedTimer::enableCompilationTimers();
 
diff --git a/lib/Immediate/REPL.cpp b/lib/Immediate/REPL.cpp
index cf9315ac6de..e913d0217a6 100644
--- a/lib/Immediate/REPL.cpp
+++ b/lib/Immediate/REPL.cpp
@@ -850,6 +850,11 @@ private:
   }
 
   bool executeSwiftSource(llvm::StringRef Line, const ProcessCmdLine &CmdLine) {
+    SWIFT_DEFER {
+      // Always flush diagnostic consumers after executing a line.
+      CI.getDiags().flushConsumers();
+    };
+
     // Parse the current line(s).
     auto InputBuf = llvm::MemoryBuffer::getMemBufferCopy(Line, "<REPL Input>");
     SmallString<8> Name{"REPL_"};
diff --git a/test/SourceKit/Sema/educational_note_diags.swift b/test/SourceKit/Sema/educational_note_diags.swift
index 0965ad345b8..b0f859292cb 100644
--- a/test/SourceKit/Sema/educational_note_diags.swift
+++ b/test/SourceKit/Sema/educational_note_diags.swift
@@ -1,6 +1,6 @@
 extension (Int, Int) {}
 
-// RUN: %sourcekitd-test -req=sema %s -- -Xfrontend -enable-descriptive-diagnostics -Xfrontend -diagnostic-documentation-path -Xfrontend /educational/notes/path/prefix %s | %FileCheck %s -check-prefix=DESCRIPTIVE
+// RUN: %sourcekitd-test -req=sema %s -- -Xfrontend -enable-educational-notes -Xfrontend -diagnostic-documentation-path -Xfrontend /educational/notes/path/prefix %s | %FileCheck %s -check-prefix=DESCRIPTIVE
 
 // DESCRIPTIVE:      key.description: "non-nominal type
 // DESCRIPTIVE:      key.educational_note_paths: [
diff --git a/test/diagnostics/educational-notes.swift b/test/diagnostics/educational-notes.swift
index a74ceb455da..8cafa26c289 100644
--- a/test/diagnostics/educational-notes.swift
+++ b/test/diagnostics/educational-notes.swift
@@ -1,4 +1,4 @@
-// RUN: not %target-swift-frontend -enable-descriptive-diagnostics -diagnostic-documentation-path %S/test-docs/ -typecheck %s 2>&1 | %FileCheck %s
+// RUN: not %target-swift-frontend -enable-educational-notes -diagnostic-documentation-path %S/test-docs/ -typecheck %s 2>&1 | %FileCheck %s
 
 // A diagnostic with no educational notes
 let x = 1 +
diff --git a/test/diagnostics/pretty-printed-diagnostics.swift b/test/diagnostics/pretty-printed-diagnostics.swift
new file mode 100644
index 00000000000..e58a5691738
--- /dev/null
+++ b/test/diagnostics/pretty-printed-diagnostics.swift
@@ -0,0 +1,165 @@
+// RUN: not %target-swift-frontend -enable-experimental-diagnostic-formatting -typecheck %s 2>&1 | %FileCheck %s
+
+1 + 2
+
+func foo(a: Int, b: Int) {
+  a + b
+}
+
+foo(b: 1, a: 2)
+
+
+func baz() {
+  bar(a: "hello, world!")
+}
+
+struct Foo {
+  var x: Int
+  var x: Int
+}
+
+func bar(a: Int) {}
+func bar(a: Float) {}
+
+
+func bazz() throws {
+
+}
+bazz()
+
+struct A {}
+extension A {
+  let x: Int = { 42 }
+}
+
+let abc = "ðŸ‘
+
+let x = {
+  let y = 1
+  return y
+}
+
+struct B: Decodable {
+  let a: Foo
+}
+
+// The line below is indented with tabs, not spaces.
+			foo(b: 1, a: 2)
+
+let ðŸ‘ðŸ‘ðŸ‘ = {
+  let y = 1
+  return y
+}
+
+// Test fallback for non-ASCII characters.
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:11
+// CHECK: [[#LINE-1]] |
+// CHECK: [[#LINE]]   | let abc = "ðŸ‘
+// CHECK:             | --> error: unterminated string literal
+// CHECK: [[#LINE+1]] |
+
+// Test underlining.
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:3
+// CHECK: [[#LINE-1]] |
+// CHECK: [[#LINE]]   | 1 + 2
+// CHECK:             | ~   ~
+// CHECK:             |   ^ warning: result of operator '+' is unused
+// CHECK: [[#LINE+1]] |
+
+// Test inline fix-it rendering.
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:11
+// CHECK:  [[#LINE-1]] |
+// CHECK:  [[#LINE]]   | foo(a: 2, b: 1, a: 2)
+// CHECK:              |     ++++++~~~~------
+// CHECK:              |                 ^ error: argument 'a' must precede argument 'b' [remove ', a: 2' and insert 'a: 2, ']
+// CHECK: [[#LINE+1]]  |
+
+
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:7
+// CHECK: [[#LINE-2]] | struct Foo {
+// CHECK: [[#LINE-1]] |   var x: Int
+// CHECK:             |       ^ note: 'x' previously declared here
+// CHECK: [[#LINE]]   |   var x: Int
+// CHECK:             |       ^ error: invalid redeclaration of 'x'
+// CHECK: [[#LINE+1]] | }
+
+// Test out-of-line fix-its on notes.
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:1
+// CHECK: [[#LINE-1]] | }
+// CHECK: [[#LINE]]   | bazz()
+// CHECK:             | ~~~~~~
+// CHECK:             | ^ error: call can throw but is not marked with 'try'
+// CHECK:             | ^ note: did you mean to use 'try'? [insert 'try ']
+// CHECK:             | ^ note: did you mean to handle error as optional value? [insert 'try? ']
+// CHECK:             | ^ note: did you mean to disable error propagation? [insert 'try! ']
+// CHECK: [[#LINE+1]] |
+
+
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:7
+// CHECK: [[#LINE-1]] | extension A {
+// CHECK: [[#LINE]]   |   let x: Int = { 42 }
+// CHECK:             |       ^ error: extensions must not contain stored properties
+// CHECK: [[#LINE+1]] | }
+
+// Test complex out-of-line fix-its.
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:16
+// CHECK: [[#LINE-1]] | extension A {
+// CHECK: [[#LINE]]   |   let x: Int = { 42 }()
+// CHECK:             |                ~~~~~~++
+// CHECK:             |                ^ error: function produces expected type 'Int'; did you mean to call it with '()'?
+// CHECK:             |                ^ note: Remove '=' to make 'x' a computed property [remove '= ' and replace 'let' with 'var']
+// CHECK: [[#LINE+1]] | }
+
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:9
+// CHECK: [[#LINE-1]] |
+// CHECK: [[#LINE]]   | let x = { () -> Result in
+// CHECK:             |          +++++++++++++++++
+// CHECK:             |         ^ error: unable to infer complex closure return type; add explicit type to disambiguate
+// CHECK: [[#LINE+1]] |   let y = 1
+
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:8
+// CHECK: [[#LINE-1]] |
+// CHECK: [[#LINE]]   | struct B: Decodable {
+// CHECK:             |        ^ error: type 'B' does not conform to protocol 'Decodable'
+// CHECK: [[#LINE+1]] |   let a: Foo
+// CHECK:             |       ^ note: cannot automatically synthesize 'Decodable' because 'Foo' does not conform to 'Decodable'
+// CHECK: [[#LINE+2]] | }
+// CHECK: Swift.Decodable:2:5
+// CHECK: 1 | public protocol Decodable {
+// CHECK: 2 |     init(from decoder: Decoder) throws
+// CHECK:   |     ^ note: protocol requires initializer 'init(from:)' with type 'Decodable'
+// CHECK: 3 | }
+
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:14
+// CHECK: [[#LINE-1]] | // The line below is indented with tabs, not spaces.
+// CHECK: [[#LINE]]   |       foo(a: 2, b: 1, a: 2)
+// CHECK:             |           ++++++~~~~------
+// CHECK:             |                       ^ error: argument 'a' must precede argument 'b' [remove ', a: 2' and insert 'a: 2, ']
+// CHECK: [[#LINE+1]] |
+
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:20
+// CHECK: [[#LINE-1]] |
+// CHECK: [[#LINE]]   | let ðŸ‘ðŸ‘ðŸ‘ = {
+// CHECK:    | --> error: unable to infer complex closure return type; add explicit type to disambiguate [insert ' () -> <#Result#> in ']
+// CHECK: [[#LINE+1]] |   let y = 1
+
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:5
+// CHECK: [[#LINE-1]] | func foo(a: Int, b: Int) {
+// CHECK: [[#LINE]]   |   a + b
+// CHECK:             |   ~   ~
+// CHECK:             |     ^ warning: result of operator '+' is unused
+// CHECK: [[#LINE+1]] | }
+
+// Test snippet truncation.
+// CHECK: SOURCE_DIR{{[/\]+}}test{{[/\]+}}diagnostics{{[/\]+}}pretty-printed-diagnostics.swift:[[#LINE:]]:3
+// CHECK: [[#LINE-1]] | func baz() {
+// CHECK: [[#LINE]]   |   bar(a: "hello, world!")
+// CHECK:             |   ^ error: no exact matches in call to global function 'bar'
+// CHECK: [[#LINE+1]] | }
+// CHECK:   ...
+// CHECK: [[#LINE:]]  |
+// CHECK: [[#LINE+1]] | func bar(a: Int) {}
+// CHECK:             |      ^ note: candidate expects value of type 'Int' for parameter #1
+// CHECK: [[#LINE+2]] | func bar(a: Float) {}
+// CHECK:             |      ^ note: candidate expects value of type 'Float' for parameter #1
+// CHECK: [[#LINE+3]] |

commit 896d4fca0c496f9207efa5b2ae0adf63c088ec0f
Author: Dave Abrahams <dave@boostpro.com>
Date:   Sat Jan 4 15:01:15 2020 -0800

    Add efficiency docs for partitionPoint

diff --git a/test/Prototypes/Algorithms.swift b/test/Prototypes/Algorithms.swift
index 35f690df2de..24d48fa2b14 100644
--- a/test/Prototypes/Algorithms.swift
+++ b/test/Prototypes/Algorithms.swift
@@ -613,6 +613,11 @@ extension Collection {
   /// The collection must already be partitioned according to the
   /// predicate, as if `self.partition(by: predicate)` had already
   /// been called.
+  ///
+  /// - Efficiency: At most log(N) invocations of `predicate`, where 
+  ///   N is the length of `self`.  At most log(N) index offsetting
+  ///   operations if `self` conforms to `RandomAccessCollection`;
+  ///   at most N such operations otherwise.
   func partitionPoint(
     where predicate: (Element) throws -> Bool
   ) rethrows -> Index {

commit f8d15143a5610edea059863e1c6428678974f669
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Mon Mar 25 14:07:43 2019 -0700

    [closure-lifetime-fixup] Expose a flag instead of allocating a new instruction.
    
    Small inefficiency I noticed.

diff --git a/include/swift/SIL/SILInstruction.h b/include/swift/SIL/SILInstruction.h
index 4d7b7a82da6..bc0eae23844 100644
--- a/include/swift/SIL/SILInstruction.h
+++ b/include/swift/SIL/SILInstruction.h
@@ -4186,9 +4186,17 @@ class ConvertEscapeToNoEscapeInst final
          SILFunction &F, SILOpenedArchetypesState &OpenedArchetypes,
          bool lifetimeGuaranteed);
 public:
+  /// Return true if we have extended the lifetime of the argument of the
+  /// convert_escape_to_no_escape to be over all uses of the trivial type.
   bool isLifetimeGuaranteed() const {
     return lifetimeGuaranteed;
   }
+
+  /// Mark that we have extended the lifetime of the argument of the
+  /// convert_escape_to_no_escape to be over all uses of the trivial type.
+  ///
+  /// NOTE: This is a one way operation.
+  void setLifetimeGuaranteed() { lifetimeGuaranteed = true; }
 };
 
 /// ThinFunctionToPointerInst - Convert a thin function pointer to a
diff --git a/lib/SILOptimizer/Mandatory/ClosureLifetimeFixup.cpp b/lib/SILOptimizer/Mandatory/ClosureLifetimeFixup.cpp
index 6cf8f88c597..fdee924c0ba 100644
--- a/lib/SILOptimizer/Mandatory/ClosureLifetimeFixup.cpp
+++ b/lib/SILOptimizer/Mandatory/ClosureLifetimeFixup.cpp
@@ -123,12 +123,7 @@ static void extendLifetimeToEndOfFunction(SILFunction &Fn,
   auto OptionalEscapingClosureTy = SILType::getOptionalType(EscapingClosureTy);
   auto loc = RegularLocation::getAutoGeneratedLocation();
 
-  SILBuilderWithScope B(Cvt);
-  auto NewCvt = B.createConvertEscapeToNoEscape(
-      Cvt->getLoc(), Cvt->getOperand(), Cvt->getType(), true);
-  Cvt->replaceAllUsesWith(NewCvt);
-  Cvt->eraseFromParent();
-  Cvt = NewCvt;
+  Cvt->setLifetimeGuaranteed();
 
   // If our Cvt is in the initial block, we do not need to use the SSA updater
   // since we know Cvt can not be in a loop and must dominate all exits
@@ -417,14 +412,7 @@ static bool tryExtendLifetimeToLastUse(
     // Insert a copy at the convert_escape_to_noescape [not_guaranteed] and
     // change the instruction to the guaranteed form.
     auto EscapingClosure = Cvt->getOperand();
-    {
-      SILBuilderWithScope B(Cvt);
-      auto NewCvt = B.createConvertEscapeToNoEscape(
-          Cvt->getLoc(), Cvt->getOperand(), Cvt->getType(), true);
-      Cvt->replaceAllUsesWith(NewCvt);
-      Cvt->eraseFromParent();
-      Cvt = NewCvt;
-    }
+    Cvt->setLifetimeGuaranteed();
 
     SILBuilderWithScope B2(Cvt);
     auto ClosureCopy = B2.createCopyValue(loc, EscapingClosure);
@@ -510,14 +498,7 @@ static bool trySwitchEnumPeephole(ConvertEscapeToNoEscapeInst *Cvt) {
   if (!onlyDestroy)
     return false;
 
-  // Replace the convert_escape_to_noescape instruction.
-  {
-    SILBuilderWithScope B(Cvt);
-    auto NewCvt = B.createConvertEscapeToNoEscape(
-        Cvt->getLoc(), Cvt->getOperand(), Cvt->getType(), true);
-    Cvt->replaceAllUsesWith(NewCvt);
-    Cvt->eraseFromParent();
-  }
+  Cvt->setLifetimeGuaranteed();
 
   // Extend the lifetime.
   SILBuilderWithScope B(SwitchEnum1);

commit dfc2d47f3b0e97035b62133365ed2a7be3100938
Author: Andrew Trick <atrick@apple.com>
Date:   Thu Mar 7 18:01:00 2019 -0800

    Redo the data flow part of AccessEnforcementOpts.
    
    Directly implement the data flow. Eliminate the extraneous work.
    Remove cubic behavior. Do a single iteration of the data flow state at
    each program point only performing the necessary set operations. At
    unidentified access, clear the sets for simplicity and efficiency.
    
    This cleanup results in significant functional changes:
    
    - Allowing scopes to merge even if they are enclosed.
    
    - Handling unidentified access conservatively.
    
    Note that most of the added lines of code are comments.
    
    Somehow this cleanup incidentally fixes:
    <rdar://problem/48514339> swift compiler hangs building project
    
    (I expected the subsequent loop summary cleanup to fix that problem.)

diff --git a/include/swift/SILOptimizer/Analysis/AccessedStorageAnalysis.h b/include/swift/SILOptimizer/Analysis/AccessedStorageAnalysis.h
index 51d5d7f678f..64f4c8d6f9c 100644
--- a/include/swift/SILOptimizer/Analysis/AccessedStorageAnalysis.h
+++ b/include/swift/SILOptimizer/Analysis/AccessedStorageAnalysis.h
@@ -122,6 +122,8 @@ template <> struct DenseMapInfo<swift::StorageAccessInfo> {
 }
 
 namespace swift {
+using AccessedStorageSet = llvm::SmallDenseSet<StorageAccessInfo, 8>;
+
 /// Records each unique AccessedStorage in a set of StorageAccessInfo
 /// objects. Hashing and equality only sees the AccesedStorage data. The
 /// additional StorageAccessInfo bits are recorded as results of this analysis.
@@ -132,17 +134,17 @@ namespace swift {
 /// large. It does not imply that all accesses have Unidentified
 /// AccessedStorage, which is never allowed for class or global access.
 class AccessedStorageResult {
-
-  using AccessedStorageSet = llvm::SmallDenseSet<StorageAccessInfo, 8>;
-
   AccessedStorageSet storageAccessSet;
   Optional<SILAccessKind> unidentifiedAccess;
+
 public:
   AccessedStorageResult() {}
 
   // ---------------------------------------------------------------------------
   // Accessing the results.
 
+  const AccessedStorageSet &getStorageSet() const { return storageAccessSet; }
+
   bool isEmpty() const {
     return storageAccessSet.empty() && !unidentifiedAccess;
   }
diff --git a/lib/SILOptimizer/Transforms/AccessEnforcementOpts.cpp b/lib/SILOptimizer/Transforms/AccessEnforcementOpts.cpp
index faa014cc2a6..33c4e46096a 100644
--- a/lib/SILOptimizer/Transforms/AccessEnforcementOpts.cpp
+++ b/lib/SILOptimizer/Transforms/AccessEnforcementOpts.cpp
@@ -310,144 +310,169 @@ protected:
                             RegionIDToLocalStateMap &localRegionStates);
 
 private:
-  void addInScopeAccess(RegionState &state, BeginAccessInst *beginAccess);
-  void removeInScopeAccess(RegionState &state, BeginAccessInst *beginAccess);
-  void recordConflicts(RegionState &state, const AccessedStorage &currStorage);
-  void addOutOfScopeAccessInsert(RegionState &state,
-                                 BeginAccessInst *beginAccess);
-  void addOutOfScopeAccessMerge(RegionState &state, BeginAccessInst *beginAccess);
+  void recordInScopeConflicts(RegionState &state,
+                              const AccessedStorage &currStorage,
+                              SILAccessKind currKind);
+  bool removeConflicts(DenseAccessSet &accessSet,
+                       const AccessedStorage &currStorage);
+  void recordUnknownConflict(RegionState &state);
+  void recordConflicts(RegionState &state,
+                       const AccessedStorageResult &accessedStorage);
+  BeginAccessInst *findMergeableOutOfScopeAccess(RegionState &state,
+                                                 BeginAccessInst *beginAccess);
+  void insertOutOfScopeAccess(RegionState &state, BeginAccessInst *beginAccess,
+                              AccessInfo &currStorageInfo);
   void mergeAccessSet(DenseAccessSet &accessSet, const DenseAccessSet &otherSet,
                       bool isInitialized);
   void mergeState(RegionState &state, const RegionState &otherState,
                   bool isInitialized);
-  void removeConflictFromStruct(RegionState &state, DenseAccessSet &accessSet,
-                                const AccessedStorage &storage, bool isInScope);
-  void visitSetForConflicts(
-      const DenseAccessSet &accessSet, RegionState &state,
-      AccessConflictAndMergeAnalysis::AccessedStorageSet &loopStorage);
-  void
-  detectApplyConflicts(const swift::FunctionAccessedStorage &callSiteAccesses,
-                       const DenseAccessSet &conflictFreeSet,
-                       const swift::FullApplySite &fullApply, RegionState &state);
-
-  void detectMayReleaseConflicts(const DenseAccessSet &conflictFreeSet,
-                                 SILInstruction *instr, RegionState &state);
 };
 } // namespace
 
-void AccessConflictAndMergeAnalysis::addInScopeAccess(
-    RegionState &state, BeginAccessInst *beginAccess) {
-  assert(state.inScopeConflictFreeAccesses.count(beginAccess) == 0
-         && "the begin_access should not have been in Vec.");
-  state.inScopeConflictFreeAccesses.insert(beginAccess);
-}
+// Mark any in-scope access that conflicts with an access to 'currStorage' for
+// the given 'beginAccess' as having a nested conflict.
+void AccessConflictAndMergeAnalysis::recordInScopeConflicts(
+    RegionState &state, const AccessedStorage &currStorage,
+    SILAccessKind currKind) {
+  // It is tempting to combine loop with the loop in removeConflicts, which also
+  // checks isDistinctFrom for each element. However, since SetVector does not
+  // support 'llvm::erase_if', it is actually more efficient to do the removal
+  // in a separate 'remove_if' loop.
+  llvm::for_each(state.inScopeConflictFreeAccesses, [&](BeginAccessInst *bai) {
+    auto &accessInfo = result.getAccessInfo(bai);
+    if (accessKindMayConflict(currKind, bai->getAccessKind())
+        && !accessInfo.isDistinctFrom(currStorage)) {
 
-void AccessConflictAndMergeAnalysis::removeInScopeAccess(
-    RegionState &state, BeginAccessInst *beginAccess) {
-  auto it = std::find(state.inScopeConflictFreeAccesses.begin(),
-                      state.inScopeConflictFreeAccesses.end(), beginAccess);
-  assert(it != state.inScopeConflictFreeAccesses.end()
-         && "the begin_access should have been in Vec.");
-  state.inScopeConflictFreeAccesses.erase(it);
+      accessInfo.setSeenNestedConflict();
+      LLVM_DEBUG(llvm::dbgs() << "  may conflict with:\n"; accessInfo.dump());
+    }
+  });
 }
 
-// Update data flow `state` by removing accesses that conflict with the
-// currently accessed `storage`. For in-scope accesses, also mark conflicting
-// scopes with SeenNestedConflict.
-void AccessConflictAndMergeAnalysis::recordConflicts(
-    RegionState &state, const AccessedStorage &currStorage) {
-  // Remove any out-of-scope conflicts.
-  state.outOfScopeConflictFreeAccesses.remove_if([&](BeginAccessInst *bai) {
+// Remove any accesses in accessSet that may conflict with the given storage
+// location, currStorageInfo.
+//
+// Return true if any set elements were removed.
+bool AccessConflictAndMergeAnalysis::removeConflicts(
+    DenseAccessSet &accessSet, const AccessedStorage &currStorage) {
+  return accessSet.remove_if([&](BeginAccessInst *bai) {
     auto &storage = result.getAccessInfo(bai);
     return !storage.isDistinctFrom(currStorage);
   });
+}
 
-  // Since SetVector does not support `llvm::erase_if`, we use two loops. One to
-  // mark conflicts and another to remove them all via `remove_if`.
+void AccessConflictAndMergeAnalysis::recordUnknownConflict(RegionState &state) {
+  // Mark all open scopes as having a nested conflict.
   llvm::for_each(state.inScopeConflictFreeAccesses, [&](BeginAccessInst *bai) {
-    auto &ai = result.getAccessInfo(bai);
-    if (!ai.isDistinctFrom(currStorage))
-      ai.setSeenNestedConflict();
-  });
-
-  state.inScopeConflictFreeAccesses.remove_if([&](BeginAccessInst *bai) {
-    auto &storage = result.getAccessInfo(bai);
-    return !storage.isDistinctFrom(currStorage);
+    auto &accessInfo = result.getAccessInfo(bai);
+    accessInfo.setSeenNestedConflict();
+    LLVM_DEBUG(llvm::dbgs() << "  may conflict with:\n"; accessInfo.dump());
   });
+  // Clear data flow.
+  state.inScopeConflictFreeAccesses.clear();
+  state.outOfScopeConflictFreeAccesses.clear();
+  // FIXME!!!: call reset() after removing RegionState.unidentifiedAccess.
 }
 
-void AccessConflictAndMergeAnalysis::addOutOfScopeAccessInsert(
-    RegionState &state, BeginAccessInst *beginAccess) {
-  auto newStorageInfo = result.getAccessInfo(beginAccess);
-  auto pred = [&](BeginAccessInst *it) {
-    auto currStorageInfo = result.getAccessInfo(it);
-    return currStorageInfo.hasIdenticalBase(newStorageInfo);
-  };
-
-  auto it = std::find_if(state.outOfScopeConflictFreeAccesses.rbegin(),
-                         state.outOfScopeConflictFreeAccesses.rend(), pred);
+// Update data flow `state` by removing accesses that conflict with the
+// currently accessed `storage`. For in-scope accesses, also mark conflicting
+// scopes with SeenNestedConflict.
+//
+// Removing access from the out-of-scope set is important for two reasons:
+//
+// 1. Let A & B be conflicting out-of-scope, where A's scope ends before B. If
+// data flow then encounters scope C with the same storage as B, it should be
+// able to merge them. This is safe regardless of whether A & B overlap because
+// it doesn't introduce any conflict that wasn't already present. However,
+// leaving A in the out-of-scope set means that we won't be able to merge B & C
+// based on this dataflow.
+//
+// 2. Without removing conflicting scopes, the access set is unbounded and this
+// data flow could scale quadratically with the function size.
+void AccessConflictAndMergeAnalysis::recordConflicts(
+    RegionState &state, const AccessedStorageResult &accessedStorage) {
 
-  if (it == state.outOfScopeConflictFreeAccesses.rend()) {
-    state.outOfScopeConflictFreeAccesses.insert(beginAccess);
-  } else {
-    // we have a nested read case:
-    /*%4 = begin_access [read] [dynamic] %0 : $*X
-     %5 = load %4 : $*X
-     %7 = begin_access [read] [dynamic] %0 : $*X
-     %8 = load %7 : $*X
-     end_access %7 : $*X
-     end_access %4 : $*X*/
-    // we should remove the current one and insert the new.
-    auto *otherBegin = *it;
-    auto rmIt =
-        std::find(state.outOfScopeConflictFreeAccesses.begin(),
-                  state.outOfScopeConflictFreeAccesses.end(), otherBegin);
-    state.outOfScopeConflictFreeAccesses.erase(rmIt);
-    state.outOfScopeConflictFreeAccesses.insert(beginAccess);
+  if (accessedStorage.hasUnidentifiedAccess()) {
+    recordUnknownConflict(state);
+    return;
   }
-}
+  for (const StorageAccessInfo &currStorage : accessedStorage.getStorageSet()) {
 
-void AccessConflictAndMergeAnalysis::addOutOfScopeAccessMerge(
-    RegionState &state, BeginAccessInst *beginAccess) {
-  auto newStorageInfo = result.getAccessInfo(beginAccess);
-  auto pred = [&](BeginAccessInst *it) {
-    auto currStorageInfo = result.getAccessInfo(it);
-    return currStorageInfo.hasIdenticalBase(newStorageInfo);
-  };
+    recordInScopeConflicts(state, currStorage, currStorage.getAccessKind());
 
-  auto it = std::find_if(state.outOfScopeConflictFreeAccesses.rbegin(),
-                         state.outOfScopeConflictFreeAccesses.rend(), pred);
+    removeConflicts(state.inScopeConflictFreeAccesses, currStorage);
 
-  if (it == state.outOfScopeConflictFreeAccesses.rend()) {
-    // We don't have a match in outOfScopeConflictFreeAccesses - return
-    return;
+    removeConflicts(state.outOfScopeConflictFreeAccesses, currStorage);
   }
+}
 
-  auto *otherBegin = *it;
-  auto rmIt = std::find(state.outOfScopeConflictFreeAccesses.begin(),
-                        state.outOfScopeConflictFreeAccesses.end(), otherBegin);
-  state.outOfScopeConflictFreeAccesses.erase(rmIt);
+// Check if the current BeginAccessInst has identical storage with an
+// out-of-scope access. If so, remove the access from the set and return it.
+BeginAccessInst *AccessConflictAndMergeAnalysis::findMergeableOutOfScopeAccess(
+    RegionState &state, BeginAccessInst *beginAccess) {
 
-  auto predDistinct = [&](BeginAccessInst *it) {
-    auto currStorageInfo = result.getAccessInfo(it);
-    return !currStorageInfo.isDistinctFrom(newStorageInfo);
-  };
+  auto currStorageInfo = result.getAccessInfo(beginAccess);
+
+  // Before removing any conflicting accesses, find one with identical storage.
+  auto identicalStorageIter = llvm::find_if(
+      state.outOfScopeConflictFreeAccesses, [&](BeginAccessInst *bai) {
+        auto storageInfo = result.getAccessInfo(bai);
+        return storageInfo.hasIdenticalBase(currStorageInfo);
+      });
+  if (identicalStorageIter == state.outOfScopeConflictFreeAccesses.end())
+    return nullptr;
+
+  // Remove the matching access before checking for other conflicts.  Since we
+  // only check for a single identical storage access above, leaving multiple
+  // accesses of the same storage in the set would appear as a conflict in the
+  // check below when processing subsequent mergeable accesses.
+  BeginAccessInst *mergeableAccess = *identicalStorageIter;
+  state.outOfScopeConflictFreeAccesses.erase(identicalStorageIter);
+
+  // Given a mergeableAccess, 'A', another out-of-scope access, 'B', and the
+  // current access, 'C' which has identical storage as 'A', the only situation
+  // in which it is illegal to merge 'A' with 'C' is when 'B' has non-distinct
+  // storage from 'A'/'C' and 'B' begins after 'A' and ends before 'C'. This
+  // would introduce a false conflict. Since it is impossible to determine here
+  // whether 'A' and 'B' overlap, we assume they do not and avoid merging. The
+  // case in which they actually do overlap is an unimportant to optimize.
+  if (llvm::any_of(state.outOfScopeConflictFreeAccesses,
+                   [&](BeginAccessInst *bai) {
+                     auto storageInfo = result.getAccessInfo(bai);
+                     return !storageInfo.isDistinctFrom(currStorageInfo);
+                   })) {
+    return nullptr;
+  }
+  return mergeableAccess;
+}
 
-  auto itNotDistinct =
-      std::find_if(state.outOfScopeConflictFreeAccesses.begin(),
-                   state.outOfScopeConflictFreeAccesses.end(), predDistinct);
-
-  if (itNotDistinct == state.outOfScopeConflictFreeAccesses.end()) {
-    LLVM_DEBUG(llvm::dbgs() << "Found mergable pair: " << *otherBegin << ", "
-                            << *beginAccess << "\n");
-    result.mergePairs.push_back(std::make_pair(otherBegin, beginAccess));
-  } else {
-    while (itNotDistinct != state.outOfScopeConflictFreeAccesses.end()) {
-      state.outOfScopeConflictFreeAccesses.erase(itNotDistinct);
-      itNotDistinct = std::find_if(state.outOfScopeConflictFreeAccesses.begin(),
-                                   state.outOfScopeConflictFreeAccesses.end(),
-                                   predDistinct);
-    }
+// Add the given access to the out-of-scope set, replacing any existing
+// out-of-scope access on the same storage. An access to the same storage may
+// already be out-of-scope, for example, if there are nested reads:
+//
+// %4 = begin_access [read] [dynamic] %0 : $*X
+// %5 = load %4 : $*X
+// %7 = begin_access [read] [dynamic] %0 : $*X
+// %8 = load %7 : $*X
+// end_access %7 : $*X
+// end_access %4 : $*X
+//
+// The inner scope needs to be replaced with the outer scope so that scope
+// nesting is preserved when merging scopes.
+void AccessConflictAndMergeAnalysis::insertOutOfScopeAccess(
+    RegionState &state, BeginAccessInst *beginAccess,
+    AccessInfo &currStorageInfo) {
+
+  auto identicalStorageIter = llvm::find_if(
+      state.outOfScopeConflictFreeAccesses, [&](BeginAccessInst *bai) {
+        auto storageInfo = result.getAccessInfo(bai);
+        return storageInfo.hasIdenticalBase(currStorageInfo);
+      });
+  if (identicalStorageIter == state.outOfScopeConflictFreeAccesses.end())
+    state.outOfScopeConflictFreeAccesses.insert(beginAccess);
+  else {
+    state.outOfScopeConflictFreeAccesses.erase(identicalStorageIter);
+    state.outOfScopeConflictFreeAccesses.insert(beginAccess);
   }
 }
 
@@ -609,58 +634,37 @@ void AccessConflictAndMergeAnalysis::visitBeginAccess(
   auto &beginAccessInfo = result.getAccessInfo(beginAccess);
   if (beginAccessInfo.getKind() == AccessedStorage::Unidentified) {
     state.unidentifiedAccess = true;
+    recordUnknownConflict(state);
+    return;
   }
-  SILAccessKind beginAccessKind = beginAccess->getAccessKind();
-  // check the current in-scope accesses for conflicts:
-  bool changed = false;
-  do {
-    changed = false;
-    for (auto *outerBeginAccess : state.getInScopeAccesses()) {
-      // If both are reads, keep the mapped access.
-      if (!accessKindMayConflict(beginAccessKind,
-                                 outerBeginAccess->getAccessKind())) {
-        continue;
-      }
-
-      auto &outerAccessInfo = result.getAccessInfo(outerBeginAccess);
-      // If there is no potential conflict, leave the outer access mapped.
-      if (outerAccessInfo.isDistinctFrom(beginAccessInfo))
-        continue;
-
-      LLVM_DEBUG(beginAccessInfo.dump();
-                 llvm::dbgs() << "  may conflict with:\n";
-                 outerAccessInfo.dump());
 
-      recordConflicts(state, outerAccessInfo);
-      changed = true;
-      break;
-    }
-  } while (changed);
-
-  // Record the current access to InScopeAccesses.
-  // It can potentially be folded
-  // regardless of whether it may conflict with an outer access.
-  addInScopeAccess(state, beginAccess);
-  // We can merge out-of-scope regardless of having a conflict within a scope,
-  // normally, it would have made more sense to add it to out-of-scope set
-  // *only* after encountering the end_access instruction.
-  // However, that will lose us some valid optimization potential:
-  // consider the following pseudo-SIL:
+  // Mark in-scope accesses that now have nested conflicts.
+  recordInScopeConflicts(state, beginAccessInfo, beginAccess->getAccessKind());
+  // Remove in-scope conflicts to avoid checking them again.
+  removeConflicts(state.inScopeConflictFreeAccesses, beginAccessInfo);
+  // Always record the current access as in-scope. It can potentially be folded
+  // to [no_nested_conflict] independent of any enclosing access conflicts.
+  bool inserted = state.inScopeConflictFreeAccesses.insert(beginAccess);
+  (void)inserted;
+  assert(inserted && "the begin_access should not have been seen yet.");
+
+  // Find an out-of-scope access that is mergeable with this access. This is
+  // done at the BeginAccess because it doesn't matter whether the merged access
+  // has any nested conflicts. Consider the following mergeable accesses:
+  //
   // begin_access %x
   // end_access %x
   // begin_access %x
   // conflict
   // end_access %x
-  // we can merge both of these scopes
-  // but, if we only add the instr. after seeing end_access,
-  // then we would not have the first begin_access in out-of-scope
-  // set when encoutnering the 2nd end_access due to "conflict"
-  // NOTE: What we really want to do here is to check if
-  // we should add the new beginAccess to 'mergePairs' structure
-  // the reason for calling this method is to check for that.
-  // logically, we only need to add an instructio to
-  // out-of-scope conflict-free set when we visit end_access
-  addOutOfScopeAccessMerge(state, beginAccess);
+  if (BeginAccessInst *mergeableAccess =
+          findMergeableOutOfScopeAccess(state, beginAccess)) {
+    LLVM_DEBUG(llvm::dbgs() << "Found mergable pair: " << *mergeableAccess
+                            << ", " << *beginAccess << "\n");
+    result.mergePairs.emplace_back(mergeableAccess, beginAccess);
+  }
+  // For the purpose of data-flow, removing the out-of-scope access does not
+  // need to be done until the corresponding EndAccess is seen.
 }
 
 void AccessConflictAndMergeAnalysis::visitEndAccess(EndAccessInst *endAccess,
@@ -668,55 +672,21 @@ void AccessConflictAndMergeAnalysis::visitEndAccess(EndAccessInst *endAccess,
   auto *beginAccess = endAccess->getBeginAccess();
   if (beginAccess->getEnforcement() != SILAccessEnforcement::Dynamic)
     return;
-  auto &inScope = state.getInScopeAccesses();
-  auto it = std::find(inScope.begin(), inScope.end(), beginAccess);
-  if (it != inScope.end()) {
+
+  // Remove the corresponding in-scope access (it is no longer in-scope).
+  if (state.inScopeConflictFreeAccesses.remove(beginAccess)) {
     LLVM_DEBUG(llvm::dbgs() << "No conflict on one path from " << *beginAccess
                             << " to " << *endAccess);
-    removeInScopeAccess(state, beginAccess);
-  }
-
-  // If this exact instruction is already in out-of-scope - skip:
-  if (state.outOfScopeConflictFreeAccesses.count(beginAccess) > 0) {
-    return;
   }
-  // Else we have the opposite situation to the one described in
-  // visitBeginAccess: the first scope is the one conflicting while the second
-  // does not - begin_access %x conflict end_access %x begin_access %x
-  // end_access %x
-  // when seeing the conflict we remove the first begin instruction
-  // but, we can still merge those scopes *UNLESS* there's a conflict
-  // between the first end_access and the second begin_access
-  LLVM_DEBUG(llvm::dbgs() << "Got out of scope from " << *beginAccess << " to "
-                          << *endAccess << "\n");
-
-  addOutOfScopeAccessInsert(state, beginAccess);
-}
-
-void AccessConflictAndMergeAnalysis::detectApplyConflicts(
-    const swift::FunctionAccessedStorage &callSiteAccesses,
-    const DenseAccessSet &conflictFreeSet,
-    const swift::FullApplySite &fullApply, RegionState &state) {
-  bool changed = false;
-  do {
-    changed = false;
-    for (auto *outerBeginAccess : conflictFreeSet) {
-      // If there is no potential conflict, leave the outer access mapped.
-      SILAccessKind accessKind = outerBeginAccess->getAccessKind();
-      AccessInfo &outerAccessInfo = result.getAccessInfo(outerBeginAccess);
-      if (!callSiteAccesses.mayConflictWith(accessKind, outerAccessInfo))
-        continue;
 
-      LLVM_DEBUG(
-          llvm::dbgs() << *fullApply.getInstruction() << "  call site access: ";
-          callSiteAccesses.dump(); llvm::dbgs() << "  may conflict with:\n";
-          outerAccessInfo.dump());
+  // Any out-of-scope access with non-distinct storage is now longer mergeable.
+  // If this access doesn't currently overlap with it, then merging it with
+  // another later access could introduce a conflict with this access.
+  auto currStorageInfo = result.getAccessInfo(beginAccess);
+  removeConflicts(state.outOfScopeConflictFreeAccesses, currStorageInfo);
 
-      recordConflicts(state, outerAccessInfo);
-      changed = true;
-      break;
-    }
-  } while (changed);
+  // This access is now out-of-scope access; inform data flow.
+  insertOutOfScopeAccess(state, beginAccess, currStorageInfo);
 }
 
 void AccessConflictAndMergeAnalysis::visitFullApply(FullApplySite fullApply,
@@ -724,45 +694,44 @@ void AccessConflictAndMergeAnalysis::visitFullApply(FullApplySite fullApply,
   FunctionAccessedStorage callSiteAccesses;
   ASA->getCallSiteEffects(callSiteAccesses, fullApply);
 
-  detectApplyConflicts(callSiteAccesses, state.getInScopeAccesses(), fullApply,
-                       state);
-  detectApplyConflicts(callSiteAccesses, state.getOutOfScopeAccesses(),
-                       fullApply, state);
+  LLVM_DEBUG(llvm::dbgs() << "Visiting: " << *fullApply.getInstruction()
+                          << "  call site accesses: ";
+             callSiteAccesses.dump());
+  recordConflicts(state, callSiteAccesses.getResult());
 }
 
-void AccessConflictAndMergeAnalysis::detectMayReleaseConflicts(
-    const DenseAccessSet &conflictFreeSet, SILInstruction *instr,
-    RegionState &state) {
+void AccessConflictAndMergeAnalysis::visitMayRelease(SILInstruction *instr,
+                                                     RegionState &state) {
   // TODO Introduce "Pure Swift" deinitializers
   // We can then make use of alias information for instr's operands
   // If they don't alias - we might get away with not recording a conflict
-  bool changed = false;
-  do {
-    changed = false;
-    for (auto *outerBeginAccess : conflictFreeSet) {
-      // Only class and global access that may alias would conflict
-      AccessInfo &outerAccessInfo = result.getAccessInfo(outerBeginAccess);
-      const AccessedStorage::Kind outerKind = outerAccessInfo.getKind();
-      if (outerKind != AccessedStorage::Class &&
-          outerKind != AccessedStorage::Global) {
-        continue;
-      }
-      // We can't prove what the deinitializer might do
-      // TODO Introduce "Pure Swift" deinitializers
-      LLVM_DEBUG(llvm::dbgs() << "MayRelease Instruction: " << *instr
-                              << "  may conflict with:\n";
-                 outerAccessInfo.dump());
-      recordConflicts(state, outerAccessInfo);
-      changed = true;
-      break;
+  LLVM_DEBUG(llvm::dbgs() << "MayRelease Instruction: " << *instr);
+
+  // This is similar to recordUnknownConflict, but only class and and global
+  // accesses can be affected by a deinitializer.
+  auto isHeapAccess = [](AccessedStorage::Kind accessKind) {
+    return accessKind == AccessedStorage::Class
+           || accessKind == AccessedStorage::Global;
+  };
+  // Mark the in-scope accesses as having a nested conflict
+  llvm::for_each(state.inScopeConflictFreeAccesses, [&](BeginAccessInst *bai) {
+    auto &accessInfo = result.getAccessInfo(bai);
+    if (isHeapAccess(accessInfo.getKind())) {
+      accessInfo.setSeenNestedConflict();
+      LLVM_DEBUG(llvm::dbgs() << "  may conflict with:\n"; accessInfo.dump());
     }
-  } while (changed);
-}
+  });
 
-void AccessConflictAndMergeAnalysis::visitMayRelease(SILInstruction *instr,
-                                                     RegionState &state) {
-  detectMayReleaseConflicts(state.getInScopeAccesses(), instr, state);
-  detectMayReleaseConflicts(state.getOutOfScopeAccesses(), instr, state);
+  // Remove both in-scope and out-of-scope accesses from
+  // the data flow state.
+  state.inScopeConflictFreeAccesses.remove_if([&](BeginAccessInst *bai) {
+    auto &accessInfo = result.getAccessInfo(bai);
+    return isHeapAccess(accessInfo.getKind());
+  });
+  state.outOfScopeConflictFreeAccesses.remove_if([&](BeginAccessInst *bai) {
+    auto &accessInfo = result.getAccessInfo(bai);
+    return isHeapAccess(accessInfo.getKind());
+  });
 }
 
 // Merge the data flow result in 'otherSet' into 'accessSet'.  If 'accessSet' is
@@ -818,39 +787,28 @@ void AccessConflictAndMergeAnalysis::mergePredAccesses(
   }
 }
 
-void AccessConflictAndMergeAnalysis::visitSetForConflicts(
-    const DenseAccessSet &accessSet, RegionState &state,
-    AccessConflictAndMergeAnalysis::AccessedStorageSet &loopStorage) {
-  bool changed = false;
-  do {
-    changed = false;
-    for (BeginAccessInst *beginAccess : accessSet) {
-      AccessInfo &accessInfo = result.getAccessInfo(beginAccess);
-
-      for (auto loopAccess : loopStorage) {
-        if (loopAccess.isDistinctFrom(accessInfo) && !state.unidentifiedAccess)
-          continue;
-
-        recordConflicts(state, loopAccess);
-        changed = true;
-        break;
-      }
-      if (changed)
-        break;
-    }
-  } while (changed);
-}
-
 void AccessConflictAndMergeAnalysis::detectConflictsInLoop(
     LoopRegion *loopRegion, RegionIDToLocalStateMap &localRegionStates,
     LoopRegionToAccessedStorage &accessSetsOfRegions) {
   assert(loopRegion->isLoop() && "Expected a loop region");
   auto loopID = loopRegion->getID();
   RegionState &state = localRegionStates.find(loopID)->getSecond();
+
+  // FIXME!!!: just call recordConflicts instead one loop summaries are fixed.
+  if (state.unidentifiedAccess) {
+    recordUnknownConflict(state);
+    return;
+  }
   AccessedStorageSet &loopStorage =
       accessSetsOfRegions.find(loopID)->getSecond();
-  visitSetForConflicts(state.getInScopeAccesses(), state, loopStorage);
-  visitSetForConflicts(state.getOutOfScopeAccesses(), state, loopStorage);
+  for (const AccessedStorage &currStorage : loopStorage) {
+    // FIXME: The access kind will be part of the loop summary soon.
+    recordInScopeConflicts(state, currStorage, SILAccessKind::Modify);
+
+    removeConflicts(state.inScopeConflictFreeAccesses, currStorage);
+
+    removeConflicts(state.outOfScopeConflictFreeAccesses, currStorage);
+  }
 }
 
 void AccessConflictAndMergeAnalysis::localDataFlowInBlock(
diff --git a/test/SILOptimizer/access_enforcement_opts.sil b/test/SILOptimizer/access_enforcement_opts.sil
index 62a67dea258..00dbbe3b6a2 100644
--- a/test/SILOptimizer/access_enforcement_opts.sil
+++ b/test/SILOptimizer/access_enforcement_opts.sil
@@ -1352,8 +1352,9 @@ class RefElemClass {
   init()
 }
 
-// Checks that we don't crash when unable to create projection paths
-// we can't merge anything here because of that
+// Merge access overlapping scopes. Scope nesting does not need to be
+// preserved unless the nested accesses are to identical storage.
+//
 // CHECK-LABEL: sil @ref_elem_c : $@convention(thin) (RefElemClass) -> () {
 // CHECK: [[GLOBAL:%.*]] = global_addr @globalX : $*X
 // CHECK-NEXT: [[BEGIN:%.*]] = begin_access [read] [dynamic] [no_nested_conflict] [[GLOBAL]] : $*X
@@ -1362,10 +1363,8 @@ class RefElemClass {
 // CHECK-NEXT: [[REFX:%.*]] = ref_element_addr %0 : $RefElemClass, #RefElemClass.x
 // CHECK-NEXT: [[REFY:%.*]] = ref_element_addr %0 : $RefElemClass, #RefElemClass.y
 // CHECK-NEXT: [[BEGINX:%.*]] = begin_access [modify] [dynamic] [[REFX]] : $*BitfieldOne
-// CHECK: end_access [[BEGINX]] : $*BitfieldOne
 // CHECK: [[BEGINY:%.*]] = begin_access [modify] [dynamic] [[REFY]] : $*Int32
-// CHECK: [[BEGINX2:%.*]] = begin_access [modify] [dynamic] [no_nested_conflict] [[REFX]] : $*BitfieldOne
-// CHECK-NEXT: end_access [[BEGINX2]] : $*BitfieldOne
+// CHECK: end_access [[BEGINX]] : $*BitfieldOne
 // CHECK-NEXT: end_access [[BEGINY]] : $*Int32
 // CHECK-LABEL: } // end sil function 'ref_elem_c'
 

commit b6517c50a1cfdfd9dae47ee9fa61a4bab76c7e39
Merge: 8675c3795ad 099743894b3
Author: David Zarzycki <dave@znu.io>
Date:   Wed Mar 6 15:35:19 2019 -0500

    Merge pull request #23014 from davezarzycki/faster_simpler_nominaltype_get
    
    [AST] NFC: Improve non-generic nominal type memory efficiency

commit 099743894b38d32601ec13c0634327e568ecf53d
Author: David Zarzycki <dave@znu.io>
Date:   Fri Mar 1 09:13:01 2019 -0500

    [AST] NFC: Improve non-generic nominal type memory efficiency
    
    The non-generic nominal type nodes do not actually need to use LLVM's
    FoldingSetNode, and on my workstation the release build of the standard
    library completes about 1/3 of a second faster after switching to LLVM
    DenseMap. This is perhaps not surprising, because Decl to Type mappings
    are only needed during early compiler stages, but the intrusive
    FoldingSetNode data decreases CPU cache efficiency during all compiler
    stages. As a bonus, the resulting code is simpler.

diff --git a/include/swift/AST/Types.h b/include/swift/AST/Types.h
index 5a51f0c5156..32c4b761896 100644
--- a/include/swift/AST/Types.h
+++ b/include/swift/AST/Types.h
@@ -2241,7 +2241,7 @@ public:
 DEFINE_EMPTY_CAN_TYPE_WRAPPER(NominalType, NominalOrBoundGenericNominalType)
 
 /// EnumType - This represents the type declared by an EnumDecl.
-class EnumType : public NominalType, public llvm::FoldingSetNode {
+class EnumType : public NominalType {
 public:
   /// getDecl() - Returns the decl which declares this type.
   EnumDecl *getDecl() const {
@@ -2252,11 +2252,6 @@ public:
   /// declaration in the parent type \c Parent.
   static EnumType *get(EnumDecl *D, Type Parent, const ASTContext &C);
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getDecl(), getParent());
-  }
-  static void Profile(llvm::FoldingSetNodeID &ID, EnumDecl *D, Type Parent);
-
   // Implement isa/cast/dyncast/etc.
   static bool classof(const TypeBase *T) {
     return T->getKind() == TypeKind::Enum;
@@ -2269,7 +2264,7 @@ private:
 DEFINE_EMPTY_CAN_TYPE_WRAPPER(EnumType, NominalType)
 
 /// StructType - This represents the type declared by a StructDecl.
-class StructType : public NominalType, public llvm::FoldingSetNode {  
+class StructType : public NominalType {
 public:
   /// getDecl() - Returns the decl which declares this type.
   StructDecl *getDecl() const {
@@ -2280,11 +2275,6 @@ public:
   /// declaration in the parent type \c Parent.
   static StructType *get(StructDecl *D, Type Parent, const ASTContext &C);
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getDecl(), getParent());
-  }
-  static void Profile(llvm::FoldingSetNodeID &ID, StructDecl *D, Type Parent);
-
   // Implement isa/cast/dyncast/etc.
   static bool classof(const TypeBase *T) {
     return T->getKind() == TypeKind::Struct;
@@ -2297,7 +2287,7 @@ private:
 DEFINE_EMPTY_CAN_TYPE_WRAPPER(StructType, NominalType)
 
 /// ClassType - This represents the type declared by a ClassDecl.
-class ClassType : public NominalType, public llvm::FoldingSetNode {  
+class ClassType : public NominalType {
 public:
   /// getDecl() - Returns the decl which declares this type.
   ClassDecl *getDecl() const {
@@ -2308,11 +2298,6 @@ public:
   /// declaration in the parent type \c Parent.
   static ClassType *get(ClassDecl *D, Type Parent, const ASTContext &C);
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getDecl(), getParent());
-  }
-  static void Profile(llvm::FoldingSetNodeID &ID, ClassDecl *D, Type Parent);
-
   // Implement isa/cast/dyncast/etc.
   static bool classof(const TypeBase *T) {
     return T->getKind() == TypeKind::Class;
@@ -4343,7 +4328,7 @@ public:
 
 /// ProtocolType - A protocol type describes an abstract interface implemented
 /// by another type.
-class ProtocolType : public NominalType, public llvm::FoldingSetNode {
+class ProtocolType : public NominalType {
 public:
   /// Retrieve the type when we're referencing the given protocol.
   /// declaration.
@@ -4376,11 +4361,6 @@ public:
   static bool visitAllProtocols(ArrayRef<ProtocolDecl *> protocols,
                                 llvm::function_ref<bool(ProtocolDecl *)> fn);
 
-  void Profile(llvm::FoldingSetNodeID &ID) {
-    Profile(ID, getDecl(), getParent());
-  }
-  static void Profile(llvm::FoldingSetNodeID &ID, ProtocolDecl *D, Type Parent);
-
 private:
   friend class NominalTypeDecl;
   ProtocolType(ProtocolDecl *TheDecl, Type Parent, const ASTContext &Ctx,
diff --git a/lib/AST/ASTContext.cpp b/lib/AST/ASTContext.cpp
index a1d3ce157a6..36be948287b 100644
--- a/lib/AST/ASTContext.cpp
+++ b/lib/AST/ASTContext.cpp
@@ -317,12 +317,12 @@ FOR_KNOWN_FOUNDATION_TYPES(CACHE_FOUNDATION_DECL)
     llvm::DenseMap<std::pair<Type, void*>, DependentMemberType *>
       DependentMemberTypes;
     llvm::DenseMap<Type, DynamicSelfType *> DynamicSelfTypes;
-    llvm::FoldingSet<EnumType> EnumTypes;
-    llvm::FoldingSet<StructType> StructTypes;
-    llvm::FoldingSet<ClassType> ClassTypes;
+    llvm::DenseMap<std::pair<EnumDecl*, Type>, EnumType*> EnumTypes;
+    llvm::DenseMap<std::pair<StructDecl*, Type>, StructType*> StructTypes;
+    llvm::DenseMap<std::pair<ClassDecl*, Type>, ClassType*> ClassTypes;
+    llvm::DenseMap<std::pair<ProtocolDecl*, Type>, ProtocolType*> ProtocolTypes;
     llvm::FoldingSet<UnboundGenericType> UnboundGenericTypes;
     llvm::FoldingSet<BoundGenericType> BoundGenericTypes;
-    llvm::FoldingSet<ProtocolType> ProtocolTypes;
     llvm::FoldingSet<ProtocolCompositionType> ProtocolCompositionTypes;
     llvm::FoldingSet<LayoutConstraintInfo> LayoutConstraints;
 
@@ -2093,11 +2093,12 @@ size_t ASTContext::Implementation::Arena::getTotalMemory() const {
     llvm::capacity_in_bytes(LValueTypes) +
     llvm::capacity_in_bytes(InOutTypes) +
     llvm::capacity_in_bytes(DependentMemberTypes) +
+    llvm::capacity_in_bytes(EnumTypes) +
+    llvm::capacity_in_bytes(StructTypes) +
+    llvm::capacity_in_bytes(ClassTypes) +
+    llvm::capacity_in_bytes(ProtocolTypes) +
     llvm::capacity_in_bytes(DynamicSelfTypes);
     // FunctionTypes ?
-    // EnumTypes ?
-    // StructTypes ?
-    // ClassTypes ?
     // UnboundGenericTypes ?
     // BoundGenericTypes ?
     // NormalConformances ?
@@ -3362,26 +3363,15 @@ EnumType::EnumType(EnumDecl *TheDecl, Type Parent, const ASTContext &C,
   : NominalType(TypeKind::Enum, &C, TheDecl, Parent, properties) { }
 
 EnumType *EnumType::get(EnumDecl *D, Type Parent, const ASTContext &C) {
-  llvm::FoldingSetNodeID id;
-  EnumType::Profile(id, D, Parent);
-
   RecursiveTypeProperties properties;
   if (Parent) properties |= Parent->getRecursiveProperties();
   auto arena = getArena(properties);
 
-  void *insertPos = nullptr;
-  if (auto enumTy
-        = C.getImpl().getArena(arena).EnumTypes.FindNodeOrInsertPos(id, insertPos))
-    return enumTy;
-
-  auto enumTy = new (C, arena) EnumType(D, Parent, C, properties);
-  C.getImpl().getArena(arena).EnumTypes.InsertNode(enumTy, insertPos);
-  return enumTy;
-}
-
-void EnumType::Profile(llvm::FoldingSetNodeID &ID, EnumDecl *D, Type Parent) {
-  ID.AddPointer(D);
-  ID.AddPointer(Parent.getPointer());
+  auto *&known = C.getImpl().getArena(arena).EnumTypes[{D, Parent}];
+  if (!known) {
+    known = new (C, arena) EnumType(D, Parent, C, properties);
+  }
+  return known;
 }
 
 StructType::StructType(StructDecl *TheDecl, Type Parent, const ASTContext &C,
@@ -3389,26 +3379,15 @@ StructType::StructType(StructDecl *TheDecl, Type Parent, const ASTContext &C,
   : NominalType(TypeKind::Struct, &C, TheDecl, Parent, properties) { }
 
 StructType *StructType::get(StructDecl *D, Type Parent, const ASTContext &C) {
-  llvm::FoldingSetNodeID id;
-  StructType::Profile(id, D, Parent);
-
   RecursiveTypeProperties properties;
   if (Parent) properties |= Parent->getRecursiveProperties();
   auto arena = getArena(properties);
 
-  void *insertPos = nullptr;
-  if (auto structTy
-        = C.getImpl().getArena(arena).StructTypes.FindNodeOrInsertPos(id, insertPos))
-    return structTy;
-
-  auto structTy = new (C, arena) StructType(D, Parent, C, properties);
-  C.getImpl().getArena(arena).StructTypes.InsertNode(structTy, insertPos);
-  return structTy;
-}
-
-void StructType::Profile(llvm::FoldingSetNodeID &ID, StructDecl *D, Type Parent) {
-  ID.AddPointer(D);
-  ID.AddPointer(Parent.getPointer());
+  auto *&known = C.getImpl().getArena(arena).StructTypes[{D, Parent}];
+  if (!known) {
+    known = new (C, arena) StructType(D, Parent, C, properties);
+  }
+  return known;
 }
 
 ClassType::ClassType(ClassDecl *TheDecl, Type Parent, const ASTContext &C,
@@ -3416,26 +3395,15 @@ ClassType::ClassType(ClassDecl *TheDecl, Type Parent, const ASTContext &C,
   : NominalType(TypeKind::Class, &C, TheDecl, Parent, properties) { }
 
 ClassType *ClassType::get(ClassDecl *D, Type Parent, const ASTContext &C) {
-  llvm::FoldingSetNodeID id;
-  ClassType::Profile(id, D, Parent);
-
   RecursiveTypeProperties properties;
   if (Parent) properties |= Parent->getRecursiveProperties();
   auto arena = getArena(properties);
 
-  void *insertPos = nullptr;
-  if (auto classTy
-        = C.getImpl().getArena(arena).ClassTypes.FindNodeOrInsertPos(id, insertPos))
-    return classTy;
-
-  auto classTy = new (C, arena) ClassType(D, Parent, C, properties);
-  C.getImpl().getArena(arena).ClassTypes.InsertNode(classTy, insertPos);
-  return classTy;
-}
-
-void ClassType::Profile(llvm::FoldingSetNodeID &ID, ClassDecl *D, Type Parent) {
-  ID.AddPointer(D);
-  ID.AddPointer(Parent.getPointer());
+  auto *&known = C.getImpl().getArena(arena).ClassTypes[{D, Parent}];
+  if (!known) {
+    known = new (C, arena) ClassType(D, Parent, C, properties);
+  }
+  return known;
 }
 
 ProtocolCompositionType *
@@ -4171,22 +4139,15 @@ OptionalType *OptionalType::get(Type base) {
 
 ProtocolType *ProtocolType::get(ProtocolDecl *D, Type Parent,
                                 const ASTContext &C) {
-  llvm::FoldingSetNodeID id;
-  ProtocolType::Profile(id, D, Parent);
-
   RecursiveTypeProperties properties;
   if (Parent) properties |= Parent->getRecursiveProperties();
   auto arena = getArena(properties);
 
-  void *insertPos = nullptr;
-  if (auto protoTy
-        = C.getImpl().getArena(arena).ProtocolTypes.FindNodeOrInsertPos(id, insertPos))
-    return protoTy;
-
-  auto protoTy = new (C, arena) ProtocolType(D, Parent, C, properties);
-  C.getImpl().getArena(arena).ProtocolTypes.InsertNode(protoTy, insertPos);
-
-  return protoTy;
+  auto *&known = C.getImpl().getArena(arena).ProtocolTypes[{D, Parent}];
+  if (!known) {
+    known = new (C, arena) ProtocolType(D, Parent, C, properties);
+  }
+  return known;
 }
 
 ProtocolType::ProtocolType(ProtocolDecl *TheDecl, Type Parent,
@@ -4194,12 +4155,6 @@ ProtocolType::ProtocolType(ProtocolDecl *TheDecl, Type Parent,
                            RecursiveTypeProperties properties)
   : NominalType(TypeKind::Protocol, &Ctx, TheDecl, Parent, properties) { }
 
-void ProtocolType::Profile(llvm::FoldingSetNodeID &ID, ProtocolDecl *D,
-                           Type Parent) {
-  ID.AddPointer(D);
-  ID.AddPointer(Parent.getPointer());
-}
-
 LValueType *LValueType::get(Type objectTy) {
   assert(!objectTy->hasError() &&
          "cannot have ErrorType wrapped inside LValueType");

commit 1ebc8e95f75d56b78484025e18e8e68f02bdda46
Merge: 0632d8763a6 77a29c9551d
Author: David Zarzycki <dave@znu.io>
Date:   Wed Jan 3 17:20:01 2018 -0500

    Merge pull request #13691 from davezarzycki/nfc_perf_getDesugaredType2
    
    [AST] Perf: Improve getDesugaredType() efficiency

commit 77a29c9551df516707b7b046699f51c8655f9270
Author: David Zarzycki <dave@znu.io>
Date:   Wed Jan 3 07:17:43 2018 -0500

    [AST] Perf: Improve getDesugaredType() efficiency
    
    Make getDesugaredType() as fast as possible for now. With the old way:
    
    1) Switching over the sugared types turned into a frequently
       mispredicted branch because the sugar in the type system is random
       as far as the processor is concerned.
    2) Storing the underlying/singlely-desugared type at different offsets
       in memory adds more code bloat and misprediction.
    
    Short of a major redesign to avoid pointer chasing, this is probably as
    fast as the method will get.

diff --git a/include/swift/AST/TypeNodes.def b/include/swift/AST/TypeNodes.def
index 2577d6fa3a4..128915e3fef 100644
--- a/include/swift/AST/TypeNodes.def
+++ b/include/swift/AST/TypeNodes.def
@@ -99,8 +99,6 @@ ABSTRACT_TYPE(Builtin, Type)
   BUILTIN_TYPE(BuiltinUnsafeValueBuffer, BuiltinType)
   BUILTIN_TYPE(BuiltinVector, BuiltinType)
   TYPE_RANGE(Builtin, BuiltinInteger, BuiltinVector)
-SUGARED_TYPE(NameAlias, Type)
-SUGARED_TYPE(Paren, Type)
 TYPE(Tuple, Type)
 ABSTRACT_TYPE(ReferenceStorage, Type)
   ARTIFICIAL_TYPE(UnownedStorage, ReferenceStorageType)
@@ -141,19 +139,23 @@ ARTIFICIAL_TYPE(SILFunction, Type)
 ARTIFICIAL_TYPE(SILBlockStorage, Type)
 ARTIFICIAL_TYPE(SILBox, Type)
 ARTIFICIAL_TYPE(SILToken, Type)
-ABSTRACT_SUGARED_TYPE(SyntaxSugar, Type)
-  ABSTRACT_SUGARED_TYPE(UnarySyntaxSugar, SyntaxSugarType)
-    SUGARED_TYPE(ArraySlice, UnarySyntaxSugarType)
-    SUGARED_TYPE(Optional, UnarySyntaxSugarType)
-    SUGARED_TYPE(ImplicitlyUnwrappedOptional, UnarySyntaxSugarType)
-    TYPE_RANGE(UnarySyntaxSugar, ArraySlice, ImplicitlyUnwrappedOptional)
-  SUGARED_TYPE(Dictionary, SyntaxSugarType)
-  TYPE_RANGE(SyntaxSugar, ArraySlice, Dictionary)
 TYPE(ProtocolComposition, Type)
 TYPE(LValue, Type)
 TYPE(InOut, Type)
 UNCHECKED_TYPE(TypeVariable, Type)
-LAST_TYPE(TypeVariable)
+ABSTRACT_SUGARED_TYPE(Sugar, Type)
+  SUGARED_TYPE(Paren, SugarType)
+  SUGARED_TYPE(NameAlias, SugarType)
+  ABSTRACT_SUGARED_TYPE(SyntaxSugar, SugarType)
+    ABSTRACT_SUGARED_TYPE(UnarySyntaxSugar, SyntaxSugarType)
+      SUGARED_TYPE(ArraySlice, UnarySyntaxSugarType)
+      SUGARED_TYPE(Optional, UnarySyntaxSugarType)
+      SUGARED_TYPE(ImplicitlyUnwrappedOptional, UnarySyntaxSugarType)
+      TYPE_RANGE(UnarySyntaxSugar, ArraySlice, ImplicitlyUnwrappedOptional)
+    SUGARED_TYPE(Dictionary, SyntaxSugarType)
+    TYPE_RANGE(SyntaxSugar, ArraySlice, Dictionary)
+  TYPE_RANGE(Sugar, Paren, Dictionary)
+LAST_TYPE(Dictionary) // Sugared types are last to make isa<SugarType>() fast.
 
 #undef TYPE_RANGE
 #undef ABSTRACT_SUGARED_TYPE
diff --git a/include/swift/AST/Types.h b/include/swift/AST/Types.h
index d46ad8a6c27..f6bb709ed90 100644
--- a/include/swift/AST/Types.h
+++ b/include/swift/AST/Types.h
@@ -286,8 +286,12 @@ protected:
     HasOriginalType : 1
   );
 
+  SWIFT_INLINE_BITFIELD(SugarType, TypeBase, 1,
+    HasCachedType : 1
+  );
+
   enum { NumFlagBits = 5 };
-  SWIFT_INLINE_BITFIELD(ParenType, TypeBase, NumFlagBits,
+  SWIFT_INLINE_BITFIELD(ParenType, SugarType, NumFlagBits,
     /// Whether there is an original type.
     Flags : NumFlagBits
   );
@@ -1461,13 +1465,62 @@ public:
 };
 DEFINE_EMPTY_CAN_TYPE_WRAPPER(BuiltinFloatType, BuiltinType)
   
+/// An abstract type for all sugared types to make getDesugaredType() fast by
+/// sharing field offsets and logic for the fast path.
+class SugarType : public TypeBase {
+  // The state of this union is known via Bits.SugarType.HasCachedType so that
+  // we can avoid masking the pointer on the fast path.
+  union {
+    TypeBase *UnderlyingType;
+    const ASTContext *Context;
+  };
+
+protected:
+  // Sugar types are never canonical.
+  SugarType(TypeKind K, const ASTContext *ctx,
+            RecursiveTypeProperties properties)
+      : TypeBase(K, nullptr, properties), Context(ctx) {
+    Bits.SugarType.HasCachedType = false;
+  }
+
+  // Sugar types are never canonical.
+  SugarType(TypeKind K, Type type, RecursiveTypeProperties properties)
+      : TypeBase(K, nullptr, properties), UnderlyingType(type.getPointer()) {
+    Bits.SugarType.HasCachedType = true;
+  }
+
+  void setUnderlyingType(Type type) {
+    assert(!Bits.SugarType.HasCachedType && "Cached type already set");
+    Bits.SugarType.HasCachedType = true;
+    UnderlyingType = type.getPointer();
+  }
+
+public:
+  /// Remove one level of top-level sugar from this type.
+  Type getSinglyDesugaredTypeSlow();
+  TypeBase *getSinglyDesugaredType() const {
+    if (LLVM_LIKELY(Bits.SugarType.HasCachedType))
+      return UnderlyingType;
+    auto Ty = const_cast<SugarType*>(this);
+    return Ty->getSinglyDesugaredTypeSlow().getPointer();
+  }
+
+  static bool classof(const TypeBase *T) {
+    if (TypeKind::Last_Type == TypeKind::Last_SugarType)
+      return T->getKind() >= TypeKind::First_SugarType;
+    return T->getKind() >= TypeKind::First_SugarType &&
+           T->getKind() <= TypeKind::Last_SugarType;
+  }
+};
+
 /// NameAliasType - An alias type is a name for another type, just like a
 /// typedef in C.
-class NameAliasType : public TypeBase {
+class NameAliasType : public SugarType {
   friend class TypeAliasDecl;
   // NameAliasType are never canonical.
   NameAliasType(TypeAliasDecl *d) 
-    : TypeBase(TypeKind::NameAlias, nullptr, RecursiveTypeProperties()),
+    : SugarType(TypeKind::NameAlias, (ASTContext*)nullptr,
+                RecursiveTypeProperties()),
       TheDecl(d) {}
   TypeAliasDecl *const TheDecl;
 
@@ -1476,9 +1529,6 @@ public:
 
   using TypeBase::setRecursiveProperties;
    
-  /// Remove one level of top-level sugar from this type.
-  TypeBase *getSinglyDesugaredType();
-
   // Implement isa/cast/dyncast/etc.
   static bool classof(const TypeBase *T) {
     return T->getKind() == TypeKind::NameAlias;
@@ -1561,23 +1611,18 @@ public:
 };
 
 /// ParenType - A paren type is a type that's been written in parentheses.
-class ParenType : public TypeBase {
-  Type UnderlyingType;
-
+class ParenType : public SugarType {
   friend class ASTContext;
   
   ParenType(Type UnderlyingType, RecursiveTypeProperties properties,
             ParameterTypeFlags flags);
 
 public:
-  Type getUnderlyingType() const { return UnderlyingType; }
+  Type getUnderlyingType() const { return getSinglyDesugaredType(); }
 
   static ParenType *get(const ASTContext &C, Type underlying,
                         ParameterTypeFlags flags = {});
 
-  /// Remove one level of top-level sugar from this type.
-  TypeBase *getSinglyDesugaredType();
-
   /// Get the parameter flags
   ParameterTypeFlags getParameterFlags() const {
     return ParameterTypeFlags::fromRaw(Bits.ParenType.Flags);
@@ -3951,25 +3996,15 @@ DEFINE_EMPTY_CAN_TYPE_WRAPPER(SILTokenType, Type)
 /// Arrays: [T] -> Array<T>
 /// Optionals: T? -> Optional<T>
 /// Dictionaries: [K : V]  -> Dictionary<K, V>
-class SyntaxSugarType : public TypeBase {
-  llvm::PointerUnion<Type, const ASTContext *> ImplOrContext;
-
-  Type getImplementationTypeSlow();
-
+class SyntaxSugarType : public SugarType {
 protected:
   // Syntax sugar types are never canonical.
   SyntaxSugarType(TypeKind K, const ASTContext &ctx,
-                     RecursiveTypeProperties properties)
-    : TypeBase(K, nullptr, properties), ImplOrContext(&ctx) {}
+                  RecursiveTypeProperties properties)
+    : SugarType(K, &ctx, properties) {}
 
 public:
-  TypeBase *getSinglyDesugaredType();
-
-  Type getImplementationType() {
-    if (ImplOrContext.is<Type>())
-      return ImplOrContext.get<Type>();
-    return getImplementationTypeSlow();
-  }
+  Type getImplementationType() const { return getSinglyDesugaredType(); }
 
   static bool classof(const TypeBase *T) {
     return T->getKind() >= TypeKind::First_SyntaxSugarType &&
@@ -5169,6 +5204,12 @@ constexpr bool TypeBase::isSugaredType<id##Type>() { \
 inline GenericParamKey::GenericParamKey(const GenericTypeParamType *p)
   : Depth(p->getDepth()), Index(p->getIndex()) { }
 
+inline TypeBase *TypeBase::getDesugaredType() {
+  if (!isa<SugarType>(this))
+    return this;
+  return cast<SugarType>(this)->getSinglyDesugaredType()->getDesugaredType();
+}
+
 } // end namespace swift
 
 namespace llvm {
diff --git a/include/swift/Basic/InlineBitfield.h b/include/swift/Basic/InlineBitfield.h
index bb506bef772..b5862919944 100644
--- a/include/swift/Basic/InlineBitfield.h
+++ b/include/swift/Basic/InlineBitfield.h
@@ -38,22 +38,27 @@ namespace swift {
   class T##Bitfield { \
     friend class T; \
     uint64_t __VA_ARGS__; \
+    uint64_t : 64 - (C); /* Better code gen */ \
   } T; \
   LLVM_PACKED_END \
   enum { Num##T##Bits = (C) }; \
   static_assert(sizeof(T##Bitfield) <= 8, "Bitfield overflow")
 
 /// Define an bitfield for type 'T' with parent class 'U' and 'C' bits used.
-#define SWIFT_INLINE_BITFIELD(T, U, C, ...) \
+#define SWIFT_INLINE_BITFIELD_TEMPLATE(T, U, C, HC, ...) \
   LLVM_PACKED_START \
   class T##Bitfield { \
     friend class T; \
     uint64_t : Num##U##Bits, __VA_ARGS__; \
+    uint64_t : 64 - (Num##U##Bits + (HC) + (C)); /* Better code gen */ \
   } T; \
   LLVM_PACKED_END \
   enum { Num##T##Bits = Num##U##Bits + (C) }; \
   static_assert(sizeof(T##Bitfield) <= 8, "Bitfield overflow")
 
+#define SWIFT_INLINE_BITFIELD(T, U, C, ...) \
+  SWIFT_INLINE_BITFIELD_TEMPLATE(T, U, C, 0, __VA_ARGS__)
+
 /// Define a full bitfield for type 'T' that uses all of the remaining bits in
 /// the inline bitfield.
 ///
diff --git a/include/swift/SIL/SILNode.h b/include/swift/SIL/SILNode.h
index c6caef3fe52..b6e0b8434ca 100644
--- a/include/swift/SIL/SILNode.h
+++ b/include/swift/SIL/SILNode.h
@@ -142,7 +142,7 @@ protected:
   );
 
 #define IBWTO_BITFIELD(T, U, C, ...) \
-  SWIFT_INLINE_BITFIELD(T, U, (C), __VA_ARGS__, : 32)
+  SWIFT_INLINE_BITFIELD_TEMPLATE(T, U, (C), 32, __VA_ARGS__)
 #define IBWTO_BITFIELD_EMPTY(T, U) \
   SWIFT_INLINE_BITFIELD_EMPTY(T, U)
 
diff --git a/lib/AST/Type.cpp b/lib/AST/Type.cpp
index f75c770c621..e37e2608c14 100644
--- a/lib/AST/Type.cpp
+++ b/lib/AST/Type.cpp
@@ -1249,56 +1249,16 @@ TypeBase *TypeBase::reconstituteSugar(bool Recursive) {
     return Func(this).getPointer();
 }
 
-TypeBase *TypeBase::getDesugaredType() {
-  switch (getKind()) {
-#define ALWAYS_CANONICAL_TYPE(id, parent) case TypeKind::id:
-#define UNCHECKED_TYPE(id, parent) case TypeKind::id:
-#define TYPE(id, parent)
-#include "swift/AST/TypeNodes.def"
-  case TypeKind::Error:
-  case TypeKind::Tuple:
-  case TypeKind::Function:
-  case TypeKind::GenericFunction:
-  case TypeKind::SILBlockStorage:
-  case TypeKind::SILBox:
-  case TypeKind::SILFunction:
-  case TypeKind::SILToken:
-  case TypeKind::LValue:
-  case TypeKind::InOut:
-  case TypeKind::ProtocolComposition:
-  case TypeKind::ExistentialMetatype:
-  case TypeKind::Metatype:
-  case TypeKind::BoundGenericClass:
-  case TypeKind::BoundGenericEnum:
-  case TypeKind::BoundGenericStruct:
-  case TypeKind::Enum:
-  case TypeKind::Struct:
-  case TypeKind::Class:
-  case TypeKind::Protocol:
-  case TypeKind::GenericTypeParam:
-  case TypeKind::DependentMember:
-  case TypeKind::UnownedStorage:
-  case TypeKind::UnmanagedStorage:
-  case TypeKind::WeakStorage:
-  case TypeKind::DynamicSelf:
-    // None of these types have sugar at the outer level.
-    return this;
-#define SUGARED_TYPE(ID, PARENT) \
-  case TypeKind::ID: \
-    return cast<ID##Type>(this)->getSinglyDesugaredType()->getDesugaredType();
-#define TYPE(id, parent)
+#define TYPE(Id, Parent)
+#define SUGARED_TYPE(Id, Parent) \
+  static_assert(std::is_base_of<SugarType, Id##Type>::value, "Sugar mismatch");
 #include "swift/AST/TypeNodes.def"
-  }
-
-  llvm_unreachable("Unknown type kind");
-}
 
 ParenType::ParenType(Type baseType, RecursiveTypeProperties properties,
                      ParameterTypeFlags flags)
-  : TypeBase(TypeKind::Paren, nullptr, properties),
-    UnderlyingType(flags.isInOut()
-                     ? InOutType::get(baseType)
-                     : baseType) {
+  : SugarType(TypeKind::Paren,
+              flags.isInOut() ? InOutType::get(baseType) : baseType,
+              properties) {
   Bits.ParenType.Flags = flags.toRaw();
   if (flags.isInOut())
     assert(!baseType->is<InOutType>() && "caller did not pass a base type");
@@ -1307,21 +1267,9 @@ ParenType::ParenType(Type baseType, RecursiveTypeProperties properties,
 }
 
 
-TypeBase *ParenType::getSinglyDesugaredType() {
-  return getUnderlyingType().getPointer();
-}
-
-TypeBase *NameAliasType::getSinglyDesugaredType() {
-  return getDecl()->getUnderlyingTypeLoc().getType().getPointer();
-}
-
-TypeBase *SyntaxSugarType::getSinglyDesugaredType() {
-  return getImplementationType().getPointer();
-}
-
-Type SyntaxSugarType::getImplementationTypeSlow() {
+Type SugarType::getSinglyDesugaredTypeSlow() {
   // Find the generic type that implements this syntactic sugar type.
-  auto &ctx = *ImplOrContext.get<const ASTContext *>();
+  auto &ctx = *Context;
   NominalTypeDecl *implDecl;
 
   // XXX -- If the Decl and Type class hierarchies agreed on spelling, then
@@ -1331,9 +1279,17 @@ Type SyntaxSugarType::getImplementationTypeSlow() {
   case TypeKind::Id: llvm_unreachable("non-sugared type?");
 #define SUGARED_TYPE(Id, Parent)
 #include "swift/AST/TypeNodes.def"
-  case TypeKind::NameAlias:
   case TypeKind::Paren:
-    llvm_unreachable("typealiases and parens are sugar, but not syntax sugar");
+    llvm_unreachable("parenthesis are sugar, but not syntax sugar");
+  case TypeKind::NameAlias: {
+    auto Ty = cast<NameAliasType>(this);
+    auto UTy = Ty->getDecl()->getUnderlyingTypeLoc().getType().getPointer();
+    if (isa<NominalOrBoundGenericNominalType>(UTy)) {
+      Bits.SugarType.HasCachedType = true;
+      UnderlyingType = UTy;
+    }
+    return UTy;
+  }
   case TypeKind::ArraySlice:
     implDecl = ctx.getArrayDecl();
     break;
@@ -1349,17 +1305,18 @@ Type SyntaxSugarType::getImplementationTypeSlow() {
   }
   assert(implDecl && "Type has not been set yet");
 
+  Bits.SugarType.HasCachedType = true;
   if (auto Ty = dyn_cast<UnarySyntaxSugarType>(this)) {
-    ImplOrContext = BoundGenericType::get(implDecl, Type(), Ty->getBaseType());
+    UnderlyingType = BoundGenericType::get(implDecl, Type(), Ty->getBaseType());
   } else if (auto Ty = dyn_cast<DictionaryType>(this)) {
-    ImplOrContext = BoundGenericType::get(implDecl, Type(),
+    UnderlyingType = BoundGenericType::get(implDecl, Type(),
                                       { Ty->getKeyType(), Ty->getValueType() });
   } else {
     llvm_unreachable("Not UnarySyntaxSugarType or DictionaryType?");
   }
 
   // Record the implementation type.
-  return ImplOrContext.get<Type>();
+  return UnderlyingType;
 }
 
 unsigned GenericTypeParamType::getDepth() const {

commit fbe34e0f6f843941b75bedb8c6b76777fb2b2c45
Author: omochimetaru <omochi.metaru@gmail.com>
Date:   Fri Dec 22 01:23:25 2017 +0900

    [Parse] improve LF handling efficiency.

diff --git a/lib/Parse/Lexer.cpp b/lib/Parse/Lexer.cpp
index 9c272bff921..4ace21763fe 100644
--- a/lib/Parse/Lexer.cpp
+++ b/lib/Parse/Lexer.cpp
@@ -2298,24 +2298,20 @@ Restart:
   // TODO: Handle invalid UTF8 sequence which is skipped in lexImpl().
   switch (*CurPtr++) {
   case '\n':
-  case '\r':
     if (IsForTrailingTrivia)
       break;
     NextToken.setAtStartOfLine(true);
-    switch (CurPtr[-1]) {
-    case '\n':
-      Pieces.appendOrSquash(TriviaPiece::newlines(1));
-      break;
-    case '\r':
-      if (CurPtr[0] == '\n') {
-        Pieces.appendOrSquash(TriviaPiece::carriageReturnLineFeeds(1));
-        CurPtr++;
-      } else {
-        Pieces.appendOrSquash(TriviaPiece::carriageReturns(1));
-      }
+    Pieces.appendOrSquash(TriviaPiece::newlines(1));
+    break;
+  case '\r':
+    if (IsForTrailingTrivia)
       break;
-    default:
-      llvm_unreachable("unexcepted char here");
+    NextToken.setAtStartOfLine(true);
+    if (CurPtr[0] == '\n') {
+      Pieces.appendOrSquash(TriviaPiece::carriageReturnLineFeeds(1));
+      ++CurPtr;
+    } else {
+      Pieces.appendOrSquash(TriviaPiece::carriageReturns(1));
     }
     goto Restart;
   case ' ':

commit 8f43cba0b5d4e7d666e1c487af8b1a0babaff43b
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Oct 31 23:27:39 2017 -0700

    [Syntax] Replace TrivialList's std::deque with a std::vector.
    
    For very large source files, the parser's syntax map---which contains a
    very large number of TrivialLists---was taking an inordinate amount of
    memory due to the inefficiency of std::deque. Specifically, a
    std::deque containing just one trivial element would allocate 4k of
    memory. With the ~120MB SIL output of one of the parse_stdlib tests,
    these std::deques would add up to > 6GB of memory, most of which is
    wasted.
    
    Replacing the std::deque with a std::vector knocks the memory required
    for one of the parse_stdlib tests from > 8GB down closer to 2 GB. The
    parser's syntax map is still large (e.g., a 512MB allocation for the
    overall vector plus a few hundred MB of raw-syntax data), but not
    prohibitively so.
    
    Part of rdar://problem/34771322.

diff --git a/include/swift/Syntax/Trivia.h b/include/swift/Syntax/Trivia.h
index 615a1767c77..75137c0f8e2 100644
--- a/include/swift/Syntax/Trivia.h
+++ b/include/swift/Syntax/Trivia.h
@@ -82,7 +82,7 @@
 #include "swift/Basic/OwnedString.h"
 #include "llvm/Support/raw_ostream.h"
 
-#include <deque>
+#include <vector>
 
 namespace swift {
 namespace syntax {
@@ -204,7 +204,7 @@ struct TriviaPiece {
   }
 };
 
-using TriviaList = std::deque<TriviaPiece>;
+using TriviaList = std::vector<TriviaPiece>;
 
 /// A collection of leading or trailing trivia. This is the main data structure
 /// for thinking about trivia.
@@ -228,7 +228,7 @@ struct Trivia {
 
   /// Add a piece to the beginning of the collection.
   void push_front(const TriviaPiece &Piece) {
-    Pieces.push_front(Piece);
+    Pieces.insert(Pieces.begin(), Piece);
   }
 
   /// Return a reference to the first piece.
diff --git a/lib/Parse/Lexer.cpp b/lib/Parse/Lexer.cpp
index 7a2f941a904..2eacbfdde3a 100644
--- a/lib/Parse/Lexer.cpp
+++ b/lib/Parse/Lexer.cpp
@@ -740,7 +740,8 @@ static bool rangeContainsPlaceholderEnd(const char *CurPtr,
 syntax::RawTokenInfo Lexer::fullLex() {
   if (NextToken.isEscapedIdentifier()) {
     LeadingTrivia.push_back(syntax::TriviaPiece::backtick());
-    TrailingTrivia.push_front(syntax::TriviaPiece::backtick());
+    TrailingTrivia.insert(TrailingTrivia.begin(),
+                          syntax::TriviaPiece::backtick());
   }
   auto Loc = NextToken.getLoc();
   auto Result = syntax::RawTokenSyntax::make(NextToken.getKind(),

commit 62f43ae75b0139cf8bdeefbab368873757bd3295
Author: Doug Gregor <dgregor@apple.com>
Date:   Tue Oct 31 23:27:39 2017 -0700

    [Syntax] Replace TrivialList's std::deque with a std::vector.
    
    For very large source files, the parser's syntax map---which contains a
    very large number of TrivialLists---was taking an inordinate amount of
    memory due to the inefficiency of std::deque. Specifically, a
    std::deque containing just one trivial element would allocate 4k of
    memory. With the ~120MB SIL output of one of the parse_stdlib tests,
    these std::deques would add up to > 6GB of memory, most of which is
    wasted.
    
    Replacing the std::deque with a std::vector knocks the memory required
    for one of the parse_stdlib tests from > 8GB down closer to 2 GB. The
    parser's syntax map is still large (e.g., a 512MB allocation for the
    overall vector plus a few hundred MB of raw-syntax data), but not
    prohibitively so.
    
    Part of rdar://problem/34771322.

diff --git a/include/swift/Syntax/Trivia.h b/include/swift/Syntax/Trivia.h
index 615a1767c77..75137c0f8e2 100644
--- a/include/swift/Syntax/Trivia.h
+++ b/include/swift/Syntax/Trivia.h
@@ -82,7 +82,7 @@
 #include "swift/Basic/OwnedString.h"
 #include "llvm/Support/raw_ostream.h"
 
-#include <deque>
+#include <vector>
 
 namespace swift {
 namespace syntax {
@@ -204,7 +204,7 @@ struct TriviaPiece {
   }
 };
 
-using TriviaList = std::deque<TriviaPiece>;
+using TriviaList = std::vector<TriviaPiece>;
 
 /// A collection of leading or trailing trivia. This is the main data structure
 /// for thinking about trivia.
@@ -228,7 +228,7 @@ struct Trivia {
 
   /// Add a piece to the beginning of the collection.
   void push_front(const TriviaPiece &Piece) {
-    Pieces.push_front(Piece);
+    Pieces.insert(Pieces.begin(), Piece);
   }
 
   /// Return a reference to the first piece.
diff --git a/lib/Parse/Lexer.cpp b/lib/Parse/Lexer.cpp
index 7a2f941a904..2eacbfdde3a 100644
--- a/lib/Parse/Lexer.cpp
+++ b/lib/Parse/Lexer.cpp
@@ -740,7 +740,8 @@ static bool rangeContainsPlaceholderEnd(const char *CurPtr,
 syntax::RawTokenInfo Lexer::fullLex() {
   if (NextToken.isEscapedIdentifier()) {
     LeadingTrivia.push_back(syntax::TriviaPiece::backtick());
-    TrailingTrivia.push_front(syntax::TriviaPiece::backtick());
+    TrailingTrivia.insert(TrailingTrivia.begin(),
+                          syntax::TriviaPiece::backtick());
   }
   auto Loc = NextToken.getLoc();
   auto Result = syntax::RawTokenSyntax::make(NextToken.getKind(),

commit 157de8c312f388f4b08b525a75e6a4bf51dadfae
Author: Andrew Trick <atrick@apple.com>
Date:   Mon Sep 11 13:28:13 2017 -0700

    NFC. Reorder a verifier check for obvious efficiency.

diff --git a/lib/SIL/SILVerifier.cpp b/lib/SIL/SILVerifier.cpp
index 9770a9921c0..4af53684c88 100644
--- a/lib/SIL/SILVerifier.cpp
+++ b/lib/SIL/SILVerifier.cpp
@@ -2380,8 +2380,8 @@ public:
       }
       return false;
     };
-    require(!isMutatingOrConsuming(OEI) ||
-                allowedAccessKind == OpenedExistentialAccess::Mutable,
+    require(allowedAccessKind == OpenedExistentialAccess::Mutable
+            || !isMutatingOrConsuming(OEI),
             "open_existential_addr uses that consumes or mutates but is not "
             "opened for mutation");
   }

commit 6bfba480552265e61e7acd826d9c15fbf5e5bf5c
Author: Xi Ge <xi_ge@apple.com>
Date:   Thu Oct 20 18:43:03 2016 -0700

    Revert "[swift-api-digester] caching the equality comparison results among SDKNodes. (#5297)" (#5391)
    
    Practical measurement suggests the efficiency improvement is
    negligible, thus revert it.

diff --git a/tools/swift-api-digester/swift-api-digester.cpp b/tools/swift-api-digester/swift-api-digester.cpp
index 0ce99bc35db..132cdc4bb05 100644
--- a/tools/swift-api-digester/swift-api-digester.cpp
+++ b/tools/swift-api-digester/swift-api-digester.cpp
@@ -808,45 +808,17 @@ NodeUniquePtr SDKNode::constructSDKNode(llvm::yaml::MappingNode *Node) {
   return Result;
 }
 
-/// This is for caching the comparison results between two SDKNodes.
-class SDKNodeEqualContext {
-  using NodePtrAndEqual = llvm::DenseMap<const SDKNode*, bool>;
-  llvm::DenseMap<const SDKNode*, llvm::DenseMap<const SDKNode*, bool>> Data;
-
-public:
-  Optional<bool> getEquality(const SDKNode* Left, const SDKNode* Right) {
-    auto &Map = Data.insert({Left, NodePtrAndEqual()}).first->getSecond();
-    if (Map.count(Right))
-      return Map[Right];
-    return None;
-  }
-
-  void addEquality(const SDKNode* Left, const SDKNode* Right, const bool Value) {
-    Data.insert(std::make_pair(Left, NodePtrAndEqual())).first->getSecond().
-      insert({Right, Value});
-  }
-};
-
 bool SDKNode::operator==(const SDKNode &Other) const {
-  static SDKNodeEqualContext EqualCache;
-  if (auto Cached = EqualCache.getEquality(this, &Other)) {
-    return Cached.getValue();
-  }
-  auto Exit = [&](const bool Result) {
-    EqualCache.addEquality(this, &Other, Result);
-    return Result;
-  };
-
   if (getKind() != Other.getKind())
-    return Exit(false);
+    return false;
 
   switch(getKind()) {
     case SDKNodeKind::TypeNominal:
     case SDKNodeKind::TypeFunc: {
       auto Left = this->getAs<SDKNodeType>();
       auto Right = (&Other)->getAs<SDKNodeType>();
-      return Exit(Left->getTypeAttributes().equals(Right->getTypeAttributes())
-        && Left->getPrintedName() == Right->getPrintedName());
+      return Left->getTypeAttributes().equals(Right->getTypeAttributes())
+        && Left->getPrintedName() == Right->getPrintedName();
     }
 
     case SDKNodeKind::Function:
@@ -856,9 +828,9 @@ bool SDKNode::operator==(const SDKNode &Other) const {
       auto Left = this->getAs<SDKNodeAbstractFunc>();
       auto Right = (&Other)->getAs<SDKNodeAbstractFunc>();
       if (Left->isMutating() ^ Right->isMutating())
-        return Exit(false);
+        return false;
       if (Left->isThrowing() ^ Right->isThrowing())
-        return Exit(false);
+        return false;
       SWIFT_FALLTHROUGH;
     }
     case SDKNodeKind::TypeDecl:
@@ -870,11 +842,11 @@ bool SDKNode::operator==(const SDKNode &Other) const {
           Children.size() == Other.Children.size()) {
         for (unsigned I = 0; I < Children.size(); ++ I) {
           if (*Children[I] != *Other.Children[I])
-            return Exit(false);
+            return false;
         }
-        return Exit(true);
+        return true;
       }
-      return Exit(false);
+      return false;
     }
   }
 }

commit 1d5ccc0d1b1e486bb48a5f14a6d794bdb476d9e2
Author: Michael Ilseman <milseman@apple.com>
Date:   Sat Oct 1 10:16:56 2016 -0700

    [ClangImporter] EnumInfos are now cached by Clang decl
    
    Now that we have a EnumInfoCache per NameImporter which is per Clang
    instance, we can just cache our EnumInfo based off of Clang decl,
    rather than using a faux-USR technique. This greatly speeds up and
    simplifies enum imports.
    
    EnumInfos are requested about 200k times when building the module
    cache for Foundation, and thousands of times for simple tests during
    normal import, so efficiency here is important. While building module
    caches, the EnumInfoCache is hit 99% of the time, and it is hit 98% of
    the time during normal import with pre-built module caches.
    
    NFC.

diff --git a/lib/ClangImporter/ImportEnumInfo.cpp b/lib/ClangImporter/ImportEnumInfo.cpp
index ccb67f04f5f..01f95bdb251 100644
--- a/lib/ClangImporter/ImportEnumInfo.cpp
+++ b/lib/ClangImporter/ImportEnumInfo.cpp
@@ -25,6 +25,11 @@
 #include "clang/Lex/MacroInfo.h"
 #include "clang/Lex/Preprocessor.h"
 
+#include "llvm/ADT/Statistic.h"
+#define DEBUG_TYPE "Enum Info"
+STATISTIC(EnumInfoNumCacheHits, "# of times the enum info cache was hit");
+STATISTIC(EnumInfoNumCacheMisses, "# of times the enum info cache was missed");
+
 using namespace swift;
 using namespace importer;
 
@@ -288,25 +293,13 @@ void EnumInfo::determineConstantNamePrefix(ASTContext &ctx,
   constantNamePrefix = ctx.AllocateCopy(commonPrefix);
 }
 
-StringRef EnumInfoCache::getEnumInfoKey(const clang::EnumDecl *decl,
-                                        SmallVectorImpl<char> &scratch) {
-  StringRef moduleName;
-  if (auto moduleOpt = getClangSubmoduleForDecl(decl)) {
-    if (*moduleOpt)
-      moduleName = (*moduleOpt)->getTopLevelModuleName();
+EnumInfo EnumInfoCache::getEnumInfo(const clang::EnumDecl *decl) {
+  if (enumInfos.count(decl)) {
+    ++EnumInfoNumCacheHits;
+    return enumInfos[decl];
   }
-  if (moduleName.empty())
-    moduleName = decl->getASTContext().getLangOpts().CurrentModule;
-
-  StringRef enumName = decl->getDeclName()
-                           ? decl->getName()
-                           : decl->getTypedefNameForAnonDecl()->getName();
-
-  if (moduleName.empty())
-    return enumName;
-
-  scratch.append(moduleName.begin(), moduleName.end());
-  scratch.push_back('.');
-  scratch.append(enumName.begin(), enumName.end());
-  return StringRef(scratch.data(), scratch.size());
+  ++EnumInfoNumCacheMisses;
+  EnumInfo enumInfo(swiftCtx, decl, clangPP);
+  enumInfos[decl] = enumInfo;
+  return enumInfo;
 }
diff --git a/lib/ClangImporter/ImportEnumInfo.h b/lib/ClangImporter/ImportEnumInfo.h
index 7260027f2f5..0ac5d263335 100644
--- a/lib/ClangImporter/ImportEnumInfo.h
+++ b/lib/ClangImporter/ImportEnumInfo.h
@@ -19,14 +19,9 @@
 
 #include "swift/AST/ASTContext.h"
 #include "swift/AST/Decl.h"
-#include "clang/AST/Attr.h"
-#include "clang/Basic/SourceLocation.h"
 #include "llvm/ADT/APSInt.h"
 #include "llvm/ADT/DenseMap.h"
 
-// TODO: drop this when we embed PP directly
-#include "clang/Sema/Sema.h"
-
 namespace clang {
 class EnumDecl;
 class Preprocessor;
@@ -105,14 +100,7 @@ class EnumInfoCache {
   ASTContext &swiftCtx;
   clang::Preprocessor &clangPP;
 
-  /// Cache enum infos, referenced with a dotted Clang name
-  /// "ModuleName.EnumName".
-  // TODO: switch to a Clang decl dense map
-  llvm::StringMap<importer::EnumInfo> enumInfos;
-
-  /// Retrieve the key to use when looking for enum information.
-  StringRef getEnumInfoKey(const clang::EnumDecl *decl,
-                           SmallVectorImpl<char> &scratch);
+  llvm::DenseMap<const clang::EnumDecl *, EnumInfo> enumInfos;
 
   // Never copy
   EnumInfoCache(const EnumInfoCache &) = delete;
@@ -122,24 +110,9 @@ public:
   EnumInfoCache(ASTContext &swiftContext, clang::Preprocessor &cpp)
       : swiftCtx(swiftContext), clangPP(cpp) {}
 
-  importer::EnumInfo getEnumInfo(const clang::EnumDecl *decl) {
-    // If there is no name for linkage, the computation is trivial and we
-    // wouldn't be able to perform name-based caching anyway.
-    if (!decl->hasNameForLinkage())
-      return importer::EnumInfo(swiftCtx, decl, clangPP);
-
-    SmallString<32> keyScratch;
-    auto key = getEnumInfoKey(decl, keyScratch);
-    auto known = enumInfos.find(key);
-    if (known != enumInfos.end())
-      return known->second;
-
-    importer::EnumInfo enumInfo(swiftCtx, decl, clangPP);
-    enumInfos[key] = enumInfo;
-    return enumInfo;
-  }
+  EnumInfo getEnumInfo(const clang::EnumDecl *decl);
 
-  importer::EnumKind getEnumKind(const clang::EnumDecl *decl) {
+  EnumKind getEnumKind(const clang::EnumDecl *decl) {
     return getEnumInfo(decl).getKind();
   }
 
diff --git a/lib/ClangImporter/ImportName.h b/lib/ClangImporter/ImportName.h
index 869ef1268fe..4ad5e201b60 100644
--- a/lib/ClangImporter/ImportName.h
+++ b/lib/ClangImporter/ImportName.h
@@ -22,6 +22,7 @@
 #include "swift/AST/ASTContext.h"
 #include "swift/AST/Decl.h"
 #include "swift/AST/ForeignErrorConvention.h"
+#include "clang/Sema/Sema.h"
 
 namespace swift {
 namespace importer {
@@ -165,7 +166,7 @@ class NameImporter {
   clang::Sema &clangSema;
   const PlatformAvailability &availability;
 
-  EnumInfoCache enumInfoCache;
+  EnumInfoCache enumInfos;
   StringScratchSpace scratch;
 
   const bool inferImportAsMember;
@@ -176,7 +177,7 @@ public:
   NameImporter(ImportNameSwiftContext ctx, clang::Sema &cSema)
       : swiftCtx(ctx.swiftCtx), clangSema(cSema),
         availability(ctx.availability),
-        enumInfoCache(swiftCtx, clangSema.getPreprocessor()),
+        enumInfos(swiftCtx, clangSema.getPreprocessor()),
         inferImportAsMember(ctx.inferImportAsMember) {}
 
   NameImporter(ASTContext &ctx, const PlatformAvailability &avail,
@@ -199,13 +200,12 @@ public:
   bool isInferImportAsMember() const { return inferImportAsMember; }
 
   EnumInfo getEnumInfo(const clang::EnumDecl *decl) {
-    return enumInfoCache.getEnumInfo(decl);
+    return enumInfos.getEnumInfo(decl);
   }
   EnumKind getEnumKind(const clang::EnumDecl *decl) {
-    return enumInfoCache.getEnumKind(decl);
+    return enumInfos.getEnumKind(decl);
   }
 
-
   clang::Sema &getClangSema() { return clangSema; }
   clang::ASTContext &getClangContext() { return getClangSema().getASTContext(); }
 

commit 6d2f11a305c1649d79334d29104bd0efc607b427
Author: Doug Gregor <dgregor@apple.com>
Date:   Mon Aug 1 21:20:41 2016 -0700

    [Type checker] Synthesize Error._code witnesses as 'final' within a class.
    
    When synthesizing the witness for Error._code, synthesize it as
    final. This isn't meant to be user-visible (and, therefore, isn't
    meant to be user-overridable), so it's a minor efficiency
    win. Moreover, we weren't making sure this member got synthesized in
    in cross-module situations, leading to runtime crashes. Fixes
    rdar://problem/27335637.

diff --git a/lib/Sema/DerivedConformanceError.cpp b/lib/Sema/DerivedConformanceError.cpp
index 4508b248f09..8776bfa029b 100644
--- a/lib/Sema/DerivedConformanceError.cpp
+++ b/lib/Sema/DerivedConformanceError.cpp
@@ -150,7 +150,9 @@ static ValueDecl *deriveError_code(TypeChecker &tc, Decl *parentDecl,
 
   // Define the getter.
   auto getterDecl = declareDerivedPropertyGetter(tc, parentDecl, nominal,
-                                                 intTy, intTy);
+                                                 intTy, intTy,
+                                                 /*isStatic=*/false,
+                                                 /*isFinal=*/true);
   if (isa<EnumDecl>(nominal))
     getterDecl->setBodySynthesizer(&deriveBodyError_enum_code);
   else
@@ -161,7 +163,8 @@ static ValueDecl *deriveError_code(TypeChecker &tc, Decl *parentDecl,
   PatternBindingDecl *pbDecl;
   std::tie(propDecl, pbDecl)
     = declareDerivedReadOnlyProperty(tc, parentDecl, nominal, C.Id_code_,
-                                     intTy, intTy, getterDecl);
+                                     intTy, intTy, getterDecl,
+                                     /*isStatic=*/false, /*isFinal=*/true);
   
   auto dc = cast<IterableDeclContext>(parentDecl);
   dc->addMember(getterDecl);
@@ -231,7 +234,8 @@ static ValueDecl *deriveBridgedNSError_enum_nsErrorDomain(TypeChecker &tc,
   // Define the getter.
   auto getterDecl = declareDerivedPropertyGetter(tc, parentDecl, enumDecl,
                                                  stringTy, stringTy,
-                                                 /*isStatic=*/true);
+                                                 /*isStatic=*/true,
+                                                 /*isFinal=*/true);
   getterDecl->setBodySynthesizer(&deriveBodyBridgedNSError_enum_nsErrorDomain);
   
   // Define the property.
@@ -241,7 +245,8 @@ static ValueDecl *deriveBridgedNSError_enum_nsErrorDomain(TypeChecker &tc,
     = declareDerivedReadOnlyProperty(tc, parentDecl, enumDecl,
                                      C.Id_nsErrorDomain,
                                      stringTy, stringTy,
-                                     getterDecl, /*isStatic=*/true);
+                                     getterDecl, /*isStatic=*/true,
+                                     /*isFinal=*/true);
   
   auto dc = cast<IterableDeclContext>(parentDecl);
   dc->addMember(getterDecl);
diff --git a/lib/Sema/DerivedConformanceRawRepresentable.cpp b/lib/Sema/DerivedConformanceRawRepresentable.cpp
index 31cf1136d41..a02147bb05d 100644
--- a/lib/Sema/DerivedConformanceRawRepresentable.cpp
+++ b/lib/Sema/DerivedConformanceRawRepresentable.cpp
@@ -133,7 +133,9 @@ static VarDecl *deriveRawRepresentable_raw(TypeChecker &tc,
   // Define the getter.
   auto getterDecl = declareDerivedPropertyGetter(tc, parentDecl, enumDecl,
                                                  rawInterfaceType,
-                                                 rawType);
+                                                 rawType,
+                                                 /*isStatic=*/false,
+                                                 /*isFinal=*/false);
   getterDecl->setBodySynthesizer(&deriveBodyRawRepresentable_raw);
 
   // Define the property.
@@ -144,7 +146,9 @@ static VarDecl *deriveRawRepresentable_raw(TypeChecker &tc,
                                      C.Id_rawValue,
                                      rawInterfaceType,
                                      rawType,
-                                     getterDecl);
+                                     getterDecl,
+                                     /*isStatic=*/false,
+                                     /*isFinal=*/false);
   
   auto dc = cast<IterableDeclContext>(parentDecl);
   dc->addMember(getterDecl);
diff --git a/lib/Sema/DerivedConformances.cpp b/lib/Sema/DerivedConformances.cpp
index 8c8b53b369e..fc7a3d890ec 100644
--- a/lib/Sema/DerivedConformances.cpp
+++ b/lib/Sema/DerivedConformances.cpp
@@ -106,7 +106,8 @@ FuncDecl *DerivedConformance::declareDerivedPropertyGetter(TypeChecker &tc,
                                                  NominalTypeDecl *typeDecl,
                                                  Type propertyInterfaceType,
                                                  Type propertyContextType,
-                                                 bool isStatic) {
+                                                 bool isStatic,
+                                                 bool isFinal) {
   auto &C = tc.Context;
   auto parentDC = cast<DeclContext>(parentDecl);
   auto selfDecl = ParamDecl::createUnboundSelf(SourceLoc(), parentDC, isStatic);
@@ -125,6 +126,12 @@ FuncDecl *DerivedConformance::declareDerivedPropertyGetter(TypeChecker &tc,
   getterDecl->setImplicit();
   getterDecl->setStatic(isStatic);
 
+  // If this is supposed to be a final method, mark it as such.
+  assert(isFinal || !parentDC->getAsClassOrClassExtensionContext());
+  if (isFinal && parentDC->getAsClassOrClassExtensionContext() &&
+      !getterDecl->isFinal())
+    getterDecl->getAttrs().add(new (C) FinalAttr(/*IsImplicit=*/true));
+
   // Compute the type of the getter.
   GenericParamList *genericParams = getterDecl->getGenericParamsOfContext();
   Type type = FunctionType::get(TupleType::getEmpty(C),
@@ -171,7 +178,8 @@ DerivedConformance::declareDerivedReadOnlyProperty(TypeChecker &tc,
                                                    Type propertyInterfaceType,
                                                    Type propertyContextType,
                                                    FuncDecl *getterDecl,
-                                                   bool isStatic) {
+                                                   bool isStatic,
+                                                   bool isFinal) {
   auto &C = tc.Context;
   auto parentDC = cast<DeclContext>(parentDecl);
 
@@ -185,6 +193,12 @@ DerivedConformance::declareDerivedReadOnlyProperty(TypeChecker &tc,
   propDecl->setAccessibility(getterDecl->getFormalAccess());
   propDecl->setInterfaceType(propertyInterfaceType);
 
+  // If this is supposed to be a final property, mark it as such.
+  assert(isFinal || !parentDC->getAsClassOrClassExtensionContext());
+  if (isFinal && parentDC->getAsClassOrClassExtensionContext() &&
+      !propDecl->isFinal())
+    propDecl->getAttrs().add(new (C) FinalAttr(/*IsImplicit=*/true));
+
   Pattern *propPat = new (C) NamedPattern(propDecl, /*implicit*/ true);
   propPat->setType(propertyContextType);
   propPat = new (C) TypedPattern(propPat,
diff --git a/lib/Sema/DerivedConformances.h b/lib/Sema/DerivedConformances.h
index fab368e5f2d..84b6cf74090 100644
--- a/lib/Sema/DerivedConformances.h
+++ b/lib/Sema/DerivedConformances.h
@@ -113,7 +113,8 @@ FuncDecl *declareDerivedPropertyGetter(TypeChecker &tc,
                                        NominalTypeDecl *typeDecl,
                                        Type propertyInterfaceType,
                                        Type propertyContextType,
-                                       bool isStatic = false);
+                                       bool isStatic,
+                                       bool isFinal);
 
 /// Declare a read-only property with an existing getter.
 std::pair<VarDecl *, PatternBindingDecl *>
@@ -124,7 +125,8 @@ declareDerivedReadOnlyProperty(TypeChecker &tc,
                                Type propertyInterfaceType,
                                Type propertyContextType,
                                FuncDecl *getterDecl,
-                               bool isStatic = false);
+                               bool isStatic,
+                               bool isFinal);
 
 
 /// Build a reference to the 'self' decl of a derived function.
diff --git a/test/SILGen/Inputs/errors_other.swift b/test/SILGen/Inputs/errors_other.swift
new file mode 100644
index 00000000000..42d7ec632e4
--- /dev/null
+++ b/test/SILGen/Inputs/errors_other.swift
@@ -0,0 +1,6 @@
+// Parse of the 'errors' test.
+import Swift
+
+class OtherError : Error { }
+
+
diff --git a/test/SILGen/errors.swift b/test/SILGen/errors.swift
index ee0124b25b2..23e08154e5e 100644
--- a/test/SILGen/errors.swift
+++ b/test/SILGen/errors.swift
@@ -1,4 +1,4 @@
-// RUN: %target-swift-frontend -parse-stdlib -emit-silgen -verify %s | FileCheck %s
+// RUN: %target-swift-frontend -parse-stdlib -emit-silgen -verify -primary-file %s %S/Inputs/errors_other.swift | FileCheck %s
 
 import Swift
 
@@ -885,3 +885,17 @@ func testOptionalTryNeverFailsAddressOnly<T>(_ obj: T) {
 func testOptionalTryNeverFailsAddressOnlyVar<T>(_ obj: T) {
   var copy = try? obj // expected-warning {{no calls to throwing functions occur within 'try' expression}} expected-warning {{initialization of variable 'copy' was never used; consider replacing with assignment to '_' or removing it}}
 }
+
+class SomeErrorClass : Error { }
+
+// CHECK-LABEL: sil_vtable SomeErrorClass
+// CHECK-NEXT:   #SomeErrorClass.deinit!deallocator: _TFC6errors14SomeErrorClassD
+// CHECK-NEXT:   #SomeErrorClass.init!initializer.1: _TFC6errors14SomeErrorClasscfT_S0_
+// CHECK-NEXT: }
+
+class OtherErrorSub : OtherError { }
+
+// CHECK-LABEL: sil_vtable OtherErrorSub {
+// CHECK-NEXT:  #OtherError.init!initializer.1: _TFC6errors13OtherErrorSubcfT_S0_     // OtherErrorSub.init() -> OtherErrorSub
+// CHECK-NEXT:  #OtherErrorSub.deinit!deallocator: _TFC6errors13OtherErrorSubD        // OtherErrorSub.__deallocating_deinit
+// CHECK-NEXT:}

commit 11e85d73a443059b9b8c00300d498947cfa84782
Author: Tony Parker <anthony.parker@apple.com>
Date:   Tue Jun 21 10:40:57 2016 -0700

    Add a new set of initializers to Data to aid efficiency and clarity when wrapping NSData.
    
    addresses:
    rdar://problem/26385078 Data() really needs a init(length:) and replaceBytes(in:withBytes:)
    rdar://problem/26508250 Add more specific init method to Data to indicate keeping a ref type

diff --git a/stdlib/public/SDK/Foundation/Data.swift b/stdlib/public/SDK/Foundation/Data.swift
index c99ed9ec297..623c62d4b94 100644
--- a/stdlib/public/SDK/Foundation/Data.swift
+++ b/stdlib/public/SDK/Foundation/Data.swift
@@ -128,14 +128,29 @@ public struct Data : ReferenceConvertible, CustomStringConvertible, Equatable, H
     public init(bytes: UnsafePointer<UInt8>, count: Int) {
         _wrapped = _SwiftNSData(immutableObject: NSData(bytes: bytes, length: count))
     }
-    
+
+    /// Initialize a `Data` with copied memory content.
+    ///
+    /// - parameter bytes: A pointer to the memory. It will be copied.
+    /// - parameter count: The number of bytes to copy.
+    public init(bytes: UnsafeMutablePointer<UInt8>, count: Int) {
+        _wrapped = _SwiftNSData(immutableObject: NSData(bytes: UnsafePointer(bytes), length: count))
+    }
+
     /// Initialize a `Data` with copied memory content.
     /// 
     /// - parameter buffer: A buffer pointer to copy. The size is calculated from `SourceType` and `buffer.count`.
     public init<SourceType>(buffer: UnsafeBufferPointer<SourceType>) {
         _wrapped = _SwiftNSData(immutableObject: NSData(bytes: buffer.baseAddress, length: strideof(SourceType) * buffer.count))
     }
-    
+
+    /// Initialize a `Data` with copied memory content.
+    ///
+    /// - parameter buffer: A buffer pointer to copy. The size is calculated from `SourceType` and `buffer.count`.
+    public init<SourceType>(buffer: UnsafeMutableBufferPointer<SourceType>) {
+        _wrapped = _SwiftNSData(immutableObject: NSData(bytes: UnsafePointer(buffer.baseAddress), length: strideof(SourceType) * buffer.count))
+    }
+
     /// Initialize a `Data` with the contents of an Array.
     ///
     /// - parameter bytes: An array of bytes to copy.
@@ -156,15 +171,33 @@ public struct Data : ReferenceConvertible, CustomStringConvertible, Equatable, H
     
     /// Initialize a `Data` with the specified size.
     ///
+    /// This initializer doesn not necessarily allocate the requested memory right away. Mutable data allocates additional memory as needed, so `capacity` simply establishes the initial capacity. When it does allocate the initial memory, though, it allocates the specified amount.
+    ///
+    /// This method sets the `count` of the data to 0.
+    /// 
+    /// If the capacity specified in `capacity` is greater than four memory pages in size, this may round the amount of requested memory up to the nearest full page.
+    ///
     /// - parameter capacity: The size of the data.
     public init?(capacity: Int) {
         if let d = NSMutableData(capacity: capacity) {
-            _wrapped = _SwiftNSData(immutableObject: d)
+            _wrapped = _SwiftNSData(mutableObject: d)
         } else {
             return nil
         }
     }
     
+    /// Initialize a `Data` with the specified count of zeroed bytes.
+    ///
+    /// - parameter count: The number of bytes the data initially contains.
+    public init?(count: Int) {
+        if let d = NSMutableData(length: count) {
+            _wrapped = _SwiftNSData(mutableObject: d)
+        } else {
+            return nil
+        }
+    }
+
+
     /// Initialize an empty `Data`.
     public init() {
         _wrapped = _SwiftNSData(immutableObject: NSData(bytes: nil, length: 0))
@@ -217,9 +250,27 @@ public struct Data : ReferenceConvertible, CustomStringConvertible, Equatable, H
         }
     }
     
-    private init(_bridged data: NSData) {
-        // We must copy the input because it might be mutable; just like storing a value type in ObjC
-        _wrapped = _SwiftNSData(immutableObject: data.copy())
+    /// Initialize a `Data` by adopting a reference type.
+    ///
+    /// You can use this initializer to create a `struct Data` that wraps a `class NSData`. `struct Data` will use the `class NSData` for all operations. Other initializers (including casting using `as Data`) may choose to hold a reference or not, based on a what is the most efficient representation.
+    ///
+    /// If the resulting value is mutated, then `Data` will invoke the `mutableCopy()` function on the reference to copy the contents. You may customize the behavior of that function if you wish to return a specialized mutable subclass.
+    ///
+    /// - parameter reference: The instance of `NSData` that you wish to wrap. This instance will be copied by `struct Data`.
+    public init(reference: NSData) {
+        _wrapped = _SwiftNSData(immutableObject: reference.copy())
+    }
+    
+    /// Initialize a `Data` by adopting a mutable reference type.
+    ///
+    /// You can use this initializer to create a `struct Data` that wraps a `class NSMutableData`. `struct Data` will use the `class NSMutableData` for all operations. Other initializers (including casting using `as Data`) may choose to hold a reference or not, based on a what is the most efficient representation.
+    ///
+    /// If the resulting value is mutated, then `Data` will invoke the `mutableCopy()` function on the reference to copy the contents. You may customize the behavior of that function if you wish to return a specialized mutable subclass.
+    ///
+    /// - warning: For performance reasons, this method does not copy the reference on initialization. It assumes that the reference is uniquely held by the struct. If you continue to mutate the reference after invoking this initializer, you may invalidate the copy-on-write and value semantics of `struct Data`. It is recommended that you do not keep the reference after initializing a `Data` with it.
+    /// - parameter mutableReference: The instance of `NSMutableData` that you wish to wrap.
+    public init(mutableReference: NSMutableData) {
+        _wrapped = _SwiftNSData(mutableObject: mutableReference)
     }
     
     // -----------------------------------
@@ -428,6 +479,9 @@ public struct Data : ReferenceConvertible, CustomStringConvertible, Equatable, H
     
     /// Replace a region of bytes in the data with new data.
     ///
+    /// This will resize the data if required, to fit the entire contents of `data`.
+    ///
+    /// - precondition: `range` must be within the range of the data.
     /// - parameter range: The range in the data to replace.
     /// - parameter data: The replacement data.
     public mutating func replaceBytes(in range: Range<Index>, with data: Data) {
@@ -439,7 +493,23 @@ public struct Data : ReferenceConvertible, CustomStringConvertible, Equatable, H
             $0.replaceBytes(in: nsRange, withBytes: bytes, length: cnt)
         }
     }
+    
+    /// Replace a region of bytes in the data with new bytes from a buffer.
+    ///
+    /// This will resize the data if required, to fit the entire contents of `buffer`.
+    ///
+    /// - precondition: `range` must be within the range of the data.
+    /// - parameter buffer: The replacement bytes.
+    public mutating func replaceBytes<SourceType>(in range: Range<Index>, with buffer: UnsafeBufferPointer<SourceType>) {
+        let nsRange = NSMakeRange(range.lowerBound, range.upperBound - range.lowerBound)
+        let bufferCount = buffer.count * strideof(SourceType)
+        
+        _applyUnmanagedMutation {
+            $0.replaceBytes(in: nsRange, withBytes: buffer.baseAddress, length: bufferCount)
+        }
 
+    }
+    
     /// Return a new copy of the data in a specified range.
     ///
     /// - parameter range: The range to copy.
@@ -506,7 +576,7 @@ public struct Data : ReferenceConvertible, CustomStringConvertible, Equatable, H
         }
     }
     
-    public subscript(bounds: Range<Int>) -> MutableRandomAccessSlice<Data> {
+    public subscript(bounds: Range<Index>) -> MutableRandomAccessSlice<Data> {
         get {
             return MutableRandomAccessSlice(base: self, bounds: bounds)
         }
@@ -568,11 +638,13 @@ extension Data : _ObjectiveCBridgeable {
     }
     
     public static func _forceBridgeFromObjectiveC(_ input: NSData, result: inout Data?) {
-        result = Data(_bridged: input)
+        // We must copy the input because it might be mutable; just like storing a value type in ObjC
+        result = Data(reference: input)
     }
     
     public static func _conditionallyBridgeFromObjectiveC(_ input: NSData, result: inout Data?) -> Bool {
-        result = Data(_bridged: input)
+        // We must copy the input because it might be mutable; just like storing a value type in ObjC
+        result = Data(reference: input)
         return true
     }
     
diff --git a/test/1_stdlib/TestData.swift b/test/1_stdlib/TestData.swift
index 03a9f7ad46d..c34389cfee2 100644
--- a/test/1_stdlib/TestData.swift
+++ b/test/1_stdlib/TestData.swift
@@ -190,7 +190,7 @@ class TestData : TestDataSuper {
     
     func testCustomData() {
         let length = 5
-        let allOnesData = AllOnesData(length: length) as Data
+        let allOnesData = Data(reference: AllOnesData(length: length))
         expectEqual(1, allOnesData[0], "First byte of all 1s data should be 1")
         
         // Double the length
@@ -246,7 +246,7 @@ class TestData : TestDataSuper {
         let allOnes = AllOnesData(length: 64)
         
         // Type-erased
-        let data = allOnes as Data
+        let data = Data(reference: allOnes)
         
         // Create a home for our test data
         let dirPath = (NSTemporaryDirectory() as NSString).appendingPathComponent(NSUUID().uuidString)
@@ -291,7 +291,7 @@ class TestData : TestDataSuper {
         expectEqual(s.count, 2, "Expected only two entries in the Set")
     }
     
-    func testSubsequences() {
+    func testReplaceBytes() {
         var hello = dataFrom("Hello")
         let world = dataFrom("World")
         
@@ -306,7 +306,7 @@ class TestData : TestDataSuper {
         expectEqual(goodbyeWorld, expected)
     }
     
-    func testSubsequences2() {
+    func testReplaceBytes2() {
         let hello = dataFrom("Hello")
         let world = dataFrom(" World")
         let goodbye = dataFrom("Goodbye")
@@ -320,8 +320,29 @@ class TestData : TestDataSuper {
         }
         expectEqual(mutateMe, expected)
     }
+    
+    func testReplaceBytes3() {
+        // The expected result
+        let expectedBytes : [UInt8] = [1, 2, 9, 10, 11, 12, 13]
+        let expected = expectedBytes.withUnsafeBufferPointer {
+            return Data(buffer: $0)
+        }
+        
+        // The data we'll mutate
+        let someBytes : [UInt8] = [1, 2, 3, 4, 5]
+        var a = someBytes.withUnsafeBufferPointer {
+            return Data(buffer: $0)
+        }
+        
+        // The bytes we'll insert
+        let b : [UInt8] = [9, 10, 11, 12, 13]
+        b.withUnsafeBufferPointer {
+            a.replaceBytes(in: 2..<5, with: $0)
+        }
+        expectEqual(expected, a)
+    }
 
-    func testSubsequences3() {
+    func testRange() {
         let helloWorld = dataFrom("Hello World")
         let goodbye = dataFrom("Goodbye")
         let hello = dataFrom("Hello")
@@ -774,9 +795,10 @@ DataTests.test("testBridgingMutable") { TestData().testBridgingMutable() }
 DataTests.test("testBridgingCustom") { TestData().testBridgingCustom() }
 DataTests.test("testEquality") { TestData().testEquality() }
 DataTests.test("testDataInSet") { TestData().testDataInSet() }
-DataTests.test("testSubsequences") { TestData().testSubsequences() }
-DataTests.test("testSubsequences2") { TestData().testSubsequences2() }
-DataTests.test("testSubsequences3") { TestData().testSubsequences3() }
+DataTests.test("testReplaceBytes") { TestData().testReplaceBytes() }
+DataTests.test("testReplaceBytes2") { TestData().testReplaceBytes2() }
+DataTests.test("testReplaceBytes3") { TestData().testReplaceBytes3() }
+DataTests.test("testRange") { TestData().testRange() }
 DataTests.test("testInsertData") { TestData().testInsertData() }
 DataTests.test("testLoops") { TestData().testLoops() }
 DataTests.test("testGenericAlgorithms") { TestData().testGenericAlgorithms() }
@@ -811,6 +833,26 @@ DataTests.test("bounding failure replace") {
     data.replaceBytes(in: 5..<200, with: Data())
 }
 
+DataTests.test("bounding failure replace2") {
+    var data = "a".data(using: .utf8)!
+    var bytes : [UInt8] = [1, 2, 3]
+    expectCrashLater()
+    bytes.withUnsafeBufferPointer {
+        // lowerBound ok, upperBound after end of data
+        data.replaceBytes(in: 0..<2, with: $0)
+    }
+}
+
+DataTests.test("bounding failure replace3") {
+    var data = "a".data(using: .utf8)!
+    var bytes : [UInt8] = [1, 2, 3]
+    expectCrashLater()
+    bytes.withUnsafeBufferPointer {
+        // lowerBound is > length
+        data.replaceBytes(in: 2..<4, with: $0)
+    }
+}
+
 DataTests.test("bounding failure reset range") {
     var data = "Hello World".data(using: .utf8)!
     expectCrashLater()

commit 8634a57696db16775d066b2a878d219efc878f9a
Author: John McCall <rjmccall@apple.com>
Date:   Mon Apr 11 14:53:29 2016 -0700

    Only check the minimal set of generic requirements when opening
    a generic function type during constraint solving, as opposed to
    checking a bunch of implicit things that we already know.  This
    should significantly improve the efficiency of checking uses of
    generic APIs by reducing the total number of type variables and
    constraints.
    
    It is becoming increasingly funny to refer to this minimized generic
    signature as the "mangling" signature.
    
    The test changes are kind of a wash: in one case, we've eliminated
    a confusing extra error, but in another we've caused the confusing
    extra error to refer to '<<error type>>'.  Not worth fighting right
    now.  The reference-dependencies change is due to not needing to
    pull in all of those associated types anymore, which seems correct.

diff --git a/lib/Sema/CSApply.cpp b/lib/Sema/CSApply.cpp
index 5f74f0cb013..d319d1e7596 100644
--- a/lib/Sema/CSApply.cpp
+++ b/lib/Sema/CSApply.cpp
@@ -141,13 +141,16 @@ Type Solution::computeSubstitutions(
                         &conformance);
       (void)isOpenedAnyObject;
       assert((conforms ||
+              replacement->is<ErrorType>() ||
               firstArchetype->getIsRecursive() ||
               isOpenedAnyObject(replacement) ||
               replacement->is<GenericTypeParamType>()) &&
              "Constraint system missed a conformance?");
       (void)conforms;
 
-      assert(conformance || replacement->hasDependentProtocolConformances());
+      assert(conformance ||
+             replacement->is<ErrorType>() ||
+             replacement->hasDependentProtocolConformances());
       currentConformances.push_back(
                   ProtocolConformanceRef(protoType->getDecl(), conformance));
       break;
diff --git a/lib/Sema/CSRanking.cpp b/lib/Sema/CSRanking.cpp
index fe785eda39b..d82368acff3 100644
--- a/lib/Sema/CSRanking.cpp
+++ b/lib/Sema/CSRanking.cpp
@@ -450,8 +450,7 @@ static bool isProtocolExtensionAsSpecializedAs(TypeChecker &tc,
   // the second protocol extension.
   ConstraintSystem cs(tc, dc1, None);
   llvm::DenseMap<CanType, TypeVariableType *> replacements;
-  cs.openGeneric(dc2, sig2->getGenericParams(), sig2->getRequirements(),
-                 false, dc2->getGenericTypeContextDepth(),
+  cs.openGeneric(dc2, sig2, false, dc2->getGenericTypeContextDepth(),
                  ConstraintLocatorBuilder(nullptr),
                  replacements);
 
diff --git a/lib/Sema/ConstraintSystem.cpp b/lib/Sema/ConstraintSystem.cpp
index b4da061262e..41a4788efd8 100644
--- a/lib/Sema/ConstraintSystem.cpp
+++ b/lib/Sema/ConstraintSystem.cpp
@@ -572,8 +572,7 @@ namespace {
       if (auto genericFn = type->getAs<GenericFunctionType>()) {
         // Open up the generic parameters and requirements.
         cs.openGeneric(dc,
-                       genericFn->getGenericParams(),
-                       genericFn->getRequirements(),
+                       genericFn->getGenericSignature(),
                        skipProtocolSelfConstraint,
                        minOpeningDepth,
                        locator,
@@ -890,6 +889,27 @@ ConstraintSystem::getTypeOfReference(ValueDecl *value,
   return { valueType, valueType };
 }
 
+void ConstraintSystem::openGeneric(
+       DeclContext *dc,
+       GenericSignature *signature,
+       bool skipProtocolSelfConstraint,
+       unsigned minOpeningDepth,
+       ConstraintLocatorBuilder locator,
+       llvm::DenseMap<CanType, TypeVariableType *> &replacements) {
+  // Use the minimized constraints; we can re-derive solutions for all the
+  // implied constraints.
+  auto minimized =
+    signature->getCanonicalManglingSignature(*DC->getParentModule());
+
+  openGeneric(dc,
+              minimized->getGenericParams(),
+              minimized->getRequirements(),
+              skipProtocolSelfConstraint,
+              minOpeningDepth,
+              locator,
+              replacements);
+}
+
 void ConstraintSystem::openGeneric(
        DeclContext *dc,
        ArrayRef<GenericTypeParamType *> params,
@@ -1111,8 +1131,7 @@ ConstraintSystem::getTypeOfMemberReference(
     if (auto sig = dc->getGenericSignatureOfContext()) {
 
       // Open up the generic parameter list for the container.
-      openGeneric(dc, sig->getGenericParams(), sig->getRequirements(),
-                  /*skipProtocolSelfConstraint=*/true, minOpeningDepth,
+      openGeneric(dc, sig, /*skipProtocolSelfConstraint=*/true, minOpeningDepth,
                   locator, replacements);
 
       // Open up the type of the member.
diff --git a/lib/Sema/ConstraintSystem.h b/lib/Sema/ConstraintSystem.h
index 7f7e06826c7..df8403cb282 100644
--- a/lib/Sema/ConstraintSystem.h
+++ b/lib/Sema/ConstraintSystem.h
@@ -1552,6 +1552,12 @@ public:
   /// declaration's signature, but they do not have to, so they might not be
   /// substituted at all. Since the inner declaration inherits the context
   /// archetypes of the outer function we do not need to open them here.
+  void openGeneric(DeclContext *dc,
+                   GenericSignature *signature,
+                   bool skipProtocolSelfConstraint,
+                   unsigned minOpeningDepth,
+                   ConstraintLocatorBuilder locator,
+                   llvm::DenseMap<CanType, TypeVariableType *> &replacements);
   void openGeneric(DeclContext *dc,
                    ArrayRef<GenericTypeParamType *> params,
                    ArrayRef<Requirement> requirements,
diff --git a/lib/Sema/TypeCheckProtocol.cpp b/lib/Sema/TypeCheckProtocol.cpp
index ba8ba8e2cc6..623c0a5afc1 100644
--- a/lib/Sema/TypeCheckProtocol.cpp
+++ b/lib/Sema/TypeCheckProtocol.cpp
@@ -1006,8 +1006,7 @@ matchWitness(TypeChecker &tc,
             continue;
           }
 
-          // Inner generic parameter of the requirement -- fall through below.
-
+        // Inner generic parameter of the requirement -- fall through below.
         } else if (auto assocType = getReferencedAssocTypeOfProtocol(
                                                     replacement.first, proto)) {
 
diff --git a/test/Generics/associated_types.swift b/test/Generics/associated_types.swift
index 4626567ae42..a69e004d9c8 100644
--- a/test/Generics/associated_types.swift
+++ b/test/Generics/associated_types.swift
@@ -149,14 +149,14 @@ struct V<T> : Fooable {
   func foo(x: T) {}
 }
 
-// FIXME: <rdar://problem/16123805> Inferred associated types can't be used in expression contexts
+// FIXME: <rdar://problem/16123805> associated Inferred types can't be used in expression contexts
 var w = W.AssocType()
 var v = V<String>.AssocType()
 
 //
 // SR-427
 protocol A {
-  func c() // expected-note {{protocol requires function 'c()' with type '() -> ()'}}
+  func c()
 }
 
 protocol B : A {
@@ -164,11 +164,11 @@ protocol B : A {
 }
 
 extension B {
-  func c() { // expected-note {{candidate has non-matching type '<Self> () -> ()' (aka '<Ï„_0_0> () -> ()')}}
+  func c() {
   }
 }
 
-struct C<a : B> : B { // expected-error {{type 'C<a>' does not conform to protocol 'B'}} expected-error {{type 'C<a>' does not conform to protocol 'A'}}
+struct C<a : B> : B { // expected-error {{type 'C<a>' does not conform to protocol 'B'}}
 }
 
 // SR-511
diff --git a/test/NameBinding/reference-dependencies.swift b/test/NameBinding/reference-dependencies.swift
index 974f0115c4c..d95c5d01e1d 100644
--- a/test/NameBinding/reference-dependencies.swift
+++ b/test/NameBinding/reference-dependencies.swift
@@ -358,9 +358,6 @@ struct Sentinel2 {}
 // CHECK-DAG: - ["V4main10IntWrapper", "deinit"]
 // CHECK-DAG: - ["Ps10Comparable", ""]
 // CHECK-DAG: - ["C4main18ClassFromOtherFile", ""]
-// CHECK-DAG: - !private ["Si", "Distance"]
-// CHECK-DAG: - !private ["Si", "IntegerLiteralType"]
-// CHECK-DAG: - !private ["Si", "Stride"]
 // CHECK-DAG: - !private ["Si", "deinit"]
 // CHECK-DAG: - !private ["Si", "max"]
 // CHECK-DAG: - ["Ps23FloatLiteralConvertible", ""]
diff --git a/test/decl/protocol/protocols.swift b/test/decl/protocol/protocols.swift
index 2a249ebc72c..7bebd1ba92c 100644
--- a/test/decl/protocol/protocols.swift
+++ b/test/decl/protocol/protocols.swift
@@ -243,7 +243,7 @@ struct WrongIsEqual : IsEqualComparable { // expected-error{{type 'WrongIsEqual'
 //===----------------------------------------------------------------------===//
 
 func existentialSequence(e: Sequence) { // expected-error{{has Self or associated type requirements}}
-  var x = e.makeIterator() // expected-error{{type 'Sequence' does not conform to protocol 'IteratorProtocol'}}
+  var x = e.makeIterator() // expected-error{{'Sequence' is not convertible to '<<error type>>'}}
   x.next()
   x.nonexistent()
 }

commit c0021e1c6294e7c22dc0e043b3ae2198b0ce61be
Author: John McCall <rjmccall@apple.com>
Date:   Mon Apr 11 14:53:29 2016 -0700

    Only check the minimal set of generic requirements when opening
    a generic function type during constraint solving, as opposed to
    checking a bunch of implicit things that we already know.  This
    should significantly improve the efficiency of checking uses of
    generic APIs by reducing the total number of type variables and
    constraints.
    
    It is becoming increasingly funny to refer to this minimized generic
    signature as the "mangling" signature.
    
    The test changes are kind of a wash: in one case, we've eliminated
    a confusing extra error, but in another we've caused the confusing
    extra error to refer to '<<error type>>'.  Not worth fighting right
    now.  The reference-dependencies change is due to not needing to
    pull in all of those associated types anymore, which seems correct.

diff --git a/lib/Sema/CSApply.cpp b/lib/Sema/CSApply.cpp
index a1cdb4b906c..04ea6800c67 100644
--- a/lib/Sema/CSApply.cpp
+++ b/lib/Sema/CSApply.cpp
@@ -141,13 +141,16 @@ Type Solution::computeSubstitutions(
                         &conformance);
       (void)isOpenedAnyObject;
       assert((conforms ||
+              replacement->is<ErrorType>() ||
               firstArchetype->getIsRecursive() ||
               isOpenedAnyObject(replacement) ||
               replacement->is<GenericTypeParamType>()) &&
              "Constraint system missed a conformance?");
       (void)conforms;
 
-      assert(conformance || replacement->hasDependentProtocolConformances());
+      assert(conformance ||
+             replacement->is<ErrorType>() ||
+             replacement->hasDependentProtocolConformances());
       currentConformances.push_back(
                   ProtocolConformanceRef(protoType->getDecl(), conformance));
       break;
diff --git a/lib/Sema/CSRanking.cpp b/lib/Sema/CSRanking.cpp
index 84881577f2a..6daeaedd104 100644
--- a/lib/Sema/CSRanking.cpp
+++ b/lib/Sema/CSRanking.cpp
@@ -450,8 +450,7 @@ static bool isProtocolExtensionAsSpecializedAs(TypeChecker &tc,
   // the second protocol extension.
   ConstraintSystem cs(tc, dc1, None);
   llvm::DenseMap<CanType, TypeVariableType *> replacements;
-  cs.openGeneric(dc2, sig2->getGenericParams(), sig2->getRequirements(),
-                 false, dc2->getGenericTypeContextDepth(),
+  cs.openGeneric(dc2, sig2, false, dc2->getGenericTypeContextDepth(),
                  ConstraintLocatorBuilder(nullptr),
                  replacements);
 
diff --git a/lib/Sema/ConstraintSystem.cpp b/lib/Sema/ConstraintSystem.cpp
index b4da061262e..41a4788efd8 100644
--- a/lib/Sema/ConstraintSystem.cpp
+++ b/lib/Sema/ConstraintSystem.cpp
@@ -572,8 +572,7 @@ namespace {
       if (auto genericFn = type->getAs<GenericFunctionType>()) {
         // Open up the generic parameters and requirements.
         cs.openGeneric(dc,
-                       genericFn->getGenericParams(),
-                       genericFn->getRequirements(),
+                       genericFn->getGenericSignature(),
                        skipProtocolSelfConstraint,
                        minOpeningDepth,
                        locator,
@@ -890,6 +889,27 @@ ConstraintSystem::getTypeOfReference(ValueDecl *value,
   return { valueType, valueType };
 }
 
+void ConstraintSystem::openGeneric(
+       DeclContext *dc,
+       GenericSignature *signature,
+       bool skipProtocolSelfConstraint,
+       unsigned minOpeningDepth,
+       ConstraintLocatorBuilder locator,
+       llvm::DenseMap<CanType, TypeVariableType *> &replacements) {
+  // Use the minimized constraints; we can re-derive solutions for all the
+  // implied constraints.
+  auto minimized =
+    signature->getCanonicalManglingSignature(*DC->getParentModule());
+
+  openGeneric(dc,
+              minimized->getGenericParams(),
+              minimized->getRequirements(),
+              skipProtocolSelfConstraint,
+              minOpeningDepth,
+              locator,
+              replacements);
+}
+
 void ConstraintSystem::openGeneric(
        DeclContext *dc,
        ArrayRef<GenericTypeParamType *> params,
@@ -1111,8 +1131,7 @@ ConstraintSystem::getTypeOfMemberReference(
     if (auto sig = dc->getGenericSignatureOfContext()) {
 
       // Open up the generic parameter list for the container.
-      openGeneric(dc, sig->getGenericParams(), sig->getRequirements(),
-                  /*skipProtocolSelfConstraint=*/true, minOpeningDepth,
+      openGeneric(dc, sig, /*skipProtocolSelfConstraint=*/true, minOpeningDepth,
                   locator, replacements);
 
       // Open up the type of the member.
diff --git a/lib/Sema/ConstraintSystem.h b/lib/Sema/ConstraintSystem.h
index d328200dbd1..95d0624f096 100644
--- a/lib/Sema/ConstraintSystem.h
+++ b/lib/Sema/ConstraintSystem.h
@@ -1552,6 +1552,12 @@ public:
   /// declaration's signature, but they do not have to, so they might not be
   /// substituted at all. Since the inner declaration inherits the context
   /// archetypes of the outer function we do not need to open them here.
+  void openGeneric(DeclContext *dc,
+                   GenericSignature *signature,
+                   bool skipProtocolSelfConstraint,
+                   unsigned minOpeningDepth,
+                   ConstraintLocatorBuilder locator,
+                   llvm::DenseMap<CanType, TypeVariableType *> &replacements);
   void openGeneric(DeclContext *dc,
                    ArrayRef<GenericTypeParamType *> params,
                    ArrayRef<Requirement> requirements,
diff --git a/lib/Sema/TypeCheckProtocol.cpp b/lib/Sema/TypeCheckProtocol.cpp
index ba8ba8e2cc6..623c0a5afc1 100644
--- a/lib/Sema/TypeCheckProtocol.cpp
+++ b/lib/Sema/TypeCheckProtocol.cpp
@@ -1006,8 +1006,7 @@ matchWitness(TypeChecker &tc,
             continue;
           }
 
-          // Inner generic parameter of the requirement -- fall through below.
-
+        // Inner generic parameter of the requirement -- fall through below.
         } else if (auto assocType = getReferencedAssocTypeOfProtocol(
                                                     replacement.first, proto)) {
 
diff --git a/test/Generics/associated_types.swift b/test/Generics/associated_types.swift
index d2a261a8a55..e0e73da0ad8 100644
--- a/test/Generics/associated_types.swift
+++ b/test/Generics/associated_types.swift
@@ -149,14 +149,14 @@ struct V<T> : Fooable {
   func foo(_ x: T) {}
 }
 
-// FIXME: <rdar://problem/16123805> Inferred associated types can't be used in expression contexts
+// FIXME: <rdar://problem/16123805> associated Inferred types can't be used in expression contexts
 var w = W.AssocType()
 var v = V<String>.AssocType()
 
 //
 // SR-427
 protocol A {
-  func c() // expected-note {{protocol requires function 'c()' with type '() -> ()'}}
+  func c()
 }
 
 protocol B : A {
@@ -164,11 +164,11 @@ protocol B : A {
 }
 
 extension B {
-  func c() { // expected-note {{candidate has non-matching type '<Self> () -> ()' (aka '<Ï„_0_0> () -> ()')}}
+  func c() {
   }
 }
 
-struct C<a : B> : B { // expected-error {{type 'C<a>' does not conform to protocol 'B'}} expected-error {{type 'C<a>' does not conform to protocol 'A'}}
+struct C<a : B> : B { // expected-error {{type 'C<a>' does not conform to protocol 'B'}}
 }
 
 // SR-511
diff --git a/test/NameBinding/reference-dependencies.swift b/test/NameBinding/reference-dependencies.swift
index 3a0c18fe86a..a62c27ad099 100644
--- a/test/NameBinding/reference-dependencies.swift
+++ b/test/NameBinding/reference-dependencies.swift
@@ -358,9 +358,6 @@ struct Sentinel2 {}
 // CHECK-DAG: - ["V4main10IntWrapper", "deinit"]
 // CHECK-DAG: - ["Ps10Comparable", ""]
 // CHECK-DAG: - ["C4main18ClassFromOtherFile", ""]
-// CHECK-DAG: - !private ["Si", "Distance"]
-// CHECK-DAG: - !private ["Si", "IntegerLiteralType"]
-// CHECK-DAG: - !private ["Si", "Stride"]
 // CHECK-DAG: - !private ["Si", "deinit"]
 // CHECK-DAG: - !private ["Si", "max"]
 // CHECK-DAG: - ["Ps23FloatLiteralConvertible", ""]
diff --git a/test/decl/protocol/protocols.swift b/test/decl/protocol/protocols.swift
index 974a7c184d3..cd936639c7a 100644
--- a/test/decl/protocol/protocols.swift
+++ b/test/decl/protocol/protocols.swift
@@ -243,7 +243,7 @@ struct WrongIsEqual : IsEqualComparable { // expected-error{{type 'WrongIsEqual'
 //===----------------------------------------------------------------------===//
 
 func existentialSequence(_ e: Sequence) { // expected-error{{has Self or associated type requirements}}
-  var x = e.makeIterator() // expected-error{{type 'Sequence' does not conform to protocol 'IteratorProtocol'}}
+  var x = e.makeIterator() // expected-error{{'Sequence' is not convertible to '<<error type>>'}}
   x.next()
   x.nonexistent()
 }

commit d55ebaec423a7b7f0af6e81273c38a42ab6edcc8
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Wed Feb 17 13:41:32 2016 -0800

    [arc] Make all *RefCountStates and RCStateTransition trivially destructable and constructable.
    
    Tested via static assert.
    
    There is no reason for these data structures to not have these properties.
    Adding these properties will improve the compile time efficiency of ARC by
    allowing for cheaper copying and 0 cost destruction.

diff --git a/lib/SILOptimizer/ARC/RCStateTransition.cpp b/lib/SILOptimizer/ARC/RCStateTransition.cpp
index 8b0681ed8be..fac9bc8ec14 100644
--- a/lib/SILOptimizer/ARC/RCStateTransition.cpp
+++ b/lib/SILOptimizer/ARC/RCStateTransition.cpp
@@ -121,18 +121,6 @@ raw_ostream &llvm::operator<<(raw_ostream &os, RCStateTransitionKind Kind) {
   }
 #include "RCStateTransition.def"
 
-RCStateTransition::RCStateTransition(const RCStateTransition &R) {
-  Kind = R.Kind;
-  if (R.isEndPoint()) {
-    EndPoint = R.EndPoint;
-    return;
-  }
-
-  if (!R.isMutator())
-    return;
-  Mutators = R.Mutators;
-}
-
 bool RCStateTransition::matchingInst(SILInstruction *Inst) const {
   // We only pair mutators for now.
   if (!isMutator())
diff --git a/lib/SILOptimizer/ARC/RCStateTransition.def b/lib/SILOptimizer/ARC/RCStateTransition.def
index 7e1d01faf10..df9ed83fd4e 100644
--- a/lib/SILOptimizer/ARC/RCStateTransition.def
+++ b/lib/SILOptimizer/ARC/RCStateTransition.def
@@ -43,6 +43,8 @@
 /// Misc |
 ///-------
 ///
+/// An invalid transition kind. This must always be first so that it is zero.
+KIND(Invalid)
 /// An unknown kind.
 KIND(Unknown)
 /// An autorelease pool call.
diff --git a/lib/SILOptimizer/ARC/RCStateTransition.h b/lib/SILOptimizer/ARC/RCStateTransition.h
index 2fecbcca7fc..48675707786 100644
--- a/lib/SILOptimizer/ARC/RCStateTransition.h
+++ b/lib/SILOptimizer/ARC/RCStateTransition.h
@@ -13,6 +13,7 @@
 #ifndef SWIFT_SILOPTIMIZER_PASSMANAGER_ARC_RCSTATETRANSITION_H
 #define SWIFT_SILOPTIMIZER_PASSMANAGER_ARC_RCSTATETRANSITION_H
 
+#include "swift/Basic/type_traits.h"
 #include "swift/Basic/ImmutablePointerSet.h"
 #include "swift/SIL/SILArgument.h"
 #include "swift/SIL/SILInstruction.h"
@@ -60,10 +61,14 @@ RCStateTransitionKind getRCStateTransitionKind(ValueBase *V);
 //===----------------------------------------------------------------------===//
 
 class RefCountState;
+class BottomUpRefCountState;
+class TopDownRefCountState;
 
 /// Represents a transition in the RC history of a ref count.
 class RCStateTransition {
   friend class RefCountState;
+  friend class BottomUpRefCountState;
+  friend class TopDownRefCountState;
 
   /// An RCStateTransition can represent either an RC end point (i.e. an initial
   /// or terminal RC transition) or a ptr set of Mutators.
@@ -73,11 +78,11 @@ class RCStateTransition {
   RCStateTransitionKind Kind;
 
   // Should only be constructed be default RefCountState.
-  RCStateTransition() {}
+  RCStateTransition() = default;
 
 public:
-  ~RCStateTransition() {}
-  RCStateTransition(const RCStateTransition &R);
+  ~RCStateTransition() = default;
+  RCStateTransition(const RCStateTransition &R) = default;
 
   RCStateTransition(ImmutablePointerSet<SILInstruction> *I) {
     assert(I->size() == 1);
@@ -132,8 +137,15 @@ public:
   /// Attempt to merge \p Other into \p this. Returns true if we succeeded,
   /// false otherwise.
   bool merge(const RCStateTransition &Other);
+
+  /// Return true if the kind of this RCStateTransition is not 'Invalid'.
+  bool isValid() const { return getKind() != RCStateTransitionKind::Invalid; }
 };
 
+// These static assert checks are here for performance reasons.
+static_assert(IsTriviallyCopyable<RCStateTransition>::value,
+              "RCStateTransitions must be trivially copyable");
+
 } // end swift namespace
 
 namespace llvm {
diff --git a/lib/SILOptimizer/ARC/RefCountState.cpp b/lib/SILOptimizer/ARC/RefCountState.cpp
index e17fc4061d9..306c7841a7a 100644
--- a/lib/SILOptimizer/ARC/RefCountState.cpp
+++ b/lib/SILOptimizer/ARC/RefCountState.cpp
@@ -124,7 +124,7 @@ void BottomUpRefCountState::clear() {
   // be removed, be conservative and clear the transition state, so we do not
   // propagate KnownSafety forward.
   if (mightRemoveMutators())
-    Transition = None;
+    Transition = RCStateTransition();
   LatState = LatticeState::None;
   SuperTy::clear();
 }
@@ -261,13 +261,13 @@ bool BottomUpRefCountState::handleGuaranteedUser(
 bool BottomUpRefCountState::isRefCountInstMatchedToTrackedInstruction(
     SILInstruction *RefCountInst) {
   // If we are not tracking any state transitions bail.
-  if (!Transition.hasValue())
+  if (!Transition.isValid())
     return false;
 
   // Otherwise, ask the transition state if this instruction causes a
   // transition that can be matched with the transition in order to eliminate
   // the transition.
-  if (!Transition->matchingInst(RefCountInst))
+  if (!Transition.matchingInst(RefCountInst))
     return false;
 
   return handleRefCountInstMatch(RefCountInst);
@@ -328,8 +328,8 @@ bool BottomUpRefCountState::merge(const BottomUpRefCountState &Other) {
     return false;
   }
 
-  if (!Transition.hasValue() || !Other.Transition.hasValue() ||
-      !Transition->merge(Other.Transition.getValue())) {
+  if (!Transition.isValid() || !Other.Transition.isValid() ||
+      !Transition.merge(Other.Transition)) {
     DEBUG(llvm::dbgs() << "            Failed merge!\n");
     clear();
     return false;
@@ -568,7 +568,7 @@ bool TopDownRefCountState::initWithMutatorInst(
 void TopDownRefCountState::initWithArg(SILArgument *Arg) {
   LatState = LatticeState::Incremented;
   Transition = RCStateTransition(Arg);
-  assert((*Transition).getKind() == RCStateTransitionKind::StrongEntrance &&
+  assert(Transition.getKind() == RCStateTransitionKind::StrongEntrance &&
          "Expected a strong entrance here");
   RCRoot = Arg;
   KnownSafe = false;
@@ -581,7 +581,7 @@ void TopDownRefCountState::initWithEntranceInst(
     ImmutablePointerSet<SILInstruction> *I, SILValue RCIdentity) {
   LatState = LatticeState::Incremented;
   Transition = RCStateTransition(I);
-  assert((*Transition).getKind() == RCStateTransitionKind::StrongEntrance &&
+  assert(Transition.getKind() == RCStateTransitionKind::StrongEntrance &&
          "Expected a strong entrance here");
   RCRoot = RCIdentity;
   KnownSafe = false;
@@ -590,7 +590,7 @@ void TopDownRefCountState::initWithEntranceInst(
 
 /// Uninitialize the current state.
 void TopDownRefCountState::clear() {
-  Transition = None;
+  Transition = RCStateTransition();
   LatState = LatticeState::None;
   SuperTy::clear();
 }
@@ -719,13 +719,13 @@ bool TopDownRefCountState::handleGuaranteedUser(
 bool TopDownRefCountState::isRefCountInstMatchedToTrackedInstruction(
     SILInstruction *RefCountInst) {
   // If we are not tracking any state transitions bail.
-  if (!Transition.hasValue())
+  if (!Transition.isValid())
     return false;
 
   // Otherwise, ask the transition state if this instruction causes a
   // transition that can be matched with the transition in order to eliminate
   // the transition.
-  if (!Transition->matchingInst(RefCountInst))
+  if (!Transition.matchingInst(RefCountInst))
     return false;
 
   return handleRefCountInstMatch(RefCountInst);
@@ -783,8 +783,8 @@ bool TopDownRefCountState::merge(const TopDownRefCountState &Other) {
     return false;
   }
 
-  if (!Transition.hasValue() || !Other.Transition.hasValue() ||
-      !Transition->merge(Other.Transition.getValue())) {
+  if (!Transition.isValid() || !Other.Transition.isValid() ||
+      !Transition.merge(Other.Transition)) {
     DEBUG(llvm::dbgs() << "            Failed merge!\n");
     clear();
     return false;
diff --git a/lib/SILOptimizer/ARC/RefCountState.h b/lib/SILOptimizer/ARC/RefCountState.h
index 71095a33f1e..e2f8cb8c804 100644
--- a/lib/SILOptimizer/ARC/RefCountState.h
+++ b/lib/SILOptimizer/ARC/RefCountState.h
@@ -14,6 +14,7 @@
 #define SWIFT_SILOPTIMIZER_PASSMANAGER_ARC_REFCOUNTSTATE_H
 
 #include "RCStateTransition.h"
+#include "swift/Basic/type_traits.h"
 #include "swift/Basic/Fallthrough.h"
 #include "swift/Basic/ImmutablePointerSet.h"
 #include "swift/SIL/SILInstruction.h"
@@ -22,7 +23,6 @@
 #include "swift/SIL/InstructionUtils.h"
 #include "swift/SILOptimizer/Analysis/ARCAnalysis.h"
 #include <algorithm>
-#include <type_traits>
 
 namespace swift {
 class AliasAnalysis;
@@ -37,9 +37,6 @@ namespace swift {
 /// A struct that abstracts over reference counts manipulated by strong_retain,
 /// retain_value, strong_release,
 class RefCountState {
-public:
-  using InstructionSet = llvm::SmallPtrSet<SILInstruction *, 1>;
-
 protected:
   /// Return the SILValue that represents the RCRoot that we are
   /// tracking.
@@ -47,7 +44,7 @@ protected:
 
   /// The last state transition that this RefCountState went through. None if we
   /// have not see any transition on this ref count yet.
-  llvm::Optional<RCStateTransition> Transition;
+  RCStateTransition Transition;
 
   /// Was the pointer we are tracking known incremented when we visited the
   /// current increment we are tracking? In that case we know that it is safe
@@ -67,8 +64,8 @@ protected:
   bool Partial = false;
 
 public:
-  RefCountState() {}
-  ~RefCountState() {}
+  RefCountState() = default;
+  ~RefCountState() = default;
   RefCountState(const RefCountState &) = default;
   RefCountState &operator=(const RefCountState &) = default;
   RefCountState(RefCountState &&) = default;
@@ -83,7 +80,7 @@ public:
     bool Nested = isTrackingRefCount();
 
     Transition = RCStateTransition(I);
-    assert((*Transition).isMutator() && "Expected I to be a mutator!\n");
+    assert(Transition.isMutator() && "Expected I to be a mutator!\n");
 
     // Initialize KnownSafe to a conservative false value.
     KnownSafe = false;
@@ -105,31 +102,29 @@ public:
   }
 
   /// Is this ref count initialized and tracking a ref count ptr.
-  bool isTrackingRefCount() const {
-    return Transition.hasValue();
-  }
+  bool isTrackingRefCount() const { return Transition.isValid(); }
 
   /// Are we tracking an instruction currently? This returns false when given an
   /// uninitialized ReferenceCountState.
   bool isTrackingRefCountInst() const {
-    return Transition.hasValue() && Transition->isMutator();
+    return Transition.isValid() && Transition.isMutator();
   }
 
   /// Are we tracking a source of ref counts? This currently means that we are
   /// tracking an argument that is @owned. In the future this will include
   /// return values of functions that are @owned.
   bool isTrackingRefCountSource() const {
-    return Transition.hasValue() && Transition->isEndPoint();
+    return Transition.isValid() && Transition.isEndPoint();
   }
 
   /// Return the increment we are tracking.
   RCStateTransition::mutator_range getInstructions() const {
-    return Transition->getMutators();
+    return Transition.getMutators();
   }
 
   /// Returns true if I is in the instructions we are tracking.
   bool containsInstruction(SILInstruction *I) const {
-    return Transition.hasValue() && Transition->containsMutator(I);
+    return Transition.isValid() && Transition.containsMutator(I);
   }
 
   /// Return the value with reference semantics that is the operand of our
@@ -200,7 +195,8 @@ private:
   bool FoundNonARCUser = false;
 
 public:
-  BottomUpRefCountState() {}
+  BottomUpRefCountState() = default;
+  ~BottomUpRefCountState() = default;
   BottomUpRefCountState(const BottomUpRefCountState &) = default;
   BottomUpRefCountState &operator=(const BottomUpRefCountState &) = default;
   BottomUpRefCountState(BottomUpRefCountState &&) = default;
@@ -344,7 +340,8 @@ private:
   LatticeState LatState = LatticeState::None;
 
 public:
-  TopDownRefCountState() {}
+  TopDownRefCountState() = default;
+  ~TopDownRefCountState() = default;
   TopDownRefCountState(const TopDownRefCountState &) = default;
   TopDownRefCountState &operator=(const TopDownRefCountState &) = default;
   TopDownRefCountState(TopDownRefCountState &&) = default;
@@ -454,6 +451,12 @@ private:
   bool handleRefCountInstMatch(SILInstruction *RefCountInst);
 };
 
+// These static asserts are here for performance reasons.
+static_assert(IsTriviallyCopyable<BottomUpRefCountState>::value,
+              "All ref count states must be trivially copyable");
+static_assert(IsTriviallyCopyable<TopDownRefCountState>::value,
+              "All ref count states must be trivially copyable");
+
 } // end swift namespace
 
 namespace llvm {

commit 925eb2e0d9717bb9a659bb36b8bbe6831c13d428
Author: Xin Tong <xin_tong@apple.com>
Date:   Tue Jan 26 10:42:13 2016 -0800

    Correct an inefficiency in initial state of the data flow in RLE

diff --git a/lib/SILOptimizer/Transforms/RedundantLoadElimination.cpp b/lib/SILOptimizer/Transforms/RedundantLoadElimination.cpp
index 50715a2d182..07cbdafbfdb 100644
--- a/lib/SILOptimizer/Transforms/RedundantLoadElimination.cpp
+++ b/lib/SILOptimizer/Transforms/RedundantLoadElimination.cpp
@@ -324,7 +324,9 @@ public:
     ForwardSetIn.resize(LocationNum, false);
     ForwardSetOut.resize(LocationNum, optimistic);
 
-    ForwardSetMax.resize(LocationNum, true);
+    // If we are running an optimsitic data flow, set forward max to true
+    // initially.
+    ForwardSetMax.resize(LocationNum, optimistic);
 
     BBGenSet.resize(LocationNum, false);
     BBKillSet.resize(LocationNum, false);

commit 7eff55dc3c56bfbec53c47f7d2f0c4235750901c
Author: Nadav Rotem <nrotem@apple.com>
Date:   Fri Jan 1 15:26:08 2016 -0800

    [Mangler Utils] Fix an off-by-one error in the calculation of the longest huffman encoding for our alphabet.
    
    We calculate a conservative estimate of how long our bitstream needs to be to
    hold the decoded string.  This commit fixes an off-by-one error in the
    calculation of the longest possible encoding sequence for our alphabet.  The bug
    was that we considered both the root of the tree and the leafs of the tree for
    the distance calculation. However, only edges should be considered for the
    length calculation because edges represent the one/zero bits.
    
    This commit can improve the efficiency of our encoding because the APInts we
    need to work with will be slightly shorter.

diff --git a/utils/name-compression/HuffGen.py b/utils/name-compression/HuffGen.py
index 9fd15eaf38d..52e1a38f7fd 100644
--- a/utils/name-compression/HuffGen.py
+++ b/utils/name-compression/HuffGen.py
@@ -47,7 +47,7 @@ class Node:
 
   def getMaxEncodingLength(self):
     """ Return the length of the longest possible encoding word"""
-    v = 1
+    v = 0
     if self.left:  v = max(v, 1 + self.left .getMaxEncodingLength())
     if self.right: v = max(v, 1 + self.right.getMaxEncodingLength())
     return v

commit 42ecad79030e6952dd36ec0a3de8e6dd5a7f79b5
Author: Xi Ge <xi_ge@apple.com>
Date:   Fri Jul 17 00:03:22 2015 +0000

    [SyntaxModel] Improve the efficiency of url regex search.
    We improve the efficiency through two ways: 1. read the protocol keywords, such as
    http, radar, or im, to select the right regex pattern to search for instead of
    trying every known patterns; and 2. reduce the regex search space by truncating
    text before protocol keywords. rdar://21009781
    
    Swift SVN r30290

diff --git a/lib/IDE/SyntaxModel.cpp b/lib/IDE/SyntaxModel.cpp
index 50a7881512b..b4208670d36 100644
--- a/lib/IDE/SyntaxModel.cpp
+++ b/lib/IDE/SyntaxModel.cpp
@@ -188,6 +188,22 @@ struct StructureElement {
     :StructureNode(StructureNode), ASTNode(ASTNode) { }
 };
 
+static const std::vector<std::string> URLPro = {
+
+  // Use RegexStrURL:
+  "acap", "afp", "afs", "cid", "data", "fax", "feed", "file", "ftp", "go",
+  "gopher", "http", "https", "imap", "ldap", "mailserver", "mid", "modem",
+  "news", "nntp", "opaquelocktoken", "pop", "prospero", "rdar", "rtsp", "service"
+  "sip", "soap.beep", "soap.beeps", "tel", "telnet", "tip", "tn3270", "urn",
+  "vemmi", "wais", "xcdoc", "z39.50r","z39.50s",
+
+  // Use RegexStrMailURL:
+  "mailto", "im",
+
+  // Use RegexStrRadarURL:
+  "radar"
+};
+
 static const char *const RegexStrURL =
   "(acap|afp|afs|cid|data|fax|feed|file|ftp|go|"
   "gopher|http|https|imap|ldap|mailserver|mid|modem|news|nntp|opaquelocktoken|"
@@ -260,6 +276,7 @@ public:
   bool shouldWalkIntoFunctionGenericParams() override { return true; }
 
 private:
+  bool findUrlStartingLoc(StringRef Text, unsigned &Start, std::regex& Regex);
   bool annotateIfConfigConditionIdentifiers(Expr *Cond);
   bool handleAttrs(const DeclAttributes &Attrs);
   bool handleAttrs(const TypeAttributes &Attrs);
@@ -1159,32 +1176,54 @@ bool ModelASTWalker::processComment(CharSourceRange Range) {
   return searchForURL(AfterMarker);  
 }
 
+bool ModelASTWalker::findUrlStartingLoc(StringRef Text,
+                                        unsigned &Start,
+                                        std::regex &Regex) {
+  static const auto MailToPosition = std::find(URLPro.begin(), URLPro.end(),
+                                               "mailto");
+  static const auto RadarPosition = std::find(URLPro.begin(), URLPro.end(),
+                                              "radar");
+  auto Index = Text.find("://");
+  if (Index != StringRef::npos) {
+    for (auto It = URLPro.begin(); It != URLPro.end(); ++ It) {
+      if (Index >= It->size() &&
+          Text.substr(Index - It->size(), It->size()) == *It) {
+        Start = Index - It->size();
+        if (It < MailToPosition)
+          Regex = URLRxs[0];
+        else if (It < RadarPosition)
+          Regex = URLRxs[1];
+        else
+          Regex = URLRxs[2];
+        return true;
+      }
+    }
+  }
+  return false;
+}
+
 bool ModelASTWalker::searchForURL(CharSourceRange Range) {
   StringRef OrigText = SM.extractText(Range, BufferID);
   SourceLoc OrigLoc = Range.getStart();
 
-  // URLs are uncommon, do a fast check before the regex one.
-  if (OrigText.find("://") == StringRef::npos)
-    return true;
-
   StringRef Text = OrigText;
   while (1) {
     std::match_results<StringRef::iterator> Matches;
-    for (auto &Rx : URLRxs) {
-      bool HadMatch = std::regex_search(Text.begin(), Text.end(), Matches, Rx);
-      if (HadMatch)
-        break;
-    }
-    if (Matches.empty())
+    std::regex &Regex = URLRxs[0];
+    unsigned Start;
+    if (findUrlStartingLoc(Text, Start, Regex) &&
+        std::regex_search(Text.substr(Start).begin(),
+                          Text.substr(Start).end(), Matches, Regex)) {
+      auto &RxMatch = Matches[0];
+      StringRef Match(RxMatch.first, RxMatch.second - RxMatch.first);
+      SourceLoc Loc = OrigLoc.getAdvancedLoc(Match.data() - OrigText.data());
+      CharSourceRange Range(Loc, Match.size());
+      SyntaxNode Node{ SyntaxNodeKind::CommentURL, Range };
+      if (!passNode(Node))
+        return false;
+      Text = Text.substr(Match.data() - Text.data() + Match.size());
+    } else
       break;
-    auto &RxMatch = Matches[0];
-    StringRef Match(RxMatch.first, RxMatch.second - RxMatch.first);
-    SourceLoc Loc = OrigLoc.getAdvancedLoc(Match.data() - OrigText.data());
-    CharSourceRange Range(Loc, Match.size());
-    SyntaxNode Node{ SyntaxNodeKind::CommentURL, Range };
-    if (!passNode(Node))
-      return false;
-    Text = Text.substr(Match.data() - Text.data() + Match.size());
   }
   return true;
 }

commit 3f375386aaadcf8b954ef93e085826370fb571a5
Author: Andrew Trick <atrick@apple.com>
Date:   Sat May 30 04:04:26 2015 +0000

    Cleanup ClosureSpecializer.
    
    Now that we use ValueLifetime, restrucure some of the code for clarity and efficiency.
    
    While we're at it, make sure we're not copying vectors and maps needlessly.
    
    Swift SVN r29166

diff --git a/lib/SILPasses/ClosureSpecializer.cpp b/lib/SILPasses/ClosureSpecializer.cpp
index 45292cc6070..d62e35e2631 100644
--- a/lib/SILPasses/ClosureSpecializer.cpp
+++ b/lib/SILPasses/ClosureSpecializer.cpp
@@ -140,9 +140,10 @@ private:
 //===----------------------------------------------------------------------===//
 
 namespace {
+struct ClosureInfo;
 
 class CallSiteDescriptor {
-  SILInstruction *Closure;
+  ClosureInfo *CInfo;
   ApplyInst *AI;
   unsigned ClosureIndex;
   SILParameterInfo ClosureParamInfo;
@@ -151,35 +152,31 @@ class CallSiteDescriptor {
   // have only one element, a return inst.
   llvm::TinyPtrVector<SILBasicBlock *> NonFailureExitBBs;
 
-  // The points where the closure is ultimately released.
-  llvm::SmallSetVector<SILInstruction *, 4> JointlyPostDomReleasePoints;
-
 public:
-  CallSiteDescriptor(SILInstruction *ClosureInst, ApplyInst *AI,
+  CallSiteDescriptor(ClosureInfo *CInfo, ApplyInst *AI,
                      unsigned ClosureIndex, SILParameterInfo ClosureParamInfo,
-                     llvm::TinyPtrVector<SILBasicBlock *> &&NonFailureExitBBs,
-                     ValueLifetime &ClosureLifetime)
-      : Closure(ClosureInst), AI(AI), ClosureIndex(ClosureIndex),
-        ClosureParamInfo(ClosureParamInfo),
-        NonFailureExitBBs(NonFailureExitBBs),
-        JointlyPostDomReleasePoints(ClosureLifetime.LastUsers) {}
+                     llvm::TinyPtrVector<SILBasicBlock *> &&NonFailureExitBBs)
+    : CInfo(CInfo), AI(AI), ClosureIndex(ClosureIndex),
+      ClosureParamInfo(ClosureParamInfo),
+      NonFailureExitBBs(NonFailureExitBBs) {}
 
-  static bool isSupportedClosure(const SILInstruction *Closure);
+  CallSiteDescriptor(CallSiteDescriptor&&) =default;
+  CallSiteDescriptor &operator=(CallSiteDescriptor &&) =default;
 
   SILFunction *getApplyCallee() const {
     return cast<FunctionRefInst>(AI->getCallee())->getReferencedFunction();
   }
 
   SILFunction *getClosureCallee() const {
-    if (auto *PAI = dyn_cast<PartialApplyInst>(Closure))
+    if (auto *PAI = dyn_cast<PartialApplyInst>(getClosure()))
       return cast<FunctionRefInst>(PAI->getCallee())->getReferencedFunction();
 
-    auto *TTTFI = cast<ThinToThickFunctionInst>(Closure);
+    auto *TTTFI = cast<ThinToThickFunctionInst>(getClosure());
     return cast<FunctionRefInst>(TTTFI->getCallee())->getReferencedFunction();
   }
 
   bool closureHasRefSemanticContext() const {
-    return isa<PartialApplyInst>(Closure);
+    return isa<PartialApplyInst>(getClosure());
   }
 
   unsigned getClosureIndex() const { return ClosureIndex; }
@@ -189,40 +186,38 @@ public:
   SILInstruction *
   createNewClosure(SILBuilder &B, SILValue V,
                    llvm::SmallVectorImpl<SILValue> &Args) const {
-    if (isa<PartialApplyInst>(Closure))
-      return B.createPartialApply(Closure->getLoc(), V, V.getType(), {}, Args,
-                                  Closure->getType(0));
+    if (isa<PartialApplyInst>(getClosure()))
+      return B.createPartialApply(getClosure()->getLoc(), V, V.getType(), {},
+                                  Args, getClosure()->getType(0));
 
-    assert(isa<ThinToThickFunctionInst>(Closure) &&
+    assert(isa<ThinToThickFunctionInst>(getClosure()) &&
            "We only support partial_apply and thin_to_thick_function");
-    return B.createThinToThickFunction(Closure->getLoc(), V,
-                                       Closure->getType(0));
+    return B.createThinToThickFunction(getClosure()->getLoc(), V,
+                                       getClosure()->getType(0));
   }
 
   ApplyInst *getApplyInst() const { return AI; }
 
-  void specializeClosure(CallGraph &CG) const;
-
   void createName(llvm::SmallString<64> &NewName) const;
 
   OperandValueArrayRef getArguments() const {
-    if (auto *PAI = dyn_cast<PartialApplyInst>(Closure))
+    if (auto *PAI = dyn_cast<PartialApplyInst>(getClosure()))
       return PAI->getArguments();
 
     // Thin to thick function has no non-callee arguments.
-    assert(isa<ThinToThickFunctionInst>(Closure) &&
+    assert(isa<ThinToThickFunctionInst>(getClosure()) &&
            "We only support partial_apply and thin_to_thick_function");
     return OperandValueArrayRef(ArrayRef<Operand>());
   }
 
-  SILInstruction *getClosure() const { return Closure; }
+  inline SILInstruction *getClosure() const;
 
   unsigned getNumArguments() const {
-    if (auto *PAI = dyn_cast<PartialApplyInst>(Closure))
+    if (auto *PAI = dyn_cast<PartialApplyInst>(getClosure()))
       return PAI->getNumArguments();
 
     // Thin to thick function has no non-callee arguments.
-    assert(isa<ThinToThickFunctionInst>(Closure) &&
+    assert(isa<ThinToThickFunctionInst>(getClosure()) &&
            "We only support partial_apply and thin_to_thick_function");
     return 0;
   }
@@ -235,7 +230,7 @@ public:
     return getClosureParameterInfo().isConsumed();
   }
 
-  SILLocation getLoc() const { return Closure->getLoc(); }
+  SILLocation getLoc() const { return getClosure()->getLoc(); }
 
   SILModule &getModule() const { return AI->getModule(); }
 
@@ -246,9 +241,25 @@ public:
   /// Extend the lifetime of 'Arg' to the lifetime of the closure.
   void extendArgumentLifetime(SILValue Arg) const;
 };
+} // end anonymous namespace
 
+namespace {
+struct ClosureInfo {
+  SILInstruction *Closure;
+  ValueLifetime Lifetime;
+  llvm::SmallVector<CallSiteDescriptor, 8> CallSites;
+
+  ClosureInfo(SILInstruction *Closure): Closure(Closure) {}
+
+  ClosureInfo(ClosureInfo &&) =default;
+  ClosureInfo &operator=(ClosureInfo &&) =default;
+};
 } // end anonymous namespace
 
+SILInstruction *CallSiteDescriptor::getClosure() const {
+  return CInfo->Closure;
+}
+
 /// Update the callsite to pass in the correct arguments.
 static void rewriteApplyInst(const CallSiteDescriptor &CSDesc,
                              SILFunction *NewF,
@@ -362,45 +373,46 @@ void CallSiteDescriptor::createName(llvm::SmallString<64> &NewName) const {
   Mangle::Mangler M(buffer);
   auto P = Mangle::SpecializationPass::ClosureSpecializer;
   Mangle::FunctionSignatureSpecializationMangler FSSM(P, M, getApplyCallee());
-  if (auto *PAI = dyn_cast<PartialApplyInst>(Closure)) {
+  if (auto *PAI = dyn_cast<PartialApplyInst>(getClosure())) {
     FSSM.setArgumentClosureProp(getClosureIndex(), PAI);
     FSSM.mangle();
     return;
   }
 
-  auto *TTTFI = cast<ThinToThickFunctionInst>(Closure);
+  auto *TTTFI = cast<ThinToThickFunctionInst>(getClosure());
   FSSM.setArgumentClosureProp(getClosureIndex(), TTTFI);
   FSSM.mangle();
 }
 
 void CallSiteDescriptor::extendArgumentLifetime(SILValue Arg) const {
-  assert(!JointlyPostDomReleasePoints.empty() &&
+  assert(!CInfo->Lifetime.getLastUsers().empty() &&
          "Need a post-dominating release(s)");
 
   // Extend the lifetime of a captured argument to cover the callee.
-  SILBuilderWithScope<2> Builder(Closure);
-  Builder.createRetainValue(Closure->getLoc(), Arg);
-  for (auto *I : JointlyPostDomReleasePoints) {
+  SILBuilderWithScope<2> Builder(getClosure());
+  Builder.createRetainValue(getClosure()->getLoc(), Arg);
+  for (auto *I : CInfo->Lifetime.getLastUsers()) {
     auto It = SILBasicBlock::iterator(*I);
     Builder.setInsertionPoint(++It);
-    Builder.createReleaseValue(Closure->getLoc(), Arg);
+    Builder.createReleaseValue(getClosure()->getLoc(), Arg);
   }
 }
 
-void CallSiteDescriptor::specializeClosure(CallGraph &CG) const {
+static void specializeClosure(CallGraph &CG, ClosureInfo &CInfo,
+                              CallSiteDescriptor &CallDesc) {
   llvm::SmallString<64> NewFName;
-  createName(NewFName);
+  CallDesc.createName(NewFName);
   DEBUG(llvm::dbgs() << "    Perform optimizations with new name " << NewFName
                      << '\n');
 
   // Then see if we already have a specialized version of this function in our
   // module.
-  SILFunction *NewF = getModule().lookUpFunction(NewFName);
+  SILFunction *NewF = CInfo.Closure->getModule().lookUpFunction(NewFName);
 
   // If not, create a specialized version of ApplyCallee calling the closure
   // directly.
   if (!NewF) {
-    NewF = ClosureSpecCloner::cloneFunction(*this, NewFName);
+    NewF = ClosureSpecCloner::cloneFunction(CallDesc, NewFName);
 
     // Update the call graph with the newly created function.
     CallGraphEditor Editor(CG);
@@ -408,10 +420,10 @@ void CallSiteDescriptor::specializeClosure(CallGraph &CG) const {
   }
 
   // Rewrite the call
-  rewriteApplyInst(*this, NewF, CG);
+  rewriteApplyInst(CallDesc, NewF, CG);
 }
 
-bool CallSiteDescriptor::isSupportedClosure(const SILInstruction *Closure) {
+static bool isSupportedClosure(const SILInstruction *Closure) {
   if (!isSupportedClosureKind(Closure))
     return false;
 
@@ -651,7 +663,7 @@ public:
   ClosureSpecializer() = default;
 
   void gatherCallSites(SILFunction *Caller,
-                       llvm::SmallVectorImpl<CallSiteDescriptor> &CallSites,
+                       llvm::SmallVectorImpl<ClosureInfo*> &ClosureCandidates,
                        llvm::SmallPtrSet<ApplyInst *, 4> &MultipleClosureAI);
   bool specialize(SILFunction *Caller, CallGraph &CG);
 
@@ -669,7 +681,8 @@ public:
 } // end anonymous namespace
 
 void ClosureSpecializer::gatherCallSites(
-    SILFunction *Caller, llvm::SmallVectorImpl<CallSiteDescriptor> &CallSites,
+    SILFunction *Caller,
+    llvm::SmallVectorImpl<ClosureInfo*> &ClosureCandidates,
     llvm::SmallPtrSet<ApplyInst *, 4> &MultipleClosureAI) {
 
   // A set of apply inst that we have associated with a closure. We use this to
@@ -679,14 +692,14 @@ void ClosureSpecializer::gatherCallSites(
   // For each basic block BB in Caller...
   for (auto &BB : *Caller) {
 
-    ValueLifetime ClosureLifetime;
-
     // For each instruction II in BB...
     for (auto &II : BB) {
       // If II is not a closure that we support specializing, skip it...
-      if (!CallSiteDescriptor::isSupportedClosure(&II))
+      if (!isSupportedClosure(&II))
         continue;
 
+      ClosureInfo *CInfo = nullptr;
+
       // Go through all uses of our closure.
       for (auto *Use : II.getUses()) {
         // If this use use is not an apply inst or an apply inst with
@@ -762,18 +775,20 @@ void ClosureSpecializer::gatherCallSites(
 
         // Compute the final release points of the closure. We will insert
         // release of the captured arguments here.
-        if (ClosureLifetime.getLastUsers().empty()) {
-          ValueLifetimeAnalysis VLA(&II);
-          ClosureLifetime = VLA.computeFromDirectUses();
+        if (!CInfo) {
+          CInfo = new ClosureInfo(&II);
+          ValueLifetimeAnalysis VLA(CInfo->Closure);
+          CInfo->Lifetime = VLA.computeFromDirectUses();
         }
 
         // Now we know that CSDesc is profitable to specialize. Add it to our
         // call site list.
-        CallSites.push_back(CallSiteDescriptor(&II, AI, ClosureIndex.getValue(),
-                                               ClosureParamInfo,
-                                               std::move(NonFailureExitBBs),
-                                               ClosureLifetime));
+        CInfo->CallSites.push_back(
+          CallSiteDescriptor(CInfo, AI, ClosureIndex.getValue(),
+                             ClosureParamInfo, std::move(NonFailureExitBBs)));
       }
+      if (CInfo)
+        ClosureCandidates.push_back(CInfo);
     }
   }
 }
@@ -785,22 +800,24 @@ bool ClosureSpecializer::specialize(SILFunction *Caller,
 
   // Collect all of the PartialApplyInsts that are used as arguments to
   // ApplyInsts. Check the profitability of specializing the closure argument.
-  llvm::SmallVector<CallSiteDescriptor, 8> CallSites;
+  llvm::SmallVector<ClosureInfo*, 8> ClosureCandidates;
   llvm::SmallPtrSet<ApplyInst *, 4> MultipleClosureAI;
-  gatherCallSites(Caller, CallSites, MultipleClosureAI);
+  gatherCallSites(Caller, ClosureCandidates, MultipleClosureAI);
 
   bool Changed = false;
-  for (auto &CSDesc : CallSites) {
-    // Do not specialize apply insts that take in multiple closures. This pass
-    // does not know how to do this yet.
-    if (MultipleClosureAI.count(CSDesc.getApplyInst()))
-      continue;
+  for (auto *CInfo : ClosureCandidates) {
+    for (auto &CSDesc : CInfo->CallSites) {
+      // Do not specialize apply insts that take in multiple closures. This pass
+      // does not know how to do this yet.
+      if (MultipleClosureAI.count(CSDesc.getApplyInst()))
+        continue;
 
-    CSDesc.specializeClosure(CG);
-    PropagatedClosures.push_back(CSDesc.getClosure());
-    Changed = true;
+      specializeClosure(CG, *CInfo, CSDesc);
+      PropagatedClosures.push_back(CSDesc.getClosure());
+      Changed = true;
+    }
+    delete CInfo;
   }
-
   return Changed;
 }
 

commit 0b6b907a0abd7613007803172b94fac43df0af3b
Author: Roman Levenstein <rlevenstein@apple.com>
Date:   Thu Apr 16 20:28:07 2015 +0000

    Do not produce an intermediate cast between the same ObjC types as it will get eliminated anyways.
    
    When casts optimizations were lowering bridged casts from ObjC to Swift, they were producing internally an intermediate cast from an ObjC type into the expected ObjC type of a bridged cast, before converting this expected ObjC type into a Swift type. In many cases, this resulted in a useless intermediate cast where both types were the same and such a cast would be eliminated afterwards. E.g.
    
    unconditional_checked_cast A to B  // where A is ObjC type and B is a Swift type
    
    was lowered into:
    
    unconditional_checked_cast A to B. _ObjectiveCType // Often useless as A is equal to B. _ObjectiveCType already.
    unconditional_checked_cast B._ObjectiveCType to B
    
    This small inefficiency is fixed now. This problem was no observable from outside the optimization pass, it just required additional processing, therefore there is no test-case.
    
    Swift SVN r27370

diff --git a/lib/SILPasses/Utils/Local.cpp b/lib/SILPasses/Utils/Local.cpp
index 40a416d12ef..4486e97063c 100644
--- a/lib/SILPasses/Utils/Local.cpp
+++ b/lib/SILPasses/Utils/Local.cpp
@@ -816,9 +816,22 @@ optimizeBridgedObjCToSwiftCast(SILInstruction *Inst,
     // Generate a load for the source argument.
     auto *Load = Builder.createLoad(Loc, Src);
     // Try to convert the source into the expected ObjC type first.
-    // TODO: If type of the source and the expected ObjC type are
-    // equal, there is no need to generate the conversion.
-    if (isConditional) {
+
+
+    if (Load->getType() == SILBridgedTy) {
+      // If type of the source and the expected ObjC type are
+      // equal, there is no need to generate the conversion
+      // from ObjCTy to _ObjectiveCBridgeable._ObjectiveCType.
+      if (isConditional) {
+        SILBasicBlock *CastSuccessBB = Inst->getFunction()->createBasicBlock();
+        CastSuccessBB->createBBArg(SILBridgedTy);
+        Builder.createBranch(Loc, CastSuccessBB, SILValue(Load,0));
+        Builder.setInsertionPoint(CastSuccessBB);
+        SrcOp = SILValue(CastSuccessBB->getBBArg(0), 0);
+      } else {
+        SrcOp = Load;
+      }
+    } else if (isConditional) {
       SILBasicBlock *CastSuccessBB = Inst->getFunction()->createBasicBlock();
       CastSuccessBB->createBBArg(SILBridgedTy);
       NewI = Builder.createCheckedCastBranch(Loc, false, SILValue(Load, 0),

commit 4d71e0874e2a361ede0129c4a9687fa5585c341b
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Thu Dec 18 20:47:40 2014 +0000

    [stdlib] Remove testing inefficiency
    
    Bring some N^3-ness back down to N^2 in this test.
    
    Swift SVN r24016

diff --git a/test/1_stdlib/NewString.swift b/test/1_stdlib/NewString.swift
index eba4baaf9b6..b5de2e9d973 100644
--- a/test/1_stdlib/NewString.swift
+++ b/test/1_stdlib/NewString.swift
@@ -403,14 +403,16 @@ do {
       expectEquality(u8i0a, u8i1, true)
 
       // Also check some positions between unicode scalars
+      var u8i0b = u8i0a
       for n0 in 0..<8 {
-        let u8i0b = advance(u8i0a, n0)
-        for n1 in n0..<8 {
-          let u8i1b = advance(u8i1, n1)
+        var u8i1b = u8i1
+        for n1 in 0..<8 {
           expectEquality(u8i0b, u8i1b, n0 == n1)
           if u8i1b == u8.endIndex { break }
+          ++u8i1b
         }
         if u8i0b == u8.endIndex { break }
+        ++u8i0b
       }
     }
   }

commit fd7ce02a514e7627ee284b515becf1e450712734
Author: Dmitri Hrybenko <dgribenko@apple.com>
Date:   Sat Oct 25 21:37:41 2014 +0000

    Runtime: emit BridgeObject metadata on all platforms
    
    Per Joe, a low level retained-pointer-with-user-controlled-spare-bits
    type would still be useful for space efficiency even on platforms that
    don't need ObjC interop.
    
    Swift SVN r22943

diff --git a/include/swift/Runtime/Metadata.h b/include/swift/Runtime/Metadata.h
index 824b761438a..8a8da37aa07 100644
--- a/include/swift/Runtime/Metadata.h
+++ b/include/swift/Runtime/Metadata.h
@@ -649,10 +649,11 @@ extern "C" const ValueWitnessTable _TWVBi128_;    // Builtin.Int128
 // pointer types.
 extern "C" const ExtraInhabitantsValueWitnessTable _TWVBo; // Builtin.NativeObject
 
+extern "C" const ExtraInhabitantsValueWitnessTable _TWVBb; // Builtin.BridgeObject
+
 #if SWIFT_OBJC_INTEROP
 // The ObjC-pointer table can be used for arbitrary ObjC pointer types.
 extern "C" const ExtraInhabitantsValueWitnessTable _TWVBO; // Builtin.UnknownObject
-extern "C" const ExtraInhabitantsValueWitnessTable _TWVBb; // Builtin.BridgeObject
 #endif
 
 // The () -> () table can be used for arbitrary function types.
diff --git a/stdlib/runtime/KnownMetadata.cpp b/stdlib/runtime/KnownMetadata.cpp
index 3de54a3b576..7f614504a31 100644
--- a/stdlib/runtime/KnownMetadata.cpp
+++ b/stdlib/runtime/KnownMetadata.cpp
@@ -126,6 +126,10 @@ const ExtraInhabitantsValueWitnessTable swift::_TWVBo =
 const ExtraInhabitantsValueWitnessTable swift::_TWVMBo =
   ValueWitnessTableForBox<PointerPointerBox>::table;
 
+/// The value-witness table for BridgeObject.
+const ExtraInhabitantsValueWitnessTable swift::_TWVBb =
+  ValueWitnessTableForBox<BridgeObjectBox>::table;
+
 #if SWIFT_OBJC_INTEROP
 /*** Objective-C pointers ****************************************************/
 
@@ -135,10 +139,6 @@ const ExtraInhabitantsValueWitnessTable swift::_TWVMBo =
 /// The basic value-witness table for ObjC object pointers.
 const ExtraInhabitantsValueWitnessTable swift::_TWVBO =
   ValueWitnessTableForBox<ObjCRetainableBox>::table;
-
-/// The value-witness table for BridgeObject.
-const ExtraInhabitantsValueWitnessTable swift::_TWVBb =
-  ValueWitnessTableForBox<BridgeObjectBox>::table;
 #endif
 
 /*** Functions ***************************************************************/
@@ -168,9 +168,9 @@ OPAQUE_METADATA(Bi32_)
 OPAQUE_METADATA(Bi64_)
 OPAQUE_METADATA(Bi128_)
 OPAQUE_METADATA(Bo)
+OPAQUE_METADATA(Bb)
 #if SWIFT_OBJC_INTEROP
 OPAQUE_METADATA(BO)
-OPAQUE_METADATA(Bb)
 #endif
 
 /// The standard metadata for the empty tuple.

commit bba2a37070b71907aa1d5e02d1807fdccb6d83e0
Author: Andrew Trick <atrick@apple.com>
Date:   Sat Aug 30 09:20:07 2014 +0000

    More COWArrayOpts cleanup.
    
    Arnold noticed during review that ArrayUserSet was not properly
    updated after previous rounds of cleanup.
    
    This also removes some innefficiency by eliminating a few hash maps.
    
    I want to make sure this is in good shape before reusing the analysis
    for immutable array properties.
    
    Swift SVN r21608

diff --git a/lib/SILPasses/COWArrayOpt.cpp b/lib/SILPasses/COWArrayOpt.cpp
index d4bb8ff6509..5543885868b 100644
--- a/lib/SILPasses/COWArrayOpt.cpp
+++ b/lib/SILPasses/COWArrayOpt.cpp
@@ -105,18 +105,18 @@ public:
   typedef SmallPtrSet<Operand*, 16> VisitedSet;
   typedef SmallVector<SILInstruction*, 16> UserList;
 
-  /// Map a user of a value or an element within that value to the operand that
-  /// directly uses the value. Multiple levels of struct_extract may exist
-  /// between the use instruction and the operand.
-  typedef llvm::MapVector<SILInstruction*, Operand*> UserMap;
+  /// Record the users of a value or an element within that value along with the
+  /// operand that directly uses the value. Multiple levels of struct_extract
+  /// may exist between the operand and the user instruction.
+  typedef SmallVector<std::pair<SILInstruction*, Operand*>, 16> UserOperList;
 
   UserList AggregateAddressUsers;
   UserList StructAddressUsers;
   UserList StructLoads;
   UserList StructValueUsers;
-  UserMap ElementAddressUsers;
-  UserMap ElementLoads;
-  UserMap ElementValueUsers;
+  UserOperList ElementAddressUsers;
+  UserOperList ElementLoads;
+  UserOperList ElementValueUsers;
   VisitedSet Visited;
 
   /// Collect all uses of the value at the given address.
@@ -137,7 +137,8 @@ public:
       for (auto DefUI : Pair.first->getUses()) {
         if (!Visited.insert(&*DefUI))
           continue;
-        ElementValueUsers[DefUI->getUser()] = Pair.second;
+        ElementValueUsers.push_back(
+          std::make_pair(DefUI->getUser(), Pair.second));
       }
     }
   }
@@ -161,11 +162,11 @@ protected:
         // Found a use of an element.
         assert(AccessPathSuffix.empty() && "should have accessed struct");
         if (LoadInst *LoadI = dyn_cast<LoadInst>(UseInst))
-          ElementLoads[LoadI] = StructVal;
+          ElementLoads.push_back(std::make_pair(LoadI, StructVal));
         else if (isa<StructElementAddrInst>(UseInst))
           collectAddressUses(UseInst, AccessPathSuffix, StructVal);
         else
-          ElementAddressUsers[UseInst] = StructVal;
+          ElementAddressUsers.push_back(std::make_pair(UseInst,StructVal));
         continue;
 
       } else if (AccessPathSuffix.empty()) {
@@ -259,7 +260,7 @@ namespace {
 /// guard are already guarded by either "init" or "mutate_unknown".
 class COWArrayOpt {
   typedef StructUseCollector::UserList UserList;
-  typedef StructUseCollector::UserMap UserMap;
+  typedef StructUseCollector::UserOperList UserOperList;
 
   SILFunction *Function;
   SILLoop *Loop;
@@ -280,10 +281,12 @@ class COWArrayOpt {
 
   // \brief Transient per-Array user set.
   //
-  // Track all known array users. During analysis of retains/releases within the
-  // loop body, the users in this set are assumed to cover all possible mutating
-  // operations on the array. If the array escaped through an unknown use, the
-  // analysis must abort earlier.
+  // Track all known array users with the exception of struct_extract users
+  // (checkSafeArrayElementUse prohibits struct_extract users from mutating the
+  // array). During analysis of retains/releases within the loop body, the
+  // users in this set are assumed to cover all possible mutating operations on
+  // the array. If the array escaped through an unknown use, the analysis must
+  // abort earlier.
   SmallPtrSet<SILInstruction*, 8> ArrayUserSet;
 
 public:
@@ -303,7 +306,7 @@ protected:
   bool checkSafeArrayAddressUses(UserList &AddressUsers);
   bool checkSafeArrayValueUses(UserList &ArrayValueUsers);
   bool checkSafeArrayElementUse(SILInstruction *UseInst, SILValue ArrayVal);
-  bool checkSafeElementValueUses(UserMap &ElementValueUsers);
+  bool checkSafeElementValueUses(UserOperList &ElementValueUsers);
   bool hoistMakeMutable(ApplyInst *MakeMutable, SILValue ArrayAddr);
 };
 } // namespace
@@ -479,6 +482,12 @@ bool COWArrayOpt::checkSafeArrayValueUses(UserList &ArrayValueUsers) {
 /// Given an array value, recursively check that uses of elements within the
 /// array are safe.
 ///
+/// Consider any potentially mutating operation unsafe. Mutation would not
+/// prevent make_mutable hoisting, but it would interfere with
+/// isRetainReleasedBeforeMutate. Since struct_extract users are not visited by
+/// StructUseCollector, they are never added to ArrayUserSet. Thus we check here
+/// that no mutating struct_extract users exist.
+///
 /// After the lower aggregates pass, SIL contains chains of struct_extract and
 /// retain_value instructions. e.g.
 ///   %a = load %0 : $*Array<Int>
@@ -520,7 +529,7 @@ bool COWArrayOpt::checkSafeArrayElementUse(SILInstruction *UseInst,
 /// Check that the use of an Array element is safe w.r.t. make_mutable hoisting.
 ///
 /// This logic should be similar to checkSafeArrayElementUse
-bool COWArrayOpt::checkSafeElementValueUses(UserMap &ElementValueUsers) {
+bool COWArrayOpt::checkSafeElementValueUses(UserOperList &ElementValueUsers) {
   for (auto &Pair : ElementValueUsers) {
     SILInstruction *UseInst = Pair.first;
     Operand *ArrayValOper = Pair.second;
@@ -560,6 +569,8 @@ bool COWArrayOpt::hoistMakeMutable(ApplyInst *MakeMutable, SILValue ArrayAddr) {
   // not escape within this function.
   StructUseCollector StructUses;
   StructUses.collectUses(ArrayContainer.getDef(), AccessPath);
+  for (auto *Oper : StructUses.Visited)
+    ArrayUserSet.insert(Oper->getUser());
 
   if (!checkSafeArrayAddressUses(StructUses.AggregateAddressUsers) ||
       !checkSafeArrayAddressUses(StructUses.StructAddressUsers) ||

commit 6bb6e1b0b43cd5e84edb8acafb52d0b64fb537fd
Author: Dmitri Hrybenko <dgribenko@apple.com>
Date:   Wed Jun 25 13:24:15 2014 +0000

    stdlib/String: if we can not get a contiguous data buffer out of NSString,
    don't call into CoreFoundation to perform UTF-8 transcoding.  CoreFoundation
    can replace ill-formed sequences with a single byte, which is not good enough
    to implement U+FFFD insertion.  Instead, use the same transcoding routine as
    for contiguous buffer.
    
    Pulled out the transcoding routine into a generic function that should be
    specialized and simplified for the case when input is UnsafeArray; we should
    not be losing efficiency here.
    
    Fixes <rdar://problem/17297055> [unicode] println crashes when given string
    with unpaired surrogate
    
    
    
    Swift SVN r19157

diff --git a/stdlib/core/Existential.swift b/stdlib/core/Existential.swift
index f1cd897743c..b478e65ea1d 100644
--- a/stdlib/core/Existential.swift
+++ b/stdlib/core/Existential.swift
@@ -51,6 +51,36 @@
   let _generate: ()->GeneratorOf<T>
 }
 
+@internal struct _CollectionOf<IndexType_ : ForwardIndex, T> : Collection {
+  init(startIndex: IndexType_, endIndex: IndexType_,
+      _ subscriptImpl: (IndexType_)->T) {
+    self.startIndex = startIndex
+    self.endIndex = endIndex
+    _subscriptImpl = subscriptImpl
+  }
+
+  func generate() -> GeneratorOf<T> {
+    var index = startIndex
+    return GeneratorOf {
+      () -> T? in
+      if _fastPath(index != self.endIndex) {
+        ++index
+        return self._subscriptImpl(index)
+      }
+      return .None
+    }
+  }
+
+  let startIndex: IndexType_
+  let endIndex: IndexType_
+
+  subscript(i: IndexType_) -> T {
+    return _subscriptImpl(i)
+  }
+
+  let _subscriptImpl: (IndexType_)->T
+}
+
 @public struct SinkOf<T> : Sink {
   @public init(_ put: (T)->()) {
     _put = put
diff --git a/stdlib/core/OutputStream.swift b/stdlib/core/OutputStream.swift
index 2d9824ac8cd..c3e0ecb0ef3 100644
--- a/stdlib/core/OutputStream.swift
+++ b/stdlib/core/OutputStream.swift
@@ -353,9 +353,9 @@ func _uint64ToStringImpl(
     // FIXME: buffering?
     // It is important that we use stdio routines in order to correctly
     // interoperate with stdio buffering.
-    string._encode(UTF8.self, output: SinkOf<UTF8.CodeUnit> {
-      _putchar(Int32($0)); return
-    })
+    for c in string.utf8 {
+      _putchar(Int32(c))
+    }
   }
 }
 
@@ -390,12 +390,13 @@ extension UnicodeScalar : Streamable {
 
 extension CString : Streamable {
   /// Writes the `CString` to the output stream.  If the `CString` does not
-  /// contain well-formed UTF-8, invokes a runtime trap.
+  /// contain well-formed UTF-8, replaces ill-formed code unit sequences
+  /// with U+FFFD.
   @public func writeTo<Target : OutputStream>(inout target: Target) {
     if _isNull {
       return
     }
-    target.write(String.fromCString(self)!)
+    target.write(String.fromCStringRepairingIllFormedUTF8(self).0!)
   }
 }
 
diff --git a/stdlib/core/StringBridge.swift b/stdlib/core/StringBridge.swift
index 909c594fbe8..f89b8832e3d 100644
--- a/stdlib/core/StringBridge.swift
+++ b/stdlib/core/StringBridge.swift
@@ -76,14 +76,3 @@ func _cocoaStringSubscriptNotInitialized(
   _fatalError("_cocoaStringSubscript not initialized")
 }
 
-@public var _cocoaStringEncodeSomeUTF8: (
-  target: _StringCore, position: Int
-) -> (_StringCore.IndexType, _StringCore.UTF8Chunk)
-  = _cocoaStringEncodeSomeUTF8NotInitialized
-
-func _cocoaStringEncodeSomeUTF8NotInitialized(
-  target: _StringCore, position: Int 
-) -> (_StringCore.IndexType, _StringCore.UTF8Chunk) {
-  _fatalError("_cocoaStringEncodeSomeUTF8 not initialized")
-}
-
diff --git a/stdlib/core/StringUTF8.swift b/stdlib/core/StringUTF8.swift
index d9f67d0a893..8826dced9cf 100644
--- a/stdlib/core/StringUTF8.swift
+++ b/stdlib/core/StringUTF8.swift
@@ -43,104 +43,35 @@ extension _StringCore {
         size: numericCast(utf16Count))
       
       return (i + utf16Count, result)
-    }
-    else {
-      return _encodeSomeUTF16AsUTF8(i)
+    } else if _fastPath(!_baseAddress._isNull) {
+      return _encodeSomeContiguousUTF16AsUTF8(i)
+    } else {
+      return _encodeSomeNonContiguousUTF16AsUTF8(i)
     }
   }
 
-  /// Helper for _encodeSomeUTF8, above.  Handles the case where we
-  /// don't have contiguous ASCII storage.
-  func _encodeSomeUTF16AsUTF8(i: Int) -> (Int, UTF8Chunk) {
+  /// Helper for `_encodeSomeUTF8`, above.  Handles the case where the
+  /// storage is contiguous UTF-16.
+  func _encodeSomeContiguousUTF16AsUTF8(i: Int) -> (Int, UTF8Chunk) {
     _sanityCheck(elementWidth == 2)
+    _sanityCheck(!_baseAddress._isNull)
 
-    if _fastPath(!_baseAddress._isNull) {
-      let utf16Count = self.count
-      let utf8Max = sizeof(UTF8Chunk.self)
-      var result: UTF8Chunk = 0
-      var utf8Count = 0
-      var nextIndex = i
-      while (nextIndex < utf16Count && utf8Count != utf8Max) {
-        let u = UInt(startUTF16[nextIndex])
-        let shift = UTF8Chunk(utf8Count * 8)
-        var utf16Length = 1
+    let storage = UnsafeArray(start: startUTF16, length: self.count)
+    return _transcodeSomeUTF16AsUTF8(storage, i)
+  }
 
-        if _fastPath(u <= 0x7f) {
-          result |= UTF8Chunk(u) << shift
-          ++utf8Count
-        }
-        else {
-          var scalarUtf8Length: Int
-          var r: UInt
-          if _fastPath((u >> 11) != 0b1101_1) {
-            // Neither high-surrogate, nor low-surrogate -- sequence of 1 code
-            // unit, decoding is trivial.
-            if u < 0x800 {
-              r = 0b10__00_0000__110__0_0000
-              r |= u >> 6
-              r |= (u & 0b11_1111) << 8
-              scalarUtf8Length = 2
-            }
-            else {
-              r = 0b10__00_0000__10__00_0000__1110__0000
-              r |= u >> 12
-              r |= ((u >> 6) & 0b11_1111) << 8
-              r |= (u        & 0b11_1111) << 16
-              scalarUtf8Length = 3
-            }
-          }
-          else {
-            var unit0 = u
-            if _slowPath((unit0 >> 10) == 0b1101_11) {
-              // `unit0` is a low-surrogate.  We have an ill-formed sequence.
-              // Replace it with U+FFFD.
-              r = 0xbdbfef
-              scalarUtf8Length = 3
-            } else if _slowPath(nextIndex + 1 == utf16Count) {
-              // We have seen a high-surrogate and EOF, so we have an
-              // ill-formed sequence.  Replace it with U+FFFD.
-              r = 0xbdbfef
-              scalarUtf8Length = 3
-            } else {
-              let unit1 = UInt(startUTF16[nextIndex + 1])
-              if _fastPath((unit1 >> 10) == 0b1101_11) {
-                // `unit1` is a low-surrogate.  We have a well-formed surrogate
-                // pair.
-                let v = 0x10000 + (((unit0 & 0x03ff) << 10) | (unit1 & 0x03ff))
+  /// Helper for `_encodeSomeUTF8`, above.  Handles the case where the
+  /// storage is non-contiguous UTF-16.
+  func _encodeSomeNonContiguousUTF16AsUTF8(i: Int) -> (Int, UTF8Chunk) {
+    _sanityCheck(elementWidth == 2)
+    _sanityCheck(_baseAddress._isNull)
 
-                r = 0b10__00_0000__10__00_0000__10__00_0000__1111_0__000
-                r |= v >> 18
-                r |= ((v >> 12) & 0b11_1111) << 8
-                r |= ((v >> 6) & 0b11_1111) << 16
-                r |= (v        & 0b11_1111) << 24
-                scalarUtf8Length = 4
-                utf16Length = 2
-              } else {
-                // Otherwise, we have an ill-formed sequence.  Replace it with
-                // U+FFFD.
-                r = 0xbdbfef
-                scalarUtf8Length = 3
-              }
-            }
-          }
-          // Don't overrun the buffer
-          if utf8Count + scalarUtf8Length > utf8Max {
-            break
-          }
-          result |= numericCast(r) << shift
-          utf8Count += scalarUtf8Length
-        }
-        nextIndex += utf16Length
-      }
-      // FIXME: Annoying check, courtesy of <rdar://problem/16740169>
-      if utf8Count < sizeofValue(result) {
-        result |= ~0 << numericCast(utf8Count * 8)
-      }
-      return (nextIndex, result)
-    }
-    else {
-      return _cocoaStringEncodeSomeUTF8(target: self, position: i)
+    let storage = _CollectionOf<Int, UInt16>(
+      startIndex: 0, endIndex: self.count) {
+      (i: Int) -> UInt16 in
+      return _cocoaStringSubscript(target: self, position: i)
     }
+    return _transcodeSomeUTF16AsUTF8(storage, i)
   }
 }
 
diff --git a/stdlib/core/StringUnicodeScalarView.swift b/stdlib/core/StringUnicodeScalarView.swift
index 06d320fff32..c7eb65023c7 100644
--- a/stdlib/core/StringUnicodeScalarView.swift
+++ b/stdlib/core/StringUnicodeScalarView.swift
@@ -106,7 +106,7 @@ extension String {
         case .EmptyInput:
           return .None
         case .Error:
-          _fatalError("unpaired surrogates are ill-formed in UTF-16")
+          return UnicodeScalar(0xfffd)
         }
       }
       var _decoder: UTF16 = UTF16()
diff --git a/stdlib/core/Unicode.swift b/stdlib/core/Unicode.swift
index 4e05738d0e5..3813453119a 100644
--- a/stdlib/core/Unicode.swift
+++ b/stdlib/core/Unicode.swift
@@ -637,6 +637,99 @@ enum UTFDecodeResult {
   return (hadError: hadError)
 }
 
+/// Transcode UTF-16 to UTF-8, replacing ill-formed sequences with U+FFFD.
+///
+/// Returns the index of the first unhandled code unit and the UTF-8 data
+/// that was encoded.
+@internal func _transcodeSomeUTF16AsUTF8<
+  Input : Collection
+  where Input.GeneratorType.Element == UInt16>(
+  input: Input, startIndex: Input.IndexType
+) -> (Input.IndexType, _StringCore.UTF8Chunk) {
+  typealias UTF8Chunk = _StringCore.UTF8Chunk
+
+  let endIndex = input.endIndex
+  let utf8Max = sizeof(UTF8Chunk.self)
+  var result: UTF8Chunk = 0
+  var utf8Count = 0
+  var nextIndex = startIndex
+  while nextIndex != input.endIndex && utf8Count != utf8Max {
+    let u = UInt(input[nextIndex])
+    let shift = UTF8Chunk(utf8Count * 8)
+    var utf16Length: Input.IndexType.DistanceType = 1
+
+    if _fastPath(u <= 0x7f) {
+      result |= UTF8Chunk(u) << shift
+      ++utf8Count
+    } else {
+      var scalarUtf8Length: Int
+      var r: UInt
+      if _fastPath((u >> 11) != 0b1101_1) {
+        // Neither high-surrogate, nor low-surrogate -- sequence of 1 code
+        // unit, decoding is trivial.
+        if u < 0x800 {
+          r = 0b10__00_0000__110__0_0000
+          r |= u >> 6
+          r |= (u & 0b11_1111) << 8
+          scalarUtf8Length = 2
+        }
+        else {
+          r = 0b10__00_0000__10__00_0000__1110__0000
+          r |= u >> 12
+          r |= ((u >> 6) & 0b11_1111) << 8
+          r |= (u        & 0b11_1111) << 16
+          scalarUtf8Length = 3
+        }
+      } else {
+        var unit0 = u
+        if _slowPath((unit0 >> 10) == 0b1101_11) {
+          // `unit0` is a low-surrogate.  We have an ill-formed sequence.
+          // Replace it with U+FFFD.
+          r = 0xbdbfef
+          scalarUtf8Length = 3
+        } else if _slowPath(advance(nextIndex, 1) == endIndex) {
+          // We have seen a high-surrogate and EOF, so we have an ill-formed
+          // sequence.  Replace it with U+FFFD.
+          r = 0xbdbfef
+          scalarUtf8Length = 3
+        } else {
+          let unit1 = UInt(input[advance(nextIndex, 1)])
+          if _fastPath((unit1 >> 10) == 0b1101_11) {
+            // `unit1` is a low-surrogate.  We have a well-formed surrogate
+            // pair.
+            let v = 0x10000 + (((unit0 & 0x03ff) << 10) | (unit1 & 0x03ff))
+
+            r = 0b10__00_0000__10__00_0000__10__00_0000__1111_0__000
+            r |= v >> 18
+            r |= ((v >> 12) & 0b11_1111) << 8
+            r |= ((v >> 6) & 0b11_1111) << 16
+            r |= (v        & 0b11_1111) << 24
+            scalarUtf8Length = 4
+            utf16Length = 2
+          } else {
+            // Otherwise, we have an ill-formed sequence.  Replace it with
+            // U+FFFD.
+            r = 0xbdbfef
+            scalarUtf8Length = 3
+          }
+        }
+      }
+      // Don't overrun the buffer
+      if utf8Count + scalarUtf8Length > utf8Max {
+        break
+      }
+      result |= numericCast(r) << shift
+      utf8Count += scalarUtf8Length
+    }
+    nextIndex = advance(nextIndex, utf16Length)
+  }
+  // FIXME: Annoying check, courtesy of <rdar://problem/16740169>
+  if utf8Count < sizeofValue(result) {
+    result |= ~0 << numericCast(utf8Count * 8)
+  }
+  return (nextIndex, result)
+}
+
 protocol StringElement {
   class func toUTF16CodeUnit(_: Self) -> UTF16.CodeUnit
   class func fromUTF16CodeUnit(utf16: UTF16.CodeUnit) -> Self
diff --git a/stdlib/objc/Foundation/Foundation.swift b/stdlib/objc/Foundation/Foundation.swift
index cc5823067d7..e9af95b2849 100644
--- a/stdlib/objc/Foundation/Foundation.swift
+++ b/stdlib/objc/Foundation/Foundation.swift
@@ -135,7 +135,6 @@ func __swift_initializeCocoaStringBridge() -> COpaquePointer {
   _cocoaStringLength = _cocoaStringLengthImpl
   _cocoaStringSlice = _cocoaStringSliceImpl
   _cocoaStringSubscript = _cocoaStringSubscriptImpl
-  _cocoaStringEncodeSomeUTF8 = _cocoaStringEncodeSomeUTF8Impl
   return COpaquePointer()
 }
 
@@ -203,28 +202,11 @@ func _cocoaStringSliceImpl(
 
 func _cocoaStringSubscriptImpl(
   target: _StringCore, position: Int) -> UTF16.CodeUnit {
-  // FIXME: implement this in terms of CFString
-  return (target.cocoaBuffer! as NSString).characterAtIndex(position)
-}
-
-func _cocoaStringEncodeSomeUTF8Impl(
-  target: _StringCore, position: Int
-) -> (_StringCore.IndexType, _StringCore.UTF8Chunk) {
   let cfSelf: CFString = reinterpretCast(target.cocoaBuffer!)
-  let utf16Length = CFStringGetLength(cfSelf)
-  var buffer: _StringCore.UTF8Chunk = ~0
-  let utf16CodeUnitsConverted = withUnsafePointer(&buffer) {
-    (bufferPtr: UnsafePointer<_StringCore.UTF8Chunk>) -> CFIndex
-  in
-    let uint8Buffer: CMutablePointer<UInt8> = UnsafePointer(bufferPtr)
-    let range = CFRange(location: position,
-        length: min(sizeofValue(buffer), utf16Length - position))
-    return CFStringGetBytes(
-        cfSelf, range, CFStringBuiltInEncodings.UTF8.toRaw(),
-        /*lossByte:*/ 0, /*isExternalRepresentation:*/ 0, uint8Buffer,
-        /*maxBufLen:*/ sizeofValue(buffer), nil)
-  }
-  return (position + utf16CodeUnitsConverted, buffer)
+  _sanityCheck(CFStringGetCharactersPtr(cfSelf)._isNull,
+    "Known contiguously-stored strings should already be converted to Swift")
+
+  return CFStringGetCharacterAtIndex(cfSelf, position)
 }
 
 //
diff --git a/test/stdlib/Unicode.swift b/test/stdlib/Unicode.swift
index 4efddba903a..270081d5c75 100644
--- a/test/stdlib/Unicode.swift
+++ b/test/stdlib/Unicode.swift
@@ -2305,9 +2305,32 @@ func checkUTF8View(expected: UInt8[], stringUnderTest: String) {
   expectEqual(expected, utf8Bytes)
 }
 
-var StringUTFViews = TestCase("StringUTFViews")
+func forStringsWithUnpairedSurrogates(checkClosure: (UTF16Test, String) -> ()) {
+  for (name, batch) in UTF16Tests {
+    println("Batch: \(name)")
+    for test in batch {
+      test.encoded.withUnsafePointerToElements {
+        (ptr) -> () in
+        let cfstring = CFStringCreateWithCharactersNoCopy(kCFAllocatorDefault,
+            ptr, test.encoded.count, kCFAllocatorNull)
+        if !test.encoded.isEmpty {
+          // Exclude the empty testcase from this check because it will fail --
+          // CoreFoundation will return NULL for character data of an empty
+          // string.
+          expectTrue(CFStringGetCharactersPtr(cfstring).getLogicValue())
+        }
+        let subject: String = cfstring
 
-StringUTFViews.test("UTF8NativeImplementation") {
+        checkClosure(test, subject)
+        return ()
+      }
+    }
+  }
+}
+
+var StringCookedViews = TestCase("StringCookedViews")
+
+StringCookedViews.test("UTF8ForContiguousUTF16") {
   for test in UTF8TestsSmokeTest {
     // Add a non-ASCII character at the beginning to force Swift String and
     // CoreFoundation off the ASCII fast path.
@@ -2335,32 +2358,18 @@ StringUTFViews.test("UTF8NativeImplementation") {
     }
   }
 
-  for (name, batch) in UTF16Tests {
-    println("Batch: \(name)")
-    for test in batch {
-      var expected: UInt8[] = []
-      var expectedScalars = test.scalarsHead + test.scalarsRepairedTail
-      var g = expectedScalars.generate()
-      transcode(UTF32.self, UTF8.self, g,
-          SinkOf {
-            expected += $0
-          },
-          stopOnError: false)
+  forStringsWithUnpairedSurrogates {
+    (test: UTF16Test, subject: String) -> () in
+    var expected: UInt8[] = []
+    var expectedScalars = test.scalarsHead + test.scalarsRepairedTail
+    var g = expectedScalars.generate()
+    transcode(UTF32.self, UTF8.self, g,
+        SinkOf {
+          expected += $0
+        },
+        stopOnError: false)
 
-      test.encoded.withUnsafePointerToElements {
-        (ptr) -> () in
-        let cfstring = CFStringCreateWithCharactersNoCopy(kCFAllocatorDefault,
-            ptr, test.encoded.count, kCFAllocatorNull)
-        if !test.encoded.isEmpty {
-          // Exclude the empty testcase from this check because it will fail --
-          // CoreFoundation will return NULL for character data of an empty
-          // string.
-          expectTrue(CFStringGetCharactersPtr(cfstring).getLogicValue())
-        }
-        checkUTF8View(expected, cfstring)
-        return ()
-      }
-    }
+    checkUTF8View(expected, subject)
   }
 }
 
@@ -2385,15 +2394,13 @@ func verifyThatStringIsOpaqueForCoreFoundation(nss: NSString) {
   assert(!CFStringGetCharactersPtr(copy))
 }
 
-StringUTFViews.test("UTF8CocoaImplementation") {
+StringCookedViews.test("UTF8ForNonContiguousUTF16") {
   for test in UTF8TestsSmokeTest {
     var nss = NonContiguousNSString(test.scalars)
     verifyThatStringIsOpaqueForCoreFoundation(nss)
     checkUTF8View(test.encoded, nss)
   }
 
-  /*
-  FIXME: This crashes now.
   for (name, batch) in UTF16Tests {
     println("Batch: \(name)")
     for test in batch {
@@ -2406,16 +2413,14 @@ StringUTFViews.test("UTF8CocoaImplementation") {
           },
           stopOnError: false)
 
-      dump(test.encoded)
       var nss = NonContiguousNSString(test.encoded)
       verifyThatStringIsOpaqueForCoreFoundation(nss)
       checkUTF8View(expected, nss)
     }
   }
-  */
 }
 
-StringUTFViews.test("UTF8CocoaImplementationExtra") {
+StringCookedViews.test("UTF8ForNonContiguousUTF16Extra") {
   // These tests don't add much additional value as long as tests above
   // actually test the code path we care about.
   if true {
@@ -2459,7 +2464,31 @@ StringUTFViews.test("UTF8CocoaImplementationExtra") {
   }
 }
 
+StringCookedViews.test("UnicodeScalars") {
+  forStringsWithUnpairedSurrogates {
+    (test: UTF16Test, subject: String) -> () in
+    let expected = test.scalarsHead + test.scalarsRepairedTail
+    let actual: UInt32[] = Array(map(subject.unicodeScalars) { $0.value })
+    expectEqual(expected, actual)
+  }
+}
+
+StringCookedViews.run()
+// CHECK: {{^}}StringCookedViews: All tests passed
+
+var StringTests = TestCase("StringTests")
+
+StringTests.test("StreamableConformance") {
+  forStringsWithUnpairedSurrogates {
+    (test: UTF16Test, subject: String) -> () in
+    let expected = test.scalarsHead + test.scalarsRepairedTail
+    let printedSubject = toString(subject)
+    let actual: UInt32[] =
+        Array(map(printedSubject.unicodeScalars) { $0.value })
+    expectEqual(expected, actual)
+  }
+}
 
-StringUTFViews.run()
-// CHECK: {{^}}StringUTFViews: All tests passed
+StringTests.run()
+// CHECK: {{^}}StringTests: All tests passed
 

commit a79a06fee4da0cd61cb2d73ef4250b685d3ab8c4
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Fri May 23 20:27:55 2014 +0000

    [stdlib] Improve efficiency of Array literals
    
    Before this change, the dispatching hacks sent convertFromArrayLiteral
    through the "Sequence" path, wherein we could not assume the ability to
    non-destructively measure the length of the Sequence before beginning to
    add the elements, which resulted in buffer reallocations as elements
    were added.  Now we the sequence is measured and storage is
    pre-allocated.
    
    Swift SVN r18603

diff --git a/stdlib/core/ContiguousArrayBuffer.swift b/stdlib/core/ContiguousArrayBuffer.swift
index 8f10162f918..d5ec3be2951 100644
--- a/stdlib/core/ContiguousArrayBuffer.swift
+++ b/stdlib/core/ContiguousArrayBuffer.swift
@@ -362,7 +362,7 @@ func ~> <
 }
 
 func ~> <  
-  C: Collection
+  C: protocol<_Collection,_Sequence_>
 >(
   source: C, _:(_CopyToNativeArrayBuffer, ())
 ) -> ContiguousArrayBuffer<C.GeneratorType.Element>

commit 32dee228f80ad88938c04106a24bbaec36656e2a
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Thu May 15 00:59:55 2014 +0000

    [sil-aa] Change TBAA to use type oracle instructions instead of the raw types of instructions.
    
    The reason that this is important is that we *ARE* allowing the stdlib
    to perform certain types of type punning for efficiency implying we need
    to have a type oracle instruction to have safety.
    
    A type oracle instruction is an instruction which implies undefined behavior
    unless its operand/result is of a certain type (allowing us to ignore that
    possibility).
    
    In a following commit I am going to go over and fix any problems in the
    actual TBAA implementation and give all the various checks tests.
    
    rdar://16651852
    
    Swift SVN r18090

diff --git a/include/swift/SILAnalysis/AliasAnalysis.h b/include/swift/SILAnalysis/AliasAnalysis.h
index 7ce5a364967..47189d1735d 100644
--- a/include/swift/SILAnalysis/AliasAnalysis.h
+++ b/include/swift/SILAnalysis/AliasAnalysis.h
@@ -53,21 +53,25 @@ public:
   }
 
   /// Perform an alias query to see if V1, V2 refer to the same values.
-  AliasResult alias(SILValue V1, SILValue V2);
+  AliasResult alias(SILValue V1, SILValue V2, SILType TBAAType1 = SILType(),
+                    SILType TBAAType2 = SILType());
 
   /// Convenience method that returns true if V1 and V2 must alias.
-  bool isMustAlias(SILValue V1, SILValue V2) {
-    return alias(V1, V2) == AliasResult::MustAlias;
+  bool isMustAlias(SILValue V1, SILValue V2, SILType TBAAType1 = SILType(),
+                   SILType TBAAType2 = SILType()) {
+    return alias(V1, V2, TBAAType1, TBAAType2) == AliasResult::MustAlias;
   }
 
   /// Convenience method that returns true if V1, V2 can not alias.
-  bool isNoAlias(SILValue V1, SILValue V2) {
-    return alias(V1, V2) == AliasResult::NoAlias;
+  bool isNoAlias(SILValue V1, SILValue V2, SILType TBAAType1 = SILType(),
+                 SILType TBAAType2 = SILType()) {
+    return alias(V1, V2, TBAAType1, TBAAType2) == AliasResult::NoAlias;
   }
 
   /// Convenience method that returns true if V1, V2 may alias.
-  bool isMayAlias(SILValue V1, SILValue V2) {
-    return alias(V1, V2) == AliasResult::MayAlias;
+  bool isMayAlias(SILValue V1, SILValue V2, SILType TBAAType1 = SILType(),
+                  SILType TBAAType2 = SILType()) {
+    return alias(V1, V2, TBAAType1, TBAAType2) == AliasResult::MayAlias;
   }
 
   /// Use the alias analysis to determine the memory behavior of Inst with
diff --git a/lib/SILAnalysis/AliasAnalysis.cpp b/lib/SILAnalysis/AliasAnalysis.cpp
index 853581c5ad2..f5b4a056e17 100644
--- a/lib/SILAnalysis/AliasAnalysis.cpp
+++ b/lib/SILAnalysis/AliasAnalysis.cpp
@@ -330,12 +330,13 @@ static bool aggregateContainsRecord(NominalTypeDecl *Aggregate, Type Record,
 ///
 /// Currently this only implements typed access based TBAA. See the TBAA section
 /// in the SIL reference manual.
-static bool typesMayAlias(SILType T1, SILType T2, SILModule &Mod) {
+static bool typedAccessTBAAMayAlias(SILType T1, SILType T2, SILModule &Mod) {
 #ifndef NDEBUG
   if (!shouldRunTypedAccessTBAA())
     return true;
 #endif
 
+  // If the two types are the same
   if (T1 == T2)
     return true;
 
@@ -400,13 +401,26 @@ static bool typesMayAlias(SILType T1, SILType T2, SILModule &Mod) {
   return true;
 }
 
+static bool typesMayAlias(SILType T1, SILType T2, SILType TBAA1Ty,
+                          SILType TBAA2Ty, SILModule &Mod) {
+  // Perform type access based TBAA if we have TBAA info.
+  if (TBAA1Ty && TBAA2Ty)
+    return typedAccessTBAAMayAlias(TBAA1Ty, TBAA2Ty, Mod);
+
+  // Otherwise perform class based TBAA. This is not implemented currently so
+  // just return true.
+  return true;
+}
+
 //===----------------------------------------------------------------------===//
 //                                Entry Points
 //===----------------------------------------------------------------------===//
 
 /// The main AA entry point. Performs various analyses on V1, V2 in an attempt
 /// to disambiguate the two values.
-AliasAnalysis::AliasResult AliasAnalysis::alias(SILValue V1, SILValue V2) {
+AliasAnalysis::AliasResult AliasAnalysis::alias(SILValue V1, SILValue V2,
+                                                SILType TBAAType1,
+                                                SILType TBAAType2) {
 #ifndef NDEBUG
   // If alias analysis is disabled, always return may alias.
   if (!shouldRunAA())
@@ -420,7 +434,9 @@ AliasAnalysis::AliasResult AliasAnalysis::alias(SILValue V1, SILValue V2) {
   DEBUG(llvm::dbgs() << "ALIAS ANALYSIS:\n    V1: " << *V1.getDef()
         << "    V2: " << *V2.getDef());
 
-  if (!typesMayAlias(V1.getType(), V2.getType(), *Mod))
+  // Pass in both the TBAA types so we can perform typed access TBAA and the
+  // actual types of V1, V2 so we can perform class based TBAA.
+  if (!typesMayAlias(V1.getType(), V2.getType(), TBAAType1, TBAAType2, *Mod))
     return AliasResult::NoAlias;
 
 #ifndef NDEBUG
@@ -523,6 +539,7 @@ public:
     return Inst->getMemoryBehavior();
   }
 
+  MemBehavior visitLoadInst(LoadInst *LI);
   MemBehavior visitStoreInst(StoreInst *SI);
   MemBehavior visitApplyInst(ApplyInst *AI);
 
@@ -544,7 +561,6 @@ public:
     return MemBehavior::None;                                           \
   }
 
-  OPERANDALIAS_MEMBEHAVIOR_INST(LoadInst)
   OPERANDALIAS_MEMBEHAVIOR_INST(InjectEnumAddrInst)
   OPERANDALIAS_MEMBEHAVIOR_INST(UncheckedTakeEnumDataAddrInst)
   OPERANDALIAS_MEMBEHAVIOR_INST(InitExistentialInst)
@@ -584,10 +600,60 @@ public:
 
 } // end anonymous namespace
 
+/// Is this an instruction that can act as a type "oracle" allowing typed access
+/// TBAA to know what the real types associated with the SILInstruction are.
+static bool isTypedAccessOracle(SILInstruction *I) {
+  switch (I->getKind()) {
+  case ValueKind::RefElementAddrInst:
+  case ValueKind::StructElementAddrInst:
+  case ValueKind::TupleElementAddrInst:
+  case ValueKind::UncheckedTakeEnumDataAddrInst:
+  case ValueKind::LoadInst:
+  case ValueKind::StoreInst:
+    return true;
+  default:
+    return false;
+  }
+}
+
+/// Look at the origin/user ValueBase of V to see if any of them are
+/// TypedAccessOracle which enable one to ascertain via undefined behavior the
+/// "true" type of the instruction.
+static SILType findTypedAccessType(SILValue V) {
+  // First look at the origin of V and see if we have any instruction that is a
+  // typed oracle.
+  if (auto *I = dyn_cast<SILInstruction>(V))
+    if (isTypedAccessOracle(I))
+      return V.getType();
+
+  // Then look at any uses of V that potentially could act as a typed access
+  // oracle.
+  for (auto Use : V.getUses())
+    if (isTypedAccessOracle(Use->getUser()))
+      return V.getType();
+
+  // Otherwise return an empty SILType
+  return SILType();
+}
+
+MemBehavior MemoryBehaviorVisitor::visitLoadInst(LoadInst *LI) {
+  if (AA.isNoAlias(LI->getOperand(), V, LI->getOperand().getType(),
+                   findTypedAccessType(V))) {
+    DEBUG(llvm::dbgs() << "  Load Operand does not alias inst. Returning "
+                          "None.\n");
+    return MemBehavior::None;
+  }
+
+  DEBUG(llvm::dbgs() << "  Could not prove that load inst does not alias "
+        "pointer. Returning may read.");
+  return MemBehavior::MayRead;
+}
+
 MemBehavior MemoryBehaviorVisitor::visitStoreInst(StoreInst *SI) {
   // If the store dest cannot alias the pointer in question, then the
   // specified value can not be modified by the store.
-  if (AA.isNoAlias(SI->getDest(), V)) {
+  if (AA.isNoAlias(SI->getDest(), V, SI->getDest().getType(),
+                   findTypedAccessType(V))) {
     DEBUG(llvm::dbgs() << "  Store Dst does not alias inst. Returning "
                           "None.\n");
     return MemBehavior::None;
@@ -618,6 +684,7 @@ MemBehavior MemoryBehaviorVisitor::visitApplyInst(ApplyInst *AI) {
                           " None.\n");
     return MemBehavior::None;
   }
+
   // If the builtin is side effect free, then it can only read memory.
   if (isSideEffectFree(BFR)) {
     DEBUG(llvm::dbgs() << "  Found apply of side effect free builtin. "
@@ -625,6 +692,10 @@ MemBehavior MemoryBehaviorVisitor::visitApplyInst(ApplyInst *AI) {
     return MemBehavior::MayRead;
   }
 
+  // FIXME: If the value (or any other values from the instruction that the
+  // value comes from) that we are tracking does not escape and we don't alias
+  // any of the arguments of the apply inst, we should be ok.
+
   // Otherwise be conservative and return that we may have side effects.
   DEBUG(llvm::dbgs() << "  Found apply of side effect builtin. "
                         "Returning MayHaveSideEffects.\n");

commit 3e917be738e3a46f7fae8a32c03b8da8a1c18807
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Tue May 6 02:35:08 2014 +0000

    Use full DeclNames for dynamic lookup of calls.
    
    Previously, we were just using the base name, which resulted in massive
    inefficiency when dealing with Clang (we basically had to check every
    selector in the system to see if it had the same first selector piece).
    I've hacked ConstraintSystem a bit to carry a map from UnresolvedDotExpr
    to the ApplyExpr that consumes it, so that we can use the full DeclName
    and look up methods by full selector.
    
    Now that dynamic lookup is fast, re-enable it for the
    Foundation_bridge.swift test. (r17520 actually provided most of the benefit.)
    
    This does break selector lookup on AnyObject when doing selector splitting,
    and slightly regresses diagnostics when you try to call a method on AnyObject
    and forget a parameter name.
    
    <rdar://problem/16808651>. Part of the Playground performance efforts.
    
    Swift SVN r17524

diff --git a/lib/ClangImporter/ClangImporter.cpp b/lib/ClangImporter/ClangImporter.cpp
index bc1e042a473..e2840bfe79b 100644
--- a/lib/ClangImporter/ClangImporter.cpp
+++ b/lib/ClangImporter/ClangImporter.cpp
@@ -885,6 +885,31 @@ ObjCSelector ClangImporter::Implementation::importSelector(
   return ObjCSelector(ctx, pieces.size(), pieces);
 }
 
+clang::Selector
+ClangImporter::Implementation::importSelector(DeclName name,
+                                              bool allowSimpleName) {
+  if (!allowSimpleName && name.isSimpleName())
+    return {};
+
+  clang::ASTContext &ctx = getClangASTContext();
+
+  SmallVector<clang::IdentifierInfo *, 8> pieces;
+  pieces.push_back(importName(name.getBaseName()).getAsIdentifierInfo());
+
+  auto argNames = name.getArgumentNames();
+  if (argNames.empty())
+    return ctx.Selectors.getNullarySelector(pieces.front());
+
+  if (!argNames.front().empty())
+    return {};
+  argNames = argNames.slice(1);
+
+  for (Identifier argName : argNames)
+    pieces.push_back(importName(argName).getAsIdentifierInfo());
+
+  return ctx.Selectors.getSelector(pieces.size(), pieces.data());
+}
+
 DeclName ClangImporter::Implementation::mapSelectorToDeclName(
            ObjCSelector selector,
            bool isInitializer)
@@ -1564,100 +1589,16 @@ void ClangModuleUnit::getImportedModules(
                        getASTContext().getStdlibModule()});
 }
 
-/// Returns true if the first selector piece matches the given identifier.
-static bool selectorMatchesName(clang::Selector sel, DeclName name) {
-  if (name.isSimpleName())
-    return sel.getNameForSlot(0) == name.getBaseName().str();
-  
-  if (sel.getNumArgs() != name.getArgumentNames().size() + 1)
-    return false;
-  if (sel.getNameForSlot(0) != name.getBaseName().str())
-    return false;
-
-  for (unsigned i = 1, e = sel.getNumArgs(); i < e; ++i)
-    if (sel.getNameForSlot(i) != name.getArgumentNames()[i-1].str())
-      return false;
-  return true;
-}
-
 static void lookupClassMembersImpl(ClangImporter::Implementation &Impl,
                                    VisibleDeclConsumer &consumer,
-                                   DeclName name = DeclName()) {
-  clang::Sema &S = Impl.getClangSema();
-  clang::ExternalASTSource *source = S.getExternalSource();
-
+                                   DeclName name) {
   // When looking for a subscript, we actually look for the getters
   // and setters.
-  clang::IdentifierInfo *objectAtIndexedSubscriptId = nullptr;
-  clang::IdentifierInfo *objectForKeyedSubscriptId = nullptr;
-  clang::IdentifierInfo *setObjectId = nullptr;
-  clang::IdentifierInfo *atIndexedSubscriptId = nullptr;
-  clang::IdentifierInfo *forKeyedSubscriptId = nullptr;
   bool isSubscript = name.isSimpleName(Impl.SwiftContext.Id_subscript);
-  if (isSubscript) {
-    auto &identTable = S.Context.Idents;
-    objectAtIndexedSubscriptId = &identTable.get("objectAtIndexedSubscript");
-    objectForKeyedSubscriptId = &identTable.get("objectForKeyedSubscript");
-    setObjectId = &identTable.get("setObject");
-    atIndexedSubscriptId = &identTable.get("atIndexedSubscript");
-    forKeyedSubscriptId = &identTable.get("forKeyedSubscript");
-  }
-
-  // Function that determines whether the given selector is acceptable.
-  auto acceptableSelector = [&](clang::Selector sel) -> bool {
-    if (!name)
-      return true;
-
-    switch (sel.getNumArgs()) {
-    case 0:
-      if (isSubscript)
-        return false;
-
-      break;
-
-    case 1:
-      if (isSubscript)
-        return sel.getIdentifierInfoForSlot(0) == objectAtIndexedSubscriptId ||
-               sel.getIdentifierInfoForSlot(0) == objectForKeyedSubscriptId;
-
-      break;
-
-    case 2:
-      if (isSubscript)
-        return (sel.getIdentifierInfoForSlot(0) == setObjectId &&
-                (sel.getIdentifierInfoForSlot(1) == atIndexedSubscriptId ||
-                 sel.getIdentifierInfoForSlot(1) == forKeyedSubscriptId));
-
-      break;
-
-    default:
-      if (isSubscript)
-        return false;
-
-      break;
-    }
-
-    return selectorMatchesName(sel, name);
-  };
-
-  // Force load all external methods.
-  // FIXME: Copied from Clang's SemaCodeComplete.
-  for (uint32_t i = 0, n = source->GetNumExternalSelectors(); i != n; ++i) {
-    clang::Selector sel = source->GetExternalSelector(i);
-    if (sel.isNull() || S.MethodPool.count(sel))
-      continue;
-    if (!acceptableSelector(sel))
-      continue;
-
-    S.ReadMethodPool(sel);
-  }
 
   // FIXME: Does not include methods from protocols.
-  // FIXME: Do we really have to import every single method?
-  // FIXME: Need a more efficient table in Clang to find "all selectors whose
-  // first piece is this name".
-  auto importMethods = [&](const clang::ObjCMethodList *list) {
-    for (; list != nullptr; list = list->getNext()) {
+  auto importMethodsImpl = [&](const clang::ObjCMethodList &start) {
+    for (auto *list = &start; list != nullptr; list = list->getNext()) {
       if (list->Method->isUnavailable())
         continue;
 
@@ -1675,36 +1616,85 @@ static void lookupClassMembersImpl(ClangImporter::Implementation &Impl,
         }
       }
 
-      if (auto VD = cast_or_null<ValueDecl>(Impl.importDeclReal(searchForDecl))) {
-        if (isSubscript || !name) {
-          // When searching for a subscript, we may have found a getter.  If so,
-          // use the subscript instead.
-          if (auto func = dyn_cast<FuncDecl>(VD)) {
-            auto known = Impl.Subscripts.find({func, nullptr});
-            if (known != Impl.Subscripts.end()) {
-              consumer.foundDecl(known->second, DeclVisibilityKind::DynamicLookup);
-            }
-          }
+      auto VD = cast_or_null<ValueDecl>(Impl.importDeclReal(searchForDecl));
+      if (!VD)
+        continue;
 
-          // If we were looking only for subscripts, don't report the getter.
-          if (isSubscript)
-            continue;
+      if (isSubscript || !name) {
+        // When searching for a subscript, we may have found a getter.  If so,
+        // use the subscript instead.
+        if (auto func = dyn_cast<FuncDecl>(VD)) {
+          auto known = Impl.Subscripts.find({func, nullptr});
+          if (known != Impl.Subscripts.end()) {
+            consumer.foundDecl(known->second,
+                               DeclVisibilityKind::DynamicLookup);
+          }
         }
 
-        consumer.foundDecl(VD, DeclVisibilityKind::DynamicLookup);
+        // If we were looking only for subscripts, don't report the getter.
+        if (isSubscript)
+          continue;
       }
+
+      consumer.foundDecl(VD, DeclVisibilityKind::DynamicLookup);
     }
   };
 
-  for (auto entry : S.MethodPool) {
-    if (!acceptableSelector(entry.first))
-      continue;
-  
-    auto &methodListPair = entry.second;
+  auto importMethods = [=](const clang::Sema::GlobalMethods &methodListPair) {
     if (methodListPair.first.Method)
-      importMethods(&methodListPair.first);
+      importMethodsImpl(methodListPair.first);
     if (methodListPair.second.Method)
-      importMethods(&methodListPair.second);
+      importMethodsImpl(methodListPair.second);
+  };
+
+  clang::Sema &S = Impl.getClangSema();
+
+  if (isSubscript) {
+    clang::Selector sels[] = {
+      Impl.objectAtIndexedSubscript,
+      Impl.setObjectAtIndexedSubscript,
+      Impl.objectForKeyedSubscript,
+      Impl.setObjectForKeyedSubscript
+    };
+    for (auto sel : sels) {
+      S.ReadMethodPool(sel);
+      importMethods(S.MethodPool[sel]);
+    }
+
+  } else if (name) {
+    auto sel = Impl.importSelector(name);
+    if (!sel.isNull()) {
+      S.ReadMethodPool(sel);
+      importMethods(S.MethodPool[sel]);
+
+      // If this is a simple name, we only checked nullary selectors. Check
+      // unary ones as well.
+      // Note: If this is ever used to look up init methods, we'd need to do
+      // the reverse as well.
+      if (name.isSimpleName()) {
+        auto *II = Impl.importName(name.getBaseName()).getAsIdentifierInfo();
+        sel = Impl.getClangASTContext().Selectors.getUnarySelector(II);
+        assert(!sel.isNull());
+
+        S.ReadMethodPool(sel);
+        importMethods(S.MethodPool[sel]);
+      }
+    }
+
+  } else {
+    // Force load all external methods.
+    // FIXME: Copied from Clang's SemaCodeComplete.
+    clang::ExternalASTSource *source = S.getExternalSource();
+    for (uint32_t i = 0, n = source->GetNumExternalSelectors(); i != n; ++i) {
+      clang::Selector sel = source->GetExternalSelector(i);
+      if (sel.isNull() || S.MethodPool.count(sel))
+        continue;
+
+      S.ReadMethodPool(sel);
+    }
+
+    for (auto entry : S.MethodPool)
+      importMethods(entry.second);
   }
 }
 
@@ -1728,7 +1718,7 @@ void ClangModuleUnit::lookupClassMembers(Module::AccessPathTy accessPath,
     return;
 
   // FIXME: Not limited by module.
-  lookupClassMembersImpl(owner.Impl, consumer);
+  lookupClassMembersImpl(owner.Impl, consumer, {});
 }
 
 void ClangModuleUnit::collectLinkLibraries(
diff --git a/lib/ClangImporter/ImporterImpl.h b/lib/ClangImporter/ImporterImpl.h
index b3195370700..bd2b9f37b2c 100644
--- a/lib/ClangImporter/ImporterImpl.h
+++ b/lib/ClangImporter/ImporterImpl.h
@@ -479,6 +479,9 @@ public:
   /// Import an Objective-C selector.
   ObjCSelector importSelector(clang::Selector selector);
 
+  /// Import a Swift name as a Clang selector.
+  clang::Selector importSelector(DeclName name, bool allowSimpleName = true);
+
   /// Map the given selector to a declaration name.
   ///
   /// \param selector The selector to map.
diff --git a/lib/Sema/CSGen.cpp b/lib/Sema/CSGen.cpp
index aa9cb8985db..bc64dab2dc0 100644
--- a/lib/Sema/CSGen.cpp
+++ b/lib/Sema/CSGen.cpp
@@ -1361,6 +1361,22 @@ namespace {
       if (isa<DefaultValueExpr>(expr)) {
         return { false, expr };
       }
+
+      // FIXME: This is a bit of a hack, recording the CallExpr that consumes
+      // an UnresolvedDotExpr so that we can do dynamic lookups more
+      // efficiently. Really we should just have the arguments be part of the
+      // UnresolvedDotExpr from the start.
+      if (auto call = dyn_cast<CallExpr>(expr)) {
+        if (Expr *fn = call->getFn()) {
+          if (auto optionalWrapper = dyn_cast<BindOptionalExpr>(fn))
+            fn = optionalWrapper->getSubExpr();
+          else if (auto forceWrapper = dyn_cast<ForceValueExpr>(fn))
+            fn = forceWrapper->getSubExpr();
+
+          if (auto UDE = dyn_cast<UnresolvedDotExpr>(fn))
+            CG.getConstraintSystem().recordPossibleDynamicCall(UDE, call);
+        }
+      }
       
       return { true, expr };
     }
diff --git a/lib/Sema/CSSimplify.cpp b/lib/Sema/CSSimplify.cpp
index 276c8153c4e..a1c2d90e7b2 100644
--- a/lib/Sema/CSSimplify.cpp
+++ b/lib/Sema/CSSimplify.cpp
@@ -2027,6 +2027,45 @@ static bool isUnavailableInExistential(TypeChecker &tc, ValueDecl *decl) {
   return containsProtocolSelf(type);
 }
 
+static DeclName
+getNameForValueLookup(
+    ASTContext &ctx,
+    const Constraint &constraint,
+    Type baseObjTy,
+    const llvm::DenseMap<UnresolvedDotExpr *, ApplyExpr *> &callMap) {
+  auto origName = constraint.getMember();
+  if (origName.isCompoundName())
+    return origName;
+
+  auto protoTy = baseObjTy->getAs<ProtocolType>();
+  if (!protoTy ||
+      !protoTy->getDecl()->isSpecificProtocol(KnownProtocolKind::AnyObject))
+    return origName;
+
+  // FIXME: Pulling out the expr from the locator is pretty hacky.
+  const ConstraintLocator *loc = constraint.getLocator();
+  if (!loc)
+    return origName;
+
+  auto UDE = dyn_cast_or_null<UnresolvedDotExpr>(loc->getAnchor());
+  if (!UDE)
+    return origName;
+
+  const ApplyExpr *call = callMap.lookup(UDE);
+  if (!call)
+    return origName;
+
+  // Handle the one-argument, no-argument-name case.
+  if (isa<ParenExpr>(call->getArg()))
+    return DeclName(ctx, origName.getBaseName(), Identifier());
+
+  auto args = dyn_cast<TupleExpr>(call->getArg());
+  if (!args)
+    return origName;
+
+  return DeclName(ctx, origName.getBaseName(), args->getElementNames());
+}
+
 ConstraintSystem::SolutionKind
 ConstraintSystem::simplifyMemberConstraint(const Constraint &constraint) {
   // Resolve the base type, if we can. If we can't resolve the base type,
@@ -2177,6 +2216,11 @@ ConstraintSystem::simplifyMemberConstraint(const Constraint &constraint) {
     return SolutionKind::Solved;
   }
 
+  // If we're doing dynamic lookup, and this is a call, use the full name of
+  // the member.
+  name = getNameForValueLookup(getASTContext(), constraint, baseObjTy,
+                               PossibleDynamicLookupCalls);
+
   // Look for members within the base.
   LookupResult &lookup = lookupMember(baseObjTy, name);
   if (!lookup) {
diff --git a/lib/Sema/ConstraintSystem.h b/lib/Sema/ConstraintSystem.h
index 2ba1c4422cc..046ef459bdf 100644
--- a/lib/Sema/ConstraintSystem.h
+++ b/lib/Sema/ConstraintSystem.h
@@ -1068,7 +1068,7 @@ private:
   ///
   /// These failures are unavoidable, in the sense that they occur before
   /// we have made any (potentially incorrect) assumptions at all.
-  SmallVector<Failure *, 1> unavoidableFailures;
+  TinyPtrVector<Failure *> unavoidableFailures;
 
   /// \brief Failures that occured while solving.
   ///
@@ -1086,6 +1086,8 @@ private:
 
   SmallVector<TypeVariableType *, 16> TypeVariables;
 
+  llvm::DenseMap<UnresolvedDotExpr *, ApplyExpr *> PossibleDynamicLookupCalls;
+
   /// \brief The set of constraint restrictions used to reach the
   /// current constraint system.
   ///
@@ -1524,6 +1526,10 @@ public:
                                          locator));
   }
 
+  void recordPossibleDynamicCall(UnresolvedDotExpr *UDE, ApplyExpr *AE) {
+    PossibleDynamicLookupCalls[UDE] = AE;
+  }
+
   /// Retrieve the type that corresponds to the given member of the
   /// given base type, which may be a newly-created type variable.
   ///

commit 5e045b4453911d915687460b928d2955374c3c63
Author: Dave Abrahams <dabrahams@apple.com>
Date:   Sat May 3 19:49:53 2014 +0000

    [stdlib] Limit array casts to Array<T>
    
    There's no need to support up- or down-casts for NativeArray<T> or
    Slice<T>.  Slice might turn out to be easy, but we don't have time right
    now.  Doing it for NativeArray<T> would have efficiency costs that we
    don't want to pay.
    
    Swift SVN r17323

diff --git a/docs/Arrays.rst b/docs/Arrays.rst
index 00f4915863d..df67d0e463c 100644
--- a/docs/Arrays.rst
+++ b/docs/Arrays.rst
@@ -129,42 +129,42 @@ it is statically known to be unneeded.
 
 .. image:: ArrayBridge.png
 
-Array Casts
------------
+``Array`` Casts
+---------------
 
-We can essentially reinterpret an array buffer containing elements of
+We can essentially reinterpret an ``Array`` buffer containing elements of
 dynamic type ``Derived`` as a buffer of elements of type ``Base``,
 where ``Derived`` is a subclass of ``Base``.  However, we cannot allow
 arbitrary ``Base`` elements to be inserted in the buffer without
 compromising type safety.  Also, our shared subscript assignment
-semantics imply that all copies of the resulting array of ``Base``
-elements see its subscript mutations.
+semantics imply that all copies of the resulting ``Array<Base>``
+see its subscript mutations.
 
-Therefore, casting one array type to another is akin to resizing an
-array: the new copy becomes independent.  To avoid an O(N) conversion
-cost, we use a layer of indirection in the data structure.  The
-indirection object is marked to prevent in-place mutation of the
-buffer; it will be copied upon its first mutation:
+Therefore, casting ``Array<T>`` to ``Array<U>`` is akin to resizing:
+the new copy becomes independent.  To avoid an O(N) conversion cost,
+we use a layer of indirection in the data structure.  The indirection
+object is marked to prevent in-place mutation of the buffer; it will
+be copied upon its first mutation:
 
 .. image:: ArrayCast.png
 
 The specific rules for casting are as follows:
 
-* An *ArrayType*\ ``<T>`` references a buffer of elements dynamically
+* An ``Array<T>`` references a buffer of elements dynamically
   known to have type ``T``
 
-* In O(1), *ArrayType*\ ``<T>`` implicitly converts to *ArrayType*\
-  ``<U>`` iff ``T`` is derived from ``U`` or if ``T`` is *bridged* to
-  ``U`` or a subclass thereof, including ``AnyObject``â€”see below__.  The
+* In O(1), ``Array<T>`` implicitly converts to ``Array<U>`` iff ``T``
+  is derived from ``U`` or if ``T`` is *bridged* to ``U`` or a
+  subclass thereof, including ``AnyObject``\ â€”see below__.  The
   resulting array references the same buffer as the original.
 
   __ `bridging to objective-c`_
 
-* In O(1), *ArrayType*\ ``<U>`` explicitly converts to *ArrayType*\
-  ``<T>?`` via ``x as ArrayType<T>``.  The cast succeeds, yielding a
-  non-nil result, iff the array buffer elements are dynamically known
-  to have type ``T`` or a type derived from ``T``.  The resulting
-  array references the same buffer as the original.
+* In O(1), ``Array<U>`` explicitly converts to ``Array<T>?`` via ``x
+  as Array<T>``.  The cast succeeds, yielding a non-nil result, iff
+  the array buffer elements are dynamically known to have type ``T``
+  or a type derived from ``T``.  The resulting ``Array<T>`` references
+  the same buffer as the original.
 
 Bridging Rules and Terminology for all Types
 --------------------------------------------

commit be12d86dddf5d405770ef1199bd1fe4f402268ae
Author: Jordan Rose <jordan_rose@apple.com>
Date:   Thu Dec 5 01:51:11 2013 +0000

    Turn ClangModule into ClangModuleUnit.
    
    Part of the FileUnit restructuring. A Clang module (whether from a framework
    or a simple collection of headers) is now imported as a TranslationUnit
    containing a single ClangModuleUnit.
    
    One wrinkle in all this is that Swift very much wants to do searches on a
    per-module basis, but Clang can only do lookups across the entire
    TranslationUnit. Unless and until we get a better way to deal with this,
    we're stuck with an inefficiency here. Previously, we used to hack around
    this by ignoring the "per-module" bit and only performing one lookup into
    all Clang modules, but that's not actually correct with respect to visibility.
    
    Now, we're just taking the filtering hit for looking up a particular name,
    and caching the results when we look up everything (for code completion).
    This isn't ideal, but it doesn't seem to be costing too much in performance,
    at least not right now, and it means we can get visibility correct.
    
    In the future, it might make sense to include a ClangModuleUnit alongside a
    SerializedASTFile for adapter modules, rather than having two separate
    modules with the same name. I haven't really thought through this yet, though.
    
    Swift SVN r10834

diff --git a/include/swift/AST/ASTContext.h b/include/swift/AST/ASTContext.h
index 49709229b50..be4ceab3bba 100644
--- a/include/swift/AST/ASTContext.h
+++ b/include/swift/AST/ASTContext.h
@@ -49,7 +49,6 @@ namespace swift {
   class ASTContext;
   class ASTMutationListener;
   class BoundGenericType;
-  class ClangModule;
   class Decl;
   class ExtensionDecl;
   class FuncDecl;
@@ -506,8 +505,8 @@ public:
 
 private:
   friend class Decl;
-  ClangNode getClangNode(Decl *decl);
-  void setClangNode(Decl *decl, ClangNode node);
+  ClangNode getClangNode(const Decl *decl);
+  void setClangNode(const Decl *decl, ClangNode node);
 
   friend class BoundGenericType;
 
diff --git a/include/swift/AST/Decl.h b/include/swift/AST/Decl.h
index 15c0a5120cf..1b45cbe1814 100644
--- a/include/swift/AST/Decl.h
+++ b/include/swift/AST/Decl.h
@@ -299,7 +299,7 @@ protected:
     DeclBits.FromClang = false;
   }
 
-  ClangNode getClangNodeSlow();
+  ClangNode getClangNodeSlow() const;
 
 public:
   DeclKind getKind() const { return DeclKind(DeclBits.Kind); }
@@ -394,7 +394,7 @@ public:
 
   /// \brief Retrieve the Clang AST node from which this declaration was
   /// synthesized, if any.
-  ClangNode getClangNode() {
+  ClangNode getClangNode() const {
     if (!DeclBits.FromClang)
       return ClangNode();
 
@@ -403,7 +403,7 @@ public:
 
   /// \brief Retrieve the Clang declaration from which this declaration was
   /// synthesized, if any.
-  const clang::Decl *getClangDecl() {
+  const clang::Decl *getClangDecl() const {
     if (!DeclBits.FromClang)
       return nullptr;
 
diff --git a/include/swift/AST/Module.h b/include/swift/AST/Module.h
index 4213ddb6e58..deb6943fac7 100644
--- a/include/swift/AST/Module.h
+++ b/include/swift/AST/Module.h
@@ -327,8 +327,10 @@ enum class FileUnitKind {
   Source,
   /// For the compiler Builtin module.
   Builtin,
-  /// Any other loaded module.
-  Loaded
+  /// A serialized Swift AST.
+  SerializedAST,
+  /// An imported Clang module.
+  ClangModule
 };
 
 /// A container for module-scope declarations that itself provides a scope; the
@@ -737,7 +739,9 @@ public:
 class LoadedFile : public FileUnit {
 protected:
   LoadedFile(FileUnitKind Kind, TranslationUnit &TU) noexcept
-    : FileUnit(Kind, TU) {}
+    : FileUnit(Kind, TU) {
+    assert(classof(this) && "invalid kind");
+  }
 
 public:
   /// Returns an arbitrary string representing the storage backing this file.
@@ -754,8 +758,9 @@ public:
     return nullptr;
   }
 
-  static bool classof(const FileUnit *M) {
-    return M->getKind() == FileUnitKind::Loaded;
+  static bool classof(const FileUnit *file) {
+    return file->getKind() == FileUnitKind::SerializedAST ||
+           file->getKind() == FileUnitKind::ClangModule;
   }
   static bool classof(const DeclContext *DC) {
     return isa<FileUnit>(DC) && classof(cast<FileUnit>(DC));
diff --git a/include/swift/ClangImporter/ClangImporter.h b/include/swift/ClangImporter/ClangImporter.h
index c550a4b3082..425c236064c 100644
--- a/include/swift/ClangImporter/ClangImporter.h
+++ b/include/swift/ClangImporter/ClangImporter.h
@@ -27,18 +27,20 @@ namespace clang {
 namespace swift {
 
 class ASTContext;
+class ClangModuleUnit;
 class Module;
 class NominalTypeDecl;
 
 /// \brief Class that imports Clang modules into Swift, mapping directly
 /// from Clang ASTs over to Swift ASTs.
 class ClangImporter : public ModuleLoader {
+  friend class ClangModuleUnit;
+
 public:
-  struct Implementation;
+  class Implementation;
 
 private:
   Implementation &Impl;
-  friend class ClangModule;
 
   ClangImporter(ASTContext &ctx);
 
@@ -105,21 +107,8 @@ public:
 
   /// \brief Look for declarations associated with the given name.
   ///
-  /// \param module The module to search.
-  ///
-  /// \param accessPath The access path used to refer to the name within this
-  /// (top-level) module.
-  ///
   /// \param name The name we're searching for.
-  ///
-  /// \param lookupKind Whether we're performing qualified vs. unqualified
-  /// lookup.
-  ///
-  /// \param result Will be populated with the results of name lookup.
-  virtual void lookupValue(const Module *module,
-                           Module::AccessPathTy accessPath, Identifier name,
-                           NLKind lookupKind,
-                           SmallVectorImpl<ValueDecl*> &result) override;
+  void lookupValue(Identifier name, VisibleDeclConsumer &consumer);
   
   /// \brief Look for visible declarations in the Clang translation unit.
   ///
@@ -134,11 +123,6 @@ public:
   /// are found and imported.
   void lookupVisibleDecls(VisibleDeclConsumer &Consumer) const;
 
-  void lookupVisibleDecls(const Module *M,
-                          Module::AccessPathTy AccessPath,
-                          VisibleDeclConsumer &Consumer,
-                          NLKind LookupKind) override;
-
   /// \brief Load extensions to the given nominal type.
   ///
   /// \param nominal The nominal type whose extensions should be loaded.
@@ -149,32 +133,6 @@ public:
   virtual void loadExtensions(NominalTypeDecl *nominal,
                               unsigned previousGeneration) override;
 
-  virtual void getImportedModules(
-    const Module *module,
-    SmallVectorImpl<Module::ImportedModule> &exports,
-    bool includePrivate) override;
-  
-  virtual void lookupClassMembers(const Module *module,
-                                  Module::AccessPathTy accessPath,
-                                  VisibleDeclConsumer &consumer) override;
-  
-  virtual void lookupClassMember(const Module *module,
-                                 Module::AccessPathTy accessPath,
-                                 Identifier name,
-                                 SmallVectorImpl<ValueDecl*> &results) override;
-
-  virtual void
-  collectLinkLibraries(const Module *module,
-                       Module::LinkLibraryCallback callback) override;
-
-  virtual void getTopLevelDecls(const Module *Module,
-                                SmallVectorImpl<Decl*> &Results) override;
-
-  virtual void getDisplayDecls(const Module *module,
-                               SmallVectorImpl<Decl*> &results) override;
-
-  StringRef getModuleFilename(const Module *Module) override;
-
   void verifyAllModules() override;
 
   clang::TargetInfo &getTargetInfo() const;
diff --git a/include/swift/ClangImporter/ClangModule.h b/include/swift/ClangImporter/ClangModule.h
index 4d556616925..639250f0e5b 100644
--- a/include/swift/ClangImporter/ClangModule.h
+++ b/include/swift/ClangImporter/ClangModule.h
@@ -29,35 +29,54 @@ class ClangImporter;
 class ModuleLoader;
 
 /// \brief Represents a Clang module that has been imported into Swift.
-class ClangModule : public LoadedModule {
-  friend class ClangImporter;
+class ClangModuleUnit final : public LoadedFile {
+  ClangImporter &owner;
   clang::Module *clangModule;
   llvm::PointerIntPair<Module *, 1, bool> adapterModule;
 
   Module *getAdapterModule() const;
 
 public:
-  ClangModule(ASTContext &ctx, StringRef DebugModuleName,
-              ModuleLoader &owner, clang::Module *clangModule);
+  ClangModuleUnit(TranslationUnit &TU, ClangImporter &owner,
+                  clang::Module *clangModule);
 
   /// \brief Retrieve the underlying Clang module.
-  // FIXME: Remove this.
   clang::Module *getClangModule() const { return clangModule; }
 
   /// Returns true if this is a top-level Clang module (not a submodule).
   bool isTopLevel() const;
 
-  /// Returns the name of the enclosing top-level module without importing it.
-  ///
-  /// If this module is itself a top-level module, returns this module's name.
-  StringRef getTopLevelModuleName() const;
+  virtual void lookupValue(Module::AccessPathTy accessPath,
+                           Identifier name, NLKind lookupKind,
+                           SmallVectorImpl<ValueDecl*> &results) const override;
 
-  static bool classof(const Module *M) {
-    return M->getKind() == ModuleKind::Clang;
-  }
+  virtual void lookupVisibleDecls(Module::AccessPathTy accessPath,
+                                  VisibleDeclConsumer &consumer,
+                                  NLKind lookupKind) const override;
+
+  virtual void lookupClassMembers(Module::AccessPathTy accessPath,
+                                  VisibleDeclConsumer &consumer) const override;
+
+  virtual void
+  lookupClassMember(Module::AccessPathTy accessPath, Identifier name,
+                    SmallVectorImpl<ValueDecl*> &decls) const override;
+
+  virtual void getTopLevelDecls(SmallVectorImpl<Decl*> &results) const override;
 
+  virtual void
+  getImportedModules(SmallVectorImpl<Module::ImportedModule> &imports,
+                     bool includePrivate) const override;
+
+  virtual void
+  collectLinkLibraries(Module::LinkLibraryCallback callback) const override;
+
+  virtual StringRef getFilename() const override;
+
+  static bool classof(const FileUnit *file) {
+    return file->getKind() == FileUnitKind::ClangModule;
+  }
   static bool classof(const DeclContext *DC) {
-    return isa<Module>(DC) && classof(cast<Module>(DC));
+    return isa<FileUnit>(DC) && classof(cast<FileUnit>(DC));
   }
 };
 
diff --git a/include/swift/IDE/CodeCompletion.h b/include/swift/IDE/CodeCompletion.h
index 34db95139c9..e46605a8f31 100644
--- a/include/swift/IDE/CodeCompletion.h
+++ b/include/swift/IDE/CodeCompletion.h
@@ -23,7 +23,6 @@
 
 namespace swift {
 class CodeCompletionCallbacksFactory;
-class ClangModule;
 class Decl;
 
 namespace ide {
diff --git a/lib/AST/ASTContext.cpp b/lib/AST/ASTContext.cpp
index 14efe9242a0..31ac457e130 100644
--- a/lib/AST/ASTContext.cpp
+++ b/lib/AST/ASTContext.cpp
@@ -94,7 +94,7 @@ struct ASTContext::Implementation {
 
   /// \brief Map from Swift declarations to the Clang nodes from which
   /// they were imported.
-  llvm::DenseMap<swift::Decl *, ClangNode> ClangNodes;
+  llvm::DenseMap<const Decl *, ClangNode> ClangNodes;
 
   /// \brief Map from local declarations to their discriminators.
   /// Missing entries implicitly have value 0.
@@ -701,13 +701,13 @@ Module *ASTContext::getStdlibModule() const {
   return TheStdlibModule;
 }
 
-ClangNode ASTContext::getClangNode(Decl *decl) {
+ClangNode ASTContext::getClangNode(const Decl *decl) {
   auto known = Impl.ClangNodes.find(decl);
   assert(known != Impl.ClangNodes.end() && "No Clang node?");
   return known->second;
 }
 
-void ASTContext::setClangNode(Decl *decl, ClangNode node) {
+void ASTContext::setClangNode(const Decl *decl, ClangNode node) {
   Impl.ClangNodes[decl] = node;
 }
 
diff --git a/lib/AST/Decl.cpp b/lib/AST/Decl.cpp
index f426f452cea..a72267babdd 100644
--- a/lib/AST/Decl.cpp
+++ b/lib/AST/Decl.cpp
@@ -95,7 +95,7 @@ case DeclKind::ID: return cast<ID##Decl>(this)->getLoc();
   llvm_unreachable("Unknown decl kind");
 }
 
-ClangNode Decl::getClangNodeSlow() {
+ClangNode Decl::getClangNodeSlow() const {
   return getASTContext().getClangNode(this);
 }
 
diff --git a/lib/AST/DeclContext.cpp b/lib/AST/DeclContext.cpp
index cb7003d8b78..f07a5fb59be 100644
--- a/lib/AST/DeclContext.cpp
+++ b/lib/AST/DeclContext.cpp
@@ -312,7 +312,8 @@ unsigned DeclContext::printContext(raw_ostream &OS) const {
     case FileUnitKind::Source:
       OS << " file=\"" << cast<SourceFile>(this)->getFilename() << "\"";
       break;
-    case FileUnitKind::Loaded:
+    case FileUnitKind::SerializedAST:
+    case FileUnitKind::ClangModule:
       OS << " file=\"" << cast<LoadedFile>(this)->getFilename() << "\"";
     }
     break;
diff --git a/lib/AST/Mangle.cpp b/lib/AST/Mangle.cpp
index 9a0ca40a0c2..b347254c9f5 100644
--- a/lib/AST/Mangle.cpp
+++ b/lib/AST/Mangle.cpp
@@ -21,7 +21,6 @@
 #include "swift/AST/Module.h"
 #include "swift/AST/ProtocolConformance.h"
 #include "swift/Basic/Punycode.h"
-#include "swift/ClangImporter/ClangModule.h"
 #include "clang/AST/Decl.h"
 #include "clang/AST/DeclObjC.h"
 #include "llvm/ADT/DenseMap.h"
@@ -173,9 +172,12 @@ void Mangler::mangleContextOf(ValueDecl *decl) {
 
   // Declarations provided by a C module have a special context mangling.
   //   known-context ::= 'SC'
-  if (isa<ClangModule>(decl->getDeclContext())) {
-    Buffer << "SC";
-    return;
+  // Do a dance to avoid a layering dependency.
+  if (auto file = dyn_cast<FileUnit>(decl->getDeclContext())) {
+    if (file->getKind() == FileUnitKind::ClangModule) {
+      Buffer << "SC";
+      return;
+    }
   }
 
   // Otherwise, just mangle the decl's DC.
diff --git a/lib/AST/Module.cpp b/lib/AST/Module.cpp
index 323c39061b9..a2244a3ce8d 100644
--- a/lib/AST/Module.cpp
+++ b/lib/AST/Module.cpp
@@ -857,7 +857,8 @@ lookupOperatorDeclForName(const FileUnit &File, SourceLoc Loc, Identifier Name,
     return nullptr;
   case FileUnitKind::Source:
     break;
-  case FileUnitKind::Loaded:
+  case FileUnitKind::SerializedAST:
+  case FileUnitKind::ClangModule:
     return lookupOperator<OP_DECL>(cast<LoadedFile>(File), Name);
   }
 
diff --git a/lib/AST/ModuleNameLookup.cpp b/lib/AST/ModuleNameLookup.cpp
index 342ef2056d9..9640e39e8ae 100644
--- a/lib/AST/ModuleNameLookup.cpp
+++ b/lib/AST/ModuleNameLookup.cpp
@@ -19,15 +19,9 @@ using namespace swift;
 using namespace namelookup;
 
 namespace {
-  /// A cache used by lookupInModule().
-  class ModuleLookupCache {
-  public:
-    using MapTy = llvm::SmallDenseMap<Module::ImportedModule,
-                                      TinyPtrVector<ValueDecl *>,
-                                      32>;
-    MapTy Map;
-    bool SearchedClangModule = false;
-  };
+  using ModuleLookupCache = llvm::SmallDenseMap<Module::ImportedModule,
+                                                TinyPtrVector<ValueDecl *>,
+                                                32>;
 
   class SortCanType {
   public:
@@ -36,7 +30,7 @@ namespace {
     }
   };
 
-  using CanTypeSet = llvm::SmallSet<CanType, 8, SortCanType>;
+  using CanTypeSet = llvm::SmallSet<CanType, 4, SortCanType>;
   using NamedCanTypeSet =
     llvm::DenseMap<Identifier, std::pair<ResolutionKind, CanTypeSet>>;
   static_assert(ResolutionKind() == ResolutionKind::Overloadable,
@@ -151,9 +145,9 @@ static void lookupInModule(Module *module, Module::AccessPathTy accessPath,
                            ModuleLookupCache &cache,
                            ArrayRef<Module::ImportedModule> extraImports,
                            CallbackTy callback) {
-  ModuleLookupCache::MapTy::iterator iter;
+  ModuleLookupCache::iterator iter;
   bool isNew;
-  std::tie(iter, isNew) = cache.Map.insert({{accessPath, module}, {}});
+  std::tie(iter, isNew) = cache.insert({{accessPath, module}, {}});
   if (!isNew) {
     decls.append(iter->second.begin(), iter->second.end());
     return;
@@ -161,19 +155,8 @@ static void lookupInModule(Module *module, Module::AccessPathTy accessPath,
 
   size_t initialCount = decls.size();
 
-  // Only perform unscoped searches once in Clang modules.
-  // FIXME: This is a weird hack. ClangImporter should just filter the results
-  // for us.
-  bool isClangModule = false;
-  if (accessPath.empty())
-    isClangModule = module->getKind() == ModuleKind::Clang;
-
   SmallVector<ValueDecl *, 4> localDecls;
-  if (!isClangModule || !cache.SearchedClangModule) {
-    callback(module, accessPath, localDecls);
-    if (isClangModule)
-      cache.SearchedClangModule = true;
-  }
+  callback(module, accessPath, localDecls);
 
   OverloadSetTy overloads;
   // FIXME: Pass TypeResolver down instead of getting it from the AST.
@@ -229,7 +212,7 @@ static void lookupInModule(Module *module, Module::AccessPathTy accessPath,
   auto afterUnique = std::unique(decls.begin() + initialCount, decls.end());
   decls.erase(afterUnique, decls.end());
 
-  auto &cachedValues = cache.Map[{accessPath, module}];
+  auto &cachedValues = cache[{accessPath, module}];
   cachedValues.insert(cachedValues.end(),
                       decls.begin() + initialCount,
                       decls.end());
diff --git a/lib/ClangImporter/ClangImporter.cpp b/lib/ClangImporter/ClangImporter.cpp
index 93948ee0fcd..45f8d584a9a 100644
--- a/lib/ClangImporter/ClangImporter.cpp
+++ b/lib/ClangImporter/ClangImporter.cpp
@@ -265,71 +265,90 @@ Module *ClangImporter::loadModule(
                                                /*IsInclusionDirective=*/false);
   if (!clangModule)
     return nullptr;
-  auto &cachedResult = Impl.ModuleWrappers[clangModule];
-  if (Module *result = cachedResult.getPointer()) {
-    if (!cachedResult.getInt()) {
+
+  // Bump the generation count.
+  ++Impl.Generation;
+  Impl.SwiftContext.bumpGeneration();
+  Impl.CachedVisibleDecls.clear();
+  Impl.CacheIsValid = false;
+
+  auto &cacheEntry = Impl.ModuleWrappers[clangModule];
+  if (ClangModuleUnit *cached = cacheEntry.getPointer()) {
+    Module *M = cached->getParentModule();
+    if (!cacheEntry.getInt()) {
       // Force load adapter modules for all imported modules.
       // FIXME: This forces the creation of wrapper modules for all imports as
       // well, and may do unnecessary work.
-      cachedResult.setInt(true);
-      result->forAllVisibleModules(path, [&](Module::ImportedModule import) {});
+      cacheEntry.setInt(true);
+      M->forAllVisibleModules(path, [&](Module::ImportedModule import) {});
     }
-    return result;
+    return M;
   }
 
   // Build the representation of the Clang module in Swift.
   // FIXME: The name of this module could end up as a key in the ASTContext,
   // but that's not correct for submodules.
-  auto result = new (Impl.SwiftContext)
-    ClangModule(Impl.SwiftContext, (*clangModule).getFullModuleName(),
-                *this, clangModule);
-  cachedResult.setPointer(result);
+  Identifier name = Impl.SwiftContext.getIdentifier((*clangModule).Name);
+  auto result = new (Impl.SwiftContext) TranslationUnit(name,Impl.SwiftContext);
+
+  auto file = new (Impl.SwiftContext) ClangModuleUnit(*result, *this,
+                                                      clangModule);
+  result->addFile(*file);
+  cacheEntry.setPointerAndInt(file, true);
 
   // FIXME: Total hack.
   if (!Impl.firstClangModule)
-    Impl.firstClangModule = result;
+    Impl.firstClangModule = file;
 
   // Force load adapter modules for all imported modules.
   // FIXME: This forces the creation of wrapper modules for all imports as
   // well, and may do unnecessary work.
-  cachedResult.setInt(true);
   result->forAllVisibleModules(path, [](Module::ImportedModule import) {});
 
-  // Bump the generation count.
-  ++Impl.Generation;
-  Impl.SwiftContext.bumpGeneration();
-
   return result;
 }
 
-ClangModule *
-ClangImporter::Implementation::getWrapperModule(ClangImporter &importer,
-                                                clang::Module *underlying) {
+ClangModuleUnit *
+ClangImporter::Implementation::getWrapperForModule(ClangImporter &importer,
+                                                   clang::Module *underlying) {
   auto &cacheEntry = ModuleWrappers[underlying];
-  if (ClangModule *cachedModule = cacheEntry.getPointer())
-    return cachedModule;
+  if (ClangModuleUnit *cached = cacheEntry.getPointer())
+    return cached;
 
-  auto result = new (SwiftContext) ClangModule(SwiftContext,
-                                               underlying->getFullModuleName(),
-                                               importer, underlying);
-  cacheEntry.setPointer(result);
-  return result;
+  // FIXME: Handle hierarchical names better.
+  Identifier name = SwiftContext.getIdentifier(underlying->Name);
+  auto wrapper = new (SwiftContext) TranslationUnit(name, SwiftContext);
+
+  auto file = new (SwiftContext) ClangModuleUnit(*wrapper, importer,
+                                                 underlying);
+  wrapper->addFile(*file);
+  cacheEntry.setPointer(file);
+
+  return file;
 }
 
-ClangModule *ClangImporter::Implementation::getClangModuleForDecl(
-    const clang::Decl *D) {
+static clang::Module *getBestOwningModule(const clang::Decl *D) {
   if (auto OID = dyn_cast<clang::ObjCInterfaceDecl>(D))
     // Put the Objective-C class into the module that contains the @interface
     // definition, not just @class forward declaration.
     D = OID->getDefinition();
+  else if (auto RD = dyn_cast<clang::RecordDecl>(D))
+    D = RD->getDefinition();
   else
     D = D->getCanonicalDecl();
+
   if (!D)
     return nullptr;
 
-  clang::Module *M = D->getOwningModule();
+  return D->getOwningModule();
+}
+
+ClangModuleUnit *ClangImporter::Implementation::getClangModuleForDecl(
+    const clang::Decl *D) {
+  clang::Module *M = getBestOwningModule(D);
   if (!M)
     return nullptr;
+
   // Get the parent module because currently we don't represent submodules with
   // ClangModule.
   // FIXME: this is just a workaround until we can import submodules.
@@ -337,7 +356,7 @@ ClangModule *ClangImporter::Implementation::getClangModuleForDecl(
 
   auto &importer =
     static_cast<ClangImporter &>(*SwiftContext.getClangModuleLoader());
-  return getWrapperModule(importer, M);
+  return getWrapperForModule(importer, M);
 }
 
 #pragma mark Source locations
@@ -424,15 +443,7 @@ ClangImporter::Implementation::importName(clang::DeclarationName name,
 
 
 #pragma mark Name lookup
-void ClangImporter::lookupValue(const Module *module,
-                                Module::AccessPathTy accessPath,
-                                Identifier name,
-                                NLKind lookupKind,
-                                SmallVectorImpl<ValueDecl*> &results) {
-  assert(accessPath.size() <= 1 && "can only refer to top-level decls");
-  if (accessPath.size() == 1 && accessPath.front().first != name)
-    return;
-
+void ClangImporter::lookupValue(Identifier name, VisibleDeclConsumer &consumer){
   auto &pp = Impl.Instance->getPreprocessor();
   auto &sema = Impl.Instance->getSema();
 
@@ -460,7 +471,7 @@ void ClangImporter::lookupValue(const Module *module,
   if (clangID && clangID->hasMacroDefinition()) {
     if (auto clangMacro = pp.getMacroInfo(clangID)) {
       if (auto valueDecl = Impl.importMacro(name, clangMacro)) {
-        results.push_back(valueDecl);
+        consumer.foundDecl(valueDecl, DeclVisibilityKind::VisibleAtTopLevel);
       }
     }
   }
@@ -481,7 +492,7 @@ void ClangImporter::lookupValue(const Module *module,
               valueDecl->getModuleContext() == Impl.getSwiftModule())
             continue;
 
-          results.push_back(valueDecl);
+          consumer.foundDecl(valueDecl, DeclVisibilityKind::VisibleAtTopLevel);
           FoundType = FoundType || isa<TypeDecl>(valueDecl);
         }
     }
@@ -498,7 +509,7 @@ void ClangImporter::lookupValue(const Module *module,
     for (auto decl : lookupResult) {
       if (auto swiftDecl = Impl.importDecl(decl->getUnderlyingDecl()))
         if (auto valueDecl = dyn_cast<ValueDecl>(swiftDecl))
-          results.push_back(valueDecl);
+          consumer.foundDecl(valueDecl, DeclVisibilityKind::VisibleAtTopLevel);
     }
   }
 }
@@ -511,79 +522,133 @@ ClangImporter::lookupVisibleDecls(clang::VisibleDeclConsumer &consumer) const {
                           consumer);
 }
 
+static bool isVisibleFromModule(const ClangModuleUnit *ModuleFilter,
+                                const ValueDecl *VD) {
+  // Include a value from module X if:
+  // * no particular module was requested, or
+  // * module X was specifically requested.
+  if (!ModuleFilter)
+    return true;
+
+  auto ContainingUnit = VD->getDeclContext()->getModuleScopeContext();
+  if (ModuleFilter == ContainingUnit)
+    return true;
+
+  auto Wrapper = dyn_cast<ClangModuleUnit>(ContainingUnit);
+  if (!Wrapper)
+    return false;
+
+  auto *ClangDecl = VD->getClangDecl();
+  assert(ClangDecl);
+
+  auto OwningClangModule = ClangDecl->getOwningModule();
+
+  // FIXME: This only triggers for implicitly-generated decls like
+  // __builtin_va_list, which we probably shouldn't be importing anyway.
+  // But it also includes the builtin declarations for 'id', 'Class', 'SEL',
+  // and '__int128_t'.
+  if (!OwningClangModule)
+    return true;
+
+  // FIXME: If this is in another module, we shouldn't really be considering
+  // it here, but the recursive lookup through exports doesn't seem to be
+  // working right yet.
+  return ModuleFilter->getClangModule()->isModuleVisible(OwningClangModule);
+}
+
+
 namespace {
 class ImportingVisibleDeclConsumer : public clang::VisibleDeclConsumer {
-  ClangImporter &TheClangImporter;
   ClangImporter::Implementation &Impl;
   swift::VisibleDeclConsumer &NextConsumer;
-  const ClangModule *ModuleFilter = nullptr;
+  const ClangModuleUnit *ModuleFilter = nullptr;
 
 public:
-  ImportingVisibleDeclConsumer(ClangImporter &TheClangImporter,
-                               ClangImporter::Implementation &Impl,
+  ImportingVisibleDeclConsumer(ClangImporter::Implementation &Impl,
                                swift::VisibleDeclConsumer &NextConsumer)
-      : TheClangImporter(TheClangImporter), Impl(Impl),
-        NextConsumer(NextConsumer) {}
+      : Impl(Impl), NextConsumer(NextConsumer) {}
 
-  void filterByModule(const ClangModule *M) {
-    ModuleFilter = M;
+  void filterByModule(const ClangModuleUnit *CMU) {
+    ModuleFilter = CMU;
   }
 
   void FoundDecl(clang::NamedDecl *ND, clang::NamedDecl *Hiding,
                  clang::DeclContext *Ctx,
                  bool InBaseClass) override {
-    if (ND->getName().empty())
+    if (!ND->getIdentifier())
       return;
 
     if (ND->isModulePrivate())
       return;
 
-    SmallVector<ValueDecl *, 4> Results;
-    TheClangImporter.lookupValue(/*module*/ nullptr,
-                                 Module::AccessPathTy(),
-                                 Impl.SwiftContext.getIdentifier(ND->getName()),
-                                 NLKind::UnqualifiedLookup,
-                                 Results);
-    for (auto *VD : Results) {
-      // Include results from this module if:
-      // * no particular module was requested, or
-      // * this module was specifically requested, or
-      // * this module is re-exported from the requested module.
-      bool ShouldReport = !ModuleFilter ||
-                          VD->getModuleContext() == ModuleFilter;
-      if (!ShouldReport) {
-        clang::Module *ThisDeclClangModule = nullptr;
-        if (auto *ClangDecl = VD->getClangDecl()) {
-          ThisDeclClangModule = ClangDecl->getOwningModule();
-        }
-        if (!ThisDeclClangModule) {
-          ThisDeclClangModule =
-              cast<ClangModule>(VD->getModuleContext())->getClangModule();
-        }
-        ShouldReport = ModuleFilter->getClangModule()->isModuleVisible(
-            ThisDeclClangModule);
-      }
-      if (ShouldReport)
-        NextConsumer.foundDecl(VD, DeclVisibilityKind::VisibleAtTopLevel);
-    }
+    if (auto Imported = cast_or_null<ValueDecl>(Impl.importDecl(ND)))
+      if (isVisibleFromModule(ModuleFilter, Imported))
+        NextConsumer.foundDecl(Imported, DeclVisibilityKind::VisibleAtTopLevel);
   }
 };
+
+class FilteringVisibleDeclConsumer : public swift::VisibleDeclConsumer {
+  swift::VisibleDeclConsumer &NextConsumer;
+  const ClangModuleUnit *ModuleFilter = nullptr;
+
+public:
+  FilteringVisibleDeclConsumer(swift::VisibleDeclConsumer &consumer,
+                               const ClangModuleUnit *CMU)
+      : NextConsumer(consumer), ModuleFilter(CMU) {}
+
+  virtual void foundDecl(ValueDecl *VD, DeclVisibilityKind Reason) override {
+    if (isVisibleFromModule(ModuleFilter, VD))
+      NextConsumer.foundDecl(VD, Reason);
+  }
+};
+
+/// Inserts found decls into an externally-owned SmallVector.
+class VectorDeclConsumer : public VisibleDeclConsumer {
+public:
+  SmallVectorImpl<ValueDecl *> &results;
+  explicit VectorDeclConsumer(SmallVectorImpl<ValueDecl *> &decls)
+    : results(decls) {}
+
+  void foundDecl(ValueDecl *VD, DeclVisibilityKind Reason) override {
+    results.push_back(VD);
+  }
+};
+
 } // unnamed namespace
 
 void ClangImporter::lookupVisibleDecls(VisibleDeclConsumer &Consumer) const {
-  ImportingVisibleDeclConsumer ImportingConsumer(
-      const_cast<ClangImporter &>(*this), Impl, Consumer);
-  lookupVisibleDecls(ImportingConsumer);
+  if (!Impl.CacheIsValid) {
+    VectorDeclConsumer CacheConsumer(Impl.CachedVisibleDecls);
+    ImportingVisibleDeclConsumer ImportingConsumer(Impl, Consumer);
+
+    auto &sema = Impl.getClangSema();
+    sema.LookupVisibleDecls(Impl.getClangASTContext().getTranslationUnitDecl(),
+                            clang::Sema::LookupNameKind::LookupAnyName,
+                            ImportingConsumer);
+    Impl.CacheIsValid = true;
+  }
+
+  for (auto VD : Impl.CachedVisibleDecls)
+    Consumer.foundDecl(VD, DeclVisibilityKind::VisibleAtTopLevel);
+}
+
+void ClangModuleUnit::lookupVisibleDecls(Module::AccessPathTy AccessPath,
+                                         VisibleDeclConsumer &Consumer,
+                                         NLKind LookupKind) const {
+  FilteringVisibleDeclConsumer filterConsumer(Consumer, this);
+  owner.lookupVisibleDecls(filterConsumer);
 }
 
-void ClangImporter::lookupVisibleDecls(const Module *M,
-                                       Module::AccessPathTy AccessPath,
-                                       VisibleDeclConsumer &Consumer,
-                                       NLKind LookupKind) {
-  ImportingVisibleDeclConsumer ImportingConsumer(
-      const_cast<ClangImporter &>(*this), Impl, Consumer);
-  ImportingConsumer.filterByModule(cast<ClangModule>(M));
-  lookupVisibleDecls(ImportingConsumer);
+void ClangModuleUnit::lookupValue(Module::AccessPathTy accessPath,
+                                  Identifier name, NLKind lookupKind,
+                                  SmallVectorImpl<ValueDecl*> &results) const {
+  assert(accessPath.size() <= 1 && "can only refer to top-level decls");
+  if (accessPath.size() == 1 && accessPath.front().first != name)
+    return;
+
+  VectorDeclConsumer vectorWriter(results);
+  FilteringVisibleDeclConsumer filteringConsumer(vectorWriter, this);
+  owner.lookupValue(name, filteringConsumer);
 }
 
 void ClangImporter::loadExtensions(NominalTypeDecl *nominal,
@@ -602,30 +667,28 @@ void ClangImporter::loadExtensions(NominalTypeDecl *nominal,
   }
 }
 
-void ClangImporter::getImportedModules(
-    const Module *module,
+void ClangModuleUnit::getImportedModules(
     SmallVectorImpl<Module::ImportedModule> &imports,
-    bool includePrivate) {
+    bool includePrivate) const {
 
-  auto underlying = cast<ClangModule>(module)->clangModule;
-  auto topLevelAdapter = cast<ClangModule>(module)->getAdapterModule();
+  auto topLevelAdapter = getAdapterModule();
 
   SmallVector<clang::Module *, 8> imported;
   if (includePrivate) {
-    imported.append(underlying->Imports.begin(), underlying->Imports.end());
+    imported.append(clangModule->Imports.begin(), clangModule->Imports.end());
     // FIXME: The parent module isn't exactly a private import, but it is
     // needed for link dependencies.
-    if (underlying->Parent)
-      imported.push_back(underlying->Parent);
+    if (clangModule->Parent)
+      imported.push_back(clangModule->Parent);
   } else
-    underlying->getExportedModules(imported);
+    clangModule->getExportedModules(imported);
 
   for (auto importMod : imported) {
-    auto wrapper = Impl.getWrapperModule(*this, importMod);
+    auto wrapper = owner.Impl.getWrapperForModule(owner, importMod);
 
     auto actualMod = wrapper->getAdapterModule();
     if (!actualMod || actualMod == topLevelAdapter)
-      actualMod = wrapper;
+      actualMod = wrapper->getParentModule();
 
     imports.push_back({Module::AccessPathTy(), actualMod});
   }
@@ -636,24 +699,10 @@ static bool selectorStartsWithName(clang::Selector sel, Identifier name) {
   return sel.getNameForSlot(0) == name.str();
 }
 
-namespace {
-  /// Inserts found decls into an externally-owned SmallVector.
-  class VectorDeclConsumer : public VisibleDeclConsumer {
-  public:
-    SmallVectorImpl<ValueDecl *> &results;
-    explicit VectorDeclConsumer(SmallVectorImpl<ValueDecl *> &decls)
-      : results(decls) {}
-
-    void foundDecl(ValueDecl *VD, DeclVisibilityKind Reason) override {
-      results.push_back(VD);
-    }
-  };
-}
-
 static void lookupClassMembersImpl(ClangImporter::Implementation &Impl,
                                    VisibleDeclConsumer &consumer,
                                    Identifier name = Identifier()) {
-  clang::Sema &S = Impl.Instance->getSema();
+  clang::Sema &S = Impl.getClangSema();
   clang::ExternalASTSource *source = S.getExternalSource();
 
   // When looking for a subscript, we actually look for the getters
@@ -778,27 +827,24 @@ static void lookupClassMembersImpl(ClangImporter::Implementation &Impl,
   }
 }
 
-void ClangImporter::lookupClassMember(const Module *module,
-                                      Module::AccessPathTy accessPath,
-                                      Identifier name,
-                                      SmallVectorImpl<ValueDecl*> &results) {
+void
+ClangModuleUnit::lookupClassMember(Module::AccessPathTy accessPath,
+                                   Identifier name,
+                                   SmallVectorImpl<ValueDecl*> &results) const {
   // FIXME: Not limited by module.
   VectorDeclConsumer consumer(results);
-  lookupClassMembersImpl(Impl, consumer, name);
+  lookupClassMembersImpl(owner.Impl, consumer, name);
 }
 
-void ClangImporter::lookupClassMembers(const Module *module,
-                                       Module::AccessPathTy accessPath,
-                                       VisibleDeclConsumer &consumer) {
+void ClangModuleUnit::lookupClassMembers(Module::AccessPathTy accessPath,
+                                         VisibleDeclConsumer &consumer) const {
   // FIXME: Not limited by module.
-  lookupClassMembersImpl(Impl, consumer);
+  lookupClassMembersImpl(owner.Impl, consumer);
 }
 
-void
-ClangImporter::collectLinkLibraries(const Module *module,
-                                    Module::LinkLibraryCallback callback) {
-  auto underlying = cast<ClangModule>(module)->clangModule;
-  for (auto clangLinkLib : underlying->LinkLibraries) {
+void ClangModuleUnit::collectLinkLibraries(
+    Module::LinkLibraryCallback callback) const {
+  for (auto clangLinkLib : clangModule->LinkLibraries) {
     LibraryKind kind;
     if (clangLinkLib.IsFramework)
       kind = LibraryKind::Framework;
@@ -809,9 +855,8 @@ ClangImporter::collectLinkLibraries(const Module *module,
   }
 }
 
-void ClangImporter::getTopLevelDecls(const Module *module,
-                                     SmallVectorImpl<Decl*> &results) {
-  clang::ASTContext &clangCtx = Impl.Instance->getASTContext();
+void ClangModuleUnit::getTopLevelDecls(SmallVectorImpl<Decl*> &results) const {
+  clang::ASTContext &clangCtx = owner.Impl.getClangASTContext();
   const clang::TranslationUnitDecl *clangTU = clangCtx.getTranslationUnitDecl();
   
   // FIXME: Do we want a noload variant here?
@@ -820,21 +865,17 @@ void ClangImporter::getTopLevelDecls(const Module *module,
     auto named = dyn_cast<clang::NamedDecl>(D);
     if (!named)
        continue;
-    if (Impl.getClangModuleForDecl(D) != module)
+    // FIXME: We shouldn't have to jump up to the top-level module here.
+    clang::Module *owningModule = getBestOwningModule(D);
+    if (!owningModule || owningModule->getTopLevelModule() != clangModule)
       continue;
-    if (auto imported = Impl.importDecl(named))
+    if (auto imported = owner.Impl.importDecl(named))
       results.push_back(imported);
   }
 }
 
-void ClangImporter::getDisplayDecls(const Module *module,
-                                    SmallVectorImpl<Decl*> &results) {
-  return getTopLevelDecls(module, results);
-}
-
-StringRef ClangImporter::getModuleFilename(const Module *Module) {
-  auto Underlying = cast<ClangModule>(Module)->clangModule;
-  return Underlying->getASTFile()->getName();
+StringRef ClangModuleUnit::getFilename() const {
+  return clangModule->getASTFile()->getName();
 }
 
 clang::TargetInfo &ClangImporter::getTargetInfo() const {
@@ -853,49 +894,48 @@ void ClangImporter::verifyAllModules() {
     if (Decl *D = I.second)
       verify(D);
   }
-  Impl.ImportCounter = Impl.VerifiedImportCounter;
+  Impl.VerifiedImportCounter = Impl.ImportCounter;
 }
 
 //===----------------------------------------------------------------------===//
 // ClangModule Implementation
 //===----------------------------------------------------------------------===//
 
-ClangModule::ClangModule(ASTContext &ctx, StringRef DebugModuleName,
-                         ModuleLoader &owner, clang::Module *clangModule)
-  : LoadedModule(ModuleKind::Clang, ctx.getIdentifier(clangModule->Name),
-                 DebugModuleName, ctx, owner), clangModule(clangModule) {
+ClangModuleUnit::ClangModuleUnit(TranslationUnit &TU,
+                                 ClangImporter &owner,
+                                 clang::Module *clangModule)
+  : LoadedFile(FileUnitKind::ClangModule, TU), owner(owner),
+    clangModule(clangModule) {
 }
 
-bool ClangModule::isTopLevel() const {
+bool ClangModuleUnit::isTopLevel() const {
   return !clangModule->isSubModule();
 }
 
-StringRef ClangModule::getTopLevelModuleName() const {
-  return clangModule->getTopLevelModuleName();
-}
-
-Module *ClangModule::getAdapterModule() const {
+Module *ClangModuleUnit::getAdapterModule() const {
   if (!isTopLevel()) {
     // FIXME: Is this correct for submodules?
-    auto &importer = static_cast<ClangImporter&>(getOwner());
     auto topLevel = clangModule->getTopLevelModule();
-    auto wrapper = importer.Impl.getWrapperModule(importer, topLevel);
+    auto wrapper = owner.Impl.getWrapperForModule(owner, topLevel);
     return wrapper->getAdapterModule();
+
   }
 
   if (!adapterModule.getInt()) {
     // FIXME: Include proper source location.
-    auto adapter = Ctx.getModule(Module::AccessPathTy({Name, SourceLoc()}));
-    if (isa<ClangModule>(adapter)) {
+    Module *M = getParentModule();
+    ASTContext &Ctx = M->Ctx;
+    auto adapter = Ctx.getModule(Module::AccessPathTy({M->Name, SourceLoc()}));
+    if (adapter == M) {
       adapter = nullptr;
     } else {
-      auto &sharedModuleRef = Ctx.LoadedModules[Name.str()];
+      auto &sharedModuleRef = Ctx.LoadedModules[M->Name.str()];
       assert(!sharedModuleRef || sharedModuleRef == adapter ||
-             sharedModuleRef == this);
+             sharedModuleRef == M);
       sharedModuleRef = adapter;
     }
 
-    auto mutableThis = const_cast<ClangModule *>(this);
+    auto mutableThis = const_cast<ClangModuleUnit *>(this);
     mutableThis->adapterModule.setPointerAndInt(adapter, true);
   }
 
diff --git a/lib/ClangImporter/ImportDecl.cpp b/lib/ClangImporter/ImportDecl.cpp
index c343f72a299..1bdd25372c7 100644
--- a/lib/ClangImporter/ImportDecl.cpp
+++ b/lib/ClangImporter/ImportDecl.cpp
@@ -1936,9 +1936,8 @@ namespace {
       // reasons. Fix it.
       auto containerTy = dc->getDeclaredTypeInContext();
       SmallVector<ValueDecl *, 2> lookup;
-      Impl.firstClangModule->lookupQualified(containerTy, name,
-                                             NL_QualifiedDefault, nullptr,
-                                             lookup);
+      dc->lookupQualified(containerTy, name, NL_QualifiedDefault, nullptr,
+                          lookup);
       Type unlabeledIndices;
       for (auto result : lookup) {
         auto parentSub = dyn_cast<SubscriptDecl>(result);
@@ -2406,9 +2405,8 @@ namespace {
       auto containerTy = dc->getDeclaredTypeInContext();
       VarDecl *overridden = nullptr;
       SmallVector<ValueDecl *, 2> lookup;
-      Impl.firstClangModule->lookupQualified(containerTy, name,
-                                             NL_QualifiedDefault, nullptr,
-                                             lookup);
+      dc->lookupQualified(containerTy, name, NL_QualifiedDefault, nullptr,
+                          lookup);
       for (auto result : lookup) {
         if (isa<FuncDecl>(result))
           return nullptr;
diff --git a/lib/ClangImporter/ImportMacro.cpp b/lib/ClangImporter/ImportMacro.cpp
index 9cb4b67837e..ce8a26f05df 100644
--- a/lib/ClangImporter/ImportMacro.cpp
+++ b/lib/ClangImporter/ImportMacro.cpp
@@ -35,12 +35,13 @@ static ValueDecl *importNumericLiteral(ClangImporter::Implementation &Impl,
                                        Identifier name,
                                        clang::Token const *signTok,
                                        clang::Token const &tok) {
+  // FIXME: This constant should live in the correct module for the macro.
   DeclContext *dc = Impl.firstClangModule;
   
   assert(tok.getKind() == clang::tok::numeric_constant &&
          "not a numeric token");
   clang::ActionResult<clang::Expr*> result =
-    Impl.Instance->getSema().ActOnNumericConstant(tok);
+    Impl.getClangSema().ActOnNumericConstant(tok);
   if (result.isUsable()) {
     clang::Expr *parsed = result.get();
     if (auto *integer = dyn_cast<clang::IntegerLiteral>(parsed)) {
@@ -124,7 +125,7 @@ static ValueDecl *importMacro(ClangImporter::Implementation &impl,
       auto clangID = tok.getIdentifierInfo();
       // If it's an identifier that is itself a macro, look into that macro.
       if (clangID->hasMacroDefinition()) {
-        auto macroID = impl.Instance->getPreprocessor().getMacroInfo(clangID);
+        auto macroID = impl.getClangPreprocessor().getMacroInfo(clangID);
         return impl.importMacro(name, macroID);
       }
 
diff --git a/lib/ClangImporter/ImporterImpl.h b/lib/ClangImporter/ImporterImpl.h
index 9bf9fca15d0..6dc4836fa26 100644
--- a/lib/ClangImporter/ImporterImpl.h
+++ b/lib/ClangImporter/ImporterImpl.h
@@ -43,7 +43,7 @@ class QualType;
 namespace swift {
 
 class ASTContext;
-class ClangModule;
+class ClangModuleUnit;
 class ClassDecl;
 class ExtensionDecl;
 class FuncDecl;
@@ -113,7 +113,10 @@ enum class SpecialMethodKind {
 };
 
 /// \brief Implementation of the Clang importer.
-struct ClangImporter::Implementation {
+class ClangImporter::Implementation {
+  friend class ClangImporter;
+
+public:
   /// \brief Describes how a particular C enumeration type will be imported
   /// into Swift. All of the possibilities have the same storage
   /// representation, but can be used in different ways.
@@ -138,6 +141,7 @@ struct ClangImporter::Implementation {
   /// \brief Swift AST context.
   ASTContext &SwiftContext;
 
+private:
   /// \brief A count of the number of load module operations.
   /// FIXME: Horrible, horrible hack for \c loadModule().
   unsigned ImportCounter = 0;
@@ -157,6 +161,7 @@ struct ClangImporter::Implementation {
   /// parser.
   std::unique_ptr<clang::SyntaxOnlyAction> Action;
 
+public:
   /// \brief Mapping of already-imported declarations.
   llvm::DenseMap<const clang::Decl *, Decl *> ImportedDecls;
 
@@ -179,6 +184,12 @@ struct ClangImporter::Implementation {
   /// \brief Mapping of already-imported macros.
   llvm::DenseMap<clang::MacroInfo *, ValueDecl *> ImportedMacros;
 
+  // FIXME: An extra level of caching of visible decls, since lookup needs to
+  // be filtered by module after the fact.
+  SmallVector<ValueDecl *, 0> CachedVisibleDecls;
+  bool CacheIsValid = false;
+
+private:
   /// \brief Generation number that is used for crude versioning.
   ///
   /// This value is incremented every time a new module is imported.
@@ -220,6 +231,7 @@ struct ClangImporter::Implementation {
   /// \brief Cache of the class extensions.
   llvm::DenseMap<ClassDecl *, CachedExtensions> ClassExtensions;
 
+public:
   /// \brief Keep track of subscript declarations based on getter/setter
   /// pairs.
   llvm::DenseMap<std::pair<FuncDecl *, FuncDecl *>, SubscriptDecl *> Subscripts;
@@ -230,14 +242,15 @@ struct ClangImporter::Implementation {
   /// \brief Keep track of enum constant values that have been imported.
   std::set<std::pair<const clang::EnumDecl *, llvm::APSInt>>
     EnumConstantValues;
-  
+
 private:
   /// \brief NSObject, imported into Swift.
   Type NSObjectTy;
 
-  /// A pair containing a ClangModule and whether the adapters of its
-  /// re-exported modules have all been forced to load already.
-  using ModuleInitPair = llvm::PointerIntPair<ClangModule *, 1, bool>;
+  /// A pair containing a ClangModuleUnit,
+  /// and whether the adapters of its re-exported modules have all been forced
+  /// to load already.
+  using ModuleInitPair = llvm::PointerIntPair<ClangModuleUnit *, 1, bool>;
 
 public:
   /// A map from Clang modules to their Swift wrapper modules.
@@ -249,7 +262,7 @@ public:
   /// map from a Decl in the tree back to the appropriate Clang module.
   /// It also means building ClangModules for all of the dependencies of a
   /// Clang module.
-  ClangModule *firstClangModule = nullptr;
+  ClangModuleUnit *firstClangModule = nullptr;
 
   /// \brief Clang's objectAtIndexedSubscript: selector.
   clang::Selector objectAtIndexedSubscript;
@@ -272,9 +285,19 @@ public:
     return Instance->getASTContext();
   }
 
-  /// \brief Retrieve the imported ClangModule that should contain the given
+  /// \brief Retrieve the Clang Sema object.
+  clang::Sema &getClangSema() const {
+    return Instance->getSema();
+  }
+
+  /// \brief Retrieve the Clang AST context.
+  clang::Preprocessor &getClangPreprocessor() const {
+    return Instance->getPreprocessor();
+  }
+
+  /// \brief Retrieve the imported module that should contain the given
   /// Clang decl.
-  ClangModule *getClangModuleForDecl(const clang::Decl *D);
+  ClangModuleUnit *getClangModuleForDecl(const clang::Decl *D);
 
   /// \brief Import the given Swift identifier into Clang.
   clang::DeclarationName importName(Identifier name);
@@ -369,8 +392,8 @@ public:
 
   /// \brief Retrieves the Swift wrapper for the given Clang module, creating
   /// it if necessary.
-  ClangModule *getWrapperModule(ClangImporter &importer,
-                                clang::Module *underlying);
+  ClangModuleUnit *getWrapperForModule(ClangImporter &importer,
+                                       clang::Module *underlying);
 
   /// \brief Retrieve the named Swift type, e.g., Int32.
   ///
diff --git a/lib/IRGen/GenDecl.cpp b/lib/IRGen/GenDecl.cpp
index 7abd20e6ad6..111e651ec61 100644
--- a/lib/IRGen/GenDecl.cpp
+++ b/lib/IRGen/GenDecl.cpp
@@ -640,7 +640,7 @@ bool LinkEntity::isThunk() const {
 
   if (isDeclKind(getKind())) {
     ValueDecl *D = static_cast<ValueDecl *>(Pointer);
-    if (!isa<ClangModule>(D->getDeclContext()->getParentModule()))
+    if (!isa<ClangModuleUnit>(D->getDeclContext()->getModuleScopeContext()))
       return false;
     
     // Nominal type descriptors for Clang-imported types are always given
@@ -659,7 +659,8 @@ bool LinkEntity::isThunk() const {
     if (!decl)
       return false;
 
-    return isa<ClangModule>(decl->getDeclContext()->getParentModule());
+    const DeclContext *DC = decl->getDeclContext();
+    return isa<ClangModuleUnit>(DC->getModuleScopeContext());
   }
 }
 
diff --git a/lib/SILGen/SILGen.cpp b/lib/SILGen/SILGen.cpp
index a9a70f29b5c..10a66d4cc2a 100644
--- a/lib/SILGen/SILGen.cpp
+++ b/lib/SILGen/SILGen.cpp
@@ -220,7 +220,7 @@ SILLinkage SILGenModule::getConstantLinkage(SILDeclRef constant) {
   // Function-local declarations always have internal linkage.
   ValueDecl *d = constant.getDecl();
   DeclContext *dc = d->getDeclContext();
-  while (!dc->isModuleContext()) {
+  while (!dc->isModuleScopeContext()) {
     if (dc->isLocalContext())
       return SILLinkage::Internal;
     dc = dc->getParent();
@@ -232,7 +232,7 @@ SILLinkage SILGenModule::getConstantLinkage(SILDeclRef constant) {
   
   // Declarations imported from Clang modules have thunk linkage.
   // FIXME: They shouldn't.
-  if(isa<ClangModule>(dc) &&
+  if(isa<ClangModuleUnit>(dc) &&
      (isa<ConstructorDecl>(d) ||
       isa<SubscriptDecl>(d) ||
       isa<EnumElementDecl>(d) ||
diff --git a/lib/Serialization/ModuleFile.h b/lib/Serialization/ModuleFile.h
index 3e9fff33bec..b753867e805 100644
--- a/lib/Serialization/ModuleFile.h
+++ b/lib/Serialization/ModuleFile.h
@@ -396,7 +396,7 @@ public:
   ModuleFile &File;
 
   SerializedASTFile(TranslationUnit &TU, ModuleFile &file)
-    : LoadedFile(FileUnitKind::Loaded, TU), File(file) {}
+    : LoadedFile(FileUnitKind::SerializedAST, TU), File(file) {}
 
   virtual void lookupValue(Module::AccessPathTy accessPath,
                            Identifier name, NLKind lookupKind,
@@ -426,6 +426,13 @@ public:
   collectLinkLibraries(Module::LinkLibraryCallback callback) const override;
 
   virtual StringRef getFilename() const override;
+
+  static bool classof(const FileUnit *file) {
+    return file->getKind() == FileUnitKind::SerializedAST;
+  }
+  static bool classof(const DeclContext *DC) {
+    return isa<FileUnit>(DC) && classof(cast<FileUnit>(DC));
+  }
 };
 
 } // end namespace swift
diff --git a/test/ClangModules/Inputs/usr/include/blocks.h b/test/ClangModules/Inputs/usr/include/blocks.h
index bf797d74023..d22515f8fc8 100644
--- a/test/ClangModules/Inputs/usr/include/blocks.h
+++ b/test/ClangModules/Inputs/usr/include/blocks.h
@@ -1,11 +1,11 @@
-@import ObjectiveC;
+@import Foundation;
 
 typedef struct dispatch_queue_t {} dispatch_queue_t;
 
 dispatch_queue_t dispatch_get_current_queue(void);
 void dispatch_async(dispatch_queue_t q, void (^f)(void));
 
-@interface NSString : NSObject
+@interface NSString ()
 
 - (void)enumerateLinesUsingBlock:(void (^)(NSString *line)) f;
 // FIXME: The importer drops this.

commit f1c977889ce1be47688c1d8f67bf8dbd781fae9c
Author: Andrew Trick <atrick@apple.com>
Date:   Sat Oct 26 09:03:36 2013 +0000

    Added a ReachingBlockSet and ReachingBlockMatrix for efficiency.
    
    Eventually, we may decide not to compute global reachability, but this
    should be reasonably efficient in the meantime.
    
    Swift SVN r9691

diff --git a/lib/SILPasses/CapturePromotion.cpp b/lib/SILPasses/CapturePromotion.cpp
index a030205b24d..0f3bb6de0f6 100644
--- a/lib/SILPasses/CapturePromotion.cpp
+++ b/lib/SILPasses/CapturePromotion.cpp
@@ -11,11 +11,12 @@
 //===----------------------------------------------------------------------===//
 
 #define DEBUG_TYPE "capture-promotion"
-#include "swift/Subsystems.h"
-#include "swift/SIL/SILCloner.h"
 #include "llvm/ADT/BitVector.h"
 #include "llvm/ADT/SmallSet.h"
 #include "llvm/ADT/Statistic.h"
+#include "llvm/Support/Debug.h"
+#include "swift/SIL/SILCloner.h"
+#include "swift/Subsystems.h"
 using namespace swift;
 
 typedef llvm::SmallSet<unsigned, 4> IndicesSet;
@@ -25,15 +26,124 @@ typedef llvm::DenseMap<PartialApplyInst*, IndicesSet> PartialApplyIndicesMap;
 STATISTIC(NumCapturesPromoted, "Number of captures promoted");
 
 namespace {
+/// \brief Transient reference to a block set within ReachabilityInfo.
+///
+/// This is a bitset that conveniently flattens into a matrix allowing bit-wise
+/// operations without masking.
+///
+/// TODO: If this sticks around, maybe we'll make a BitMatrix ADT.
+class ReachingBlockSet {
+public:
+  enum { BITWORD_SIZE = (unsigned)sizeof(uint64_t) * CHAR_BIT };
+
+  static size_t numBitWords(unsigned NumBlocks) {
+    return (NumBlocks + BITWORD_SIZE - 1) / BITWORD_SIZE;
+  }
+
+  /// \brief Transient reference to a reaching block matrix.
+  struct ReachingBlockMatrix {
+    uint64_t *Bits;
+    unsigned NumBitWords; // Words per row.
+
+    ReachingBlockMatrix(): Bits(0), NumBitWords(0) {}
+
+    bool empty() const { return !Bits; }
+  };
+
+  static ReachingBlockMatrix allocateMatrix(unsigned NumBlocks) {
+    ReachingBlockMatrix M;
+    M.NumBitWords = numBitWords(NumBlocks);
+    M.Bits = new uint64_t[NumBlocks * M.NumBitWords];
+    memset(M.Bits, 0, NumBlocks * M.NumBitWords * sizeof(uint64_t));
+    return M;
+  }
+  static void deallocateMatrix(ReachingBlockMatrix &M) {
+    delete [] M.Bits;
+    M.Bits = 0;
+    M.NumBitWords = 0;
+  }
+  static ReachingBlockSet allocateSet(unsigned NumBlocks) {
+    ReachingBlockSet S;
+    S.NumBitWords = numBitWords(NumBlocks);
+    S.Bits = new uint64_t[S.NumBitWords];
+    return S;
+  }
+  static void deallocateSet(ReachingBlockSet &S) {
+    delete [] S.Bits;
+    S.Bits = 0;
+    S.NumBitWords = 0;
+  }
+
+private:
+  uint64_t *Bits;
+  unsigned NumBitWords;
+
+public:
+  ReachingBlockSet(): Bits(0), NumBitWords(0) {}
+
+  ReachingBlockSet(unsigned BlockID, ReachingBlockMatrix &M)
+    : Bits(&M.Bits[BlockID * M.NumBitWords]),
+      NumBitWords(M.NumBitWords) {}
+
+  bool test(unsigned ID) const {
+    assert(ID / BITWORD_SIZE < NumBitWords && "block ID out-of-bounds");
+    return Bits[ID / BITWORD_SIZE] & (1L << (ID % BITWORD_SIZE));
+  }
+
+  void set(unsigned ID) {
+    assert(ID / BITWORD_SIZE < NumBitWords && "block ID out-of-bounds");
+    Bits[ID / BITWORD_SIZE] |= 1L << (ID % BITWORD_SIZE);
+  }
+
+  ReachingBlockSet &operator|=(const ReachingBlockSet &RHS) {
+    for (size_t i = 0, e = NumBitWords; i != e; ++i)
+      Bits[i] |= RHS.Bits[i];
+    return *this;
+  }
+
+  void clear() {
+    memset(Bits, 0, NumBitWords * sizeof(uint64_t));
+  }
+
+  bool operator==(const ReachingBlockSet &RHS) const {
+    assert(NumBitWords == RHS.NumBitWords && "mismatched sets");
+    for (size_t i = 0, e = NumBitWords; i != e; ++i) {
+      if (Bits[i] != RHS.Bits[i])
+        return false;
+    }
+    return true;
+  }
+
+  bool operator!=(const ReachingBlockSet &RHS) const {
+    return !(*this == RHS);
+  }
+
+  const ReachingBlockSet &operator=(const ReachingBlockSet &RHS) {
+    assert(NumBitWords == RHS.NumBitWords && "mismatched sets");
+    for (size_t i = 0, e = NumBitWords; i != e; ++i)
+      Bits[i] = RHS.Bits[i];
+    return *this;
+  }
+};
+
+/// \brief Store the reachability matrix: ToBlock -> FromBlocks.
 class ReachabilityInfo {
+  SILFunction *F;
   llvm::DenseMap<SILBasicBlock*, unsigned> BlockMap;
-  std::vector<llvm::BitVector> BitVectors;
+  ReachingBlockSet::ReachingBlockMatrix Matrix;
 
 public:
-  ReachabilityInfo(SILFunction *F);
+  ReachabilityInfo(SILFunction *f) : F(f) {}
+  ~ReachabilityInfo() { ReachingBlockSet::deallocateMatrix(Matrix); }
+
+  bool isComputed() const { return !Matrix.empty(); }
 
-  bool isReachable(SILBasicBlock *From, SILBasicBlock *To) const;
+  bool isReachable(SILBasicBlock *From, SILBasicBlock *To);
+
+private:
+  void compute();
 };
+
 } // end anonymous namespace.
 
 
@@ -66,74 +176,81 @@ private:
 };
 } // end anonymous namespace.
 
-/// \brief Initialize ReachabilityInfo so that it can answer queries about
+/// \brief Compute ReachabilityInfo so that it can answer queries about
 /// whether a given basic block in a function is reachable from another basic
 /// block in the function.
-ReachabilityInfo::ReachabilityInfo(SILFunction *F) {
-  // FIXME: Is there a better datastructure for this?  Reachability being N^2
-  // in time or space doesn't seem right at all.  If the query is infrequent
-  // enough, this could just be calculated on demand.
-  
-  // FIXME: If not, this would be better stored as a single llvm::Bitvector that
-  // is indexed into with explicit i*N+j indexes.
-  
-  // FIXME: This would be better computed lazily.  There is no reason to compute
-  // this for functions that have no partial_applys in them.
+///
+/// FIXME: Computing global reachability requires initializing an N^2
+/// bitset. This could be avoided by computing reachability on-the-fly
+/// for each alloc_box by walking backward from mutations.
+void ReachabilityInfo::compute() {
+  assert(!isComputed() && "already computed");
+
   unsigned N = 0;
   for (auto &BB : *F)
     BlockMap.insert({ &BB, N++ });
-  BitVectors.resize(N);
-  for (auto &Bits : BitVectors)
-    Bits.resize(N, false);
+  Matrix = ReachingBlockSet::allocateMatrix(N);
+  ReachingBlockSet NewSet = ReachingBlockSet::allocateSet(N);
+
+  DEBUG(llvm::dbgs() << "Computing Reachability for " << F->getName()
+        << " with " << N << " blocks.\n");
 
-  llvm::BitVector NewBits;
+  // Iterate to a fix point, two times for a topological DAG.
   bool Changed;
   do {
     Changed = false;
-    NewBits.clear();
-    for (auto &BlockPair : BlockMap) {
-      llvm::BitVector &Bits = BitVectors[BlockPair.second];
+
+    // Visit all blocks in a predictable order, hopefully close to topological.
+    for (auto &BB : *F) {
+      ReachingBlockSet CurSet(BlockMap[&BB], Matrix);
       if (!Changed) {
         // If we have not detected a change yet, then calculate new
         // reachabilities into a new bit vector so we can determine if any
         // change has occured.
-        
-        // FIXME: Shadowing "NewBits" here is suboptimal.  It should get a new
-        // name or actually reuse the bitvector.  If it remains, the bitvector
-        // should be hoisted out of the loop and cleared, to avoid redundant
-        // memory allocation each time through the loop.
-        llvm::BitVector NewBits = Bits;
-        for (auto PI = BlockPair.first->pred_begin(),
-                  PE = BlockPair.first->pred_end(); PI != PE; ++PI) {
-          unsigned I = BlockMap[*PI];
-          NewBits |= BitVectors[I];
-          NewBits.set(I);
+        NewSet = CurSet;
+        for (auto PI = BB.pred_begin(), PE = BB.pred_end(); PI != PE; ++PI) {
+          unsigned PredID = BlockMap[*PI];
+          ReachingBlockSet PredSet(PredID, Matrix);
+          NewSet |= PredSet;
+          NewSet.set(PredID);
         }
-        if (Bits != NewBits) {
-          Bits.swap(NewBits);
+        if (NewSet != CurSet) {
+          CurSet = NewSet;
           Changed = true;
         }
       } else {
         // Otherwise, just update the existing reachabilities in-place.
-        for (auto PI = BlockPair.first->pred_begin(),
-                  PE = BlockPair.first->pred_end(); PI != PE; ++PI) {
-          unsigned I = BlockMap[*PI];
-          Bits |= BitVectors[I];
-          Bits.set(I);
+        for (auto PI = BB.pred_begin(), PE = BB.pred_end(); PI != PE; ++PI) {
+          unsigned PredID = BlockMap[*PI];
+          ReachingBlockSet PredSet(PredID, Matrix);
+          CurSet |= PredSet;
+          CurSet.set(PredID);
         }
       }
+      DEBUG(llvm::dbgs() << "  Block " << BlockMap[&BB] << " reached by ";
+            for (unsigned i = 0; i < N; ++i) {
+              if (CurSet.test(i))
+                llvm::dbgs() << i << " ";
+            }
+            llvm::dbgs() << "\n");
     }
   } while (Changed);
+
+  ReachingBlockSet::deallocateSet(NewSet);
 }
 
 /// \brief Return true if the To basic block is reachable from the From basic
 /// block. A block is considered reachable from itself only if its entry can be
 /// recursively reached from its own exit.
 bool
-ReachabilityInfo::isReachable(SILBasicBlock *From, SILBasicBlock *To) const {
+ReachabilityInfo::isReachable(SILBasicBlock *From, SILBasicBlock *To) {
+  if (!isComputed())
+    compute();
+
   auto FI = BlockMap.find(From), TI = BlockMap.find(To);
   assert(FI != BlockMap.end() && TI != BlockMap.end());
-  return BitVectors[TI->second].test(FI->second);
+  ReachingBlockSet FromSet(TI->second, Matrix);
+  return FromSet.test(FI->second);
 }
 
 ClosureCloner::ClosureCloner(SILFunction *Orig, IndicesSet &PromotableIndices)
@@ -187,7 +304,7 @@ ClosureCloner::initCloned(SILFunction *Orig, IndicesSet &PromotableIndices) {
     FunctionType::get(TupleType::get(ClonedArgTys, M.getASTContext()),
                       OrigLoweredTy.getAs<AnyFunctionType>().getResult(),
                       M.getASTContext()));
-  
+
   // FIXME: This should insert the cloned function right before the existing
   // SILFunction for the closure, so that these naturally clump together and
   // so that testcases are easier to write.  This can be done by creating the
@@ -536,7 +653,7 @@ eraseRetainIfPresent(PartialApplyInst *PAI, SILValue BoxValue) {
       }
       continue;
     }
-    
+
     // FIXME: Can this be (I->getMemoryBehavior() == MayHaveSideEffects)?
     if (!isa<FunctionRefInst>(I) && !isa<LoadInst>(I) &&
         !isa<CopyValueInst>(I))
@@ -621,7 +738,7 @@ processPartialApplyInst(PartialApplyInst *PAI, IndicesSet &PromotableIndices,
   if (auto *OrigCalleeInst = dyn_cast<SILInstruction>(OrigCallee.getDef()))
     if (OrigCalleeInst->use_empty()) {
       OrigCalleeInst->eraseFromParent();
-     
+
       // TODO: If this is the last use of the closure, and if it has internal
       // linkage, we should remove it from the SILModule now.
     }
@@ -661,7 +778,7 @@ void
 swift::performSILCapturePromotion(SILModule *M) {
   SmallVector<SILFunction*, 128> Worklist;
   for (auto &F : *M)
-    Worklist.push_back(&F);
+    runOnFunction(&F, Worklist);
   while (!Worklist.empty())
     runOnFunction(Worklist.pop_back_val(), Worklist);
 }

commit f328cff0c6fb0832ec763c0d26dba8a2a1d381bd
Author: Chris Lattner <clattner@apple.com>
Date:   Fri Aug 24 21:02:46 2012 +0000

    compute a whole-function numbering for ssa values, instead of a block
    local number.  Cache them for efficiency instead of recomputing.
    
    
    Swift SVN r2759

diff --git a/lib/CFG/CFGPrinter.cpp b/lib/CFG/CFGPrinter.cpp
index 1b869763bb5..ce5195249fc 100644
--- a/lib/CFG/CFGPrinter.cpp
+++ b/lib/CFG/CFGPrinter.cpp
@@ -25,15 +25,15 @@ using namespace swift;
 
 struct ID {
   enum {
-    BasicBlock, LocalVal
+    BasicBlock, SSAValue
   } Kind;
   unsigned Number;
 };
 
 raw_ostream &operator<<(raw_ostream &OS, ID i) {
   switch (i.Kind) {
-  case ID::BasicBlock: OS << "b"; break;
-  case ID::LocalVal: OS << '%'; break;
+  case ID::BasicBlock: OS << "bb"; break;
+  case ID::SSAValue: OS << '%'; break;
   }
   return OS << i.Number;
 }
@@ -45,12 +45,13 @@ namespace {
 class CFGPrinter : public CFGVisitor<CFGPrinter> {
   raw_ostream &OS;
 
-  llvm::DenseMap<const BasicBlock *, unsigned> BlocksToIDsMap;
+  llvm::DenseMap<const BasicBlock *, unsigned> BlocksToIDMap;
   ID getID(const BasicBlock *B);
 
-  raw_ostream &printID(const Instruction *I, bool includeBBPrefix = true);
-  raw_ostream &printID(const BasicBlockArg *BBarg);
-  raw_ostream &printID(const CFGValue &V);
+  llvm::DenseMap<const Instruction*, unsigned> InstructionToIDMap;
+  ID getID(const Instruction *I);
+  ID getID(const BasicBlockArg *BBarg);
+  ID getID(const CFGValue &V);
 
 public:
   CFGPrinter(raw_ostream &OS) : OS(OS) {
@@ -74,8 +75,7 @@ public:
   // Instruction Printing Logic
 
   void print(const Instruction *I) {
-    OS << "  ";
-    printID(I, false) << " = ";
+    OS << "  " << getID(I) << " = ";
     visit(const_cast<Instruction*>(I));
     OS << '\n';
   }
@@ -85,8 +85,7 @@ public:
   }
 
   void visitCallInst(CallInst *CI) {
-    OS << "Call(fn=";
-    printID(CI->function);
+    OS << "Call(fn=" << getID(CI->function);
     auto args = CI->arguments();
     if (!args.empty()) {
       bool first = true;
@@ -96,7 +95,7 @@ public:
           first = false;
         else
           OS << ' ';
-        printID(arg);
+        OS << getID(arg);
       }
       OS << ')';
     }
@@ -111,16 +110,11 @@ public:
     OS << "Integer(val=" << lit << ",width=" << lit.getBitWidth() << ')';
   }
   void visitLoadInst(LoadInst *LI) {
-    OS << "Load(lvalue=";
-    printID(LI->lvalue);
-    OS << ')';
+    OS << "Load(lvalue=" << getID(LI->lvalue) << ')';
   }
   void visitThisApplyInst(ThisApplyInst *TAI) {
-    OS << "ThisApply(fn=";
-    printID(TAI->function);
-    OS << ",arg=";
-    printID(TAI->argument);
-    OS << ')';
+    OS << "ThisApply(fn=" << getID(TAI->function) << ",arg="
+       << getID(TAI->argument) << ')';
   }
   void visitTupleInst(TupleInst *TI) {
     OS << "Tuple(";
@@ -130,7 +124,7 @@ public:
         isFirst = false;
       else
         OS << ',';
-      printID(Elem);
+      OS << getID(Elem);
     }
     OS << ')';
   }
@@ -141,9 +135,7 @@ public:
   void visitReturnInst(ReturnInst *RI) {
     OS << "Return";
     if (RI->returnValue) {
-      OS << '(';
-      printID(RI->returnValue);
-      OS << ')';
+      OS << '(' << getID(RI->returnValue) << ')';
     }
   }
 
@@ -170,43 +162,39 @@ public:
 
 ID CFGPrinter::getID(const BasicBlock *Block) {
   // Lazily initialize the Blocks-to-IDs mapping.
-  if (BlocksToIDsMap.empty()) {
+  if (BlocksToIDMap.empty()) {
     unsigned idx = 0;
     for (const BasicBlock &B : *Block->getParent())
-      BlocksToIDsMap[&B] = idx++;
+      BlocksToIDMap[&B] = idx++;
   }
 
-  ID R = { ID::BasicBlock, BlocksToIDsMap[Block] };
+  ID R = { ID::BasicBlock, BlocksToIDMap[Block] };
   return R;
 }
 
-raw_ostream &CFGPrinter::printID(const Instruction *Inst,
-                                 bool includeBBPrefix) {
-  const BasicBlock *Block = Inst->getParent();
-  unsigned count = 1;
-  for (const Instruction &I : *Block) {
-    if (&I == Inst)
-      break;
-    ++count;
-  }
-  OS << '%';
-  if (includeBBPrefix)
-    OS << getID(Block) << '.';
-  OS << "i" << count;
-  return OS;
+ID CFGPrinter::getID(const Instruction *Inst) {
+  // Lazily initialize the instruction -> ID mapping.
+  if (InstructionToIDMap.empty()) {
+    unsigned idx = 0;
+    for (auto &BB : *Inst->getParent()->getParent())
+      for (auto &I : BB)
+        InstructionToIDMap[&I] = idx++;
+  }
+
+  ID R = { ID::SSAValue, InstructionToIDMap[Inst] };
+  return R;
 }
 
-raw_ostream &CFGPrinter::printID(const BasicBlockArg *BBArg) {
-  OS << "BBArg (unsupported)\n";
-  return OS;
+ID CFGPrinter::getID(const BasicBlockArg *BBArg) {
+  // FIXME: Not implemented yet.
+  ID R = { ID::SSAValue, ~0U };
+  return R;
 }
 
-raw_ostream &CFGPrinter::printID(const CFGValue &Val) {
+ID CFGPrinter::getID(const CFGValue &Val) {
   if (const Instruction *Inst = Val.dyn_cast<Instruction*>())
-    printID(Inst);
-  else
-    printID(Val.get<BasicBlockArg*>());
-  return OS;
+    return getID(Inst);
+  return getID(Val.get<BasicBlockArg*>());
 }
 
 //===----------------------------------------------------------------------===//

commit e7b9ae47fa272f08c3331d3083977bbaa58cc056
Author: John McCall <rjmccall@apple.com>
Date:   Thu Aug 23 05:21:31 2012 +0000

    Defer to the value witness when moving an archetype.  This
    is really a deficiency in TypeInfo::initializeWithTake, which
    is now virtual and not implemented in TypeInfo anymore.  This
    fixes rdar://problem/12153619.
    
    While I'm at it, fix an inefficiency in how we were handling
    ignored results of generic calls, and add 4 new builtins:
      Builtin.strideof is like sizeof, but guarantees that it
      returns a multiple of the alignment (i.e., like C sizeof,
      it is the appropriate allocation size for members of an
      array).
      Builtin.destroy destroys something "in place";  previously
      this was being simulated by moving and ignoring the result.
      Builtin.allocRaw allocates raw, uninitialized memory, given
      a size and alignment.
      Builtin.deallocRaw deallocates a pointer allocated with
      Builtin.allocRaw;  it must be given the allocated size.
    
    Swift SVN r2720

diff --git a/include/swift/AST/Builtins.def b/include/swift/AST/Builtins.def
index df5b3b50cbb..54019c5ea87 100644
--- a/include/swift/AST/Builtins.def
+++ b/include/swift/AST/Builtins.def
@@ -110,6 +110,13 @@ BUILTIN_LOAD(Load, "load")
 BUILTIN_LOAD(Move, "move")
 #undef BUILTIN_LOAD
 
+/// Destroy has type (metatype<T>, Builtin.RawPointer) -> ()
+#ifndef BUILTIN_DESTROY
+#define BUILTIN_DESTROY(Id, Name) BUILTIN(Id, Name)
+#endif
+BUILTIN_DESTROY(Destroy, "destroy")
+#undef BUILTIN_DESTROY
+
 /// Assign has type (T, Builtin.RawPointer) -> ()
 #ifndef BUILTIN_ASSIGN
 #define BUILTIN_ASSIGN(Id, Name) BUILTIN(Id, Name)
@@ -131,6 +138,13 @@ BUILTIN_INIT(Init, "init")
 BUILTIN_SIZEOF(Sizeof, "sizeof")
 #undef BUILTIN_SIZEOF
 
+/// Strideof has type (metatype<T>) -> Int64
+#ifndef BUILTIN_STRIDEOF
+#define BUILTIN_STRIDEOF(Id, Name) BUILTIN(Id, Name)
+#endif
+BUILTIN_STRIDEOF(Strideof, "strideof")
+#undef BUILTIN_STRIDEOF
+
 /// Alignof has type (metatype<T>) -> Int64
 #ifndef BUILTIN_ALIGNOF
 #define BUILTIN_ALIGNOF(Id, Name) BUILTIN(Id, Name)
@@ -138,6 +152,20 @@ BUILTIN_SIZEOF(Sizeof, "sizeof")
 BUILTIN_ALIGNOF(Alignof, "alignof")
 #undef BUILTIN_ALIGNOF
 
+/// AllocRaw has type (Int64, Int64) -> Builtin.RawPointer
+#ifndef BUILTIN_ALLOCRAW
+#define BUILTIN_ALLOCRAW(Id, Name) BUILTIN(Id, Name)
+#endif
+BUILTIN_ALLOCRAW(AllocRaw, "allocRaw")
+#undef BUILTIN_ALLOCRAW
+
+/// DeallocRaw has type (Builtin.RawPointer, Int64) -> ()
+#ifndef BUILTIN_DEALLOCRAW
+#define BUILTIN_DEALLOCRAW(Id, Name) BUILTIN(Id, Name)
+#endif
+BUILTIN_DEALLOCRAW(DeallocRaw, "deallocRaw")
+#undef BUILTIN_DEALLOCRAW
+
 /// CastToObjectPointer has type (T) -> Builtin.ObjectPointer.
 /// CastFromObjectPointer has type (Builtin.ObjectPointer) -> T.
 /// BridgeToRawPointer has type (T) -> Builtin.RawPointer.
diff --git a/lib/AST/Builtins.cpp b/lib/AST/Builtins.cpp
index 171ba4209d6..11d287d9e1f 100644
--- a/lib/AST/Builtins.cpp
+++ b/lib/AST/Builtins.cpp
@@ -262,6 +262,28 @@ static ValueDecl *getStoreOperation(ASTContext &Context, Identifier Id) {
                                 Context.TheBuiltinModule);
 }
 
+static ValueDecl *getDestroyOperation(ASTContext &Context, Identifier id) {
+  Type genericTy;
+  GenericParamList *paramList;
+  std::tie(genericTy, paramList) = getGenericParam(Context);
+
+  TupleTypeElt argElts[] = {
+    TupleTypeElt(MetaTypeType::get(genericTy, Context), Identifier()),
+    TupleTypeElt(Context.TheRawPointerType, Identifier())
+  };
+  Type argTy = TupleType::get(argElts, Context);
+  Type fnTy = PolymorphicFunctionType::get(argTy, TupleType::getEmpty(Context),
+                                           paramList, Context);
+  return new (Context) FuncDecl(SourceLoc(), SourceLoc(), id, SourceLoc(),
+                                paramList, fnTy, /*init*/ nullptr,
+                                Context.TheBuiltinModule);
+}
+
+static Type getPointerSizeType(ASTContext &Context) {
+  // FIXME: Size of a pointer here?
+  return BuiltinIntegerType::get(64, Context);
+}
+
 static ValueDecl *getSizeOrAlignOfOperation(ASTContext &Context, Identifier Id) {
   Type GenericTy;
   GenericParamList *ParamList;
@@ -271,15 +293,41 @@ static ValueDecl *getSizeOrAlignOfOperation(ASTContext &Context, Identifier Id)
     TupleTypeElt(MetaTypeType::get(GenericTy, Context), Identifier()),
   };
   Type Arg = TupleType::get(ArgElts, Context);
-  // FIXME: Size of a pointer here?
   Type FnTy = PolymorphicFunctionType::get(Arg,
-                                           BuiltinIntegerType::get(64, Context),
+                                           getPointerSizeType(Context),
                                            ParamList, Context);
   return new (Context) FuncDecl(SourceLoc(), SourceLoc(), Id, SourceLoc(),
                                 ParamList, FnTy, /*init*/ nullptr,
                                 Context.TheBuiltinModule);
 }
 
+static ValueDecl *getAllocOperation(ASTContext &Context, Identifier id) {
+  Type ptrSizeTy = getPointerSizeType(Context);
+  TupleTypeElt argElts[] = {
+    TupleTypeElt(ptrSizeTy, Identifier()),
+    TupleTypeElt(ptrSizeTy, Identifier())
+  };
+  Type argTy = TupleType::get(argElts, Context);
+  Type fnTy = FunctionType::get(argTy, Context.TheRawPointerType, Context);
+
+  return new (Context) FuncDecl(SourceLoc(), SourceLoc(), id, SourceLoc(),
+                                nullptr, fnTy, /*init*/ nullptr,
+                                Context.TheBuiltinModule);
+}
+
+static ValueDecl *getDeallocOperation(ASTContext &Context, Identifier id) {
+  TupleTypeElt argElts[] = {
+    TupleTypeElt(Context.TheRawPointerType, Identifier()),
+    TupleTypeElt(getPointerSizeType(Context), Identifier())
+  };
+  Type argTy = TupleType::get(argElts, Context);
+  Type fnTy = FunctionType::get(argTy, TupleType::getEmpty(Context), Context);
+
+  return new (Context) FuncDecl(SourceLoc(), SourceLoc(), id, SourceLoc(),
+                                nullptr, fnTy, /*init*/ nullptr,
+                                Context.TheBuiltinModule);
+}
+
 static ValueDecl *getObjectPointerCast(ASTContext &Context, Identifier Id,
                                        BuiltinValueKind BV) {
   Type GenericTy;
@@ -325,8 +373,12 @@ static const OverloadedBuiltinKind OverloadedBuiltinKinds[] = {
 #define BUILTIN_LOAD(id, name) OverloadedBuiltinKind::Special,
 #define BUILTIN_ASSIGN(id, name) OverloadedBuiltinKind::Special,
 #define BUILTIN_INIT(id, name) OverloadedBuiltinKind::Special,
+#define BUILTIN_DESTROY(id, name) OverloadedBuiltinKind::Special,
 #define BUILTIN_SIZEOF(id, name) OverloadedBuiltinKind::Special,
+#define BUILTIN_STRIDEOF(id, name) OverloadedBuiltinKind::Special,
 #define BUILTIN_ALIGNOF(id, name) OverloadedBuiltinKind::Special,
+#define BUILTIN_ALLOCRAW(id, name) OverloadedBuiltinKind::Special,
+#define BUILTIN_DEALLOCRAW(id, name) OverloadedBuiltinKind::Special,
 #define BUILTIN_CASTOBJECTPOINTER(id, name) OverloadedBuiltinKind::Special,
 #include "swift/AST/Builtins.def"
 };
@@ -498,15 +550,26 @@ ValueDecl *swift::getBuiltinValue(ASTContext &Context, Identifier Id) {
     if (!Types.empty()) return nullptr;
     return getLoadOperation(Context, Id);
 
+  case BuiltinValueKind::Destroy:
+    if (!Types.empty()) return nullptr;
+    return getDestroyOperation(Context, Id);
+
   case BuiltinValueKind::Assign:
   case BuiltinValueKind::Init:
     if (!Types.empty()) return nullptr;
     return getStoreOperation(Context, Id);
 
   case BuiltinValueKind::Sizeof:
+  case BuiltinValueKind::Strideof:
   case BuiltinValueKind::Alignof:
     return getSizeOrAlignOfOperation(Context, Id);
 
+  case BuiltinValueKind::AllocRaw:
+    return getAllocOperation(Context, Id);
+
+  case BuiltinValueKind::DeallocRaw:
+    return getDeallocOperation(Context, Id);
+
   case BuiltinValueKind::CastToObjectPointer:
   case BuiltinValueKind::CastFromObjectPointer:
   case BuiltinValueKind::BridgeToRawPointer:
diff --git a/lib/IRGen/FixedTypeInfo.h b/lib/IRGen/FixedTypeInfo.h
index 932ffc4d119..e69b9774d15 100644
--- a/lib/IRGen/FixedTypeInfo.h
+++ b/lib/IRGen/FixedTypeInfo.h
@@ -37,6 +37,14 @@ public:
                         OnHeap_t onHeap,
                         const llvm::Twine &name) const;
 
+  // We can give these reasonable default implementations.
+
+  void initializeWithTake(IRGenFunction &IGF, Address destAddr,
+                          Address srcAddr) const;
+
+  void initializeWithCopy(IRGenFunction &IGF, Address destAddr,
+                          Address srcAddr) const;
+
   // TODO: move the StorageSize etc. members here.
 };
 
diff --git a/lib/IRGen/GenFunc.cpp b/lib/IRGen/GenFunc.cpp
index f3a30330c8e..1edaea2f56a 100644
--- a/lib/IRGen/GenFunc.cpp
+++ b/lib/IRGen/GenFunc.cpp
@@ -1257,6 +1257,19 @@ static void emitBuiltinCall(IRGenFunction &IGF, FuncDecl *fn,
       return valueTI.load(IGF, addr, out);
     }
   }
+
+  if (BuiltinName == "destroy") {
+    // The type of the operation has to be grabbed from the substitutions.
+    Type valueTy = emission.getSubstitutions()[0].Replacement;
+    const TypeInfo &valueTI = IGF.IGM.getFragileTypeInfo(valueTy);
+
+    llvm::Value *addrValue = args.claimUnmanagedNext();
+    Address addr = getAddressForUnsafePointer(IGF, valueTI, addrValue);
+    valueTI.destroy(IGF, addr);
+
+    emission.setVoidResult();
+    return;
+  }
   
   if (BuiltinName == "assign") {
     // The type of the operation is the type of the first argument of
@@ -1292,7 +1305,9 @@ static void emitBuiltinCall(IRGenFunction &IGF, FuncDecl *fn,
     return valueTI.initialize(IGF, args, addr);
   }
 
-  if (BuiltinName == "sizeof") {
+  // FIXME: 'strideof' guarantees that it returns something that's a
+  // multiple of the alignment.
+  if (BuiltinName == "sizeof" || BuiltinName == "strideof") {
     Type valueTy = emission.getSubstitutions()[0].Replacement;
     const TypeInfo &valueTI = IGF.IGM.getFragileTypeInfo(valueTy);
     emission.setScalarUnmanagedSubstResult(valueTI.getSizeOnly(IGF));
@@ -1306,6 +1321,22 @@ static void emitBuiltinCall(IRGenFunction &IGF, FuncDecl *fn,
     return;
   }
 
+  if (BuiltinName == "allocRaw") {
+    auto size = args.claimUnmanagedNext();
+    auto align = args.claimUnmanagedNext();
+    auto result = IGF.emitAllocRawCall(size, align, "builtin-allocRaw");
+    emission.setScalarUnmanagedSubstResult(result);
+    return;
+  }
+
+  if (BuiltinName == "deallocRaw") {
+    auto pointer = args.claimUnmanagedNext();
+    auto size = args.claimUnmanagedNext();
+    IGF.emitDeallocRawCall(pointer, size);
+    emission.setVoidResult();
+    return;
+  }
+
   if (BuiltinName == "castToObjectPointer" ||
       BuiltinName == "castFromObjectPointer" ||
       BuiltinName == "bridgeToRawPointer" ||
@@ -1657,9 +1688,20 @@ void CallEmission::emitToExplosion(Explosion &out) {
     emitToMemory(temp, substResultTI);
     init.markInitialized(IGF, obj);
 
-    substResultTI.loadAsTake(IGF, temp, out);
-    if (cleanup.isValid())
-      IGF.setCleanupState(cleanup, CleanupState::Dead);
+    // If the subst result is passed as an aggregate, don't uselessly
+    // copy the temporary.
+    auto substSchema = substResultTI.getSchema(out.getKind());
+    if (substSchema.isSingleAggregate()) {
+      auto substType = substSchema.begin()->getAggregateType()->getPointerTo();
+      temp = IGF.Builder.CreateBitCast(temp, substType);
+      out.add(ManagedValue(temp.getAddress(), cleanup));
+
+    // Otherwise, we need to load.  Do a take-load and deactivate the cleanup.
+    } else {
+      substResultTI.loadAsTake(IGF, temp, out);
+      if (cleanup.isValid())
+        IGF.setCleanupState(cleanup, CleanupState::Dead);
+    }
     return;
   }
 
diff --git a/lib/IRGen/GenSequential.h b/lib/IRGen/GenSequential.h
index e35462200fd..7ece7505bf0 100644
--- a/lib/IRGen/GenSequential.h
+++ b/lib/IRGen/GenSequential.h
@@ -200,7 +200,7 @@ public:
                           Address src) const {
     // If we're POD, use the generic routine.
     if (isPOD(ResilienceScope::Local))
-      return TypeInfo::initializeWithCopy(IGF, dest, src);
+      return FixedTypeInfo::initializeWithCopy(IGF, dest, src);
 
     for (auto &field : getFields()) {
       if (field.isEmpty()) continue;
diff --git a/lib/IRGen/GenType.cpp b/lib/IRGen/GenType.cpp
index 901623e0a7f..fe18fe68dcd 100644
--- a/lib/IRGen/GenType.cpp
+++ b/lib/IRGen/GenType.cpp
@@ -48,8 +48,9 @@ ExplosionSchema TypeInfo::getSchema(ExplosionKind kind) const {
 /// Copy a value from one object to a new object, directly taking
 /// responsibility for anything it might have.  This is like C++
 /// move-initialization, except the old object will not be destroyed.
-void TypeInfo::initializeWithTake(IRGenFunction &IGF,
-                                  Address destAddr, Address srcAddr) const {
+void FixedTypeInfo::initializeWithTake(IRGenFunction &IGF,
+                                       Address destAddr,
+                                       Address srcAddr) const {
   // Prefer loads and stores if we won't make a million of them.
   // Maybe this should also require the scalars to have a fixed offset.
   ExplosionSchema schema = getSchema(ExplosionKind::Maximal);
@@ -66,8 +67,9 @@ void TypeInfo::initializeWithTake(IRGenFunction &IGF,
 
 /// Copy a value from one object to a new object.  This is just the
 /// default implementation.
-void TypeInfo::initializeWithCopy(IRGenFunction &IGF,
-                                  Address destAddr, Address srcAddr) const {
+void FixedTypeInfo::initializeWithCopy(IRGenFunction &IGF,
+                                       Address destAddr,
+                                       Address srcAddr) const {
   // Use memcpy if that's legal.
   if (isPOD(ResilienceScope::Local)) {
     return initializeWithTake(IGF, destAddr, srcAddr);
diff --git a/lib/IRGen/IRGenModule.cpp b/lib/IRGen/IRGenModule.cpp
index 7776e535f64..6746ff35d78 100644
--- a/lib/IRGen/IRGenModule.cpp
+++ b/lib/IRGen/IRGenModule.cpp
@@ -145,8 +145,9 @@ llvm::Constant *IRGenModule::getSlowAllocFn() {
   if (SlowAllocFn) return SlowAllocFn;
 
   /// void *swift_slowAlloc(size_t size, size_t flags);
+  llvm::Type *argTypes[] = { SizeTy, SizeTy };
   llvm::FunctionType *fnType =
-    llvm::FunctionType::get(Int8PtrTy, SizeTy, false);
+    llvm::FunctionType::get(Int8PtrTy, argTypes, false);
   SlowAllocFn = createRuntimeFunction(*this, "swift_slowAlloc", fnType);
   return SlowAllocFn;
 }
diff --git a/lib/IRGen/TypeInfo.h b/lib/IRGen/TypeInfo.h
index 54d1b56a32d..c689913d6ff 100644
--- a/lib/IRGen/TypeInfo.h
+++ b/lib/IRGen/TypeInfo.h
@@ -160,12 +160,12 @@ public:
   /// Perform a "take-initialization" from the given object.  A
   /// take-initialization is like a C++ move-initialization, except that
   /// the old object is actually no longer permitted to be destroyed.
-  void initializeWithTake(IRGenFunction &IGF, Address destAddr,
-                          Address srcAddr) const;
+  virtual void initializeWithTake(IRGenFunction &IGF, Address destAddr,
+                                  Address srcAddr) const = 0;
 
   /// Perform a copy-initialization from the given object.
   virtual void initializeWithCopy(IRGenFunction &IGF, Address destAddr,
-                                  Address srcAddr) const;
+                                  Address srcAddr) const = 0;
 
   /// Consume a bunch of values which have exploded at one explosion
   /// level and produce them at another.
commit a7d37a90311f0f94500d47c54818eb2351f0a85a
Author: Michael Gottesman <mgottesman@apple.com>
Date:   Sun Feb 16 14:28:38 2020 -0800

    [ownership] When we flag an over consume, improve error msg by dumping consuming user list.
    
    Specifically, there are a few places in the ownership data flow where before
    this patch we just dumped the violating user and the block where the overconsume
    occurred. Since we only dumped the block rather than consuming user list, it
    could require a little bit of time/energy to identify why the block was
    considered to already have a consume in it. So at least by dumping the consuming
    user list, the compiler writer has more information to reason about this.

diff --git a/lib/SIL/LinearLifetimeChecker.cpp b/lib/SIL/LinearLifetimeChecker.cpp
index 2bf81ce7458..c58ee3bce50 100644
--- a/lib/SIL/LinearLifetimeChecker.cpp
+++ b/lib/SIL/LinearLifetimeChecker.cpp
@@ -63,6 +63,12 @@ struct State {
   /// put in missing destroys.
   SmallVectorImpl<SILBasicBlock *> *leakingBlocks;
 
+  /// The list of passed in consuming uses.
+  ArrayRef<BranchPropagatedUser> consumingUses;
+
+  /// The list of passed in non consuming uses.
+  ArrayRef<BranchPropagatedUser> nonConsumingUses;
+
   /// The set of blocks with consuming uses.
   SmallPtrSet<SILBasicBlock *, 8> blocksWithConsumingUses;
 
@@ -80,16 +86,22 @@ struct State {
 
   State(SILValue value, SmallPtrSetImpl<SILBasicBlock *> &visitedBlocks,
         ErrorBehaviorKind errorBehavior,
-        SmallVectorImpl<SILBasicBlock *> *leakingBlocks)
+        SmallVectorImpl<SILBasicBlock *> *leakingBlocks,
+        ArrayRef<BranchPropagatedUser> consumingUses,
+        ArrayRef<BranchPropagatedUser> nonConsumingUses)
       : value(value), beginBlock(value->getParentBlock()), error(errorBehavior),
-        visitedBlocks(visitedBlocks), leakingBlocks(leakingBlocks) {}
+        visitedBlocks(visitedBlocks), leakingBlocks(leakingBlocks),
+        consumingUses(consumingUses), nonConsumingUses(nonConsumingUses) {}
 
   State(SILBasicBlock *beginBlock,
         SmallPtrSetImpl<SILBasicBlock *> &visitedBlocks,
         ErrorBehaviorKind errorBehavior,
-        SmallVectorImpl<SILBasicBlock *> *leakingBlocks)
+        SmallVectorImpl<SILBasicBlock *> *leakingBlocks,
+        ArrayRef<BranchPropagatedUser> consumingUses,
+        ArrayRef<BranchPropagatedUser> nonConsumingUses)
       : value(), beginBlock(beginBlock), error(errorBehavior),
-        visitedBlocks(visitedBlocks), leakingBlocks(leakingBlocks) {}
+        visitedBlocks(visitedBlocks), leakingBlocks(leakingBlocks),
+        consumingUses(consumingUses), nonConsumingUses(nonConsumingUses) {}
 
   void initializeAllNonConsumingUses(
       ArrayRef<BranchPropagatedUser> nonConsumingUsers);
@@ -124,6 +136,22 @@ struct State {
   /// for validity. If this is a linear typed value, return true. Return false
   /// otherwise.
   void checkDataflowEndState(DeadEndBlocks &deBlocks);
+
+  void dumpConsumingUsers() const {
+    llvm::errs() << "Consuming Users:\n";
+    for (auto user : consumingUses) {
+      llvm::errs() << *user.getInst();
+    }
+    llvm::errs() << "\n";
+  }
+
+  void dumpNonConsumingUsers() const {
+    llvm::errs() << "Non Consuming Users:\n";
+    for (auto user : nonConsumingUses) {
+      llvm::errs() << *user.getInst();
+    }
+    llvm::errs() << "\n";
+  }
 };
 
 } // end anonymous namespace
@@ -224,7 +252,8 @@ void State::initializeConsumingUse(BranchPropagatedUser consumingUser,
       llvm::errs() << "Value: N/A\n";
     }
     llvm::errs() << "User: " << *consumingUser << "Block: bb"
-                 << userBlock->getDebugID() << "\n\n";
+                 << userBlock->getDebugID() << "\n";
+    dumpConsumingUsers();
   });
 }
 
@@ -327,7 +356,8 @@ void State::checkPredsForDoubleConsume(BranchPropagatedUser consumingUser,
     }
 
     llvm::errs() << "User: " << *consumingUser << "Block: bb"
-                 << userBlock->getDebugID() << "\n\n";
+                 << userBlock->getDebugID() << "\n";
+    dumpConsumingUsers();
   });
 }
 
@@ -356,7 +386,8 @@ void State::checkPredsForDoubleConsume(SILBasicBlock *userBlock) {
       llvm::errs() << "N/A. \n";
     }
 
-    llvm::errs() << "Block: bb" << userBlock->getDebugID() << "\n\n";
+    llvm::errs() << "Block: bb" << userBlock->getDebugID() << "\n";
+    dumpConsumingUsers();
   });
 }
 
@@ -514,7 +545,8 @@ LinearLifetimeError LinearLifetimeChecker::checkValue(
     SmallVectorImpl<SILBasicBlock *> *leakingBlocks) {
   assert(!consumingUses.empty() && "Must have at least one consuming user?!");
 
-  State state(value, visitedBlocks, errorBehavior, leakingBlocks);
+  State state(value, visitedBlocks, errorBehavior, leakingBlocks, consumingUses,
+              nonConsumingUses);
 
   // First add our non-consuming uses and their blocks to the
   // blocksWithNonConsumingUses map. While we do this, if we have multiple uses

commit caa3dd4d291ec93c1a59f1db62604e703bff8468
Author: Huon Wilson <huon@apple.com>
Date:   Fri May 25 20:13:32 2018 +1000

    [Frontend] Turn symbols-missing-from-TBD validation on by default in debug builds on Apple platforms.
    
    TBD validation is effectively an expensive assertion, and is currently only
    tuned for Apple platforms. However, we don't want it to regress more, and it
    would be nice to start getting validation from people using master
    snapshots. Together, this means that turning it on by default for the cases
    mentioned above is an appropriate course of action.
    
    At the very least, this has the benefit of running validation across the stdlib,
    the overlays and the whole testsuite on each build, so people making changes to
    the compiler that change symbols are hopefully alerted.
    
    One limitation here is that this is only validating that the TBD is a superset
    of the true set of symbols: it could include spurious symbols that aren't
    actually in the binary. This case is less problematic for Swift than symbols
    missing from the TBD file, and so we've focused energy on this. Once we've fixed
    the extra-symbols problems and are confident in it, this validation can be
    upgraded to validate that too.
    
    Half of rdar://problem/40431434.

diff --git a/include/swift/Frontend/FrontendOptions.h b/include/swift/Frontend/FrontendOptions.h
index 468769712bf..e5d6a685b78 100644
--- a/include/swift/Frontend/FrontendOptions.h
+++ b/include/swift/Frontend/FrontendOptions.h
@@ -240,13 +240,14 @@ public:
 
   /// The different modes for validating TBD against the LLVM IR.
   enum class TBDValidationMode {
+    Default,        ///< Do the default validation for the current platform.
     None,           ///< Do no validation.
     MissingFromTBD, ///< Only check for symbols that are in IR but not TBD.
     All, ///< Check for symbols that are in IR but not TBD and TBD but not IR.
   };
 
   /// Compare the symbols in the IR against the TBD file we would generate.
-  TBDValidationMode ValidateTBDAgainstIR = TBDValidationMode::None;
+  TBDValidationMode ValidateTBDAgainstIR = TBDValidationMode::Default;
 
   /// The install_name to use in the TBD file.
   std::string TBDInstallName;
diff --git a/lib/FrontendTool/FrontendTool.cpp b/lib/FrontendTool/FrontendTool.cpp
index 20603e002fb..5415ac6c2b9 100644
--- a/lib/FrontendTool/FrontendTool.cpp
+++ b/lib/FrontendTool/FrontendTool.cpp
@@ -1144,9 +1144,20 @@ static bool validateTBDIfNeeded(CompilerInvocation &Invocation,
     return false;
 
   const auto &frontendOpts = Invocation.getFrontendOptions();
-  const auto mode = frontendOpts.ValidateTBDAgainstIR;
+  auto mode = frontendOpts.ValidateTBDAgainstIR;
   // Ensure all cases are covered by using a switch here.
   switch (mode) {
+  case FrontendOptions::TBDValidationMode::Default:
+#ifndef NDEBUG
+    // When a debug compiler is targeting an apple platform, we do some
+    // validation by default.
+    if (Invocation.getLangOptions().Target.getVendor() == llvm::Triple::Apple) {
+      mode = FrontendOptions::TBDValidationMode::MissingFromTBD;
+      break;
+    }
+#endif
+    // Otherwise, the default is to do nothing.
+    LLVM_FALLTHROUGH;
   case FrontendOptions::TBDValidationMode::None:
     return false;
   case FrontendOptions::TBDValidationMode::All:
diff --git a/stdlib/public/SwiftOnoneSupport/CMakeLists.txt b/stdlib/public/SwiftOnoneSupport/CMakeLists.txt
index 018b02b9eca..0cfd61182a4 100644
--- a/stdlib/public/SwiftOnoneSupport/CMakeLists.txt
+++ b/stdlib/public/SwiftOnoneSupport/CMakeLists.txt
@@ -2,6 +2,10 @@ add_swift_library(swiftSwiftOnoneSupport ${SWIFT_STDLIB_LIBRARY_BUILD_TYPES} IS_
   # This file should be listed the first.  Module name is inferred from the
   # filename.
   SwiftOnoneSupport.swift
-  SWIFT_COMPILE_FLAGS "-parse-stdlib" "-Xllvm" "-sil-inline-generics=false" "${SWIFT_RUNTIME_SWIFT_COMPILE_FLAGS}"
+  # We have to disable validation of TBD files, because this module is
+  # _explicitly_ special-cased to result in extra symbols generated by the
+  # optimizer, meaning TBDGen can't (and shouldn't: it has to run
+  # pre-optimization for performance) list them.
+  SWIFT_COMPILE_FLAGS "-parse-stdlib" "-Xllvm" "-sil-inline-generics=false" "-Xfrontend" "-validate-tbd-against-ir=none" "${SWIFT_RUNTIME_SWIFT_COMPILE_FLAGS}"
   LINK_FLAGS "${SWIFT_RUNTIME_SWIFT_LINK_FLAGS}"
   INSTALL_IN_COMPONENT stdlib)
