commit 8bb77977a92af20d53bcbbeeab916f170a78e519
Author: Dan Moldovan <mdan@google.com>
Date:   Tue Nov 9 08:08:39 2021 -0800

    Clean up placer rules.
    
    This change uses type information to determine whether a node without HostMemory outputs returns (variant) data types known to only exist on the host, such as datasets. Mainly, such nodes include common ops like Identity, control flow, etc, as most other dataset-specific nodes have HostMemory annotations in their kernel registrations.
    
    This method is more robust because we no longer need to match op names by substring checks, and it avoids inefficient DFS searches in this pass.
    
    PiperOrigin-RevId: 408612872
    Change-Id: I5db8a165fc4e429ff0bb39198216c86949103612

commit 1a01f1a2a15dff46f802fa3e37aac1041062a676
Author: Dan Moldovan <mdan@google.com>
Date:   Mon Nov 8 12:23:13 2021 -0800

    Clean up placer rules.
    
    This change uses type information to determine whether a node without HostMemory outputs returns (variant) data types known to only exist on the host, such as datasets. Mainly, such nodes include common ops like Identity, control flow, etc, as most other dataset-specific nodes have HostMemory annotations in their kernel registrations.
    
    This method is more robust because we no longer need to match op names by substring checks, and it avoids inefficient DFS searches in this pass.
    
    PiperOrigin-RevId: 408411228
    Change-Id: I092d0b1187659207d7dc89b5d4560e656b5b00b9

commit 67275fb0568b3669f9fed2d411fd38d970103c08
Author: Jacques Pienaar <jpienaar@google.com>
Date:   Wed Nov 3 10:50:49 2021 -0700

    Add resolve methods for function attributes.
    
    Enable versions that allow for more efficient queries, keep old names for
    forwarding and move towards deprecating them in follow ups.
    
    PiperOrigin-RevId: 407375469
    Change-Id: Id101bc896a35733c69550143c3d67631fc603026

commit 545fd4f34e9106e00104d10b05391cee0e8c9b20
Author: Edward Loper <edloper@google.com>
Date:   Fri Oct 8 07:01:17 2021 -0700

    Add RaggedTensor support to tf.image.resize -- allows resizing a batch of images that have different sizes to all have the same size.  For now, this uses `tf.map_fn`, but if this proves to be too slow/inefficient, then we could look into other solutions.
    
    PiperOrigin-RevId: 401762053
    Change-Id: Ia3e825219f9c2449bb0e8bfab8c1ac48833ac815

commit 6f87e9a2c2275955feae462fd2aec33448215e95
Author: Mao <qq1044467857@gmail.com>
Date:   Fri Oct 8 21:58:52 2021 +0800

    change efficientdet-lite0 --> efficientdet_lite0

commit cf6a88aa42a9490892be6c5e040b7b8cdf3e3ba8
Author: Michael Gester <mgester@google.com>
Date:   Wed Oct 6 10:51:44 2021 -0700

    Rewrite side effect analysis for TF dialect
    
    Some advantages of the new code:
    - support op-based side effects efficiently which saves control dependencies and
      results in better performance; previously they were treated like unknown side
      effects (see new tests which failed before)
    - support value-based side effects for non-resource operands/results
      efficiently; those are not supported by resource alias analysis and are now
      treated like op-based side effects; previously they were treated like unknown
      side effects (see new tests which failed before)
    - simplified/removed many special cases, unified behavior for value-based and
      op-based side effects
    - improved code efficiency and readability
    - added function for querying all resource IDs that an op potentially accesses
    
    PiperOrigin-RevId: 401286244
    Change-Id: I33e782dfd83a1ab7f7876e2e96b90b2b738cd819

commit 6dfeeb33cc4dda0cc965bbf8ce59b9853e0bb20b
Author: Mehdi Amini <aminim@google.com>
Date:   Mon Oct 4 21:14:57 2021 -0700

    Migrate `ConvolutionDimensions`attribute definition from StructAttr to be a first class attribute (NFC)
    
    This makes it more efficient to store, to access, and able to provide custom parsing/verification. The accessor are providing native view (ArrayRef<int64_t>)
    which are much nicer to work with as well.
    
    PiperOrigin-RevId: 400885874
    Change-Id: I8bfa488c25424b620d00a29bf3d4bc7b3bc48dc1

commit ac90b8f890a8cc24aeea2c49b3fbd880ab8003f8
Author: Paul Chiang <paulchiang@google.com>
Date:   Mon Oct 4 12:16:43 2021 -0700

    Hold and reuse the CheckpointReader in _CheckpointRestoreCoordinator
    
    Tiny improvement to actual execution speed, just seems rather inefficient to open the same file twice or more in quick succession.
    
    PiperOrigin-RevId: 400783565
    Change-Id: I86cf5d6b405dad5a315e57a8fa3cd407cb473112

commit f8a17851a9bbafe46fb6af81ec8b2f4e8fa8041a
Author: Geoffrey Martin-Noble <gcmn@google.com>
Date:   Thu Sep 30 17:07:48 2021 -0700

    Lowering of general mhlo.gather to linalg
    
    This is a complete lowering of the gather op and all its weirdnesses.
    Accounting for all these makes the lowering pretty fiddly and also
    means we miss more efficient representations for the common special
    cases. Some of those are already covered by the lowering via
    mhlo.torch_index_select, but this is something that should be
    revisited. In the meantime this gets us a (hopefully) correct lowering
    for the full API.
    
    In addition to the lit tests here, I also spot-checked the output of
    running this through IREE against `jax.lax.gather`.
    
    Fixes https://github.com/tensorflow/mlir-hlo/issues/16
    
    PiperOrigin-RevId: 400063077
    Change-Id: Icec8c50d891206a1a45c588a07a11f6038fb4bd0

commit aac48bdcb8665b3e21098d80a28454d84805c441
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Sep 28 12:49:25 2021 -0700

    Hold and reuse the CheckpointReader in _CheckpointRestoreCoordinator
    
    Tiny improvement to actual execution speed, just seems rather inefficient to open the same file twice or more in quick succession.
    
    PiperOrigin-RevId: 399507429
    Change-Id: I5667546be3ef3402de62ee9f0552b2762a139032

commit d12113312db4099f1d11cec85fbf25aaae6ae695
Author: Paul Chiang <paulchiang@google.com>
Date:   Tue Sep 28 11:42:24 2021 -0700

    Hold and reuse the CheckpointReader in _CheckpointRestoreCoordinator
    
    Tiny improvement to actual execution speed, just seems rather inefficient to open the same file twice or more in quick succession.
    
    PiperOrigin-RevId: 399491540
    Change-Id: I211137bd91d0a338080197f445484265977a9307

commit c1c6d04f358898ddc4d0d69b74822b8fecadbda9
Author: Mehdi Amini <aminim@google.com>
Date:   Thu Sep 23 12:55:11 2021 -0700

    Migrate `DotDimensionNumbers` attribute definition from StructAttr to be a first class attribute (NFC)
    
    This makes it more efficient to store, to access, and able to provide custom parsing/verification. The accessor are providing native view (ArrayRef<int64_t>)
    which are much nicer to work with as well.
    
    PiperOrigin-RevId: 398556364
    Change-Id: I648577d84ab61b8874be53e2e685523eae832267

commit 9789e64f1b9b46c9428a4cb4395df268a97692ea
Author: Mehdi Amini <aminim@google.com>
Date:   Wed Sep 22 19:18:57 2021 -0700

    Migrate `GatherDimensionNumbers` attribute definition from StructAttr to be a first class attribute (NFC)
    
    This makes it more efficient to store, to access, and able to provide custom parsing/verification. The accessor are providing native view (ArrayRef<int64_t>)
    which are much nicer to work with as well.
    
    PiperOrigin-RevId: 398386692
    Change-Id: I84d7d7529d6a88f9faad960296ad0a373a85cf2e

commit 8d855afa6855cca85676e783d8ce84f3addb59d1
Author: Rahul Joshi <jurahul@google.com>
Date:   Wed Sep 22 17:44:39 2021 -0700

    [XLA:GPU][SPMD] Move SPMD partitioning passes to before optimizations
    - Certain transformation and expansions of instructions can prevent optimal sharding
       propagation, so move SPMD partitioning to before optimizations.
    - Run several IR simplification passes (like inlining calls) before SPMD partitioning that
      are required for sharding propagation to happen correctly and efficiently.
    
    PiperOrigin-RevId: 398372070
    Change-Id: I6f6a0db61a15e19f8f2e64334f4655db0b700c70

commit 468636f1e914538ee9b4c91e2c79566be9a71407
Author: Mehdi Amini <aminim@google.com>
Date:   Wed Sep 22 14:44:48 2021 -0700

    Migrate `ScatterDimensionNumbers` attribute definition from StructAttr to be a first class attribute (NFC)
    
    This makes it more efficient to store, to access, and able to provide custom parsing/verification. The accessor are providing native view (ArrayRef<int64_t>)
    which are much nicer to work with as well.
    
    PiperOrigin-RevId: 398333824
    Change-Id: I5767d5e2c0303bc37aaf86d8818fc3a57629f736

commit 171ffe1ea9e0b0a33bcdd74c89a41eafa31c3827
Author: Eugene Zhulenev <ezhulenev@google.com>
Date:   Sat Aug 21 06:07:17 2021 -0700

    [tfrt:tf] Update shape optimization pass to handle const shapes
    
    - fix a bug in shape constraints optimization
    - add an option to optimize only constraints, because it allows more efficient mhlo broadcasts movement
    
    PiperOrigin-RevId: 392166188
    Change-Id: I01a653e316c3a05fed094fb47f5c33b31b160299

commit b5935d249ccd2f5c4c8c8d35251a5e29a5aa36be
Author: Matt Watson <mattdangerw@google.com>
Date:   Tue Aug 17 17:34:27 2021 -0700

    Add sparse and ragged options to TextVectorization output
    
    Sparse will only apply when output_mode is "one_hot", "multi_hot",
    "count", or "tf_idf" and the last dimension of the output contains
    bins for every element in the vocab. Sparse will be a more efficient
    output option when vocab size is large.
    
    Ragged will only apply when output_mode is "int", and will output
    a ragged tensor after string splitting where the final dimension
    contains ragged vocab indices of variable length.
    
    PiperOrigin-RevId: 391415472
    Change-Id: I6f600fb476667b8af8328b051392bdcf9f9b3266

commit afcd94a63b46ae0f3d5e249c2a696ffe3234916d
Author: Chenguang Wang <chenguangwang@google.com>
Date:   Tue Aug 10 20:37:14 2021 -0700

    Add a C++ friendly sparse->dense constructor for FormatConverter.
    
    The existing constructor uses the C API TfLiteSparsity struct, which needs to
    be freed with TfLiteSparsityFree() and allocated with malloc(). The
    TfLiteSparsity struct itself is then converted to C++ vectors.
    
    For C++ users, the TfLiteSparsity-based interface is neither easy to use or
    efficient.
    
    PiperOrigin-RevId: 390042122
    Change-Id: Iae6c00cabd6f1fbb84cdbe849823489097edeb40

commit 370888bf50de1b8f5fdfaafb64612be2babea4e1
Author: Ran Chen <crccw@google.com>
Date:   Thu Aug 5 04:52:59 2021 -0700

    Improve cancellation of MultiDeviceIterator::GetNextFromShard
    
    Cancelling GetNextFromShard shouldn't cancel the whole MultiDeviceIterator. Although in practice tf.distribute always fetch from all shards, it's possible that one shard iterator is deleted while the others are still fetching. This didn't surface any problems before because RemoteCall didn't propagate the cancellation.
    
    This change:
    1. Propagate cancellation in RemoteCall.
    2. Cancelling a GetNextFromShard only cancels itself. It no longer cancels MultiDeviceIterator. The cancellation is not efficient but it should be rare.
    
    PiperOrigin-RevId: 388906528
    Change-Id: Iadc4e34d401aecd071a7b0f86ce9f0580abe9ef5

commit a1582860e940bb8ec328ba412cf04b5ae7db210a
Author: Michael Gester <mgester@google.com>
Date:   Mon Jul 26 11:29:23 2021 -0700

    Make resource analysis more efficient by using new resource allocation trait
    
    For some resource-allocating ops we know that they will always create a unique
    resource. This CL introduces a trait to reflect that and implements
    `ResourceHandleAllocatorInterface` for the trait so resource alias analysis does
    not conservatively treat allocated resources as "unknown" in such cases anymore.
    
    PiperOrigin-RevId: 386920597
    Change-Id: Iac1fbf1f0ac0bde66041fd6ed876f857d5ccd8ad

commit c3d96abd1b48e5e0e7c3f309c7db5363e06bff9d
Author: Michael Gester <mgester@google.com>
Date:   Fri Jul 16 15:03:42 2021 -0700

    Make resource alias analysis less conservative
    
    Now we handle ops with `MemoryEffectOpInterface` less conservative which makes
    side-effect analysis more efficient. In particular, for a resource-allocating op
    with empty or anonymous `shared_name` attribute we can assume that the resource
    being created is unique which wasn't utilized before. One implication of this is
    less control dependencies in the exported GraphDef.
    Also added tests and improved some comments.
    
    PiperOrigin-RevId: 385231241
    Change-Id: I9614d050cb1a521dc34befa3e4189c0a2794493a

commit 41cc9e51b82b2655d75beaf619af6798e6a63be3
Author: Scott Zhu <scottzhu@google.com>
Date:   Mon May 17 17:30:04 2021 -0700

    Update keras metrics to use memory efficient alternative when collect values for evenly distributed thresholds.
    
    This implementation is based on the example in tf.slim. It exhibits a run time and space complexity of O(T + N), where T is the number of thresholds and N is the size of predictions. Metrics that rely on standard implementation instead exhibit a complexity of O(T * N). It could save a lot of memory when N is large.
    
    Added a unit test to verify the memory consumption. Under eager context, the ratio of memory between old and new approach is between 80 and 500. Set the limit to 50 to avoid the flakiness.
    
    PiperOrigin-RevId: 374315460
    Change-Id: If775df7031287d647a56589a7cfe9bafa7dd8cf3

commit 839928c4fcaca95eb540101f5f93a060e0995996
Author: Scott Zhu <scottzhu@google.com>
Date:   Fri May 14 16:41:03 2021 -0700

    Update keras metrics to use memory efficient alternative when collect values for evenly distributed thresholds.
    
    This implementation is based on the example in tf.slim. It exhibits a run time and space complexity of O(T + N), where T is the number of thresholds and N is the size of predictions. Metrics that rely on standard implementation instead exhibit a complexity of O(T * N). It could save a lot of memory when N is large.
    
    Added a unit test to verify the memory consumption. Under eager context, the ratio of memory between old and new approach is between 80 and 500. Set the limit to 50 to avoid the flakiness.
    
    PiperOrigin-RevId: 373889953
    Change-Id: Id0106518672d11773af051900df8cfa08a240a42

commit ffc28ef81c4235c4d5ebe77de92825db2367f579
Author: Srinivas Vasudevan <srvasude@google.com>
Date:   Thu May 13 16:41:51 2021 -0700

    Allow non-square matrices for LinearOperatorBlockDiag.
    
    We can still do matmuls efficiently in this case since we can avoid materializing zeros.
    
    PiperOrigin-RevId: 373683980
    Change-Id: If4dfe202f8c3abad3d6096175c953b3b78549871

commit bcf28627b3d8161fb53274ac0eeb11c5d59bdafe
Author: Scott Zhu <scottzhu@google.com>
Date:   Wed May 12 15:54:10 2021 -0700

    Update keras metrics to use memory efficient alternative when collect values for evenly distributed thresholds.
    
    This implementation is based on the example in tf.slim. It exhibits a run time and space complexity of O(T + N), where T is the number of thresholds and N is the size of predictions. Metrics that rely on standard implementation instead exhibit a complexity of O(T * N). It could save a lot of memory when N is large.
    
    Added a unit test to verify the memory consumption. Under eager context, the ratio of memory between old and new approach is between 80 and 500. Set the limit to 50 to avoid the flakiness.
    
    PiperOrigin-RevId: 373469207
    Change-Id: I95ed482dc4abde5de829d3018e445c9ecb62d2d6

commit 752ea9ac32ce762f272c2574a63f5379b8457c95
Author: Rajkumar Samuel <rsamuel@google.com>
Date:   Mon May 10 11:38:45 2021 -0700

    Add support in collective_rma_distributed for worker interfaces that directly populate the pointer provided in the RecvBuf request with the remote tensor, rather than populating transport_options with a serialized proto containing the remote tensor. This allows for more efficient implementation that avoids extra serialization / de-serialization costs from transport format into protos and finally to tensor buffer.
    
    PiperOrigin-RevId: 372976144
    Change-Id: I2f46d9c6b7dd30ea7003de7587703dc8666e7c27

commit 64a517a2b2421a2ae8f8655b474333591e48b726
Author: Eugene Zhulenev <ezhulenev@google.com>
Date:   Thu May 6 04:55:29 2021 -0700

    [tf:tfrt] Add ClusteringPolicy based clustering API to cluster_ops_by_policy
    
    Take the library based approach for clustering. End-user clustering pass must provide a specific clustering policies to the generic API.
    
    1. Values constraints propagation and refinement is not properly supported yet.
    2. A lot of inefficient linear lookups that will be removed once proper constraints propagation is implemented, and it will become clear what auxiliary data structures are required.
    
    tf_to_corert still relies on the generic cluster_ops_by_policy_pass.
    
    PiperOrigin-RevId: 372318535
    Change-Id: Ifdd521f0f66d84541c797a49b0c5f72f76788970

commit 311c53372c29ab23f6ca0a3bc63f40154278caf4
Author: Haoliang Zhang <haoliang@google.com>
Date:   Thu Apr 29 14:03:24 2021 -0700

    Improve the efficiency of registering FunctionDef in the flex delegate:
    Previously we add a FunctionDef for each subgraph in the tflite model, this isn't efficient since there are subgraphs that are not intended to be invoked as functions. After the change, we will collect subgraphs used by a list of ops (MapDataset, ReduceDataset) and only register functions for those subgraphs.
    
    PiperOrigin-RevId: 371199293
    Change-Id: I3a8ca3841d018b9a57e339fd8becf0d03863b974

commit e98fc97bee12655f93627604515a7140eea1cfe0
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Apr 22 11:59:02 2021 -0700

    [Grappler] Remove inefficient optimizer phase ReorderReshapeAroundUnary, and slightly extend RemoveRedundantReshapeAndBroadcast to do the same thing with fewer graph mutations.
    
    PiperOrigin-RevId: 369920081
    Change-Id: I94848f4b760081c098488b13c0b19f7e66bfcf7d

commit 0b9548386559a705308c9476dc3e562ffd0fae80
Author: Eugene Zhulenev <ezhulenev@google.com>
Date:   Wed Apr 21 14:45:46 2021 -0700

    [tf:mlir] Add clustering policy for TF operation
    
    1. Instead of passing a list of operations that could be clustered together via flags, configure clustered operations with a set of policies.
    2. Allow clustering with constraints, to guarantee that clustered operations can be compiled at runtime: no unknown ranks, guaranteed lowering to linalg indexing maps, etc...
    
    "ClusteringPolicy" is not a property of the operation itself, because different targets might have different restriction on what they can efficiently compile.
    
    PiperOrigin-RevId: 369739307
    Change-Id: Ib2e063d5fb0eb84f60c36d4d52d6e6bbc4b612cd

commit 8040cd1cca688fdfd10f954ebc7870d3c5c1883f
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Apr 15 12:06:29 2021 -0700

    [Grappler] Remove inefficient optimizer phase ReorderReshapeAroundUnary, and slightly extend RemoveRedundantReshapeAndBroadcast to do the same thing with fewer graph mutations.
    
    PiperOrigin-RevId: 368689570
    Change-Id: Iaed5c446291ceaf89a06cc42f8c1c020fe95140c

commit e4fcd304db0e6728862a62d2142596347017f32c
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Apr 14 14:22:56 2021 -0700

    [Grappler] Remove inefficient optimizer phase ReorderReshapeAroundUnary, and slightly extend RemoveRedundantReshapeAndBroadcast to do the same thing with fewer graph mutations.
    
    PiperOrigin-RevId: 368507717
    Change-Id: I6ceacd91abb058306c646ed75179810fa2f20414

commit 07da0b1e218bddc612dce603dbeb029bd9e44f18
Author: Ian Langmore <langmore@google.com>
Date:   Wed Apr 14 10:31:36 2021 -0700

    LinearOperatorLowRankUpdate now implements an efficient `diag_part`
    calculation.
    
    PiperOrigin-RevId: 368458205
    Change-Id: If752edbaf8097d90154102197e336fa9e45b4472

commit 63015de617b9286469aa6f8fd1224c9fdae69bb6
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Apr 13 19:20:40 2021 -0700

    [Grappler] Remove inefficient optimizer phase ReorderReshapeAroundUnary, and slightly extend RemoveRedundantReshapeAndBroadcast to do the same thing with fewer graph mutations.
    
    PiperOrigin-RevId: 368342596
    Change-Id: I4a5ce18508fb1aad76c1c0dac3ce5504224c6ddc

commit 03796cf1ddaa559bdca4e4f2e8fdc859bdf21a52
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Apr 13 17:22:10 2021 -0700

    [Grappler] Remove inefficient optimizer phase ReorderReshapeAroundUnary, and slightly extend RemoveRedundantReshapeAndBroadcast to do the same thing with fewer graph mutations.
    
    PiperOrigin-RevId: 368328186
    Change-Id: I73f5f45ebe4bc1387a4356aac9d3648840064855

commit 7fcf01bda7bb618fd174b17889657390d729cae4
Author: Mehdi Amini <aminim@google.com>
Date:   Fri Apr 9 21:56:39 2021 -0700

    Use tensor_content when converting constant attributes from MLIR to TensorFlow proto (NFC)
    
    This is just a more efficient storage, no functional change intended.
    
    PiperOrigin-RevId: 367753660
    Change-Id: Iec5016995eae1a88bd8df4074bfd85c85d2fe4e7

commit 7c138d4ae09bd56796a9b38f35c686756ec432c4
Author: Scott Zhu <scottzhu@google.com>
Date:   Thu Mar 18 15:23:37 2021 -0700

    Add docstring for tf.keras.applications.efficientnet.preprocess_input with more context.
    
    PiperOrigin-RevId: 363759122
    Change-Id: Ia3c12142a5e0855578a33c654cd6191869ae5e32

commit 6547de3b8e5d4c5c5f82386ad7eca616dffcc2f1
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Mar 11 09:44:05 2021 -0800

    [TF:TRT] Enable large tensor transpose for TensorRT 7.1.3.4 or higher.
    
    TensorRT 7.1.3.4 is no longer inefficient at large tensor transpose. It is better to enable the large tensor transpose by default to improve coverage and reduce fragmentation.
    
    PiperOrigin-RevId: 362313390
    Change-Id: I0add74e9996aefdeace431afad5ede9960705d68

commit 1e27427e0a74a7be489af8040a9a3378f8f8d94f
Author: Matt Watson <mattdangerw@google.com>
Date:   Wed Mar 10 19:00:15 2021 -0800

    Set TextVectorization.pad_to_max_tokens default to False
    
    This gives a consistent default across the preprocessing layers, and is the more efficient option.
    
    PiperOrigin-RevId: 362193657
    Change-Id: Id8cf48bdc501162dcd9cc9803cd7b3dce84906d4

commit 3779a51a376ac5116a9a64e3090ffd621493651f
Author: Jacques Pienaar <jpienaar@google.com>
Date:   Thu Mar 4 14:48:53 2021 -0800

    Re-enable inference for large result number ops
    
    This was made more efficient upstream by https://github.com/llvm/llvm-project/commit/3dfa86149e14ebb7fa40c55749ae435641940ff6#diff-e0acb49d6ec4a97460a1af5fe8f80abc39f04d172d6c9a08c33da1209115cd4b so re-enabling.
    
    PiperOrigin-RevId: 361008357
    Change-Id: Ie095676290a5f8b086db086a48bee9a66f7c2b32

commit 1cac769816201020fe437e84a69867781861bef0
Author: Jacques Pienaar <jpienaar@google.com>
Date:   Wed Jan 20 17:47:47 2021 -0800

    Add a prepare for export to XLA pass
    
    Add pass that prepares for more efficient export to XLA. Initially focus on splat constants: there is a mismatch in splat constant representations, where it is cheap in the dialect (and also in TF) but expensive on export to XLA/HloInstruction.
    
    PiperOrigin-RevId: 352913787
    Change-Id: I07e34a60a7e9f330c897178f8b65d75865e62d3f

commit 373f458fb66fdb709d7af828fe6200e3137942e6
Author: Chris Kennelly <ckennelly@google.com>
Date:   Mon Dec 21 08:44:36 2020 -0800

    Optimize calls to std::string::find() and friends for a single char.
    
    The character literal overload is more efficient.
    
    PiperOrigin-RevId: 348473483
    Change-Id: Ia76efa5ee243f7a92b35f1fb81d4af864fca8372

commit 4692525ffaa79982f094979f564b0b4942732ff5
Author: Chris Kennelly <ckennelly@google.com>
Date:   Thu Dec 17 17:59:05 2020 -0800

    Optimize calls to std::string::find() and friends for a single char.
    
    The character literal overload is more efficient.
    
    PiperOrigin-RevId: 348126864
    Change-Id: I12485209607a957ecb17a4ba1087473bb0c4dd06

commit c04bf06bfc9660bbe8863b35cf24646e3cad3a05
Author: Chris Kennelly <ckennelly@google.com>
Date:   Thu Dec 17 17:37:39 2020 -0800

    Optimize calls to std::string::find() and friends for a single char.
    
    The character literal overload is more efficient.
    
    PiperOrigin-RevId: 348124169
    Change-Id: I55909265a8267017210eb0deff5091da20d8ed70

commit fd0d7123b12c4b7e80468f3ce64a26687a6dc70b
Author: Chris Kennelly <ckennelly@google.com>
Date:   Thu Dec 17 10:57:49 2020 -0800

    Optimize calls to std::string::find() and friends for a single char.
    
    The character literal overload is more efficient.
    
    PiperOrigin-RevId: 348053258
    Change-Id: Ida72d7b6d860e1acf9a914e32d31a208cb23728b

commit 76a17bad5846a5bebec1b45ca29895cb50f43449
Author: Taehee Jeong <taeheej@google.com>
Date:   Wed Dec 9 11:14:46 2020 -0800

    Apply correct quantization schemes for LSTM inputs
    
    * Split LSTM quantization pass, and run it before main prepare_quantize pass.
    * Import correct quantization scheme from `operator_property.cc`
    * Added processing for input tensors, according to quantization scheme
      * Input 0, 18 (input and state): int8, handled by default quantization pass
      * Input 1~8, 16 (weights): int8, symmetric
      * Input 9~11 (coefficients for peephole): int16, symmetric
      * Input 12~15, 17 (biases): int32, depends on other scales and normalization
      * Input 19 (state): int16, quantized with power_of_two range. (already handled)
      * Input 20~23 (normalization parameters): int16, symmetric
    * Modify op definition of LSTM to recognize quantized type similarity
    
    PiperOrigin-RevId: 346593339
    Change-Id: I85dd22a9158fcb61496444ddf5bdf192447d45a0

commit e9f1da3c9b20e45ee226a835b27a252dbbd00d53
Author: Chenkai Kuang <chenkai@google.com>
Date:   Fri Nov 6 16:37:48 2020 -0800

    Improve the warning message when a large variable is not initialized in memory-efficient way.
    
    PiperOrigin-RevId: 341140947
    Change-Id: I3ffb9c61b71c9bdd78967dc77302727dd1e1462e

commit 3ac0818dea42c6699cda3cea26657f606f11a3cc
Author: Edward Loper <edloper@google.com>
Date:   Mon Oct 26 17:42:57 2020 -0700

    Add PythonTensorConverter class, which can be used in c++ to efficiently convert PyObjects to tensors.
    
    PiperOrigin-RevId: 339155091
    Change-Id: Icad20253f523e3ac3685d8d34c52e940f3889b57

commit 3be76dbd4932bddcb9101c6793b5f51359cb8b2e
Author: Chenkai Kuang <chenkai@google.com>
Date:   Wed Oct 21 17:25:39 2020 -0700

    Allow parameter server strategy sharded variable creator to efficiently use TF initializers.
    
    PiperOrigin-RevId: 338373212
    Change-Id: Id73d06faf5a950ebaad91b0dde0fba6c885aa089

commit 239fe406d30e8c8eb5387761d4e1eb48c5b532be
Author: Chenkai Kuang <chenkai@google.com>
Date:   Wed Oct 21 17:16:27 2020 -0700

    Modify some v2 initializers to be able to return a value that corresponds to a partition of the entire value. This is useful for efficiently initializing sharded variables where only a shard of the initial value is necessary at a time.
    
    PiperOrigin-RevId: 338371904
    Change-Id: Ib4320d73cbaec30f5a61793debe7755026175781

commit 9f2b92b4e94dba44177bca57351d7a9d4f441f94
Author: Michelle Casbon <michellecasbon@google.com>
Date:   Sat Oct 17 18:35:58 2020 -0700

    Add functions that return device type and ID for eager.
    
    This addition enables more efficient device handling in S4TF without needing to parse the full device string. As support for devices beyond TF eager are added, this info is needed more often and has a bigger impact on performance.
    
    Partial fix for https://github.com/tensorflow/swift/issues/524.
    
    PiperOrigin-RevId: 337696655
    Change-Id: Ifb576d37c765cced2329b77e0cebb591d8d3a46c

commit 36c1d26aef3dc5f880b23a73b4276d951c36eb43
Author: Jiri Simsa <jsimsa@google.com>
Date:   Thu Oct 15 13:29:24 2020 -0700

    [tf.data] Change implementation of `from_generator` so that generator functions that only produce dense tensors use PyFunc-based mechanism (which is more efficient) and the EagerPyFunc-based mechanism is only used when support for composite tensors is needed.
    
    PiperOrigin-RevId: 337371087
    Change-Id: Ib3c058a6ceeef330802cb41e92d61e71a3e98e8c

commit 6fdce880fa8a5f8b752323ce9d2b49d87382d80e
Author: Jose Baiocchi <jbaiocchi@google.com>
Date:   Tue Oct 6 14:20:19 2020 -0700

    Add format_utils to profiler
    
    For now just provide OneDigit implemented using StrFormat.
    A more efficient implementation could be used in the future.
    
    PiperOrigin-RevId: 335720026
    Change-Id: Iefa6ce5f5860b45533f081bbd83267347847a962

commit 7ea86e9de8a5b6426e7291e0e5477ddaee83ba88
Author: Adrian Kuegel <akuegel@google.com>
Date:   Thu Oct 1 05:12:01 2020 -0700

    Refactor the code to avoid duplication (NFC).
    
    IsFusedIrEmitterInefficient can reuse the code from
    FusionNodeIndexingEvaluation.
    
    PiperOrigin-RevId: 334791886
    Change-Id: I8bd812913355133bfcc0ea1f85792f47c550fb1c

commit dc5718967ed5f0b087cc9c6f69079d90f9ec1806
Author: Peter Hawkins <phawkins@google.com>
Date:   Wed Sep 9 06:35:02 2020 -0700

    [XLA] Use the compact WY representation in the implementation of blocked QR decompositions.
    
    Compact WY representations are described in:
    Schreiber, Robert, and Charles Van Loan. "A storage-efficient WY representation for products of Householder transformations." SIAM Journal on Scientific and Statistical Computing 10.1 (1989): 53-57.
    
    The compact WY representation is more storage efficient, requiring calculation of an nxn triangular matrix, where n is the block size (e.g., 128), instead of an mxn matrix where m is the number of matrix rows.
    
    PiperOrigin-RevId: 330711085
    Change-Id: Ideac239ff118ee6ac2fd1397b731a40e11d6ecd7

commit 4b0e555bdce3f1d78c97ff97d3c3bf037730c7ab
Author: Anna R <annarev@google.com>
Date:   Tue Sep 8 15:34:42 2020 -0700

    Addressing some of the StreamExecutor C API feedback:
    * Add block_host_until_done. Some devices might have more efficient implementation than enqueueing an event and waiting for it.
    * Plugin registration function now takes `void* dso_handle` instead of dso path. We might want to initialize multiple plugins based on `dso_handle` (for e.g. device, kernel, filesystem, etc..).
    * `visible_device_count` could be 0 at registration time. So, I removed corresponding validation.
    
    PiperOrigin-RevId: 330601080
    Change-Id: I378b0229d6c7a4311cbcd57ae6b872323edc8e82

commit 8c81dd1e6d657d92e98f8c33d7a83ab3d7122a1c
Author: Chenkai Kuang <chenkai@google.com>
Date:   Mon Aug 24 17:50:21 2020 -0700

    Add several new features to ShardedVariable in ParameterServer strategy:
    
    1. Dense layer partition. It is learned that in some cases dense layer partition could improve the model training speed.
    
    2. ShardedVariable now supports "assign", "assign_add" and "assign_sub" methods.
    
    3. ParameterServerStrategy now accepts a "variable_partitioner" parameter that controls all variable partitioning under strategy.scope(). It is compatible with tf.compat.v1 partitioner. Default partitioner is same as estimator canned models: each partition would has at least 64MB data.
    
    4. ParameterServerStrategy now is able to do memory-efficient initialization of sharded variables, but it requires a custom initializer that is partition aware.
    
    5. ParameterServerStrategy now is able to partition variables even if their `initial_value` is a Tensor (not a callable).
    
    Meanwhile, removed `strategy.experimental_variable_partitioning_scope` method. Per-layer partitioning using different partitioners is not going to be supported right now.
    
    PiperOrigin-RevId: 328241842
    Change-Id: I382743dd8d1a2f6b7ab207576aed2e77d71c5735

commit 5e9d0fe25cbd7d02a2350760e4bde00c58a803ec
Author: Rahul Joshi <jurahul@google.com>
Date:   Thu Aug 13 18:09:04 2020 -0700

    [MLIR] Extend ResourceDeviceInference to handle WhileRegion
    
    - For supporting device attribute propagation efficiently in the presence of WhileRegion,
      use tensorflow::walk() generic walker to implement a pre-order walk.
    - Extend test cases to test a inlined version of WhileRegion (where calls are inlined in
      the cond and body regions).
    
    PiperOrigin-RevId: 326565094
    Change-Id: Iac19d7f22bfd79b344fa8118115acc04fff7310e

commit ae10b73d6b70e2174613d83c3cbada09df4d4c48
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Jul 31 08:07:11 2020 -0700

    Optimize RearrangeWeights for fully connected to be cache efficient while reading weights.
    
    PiperOrigin-RevId: 324209359
    Change-Id: Ifa6dfff91b216e2e007bd4cefb8f3d814971d598

commit 8d2f9f65480a36e95e1f75956bcb40be6c08f5e8
Author: Ran Chen <crccw@google.com>
Date:   Thu Jul 30 11:29:11 2020 -0700

    Add timeout to CollectiveHints
    
    This allows users to set a timeout on the collective. This now should only be
    used for debugging purposes because it doesn't have an efficient implementation.
    
    PiperOrigin-RevId: 324044909
    Change-Id: I5d2e5c19fa5501749ebc4f13e2fd515e1de86539

commit 77787199e48764a439558e9d0b7368d96e730c2e
Author: Dmitry Volodin <mr.molkree@gmail.com>
Date:   Wed Jul 15 18:58:06 2020 +0300

    Fix indent in efficientnet.py

commit 62f54dd24004ade4a890a16c326953af3b1db87e
Author: Haoyu Zhang <haoyuzhang@google.com>
Date:   Thu Jul 9 21:37:46 2020 -0700

    Skip graph optimization passes for component functions.
    
    Before this change, the registered graph optimization passes are executed on both the main function side and the component function side when running distributed functions. This is not efficient, and can cause graph compilation problems. This change annotate component functions execution so that the graph passes will be skipped when instantiating them, avoiding the repeated graph passes that are already executed on the main function side.
    
    PiperOrigin-RevId: 320540983
    Change-Id: I4816240bcd5b54c738114c36f17ecc1b0b6c920d

commit 13e7dee685a9d7cd753ef1f6a3ac8ff54f679927
Author: Katherine Wu <kathywu@google.com>
Date:   Thu Jun 25 11:29:31 2020 -0700

    Don't trace OpLayer in SavedModel.
    
    OP Layers wrap a single Tensorflow op in a Layer class. Previously, SavedModel would wrap every internal layer call in a tf.function, so that the user can inspect individual layers in the loaded model. For TensorflowOpLayer, this is unnecessary because (1) wrapping a single op in a tf.function is very inefficient (2) the user is unlikely to individually inspect the autogenerated op layers in the loaded model.
    
    This change also resolves the saving issue that occurs when a user builds a functional model while using the eager-computed results of `tf.shape(x)` as the input shape to another op layer.
    
    An example to help illustrate:
    ```
    x = tf.keras.Input((2,))  # Shape is (None, 2)
    state = tf.zeros(4, tf.shape(x)[0])  # Expected shape is (4, None)
    LSTM(inputs, initial_state=state)
    ```
    
    Prior to this CL, the TensorFlowOpLayers generated for tf.shape and tf.zeros would be separately wrapped in tf.functions when saving. This results in `state` having a shape of `(None, None)` instead of `(4, None)`, causing potential problems when saving the rest of the model.
    
    PiperOrigin-RevId: 318311978
    Change-Id: I15099d8ba29c1d4facd3f88630f8e2651f22ae83

commit 81e6c5917e590f9c9a389d3e8825eae59fee171a
Author: Yixing Fu <yxfu93@hotmail.com>
Date:   Fri Jun 19 13:44:40 2020 -0400

    add script for updating efficientnet weights from ckpt

commit af926984871a130eec2816815cfc98a362d4f5b6
Author: Tres Popp <tpopp@google.com>
Date:   Wed Jun 17 00:44:26 2020 -0700

    [TF:XLA] Update TF:XLA tests for matrix_triangular_solve to test V1 and V2.
    
    TF:V1 raises an error on non-square coefficient matrices
    TF:V2 allows non-square coefficient matrices.
    PiperOrigin-RevId: 316839892
    Change-Id: I34c2567ba3579c8f0fd4bc6da57abe14bc6471b2

commit 580151fa26419ae583ec42cc6cbb92777e214109
Author: Berkin Ilbeyi <berkin@google.com>
Date:   Thu Jun 11 14:50:25 2020 -0700

    [XLA] Use latest to earliest order in prefetch picker.
    
    This will make more efficient use of alternate memory by trying to avoid
    prefetches that are unnecessarily early (and hence waste alternate memory).
    
    PiperOrigin-RevId: 315982838
    Change-Id: I6080a48661a5f032c0478b6d230b5b482840f2d4

commit 2b05096c356263ccd997cd01fa34d4e3aac25c21
Author: Robert Suderman <suderman@google.com>
Date:   Wed Jun 10 16:13:45 2020 -0700

    HLO Concat to Slice optimization
    
    In some Shape cases we encounter a Slice of a Concatenate optimization. This
    can be often completely removed or made more efficient for HLO optimization.
    
    PiperOrigin-RevId: 315789492
    Change-Id: I7a7384be1bc31fda41d0209eafc343f2a9190e81

commit d14b3ad658da633f15cb12e178e89fa2c7c470a2
Author: Rahul Joshi <jurahul@google.com>
Date:   Wed Jun 10 10:59:03 2020 -0700

    [NFC] Change IsExported() to be more efficient
    
    PiperOrigin-RevId: 315724467
    Change-Id: If2090deabaf0e2d387eceded3bf0a8ee1a122d5a

commit 2d4f1920d2d00775196d81073f92ad6079ee7f1a
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri May 29 05:10:42 2020 -0700

      [XLA] Transform unranked HLO
    
      All applications of a unary element-wise operation on a given tensor are
      independent from each other.
      We use this to realize these operations on a flattened, hence ranked, tensor
      when its shape is unknown at compile time.
      With only one dynamic reshape operation before and one after the targeted
      operation we can generate efficient code for the core of the operation.
      This CL realizes the transformation for `xla_hlo.sqrt` and others will follow
      analogously.
    
    PiperOrigin-RevId: 313761871
    Change-Id: I3566c2c36f1322a5833ebe49309255ba191c1bd1

commit f0ef163443b301ca913e967be566d8401c1bbf7a
Author: Mehdi Amini <aminim@google.com>
Date:   Wed May 27 02:56:50 2020 -0700

    Add an MLIR tracing implementation to the C unified API
    
    This is plumbing just enough to pass all the unit-tests.
    The conversion to the function library is quite inefficient, but it isn't
    clear if we want to optimize this or just focus on TFRT moving forward.
    
    PiperOrigin-RevId: 313356850
    Change-Id: I83815317d4958786d0103168b5d88498f89511ed

commit ad6798a2f62ae2cb7f433af7b721bf14b9850dde
Author: Berkin Ilbeyi <berkin@google.com>
Date:   Mon May 18 17:01:57 2020 -0700

    [XLA] Fix alternate memory allocation of conditional operands.
    
    Consider the following flattened HLO schedule of a conditional:
    
    1: a = fusion()
       true_computation:
    2:    parameter = parameter(0)
    3:    ...
    4:    ...
       false_computation:
    5:    parameter = parameter(0)
    6:    ...
    7:    ...
    8: conditional = conditional(pred, a, a)
    9: b = fusion(a)
    
    When we had a tensor that was a conditional operand (e.g. "a" in the example),
    we reserved the alternate memory for the entire 1-8 range. This meant that when
    we tried to allocate inside the called computations of the conditional, the
    offset we picked wasn't available since it would fall within the 1-8 range. This
    CL now reserves the conditional until the parameter of the earliest called
    computations (1-2 range).
    
    To allow efficient use of alternate memory by avoiding a very large conditional
    from claiming the offset for the entire called computation, the conditional
    operand might die within the called computation, allowing other HLOs inside the
    called computations to reclaim that alternate memory offset. This creates a
    subtlety for subsequent uses of conditional operands (e.g. "a" is used by a
    fusion at 9). These subsequent uses will force evictions (and then do another
    prefetch). After optimization, the graph might look like the following:
    
      a (Alternate Mem) = fusion()
      cs0 = copy-start(a)  # Must evict a because the allocation may die within
                           # called computation.
      cd0 (Default Mem) = copy-done(cs0)
      true_computation:
        parameter (Alternate Mem) = parameter(0)
        ...
        # parameter's alternate memory allocation may die here and another tensor
        # might use the same offset.
      false_computation:
        parameter (Alternate Mem) = parameter(0)
        ...
        # parameter's alternate memory allocation may die here and another tensor
        # might use the same offset.
      conditional = conditional(pred, a, a)
      cs1 = copy-start(cd0)  # May prefetch the value back to alternate memory.
      cd1 (Alternate Mem) = copy-done(cs1)
      b = fusion(cd1)
    
    PiperOrigin-RevId: 312182824
    Change-Id: I3ff5d019025ef96ced1aed4f6d170df677273348

commit 3e6697b916c9e775dc61375b913d21ba9d22126f
Author: Allen Lavoie <allenl@google.com>
Date:   Tue May 5 12:02:04 2020 -0700

    Forwardprop: opt the forwardprop utility function out of run_functions_eagerly
    
    If the function did execute eagerly it would be very inefficient.
    
    Fixes #39075.
    
    PiperOrigin-RevId: 309992909
    Change-Id: I3e31778390beb7a2808a33aa5fe18a5e9bd41bab

commit 958fbebe7092c1dae84e8449952b1cbdbfa8f2b1
Author: Shanqing Cai <cais@google.com>
Date:   Fri May 1 12:45:24 2020 -0700

    [tfdbg2] Various improvements to DebugDataReader for DebuggerV2
    
    This is related to https://github.com/tensorflow/tensorboard/pull/3564
    
    1. Add DebuggedGraph.get_op_creation_digest()
    2. Remove DebuggedGraph.get_op_type(), which is superseded by
       DebuggedGraph.get_op_creation_digest() and is not used anywhere.
    3. Add DebuggedGraph.add_op_consumers() and DebuggedGraph.get_op_consumers()
       to enable efficient tracking of the downstream consuming ops of a graph
       op.
    4. Add host_name and stack_frame_ids to data class GraphOpCreationDigest.
    
    PiperOrigin-RevId: 309455936
    Change-Id: I104084c1ef8b887f69733702a2f4c3190fa5402f

commit e2395e193522a5a8db1730c5409d7a1f4b15ab06
Author: Derek Murray <mrry@google.com>
Date:   Thu Apr 30 10:12:15 2020 -0700

    [Single-threaded executor] Optimize the handling of `OpKernel::const_tensor()`.
    
    This change applies an optimization that already takes place in the default `ExecutorState`: if a kernel produces a single constant tensor, we compute the `const Tensor*` once, and forward it to the downstream kernels without modifying the refcount. As a result, we invoke fewer virtual function calls and make fewer small temporary allocations. This makes `tf.constant()` and various DT_RESOURCE-producing kernels more efficient when using the single-threaded executor.
    
    As an enabling step, this change re-uses the `Entry` structure from the default executor, which has support for storing a union of a (manually-constructed) `Tensor` or a `const Tensor*` as appropriate.
    
    PiperOrigin-RevId: 309248989
    Change-Id: Id0bdf19e34caee7cd5ca3ac907f4ffaa61244934

commit ad35e8330d90a63ea4a9bc04829cbe1ce04ddb75
Author: Derek Murray <mrry@google.com>
Date:   Fri Apr 17 11:51:17 2020 -0700

    [Executor] Avoid unnecessary calls to `InlinedVector::clear()` and `resize()`.
    
    This change should benefit execution for graphs with large runs of inexpensive kernels, or where execution is dispatched by a single thread.
    
    The executor temporarily stores the outputs of an op in an `EntryVector`. `ExecutorState::ProcessOutputs()` resizes the vector to `item.num_outputs` elements before filling in the entries retrieved from the `OpKernelContext`. `ExecutorState::Process()` then clears the vector after calling `PropagateOutputs()`.
    
    This change makes two edits to avoid calls to `resize()` and `clear()`:
    
    1. We no longer clear the `EntryVector` between operations. Instead, we call `Entry::ClearVal()` on all of the elements that were touched by the last operation, which releases any leftover tensor references.
    
    2. We no longer resize the `EntryVector` to `item.num_outputs` in `ProcessOutputs()`. Instead, we resize it to the maximum of the current size and `item.num_outputs`. This avoids a deallocation/reallocation when the entries cannot be stored inline.
    
    This change also modifies the type of `outputs` passed to `ProcessOutputs()` from `EntryVector*` to a C-style `Entry*` array, to provide more efficient access (one fewer branch) to the individual elements.
    
    PiperOrigin-RevId: 307085587
    Change-Id: I0cda746b8e07e4f3f57aab3282df6b38b5653f88

commit 4c36ade963b0a7f1c8d2ea480d5b8c0922f6bebf
Author: Derek Murray <mrry@google.com>
Date:   Thu Apr 2 14:55:31 2020 -0700

    [Executor] Implement `SimplePropagatorState` and use it when a graph has no v1-style control flow.
    
    `SimplePropagatorState` provides a more efficient version of `PropagateOutputs()` thanks to the following simplifications:
    * No logic for propagating dead tensors.
    * No special propagation logic for handling control-flow nodes.
    * No dynamic allocation of or pointer chasing into `FrameState` or `IterationState` instances once the executor starts running a step.
    
    Note that `SimplePropagatorState` is compatible with graphs containing only v2-style (i.e. "functional") control flow, because the new style uses ops that require no special treatment from the executor.
    
    PiperOrigin-RevId: 304483519
    Change-Id: I7a1e8df973fc8ff1dbb0025ff89210bb658230be

commit 7d529df64cdf2cbf5d9b2b213c244df006967f42
Author: Adrian Kuegel <akuegel@google.com>
Date:   Thu Apr 2 02:02:05 2020 -0700

    Don't replace Transposes with Bitcasts on the GPU backend.
    
    We can generate the index for accessing the memory more efficiently if we still
    know which dimensions are permuted.
    Outside of fusion nodes, we still want to replace transposes with bitcasts, so
    we add another run of AlgebraicSimplifier at the end of OptimizeHloModule().
    
    PiperOrigin-RevId: 304354090
    Change-Id: I7314476397a6e24dd32b4a85f90d0fa243db382f

commit bd530a65d5712b0734c0b6c9af5aa83ccd9e7387
Author: Derek Murray <mrry@google.com>
Date:   Wed Apr 1 14:26:25 2020 -0700

    [Executor] Split `ExecutorState` into `PropagatorState` and `ExecutorState<PropagatorStateType>`.
    
    This change is part of an ongoing refactoring to simplify "executor.cc" and enable the substitution of more efficient implementations of `PropagateOutputs()`.
    
    PiperOrigin-RevId: 304262448
    Change-Id: I46a2d7fcdde89a71c502d272f35adfd34b0c4cab

commit 0c5bd2d4c77ec26b1c831c4665cf97a236496f47
Author: Derek Murray <mrry@google.com>
Date:   Fri Mar 27 14:45:30 2020 -0700

    Optimize (legacy, reference-typed) `VariableOp::Compute()`.
    
    The `cinfo_` member can be initialized at construction time, so there is no need to acquire a mutex in the `Compute()` method.
    
    The changes in "ops_testutil.cc" update the test harness to use a non-deprecated method for creating kernels in C++ tests. The deprecated API did not set the resource manager in `OpKernelConstruction`.
    
    Note that this op is still rather inefficient because it looks up the variable in the `ResourceMgr` every time it is invoked (for backwards compatibility reasons, relating to state invalidation via `Session::Reset()`). However, new code—and all TF2 code—should use resource variables, which avoid this codepath.
    
    PiperOrigin-RevId: 303410876
    Change-Id: I29e114bb863fe8a1f483d4ce1e034f5f13f1a116

commit 423c2cae269cee41adc3131e50f0d0160ec3a74e
Author: Berkin Ilbeyi <berkin@google.com>
Date:   Fri Mar 6 11:24:33 2020 -0800

    [XLA] When allocating for later uses, use an use_times dict to improve compile time complexity.
    
    We were iterating the last use time one by one. This CL makes it more efficient
    by finding longer allocations that are actually used.
    
    PiperOrigin-RevId: 299396736
    Change-Id: Iac245018bb53b8d8ea346474321bab3bd7d2909d

commit 2c88f1f9f5a71c6c2dc61f01707f8a936a89ca1f
Author: Jacques Pienaar <jpienaar@google.com>
Date:   Tue Mar 3 17:05:21 2020 -0800

    Materialize constants inside regions of TFL While
    
    If constant folding produces a constant while folding operations inside a TFL While, then keep it within the region of the While. This is more efficient on TFLite runtime when outlined to functions again.
    
    PiperOrigin-RevId: 298728560
    Change-Id: I305f5b0ce6559a94c4239d4484b81ea4470e7c68

commit 4b204acd3f297056252b81f3c3bee716dd3c871f
Author: Gaurav Jain <gjn@google.com>
Date:   Sun Mar 1 00:41:51 2020 -0800

    Speed up querying of kernel input properties
    
    * Store the input devices in the KernelAndDeviceOp class during Init.
      This allows index-based access to be faster due to the std::vector and
      the results are preserved in the kernel cache.
    * Use array-style access for inlined vectors since index-based access in
      a for loop is less efficient.
    
    PiperOrigin-RevId: 298152600
    Change-Id: If0b89ffdec78f7344c1c47b74d05cbfa2b9d01ee

commit 0e4f3a99d439c41add66513aa065e5c0656167d7
Author: Sanjoy Das <sanjoy@google.com>
Date:   Sun Feb 23 14:50:55 2020 -0800

    Add a TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE debug flag
    
    If set, TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE lets the TensorRT bridge lower
    inefficient transpose operations.
    
    PiperOrigin-RevId: 296773482
    Change-Id: Ifee5ce2d24de996336e753c48a7229fccff749ab

commit 8aa4fe59eb98147609dc690c1244dde315312cd4
Author: Derek Murray <mrry@google.com>
Date:   Thu Feb 6 13:26:38 2020 -0800

    [Rendezvous] Add a performance counter for tracking how many dead tensors are sent.
    
    This could provide useful input for identifying where a graph with control flow is partitioned in an inefficient way with a large number of cross-device dead edges, and could be refactored into a more efficient graph (e.g. with a multi-device function call inside one branch of a conditional).
    
    PiperOrigin-RevId: 293662534
    Change-Id: I4fc37a903129bbeffa38fdece17a4316fc26d8d9

commit 97b4c2c413d9c880e78e4d2616500cb30b773203
Author: Adrian Kuegel <akuegel@google.com>
Date:   Mon Feb 3 06:05:29 2020 -0800

    Replace IsFusedIrEmitterInefficient with FusionNodeIndexingEvaluation class.
    
    This avoids having to recompute everything from scratch for the whole fusion
    computation. Instead the class keeps the data structures up-to-date after each
    fusion operation.
    Also refactor FusionNodeIndexingEvaluation and support initializing it with a
    non-empty fusion node.
    For now, IsFusedIrEmitterInefficient is still used for the FusionMerger pass.
    
    PiperOrigin-RevId: 292904527
    Change-Id: I435ca6104a95e188250732f9000ea6ea7dfa1956

commit 97da75856c224853a84a1a65ef31f12143278030
Author: Adrian Kuegel <akuegel@google.com>
Date:   Mon Feb 3 01:38:46 2020 -0800

    Add a FusionNodeIndexingEvaluation class.
    
    This will replace the function IsFusedIrEmitterInefficient, at least for the
    regular GpuInstructionFusion and CpuInstructionFusion passes. It can be
    integrated in a fusion pass and avoids having to recompute everything from
    scratch for the whole fusion computation. Instead it keeps the data structures
    up-to-date after each fusion operation.
    
    PiperOrigin-RevId: 292872040
    Change-Id: Ia58a5df286b330fe5a6adb38ba63af48d8d7c74b

commit 32ac70c62a80899ff4b0dc72a608b824958566e7
Author: Terry Heo <terryheo@google.com>
Date:   Mon Jan 27 17:58:52 2020 -0800

    Add MemoryPlanner::ResetAllocationsAfter()
    
    In the previous workaround for dynamic intermediate tensors handling, it calls
    MemoryPlanner::ResetAllocations() to handle resized tensors. Instead of reset
    all the allocation plan, it's more efficient to invalidate allocations only
    after the current execution node.
    
    With the following simple LSTM keras model, benchmark_model shows that
    the execution time improves about 40%.
    
    tf.keras.models.Sequential([
        tf.keras.layers.Input(shape=(28, 28), name='input'),
        tf.keras.layers.LSTM(20),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
    ])
    
    PiperOrigin-RevId: 291840506
    Change-Id: I30e210f25b9631b9d417675d7217c8b41a07987f

commit 55b7bde1f6ee9d9be06953f15809d59ba73bc11d
Author: Adrian Kuegel <akuegel@google.com>
Date:   Thu Jan 16 03:48:13 2020 -0800

    Use DepthwiseConvolutionConverter before ConvolutionGroupConverter.
    
    A recent change in shape_inference required the usage of ConvolutionGroupConverter
    instead of DepthwiseConvolutionConverter. This meant that the filter
    shape got expanded before we called into cuDNN, which is less efficient
    than handling depthwise convolutions directly with cuDNN.
    Now that this change is reverted, go back to using DepthwiseConvolutionConverter.
    However it cannot handle cases with batch_group_count > 1 if input batch
    is not equal to batch_group_count. For this, we still need the
    ConvolutionGroupConverter.
    
    PiperOrigin-RevId: 290037172
    Change-Id: I5b4a1f8eea92392e39ae9cce8b4122f86f7e992e

commit 7835b713e766809f91913a3be2f7d8185b52d36f
Author: Derek Murray <mrry@google.com>
Date:   Fri Jan 3 09:06:14 2020 -0800

    Add Variant::emplace(), based on the C++17 `std::variant<Types...>::emplace()`.
    
    For some variant values (e.g. Tensor) the move constructor must perform a non-trivial amount of copying (e.g. copying the TensorShapeRep). In this case, creating a variant value in place with the new method will be more efficient.
    
    PiperOrigin-RevId: 288000918
    Change-Id: Ib47a9406586e562db5a5c4085d47e7b83702723b

commit 8e6539f5ea47bac3423c5d63f0cd4b18a6479558
Author: Benoit Jacob <benoitjacob@google.com>
Date:   Mon Dec 16 08:31:51 2019 -0800

    Limit rectangularness to avoid using too tiny kernel blocks in the case of highly rectangular destination matrices (gemv-ish cases), which would result in too few iterations of the kernel inner loop to be fully efficient. Now aim to have at least 8 iterations of the kernel inner loop if possible.
    
    PiperOrigin-RevId: 285778165
    Change-Id: Id8826153464b4622677f14da8b0b1b0b60a98ecf

commit 4ad5238778e3f919e9fe899eea3137b1dab81d80
Author: River Riddle <riverriddle@google.com>
Date:   Fri Dec 13 14:52:39 2019 -0800

    Refactor various canonicalization patterns as in-place folds.
    
    This is more efficient, and allows for these to fire in more situations: e.g. createOrFold, DialectConversion, etc.
    
    PiperOrigin-RevId: 285476837
    Change-Id: I510ef7dde07df380bd81dbe59942175a871826e0

commit 1aee1190ab9a4c80f281d2a2595bbc5ecc7ff342
Author: River Riddle <riverriddle@google.com>
Date:   Tue Dec 10 13:20:50 2019 -0800

    Refactor the various operand/result/type iterators to use indexed_accessor_range.
    
    This has several benefits:
    * The implementation is much cleaner and more efficient.
    * The ranges now have support for many useful operations: operator[], slice, drop_front, size, etc.
    * Value ranges can now directly query a range for their types via 'getTypes()': e.g:
       void foo(Operation::operand_range operands) {
         auto operandTypes = operands.getTypes();
       }
    
    PiperOrigin-RevId: 284834912
    Change-Id: If6e8954c8a934b378b4090e701b3fd9cc43f704e

commit 902e8f4bd534d5b440c90f2c42dd126ca90a8fd6
Author: River Riddle <riverriddle@google.com>
Date:   Mon Dec 9 15:24:10 2019 -0800

    Refactor the Block support classes.
    
    Each of the support classes for Block are now moved into a new header BlockSupport.h. The successor iterator class is also reimplemented as an indexed_accessor_range. This makes the class more efficient, and expands on its available functionality.
    
    PiperOrigin-RevId: 284646792
    Change-Id: Ib1a4385a415e3127e506c7bb1141648be97b1890

commit 8a2ad877e9100934ed3f4725424dce86185b6b36
Author: Jian Li <jianlijianli@google.com>
Date:   Fri Dec 6 11:46:24 2019 -0800

    Add quantization support to all variants of LSTM.
    - peephole coefficients are quantized to 16 bits symmetric. Int16 is used because the calculation is a 16x16 vector vector elementwise multiplication.
    - without projection, hidden tensor becomes the output and reuses the quantization parameters of the output
    - with layer normalization, the gate matmul uses intermediate result as output; without layer normalization, gate matmul is fed into activation directly so 2^(-12) is the output scale.
    
    PiperOrigin-RevId: 284230412
    Change-Id: Ibfa66dc6fc2614de28b0ba92e8fb2d42a338aab4

commit a034a3ad800056d8838309c84bc1ea8d9a58cd3e
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Dec 6 10:08:15 2019 -0800

    Add conversions of GPU func with memory attributions to LLVM/NVVM
    
    GPU functions use memory attributions, a combination of Op attributes and
    region arguments, to specify function-wide buffers placed in workgroup or
    private memory spaces. Introduce a lowering pattern for GPU functions to be
    converted to LLVM functions taking into account memory attributions. Workgroup
    attributions get transformed into module-level globals with unique names
    derived from function names. Private attributions get converted into
    llvm.allocas inside the function body. In both cases, we inject at the
    beginning of the function the IR that obtains the raw pointer to the data and
    populates a MemRef descriptor based on the MemRef type of buffer, making
    attributions compose with the rest of the MemRef lowering and transparent for
    use with std.load and std.store. While using raw pointers instead of
    descriptors might have been more efficient, it is better implemented as a
    canonicalization or a separate transformation so that non-attribution memrefs
    could also benefit from it.
    
    PiperOrigin-RevId: 284208396
    Change-Id: Ie330774f90df0c459325fd7146d81cb46da98b39

commit dc4aab853cd9883c0ff2cd181307a303abf8bca7
Author: Taylor Robie <taylorrobie@google.com>
Date:   Mon Nov 25 17:28:35 2019 -0800

    Reduce keras eager overhead by 20% by more efficiently managing the trainable state of Model sub-layers.
    
    In the Keras API, Model.compile() freezes the trainable state for a model and its sub layers. This is relevant for applications such as GANs where the the trainability of a layer depends on the model using it. However for simpler training loops, iterating over all layers to set a property and then iterating over them to set it back after is somewhat expensive. So by only updating the trainable state when it has changed (which it almost never does for normal supervised training) significantly decreases the per-step overhead.
    
    PiperOrigin-RevId: 282467340
    Change-Id: I55ea13df0abb2c2f3785fcd581dfd969e9778ac8

commit 5d2c4009987a6b33a683d6cbf1ade560e1f5b59b
Author: Derek Murray <mrry@google.com>
Date:   Mon Nov 25 14:30:31 2019 -0800

    [tf.data] Use a more efficient source in MapBenchmark.
    
    Currently, we use `Dataset.from_tensors(0).repeat(None)` as the source of dummy
    data in MapBenchmark. Consuming this dataset involves repeatedly creating and
    destroying a TensorDataset iterator, and the cost of doing this dominates the
    MapDataset execution time (for small chains). Switching to a
    `Dataset.range(num_elements)` has much lower overhead per element.
    
    From running the benchmark on my workstation (with increased num_elements), the
    execution time of "MapBenchmark.chain_length_1_single_threaded" reduces by more
    than 50%:
    
    Before:
    
    entry {
      name: "MapBenchmark.chain_length_1_single_threaded"
      iters: 5
      wall_time: 1.71906495094e-06
      extras {
        key: "num_elements"
        value {
          double_value: 1000000.0
        }
      }
    }
    
    After:
    
    entry {
      name: "MapBenchmark.chain_length_1_single_threaded"
      iters: 5
      wall_time: 8.35798978806e-07
      extras {
        key: "num_elements"
        value {
          double_value: 1000000.0
        }
      }
    }
    
    PiperOrigin-RevId: 282434351
    Change-Id: I7f726be65af35c5401c8c9a54c0b84bf27b9fa0f

commit a858c19b0c10a89639a4897155952d8c3bbd26de
Author: Elena Zhelezina <elena.zhelezina@arm.com>
Date:   Wed Sep 18 17:56:26 2019 +0100

    New implementation of TANH/Sigmoid 16-bit activation functions using LUT.
    
    We think the reference functions for 16-bit activation are too complex for
    efficient implementation on resource constrained platforms and propose
    to replace the functions with a lookup table approach as follows:
    
    First rescale the input data to fixed range of -10.7 to +10.7
    Use a 256-entry lookup table for Sigmoid followed by linear interpolation
    to efficiently derive the result.
    
    The Sigmoid LUT table is used for the TANH function,
    because tanh(x) = 2*sigmoid(2*x) -1 and we take into account the symmetry is taked.
    
    The proposed reference kernel implementation also has higher accuracy than the existing one.
    On the current functions we measure a difference of up to 6.3 for sigmoid and 11.7 for
    tanh in quantized units compared to the floating point reference implementation over
    the 16-bit input range (representing -8.0 to +8.0). For the implementation of this patch we
    see the error reduced to less than 1.5 quantized units compared to floating point
    reference for both tanh and sigmoid.
    
    Change-Id: I4d1406928db65740c1750c9cd7bfffab30771419

commit 09659f8eea1a58ac22d74a34e15a4aae2557088c
Author: Derek Murray <mrry@google.com>
Date:   Wed Nov 20 11:23:43 2019 -0800

    Add ability for Executor subclasses to override the synchronous Run() method.
    
    This change enables executors that have a more efficient synchronous implementation to use that implementation instead of requiring a callback to be created, and performing atomic operations to notify completion. This change also updates DirectSession to invoke the synchronous Run() method when the session is configured to run the graph on the caller thread (e.g. by setting `ConfigProto.inter_op_parallelism_threads = -1`).
    
    PiperOrigin-RevId: 281562211
    Change-Id: I2e8a32a440a82a32516555be8b2f64b373c21289

commit 7ef97d3b0a6734f02f4418d683f2d9a3322bff5d
Author: River Riddle <riverriddle@google.com>
Date:   Tue Nov 12 13:03:39 2019 -0800

    NFC: Change DictionaryAttr::get(StringRef) to use binary search instead of a linear scan.
    
    The elements of a DictionaryAttr are guaranteed to be sorted by name, so we can use a more efficient lookup when searching for an attribute.
    
    PiperOrigin-RevId: 280035488
    Change-Id: I6ad9f499bd0f8a26c3993b8498c29e124c56c0af

commit 14225a9f2cf9033ec54ce9bd2462b0dec34f21fe
Author: River Riddle <riverriddle@google.com>
Date:   Tue Nov 12 13:03:39 2019 -0800

    NFC: Change DictionaryAttr::get(StringRef) to use binary search instead of a linear scan.
    
    The elements of a DictionaryAttr are guaranteed to be sorted by name, so we can use a more efficient lookup when searching for an attribute.
    
    PiperOrigin-RevId: 280035488

commit c3973c78f03c50d8514c14c2866ab30e708aea24
Author: Gaurav Jain <gjn@google.com>
Date:   Sat Oct 12 01:24:25 2019 -0700

    Rename internal_convert_to_tensor for performance
    
    Calling ops.internal_convert_to_tensor is more efficient than calling
    ops.convert_to_tensor due to skipping the deprecated_argument_lookup and
    also less python function calling overhead. We thus swap these functions
    names so we can optimize most code paths.
    
    PiperOrigin-RevId: 274321742

commit d3e4201098ce96873f5d5c2688435965d886b018
Author: Shanqing Cai <cais@google.com>
Date:   Thu Oct 3 06:41:48 2019 -0700

    [tfdbg] Add Python implementation of DebugEventsWriter
    
    - Based on pybind11 wrapping of the C++ implementation.
    - Add `WriteSerializedNonExecutionDebugEvent` and `WriteSerializedExecutionDebugEvent` to support efficient writing of protos at the interface between Python and C++
    - Change the two cyclic buffer deques to hold strings. This is also to support efficient writing at the interface between Python and C++.
    
    PiperOrigin-RevId: 272649638

commit 8a2d062f08cefee2b792aa0a6aade9074d48131d
Author: Allen Lavoie <allenl@google.com>
Date:   Mon Sep 23 13:45:51 2019 -0700

    Forwardprop: enforce nesting between gradient tapes and forward accumulators
    
    Previously only accumulators were prevented from watching jvps from "outer" accumulators, but GradientTape otherwise ends up watching all jvps for tensors/variables it's watching, meaning forward-over-back isn't efficient.
    
    This doesn't really fix the issue for functions, where there's a hacky "is the tape watching any jvps" flag which only works efficiently if there's a single GradientTape. Once we generate function gradients from tapes this change should apply to them too.
    
    PiperOrigin-RevId: 270751233

commit c5fe7ab411cc40cd9c8c44d3962f825b956500e0
Author: Jacques Pienaar <jpienaar@google.com>
Date:   Mon Sep 23 11:24:19 2019 -0700

    Avoid extra space when setting attributes on import
    
    interleaveComma adds space along with the comma, which requires that the space
    either be stripped or split with string variant used. This is both inefficient
    and easy to forget and end up with spaces in names.
    
    PiperOrigin-RevId: 270720594

commit a8fcbb6c9d8f53e786b708bff86c253224596b4d
Author: Tiezhen WANG <wangtz@google.com>
Date:   Tue Sep 17 18:59:00 2019 -0700

    TFLM: support logistic (sigmoid) floating path.
    
    The current impl for quantization scheme is going to be very inefficient without opdata suppoert. They will be supported in the following work.
    
    PiperOrigin-RevId: 269704427

commit e0e1efbe0811aa0913ad8400c532b33c76425427
Author: Gaurav Jain <gjn@google.com>
Date:   Thu Sep 5 15:15:06 2019 -0700

    Add incompatible_shape_error attribute to equal op
    
    When tensor equality is enabled, if there is an incompatible shape we
    currently throw and exception. Ideally we'd like to return False when
    calling __eq__ and True when calling __ne__. We thus modify the Equal
    and NotEqual ops to return a boolean upon a shape incompatibility. Due
    to this change the shape inference logic needs to be changed to either
    return a scalar bool if the shapes are incompatible, or else return an
    unknown shape to allow for either a boolean Tensor or scalar to be
    returned.
    
    Note the behavior of tf.math.equal & tf.math.not_equal is unchanged as
    they both use optimistic shape inference logic when dealing with unknown
    dimensions which allows for more efficient graphs rather than inserting
    Rank operations.
    
    This distinction between __eq__ & tf.math.equal is also found in numpy
    and as a result the tf.debugging.assert_equal and
    tf.debugging.assert_none_equal APIs needed to be change to utilize the
    numpy operations.
    
    PiperOrigin-RevId: 267466043

commit 4eabecafdd121ec6272d91a69b23b47c08fc373c
Author: Derek Murray <mrry@google.com>
Date:   Mon Aug 26 10:48:36 2019 -0700

    Implement `Operation._get_attr_{bool,int}()` for use in op wrappers.
    
    Generated op wrappers reflect on the attrs of the created `Operation` in order to record a gradient. The default `Operation.get_attr()` method involves protobuf serialization and deserialization. For a small type, such as a boolean, it is much more efficient to invoke the specialized `TF_OperationGetAttrBool()` directly.
    
    PiperOrigin-RevId: 265491188

commit a41d8a5506f78f820f0dcab5ed786d665ba7688f
Author: Xiao Yu <fishx@google.com>
Date:   Fri Aug 23 14:18:39 2019 -0700

    Make NodeNameMapping::Uniquify more efficient.
    
    PiperOrigin-RevId: 265133264

commit 25aca37ed6cf5649b94aa884082cfe7f6eccf434
Author: Derek Murray <mrry@google.com>
Date:   Thu Aug 22 09:44:21 2019 -0700

    Change `hasattr` in `Graph.get_collection()` to use try/except.
    
    The current implementation of `hasattr` in Python 2.7 and 3.6 calls `getattr` under the hood. Since we subsequently make a second call to get the attribute, it is more efficient to "ask for forgiveness than permission" in this case.
    
    PiperOrigin-RevId: 264855401

commit 439c0df6f0202e3644c41a17c12d2505870b44f8
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Aug 21 03:27:04 2019 -0700

    Introduce `indices_are_sorted` attribute for gather/scatter HLO
    
    If the attribute is set to `true` then the backend can assume that the
    gather/scatter indices supplied by the user are sorted what should enable
    the generation of more efficient code.
    
    If the attribute is set to `true` but the indices are not sorted then
    the behavior is implementation defined.
    
    PiperOrigin-RevId: 264574093

commit 2917ad1d24cc39a228eac5248ce5d56aafe73f2d
Author: Sergei Lebedev <slebedev@google.com>
Date:   Fri Aug 16 09:23:23 2019 -0700

    Ported tf_stack.extract_stack to C++
    
    This change also removes extract_stack_file_and_line because extract_stack
    is now efficient enough to be used ~everywhere.
    
    def f(n, callback):
      if n == 0:
        return callback()
      else:
        return f(n - 1, callback)
    
    >>> %timeit f(16, lambda: None)  # Baseline
    1000000 loops, best of 3: 1.09 ?s per loop
    
    Before:
    
    >>> %timeit f(16, tf_stack.extract_stack_file_and_line)
    100000 loops, best of 3: 17.7 ?s per loop
    >>> %timeit f(16, tf_stack.extract_stack)
    100000 loops, best of 3: 18.5 ?s per loop
    
    After:
    
    >>> %timeit f(16, tf_stack.extract_stack)
    100000 loops, best of 3: 3.89 ?s per loop
    
    PiperOrigin-RevId: 263784818

commit 679babcb89537e0df8763ca09bd1b7680d959878
Author: Bixia Zheng <bixia@google.com>
Date:   Tue Aug 13 11:38:18 2019 -0700

    Rewrote the implementation of the complex sqrt and rsqrt methods.
    
    The old implementations of sqrt and rsqrt just called the pow function, which
    was very inefficient.
    
    PiperOrigin-RevId: 263180636

commit ef11a4763452c84cfa7494c713a976475c5d9cca
Author: Blake Hechtman <blakehechtman@google.com>
Date:   Wed Aug 7 10:26:14 2019 -0700

    [XLA:CLIENT] Add a dense version of TorchGather since that is sometimes more efficient.
    
    PiperOrigin-RevId: 262163732

commit 464ff69893984e9b10736b93b37d95c0a8db9d89
Author: Lei Zhang <antiagainst@google.com>
Date:   Tue Aug 6 07:02:35 2019 -0700

    [spirv] Provide decorations in batch for op construction
    
    Instead of setting the attributes for decorations one by one
    after constructing the op, this CL changes to attach all
    the attributes for decorations to the attribute vector for
    constructing the op. This should be simpler and more
    efficient.
    
    PiperOrigin-RevId: 261905578

commit fabfe4cd17f1b7ee799a0776e9c03fc20b92b96b
Author: Lei Zhang <antiagainst@google.com>
Date:   Tue Aug 6 07:02:35 2019 -0700

    [spirv] Provide decorations in batch for op construction
    
    Instead of setting the attributes for decorations one by one
    after constructing the op, this CL changes to attach all
    the attributes for decorations to the attribute vector for
    constructing the op. This should be simpler and more
    efficient.
    
    PiperOrigin-RevId: 261905578

commit e6f75bb0e0b41bb67250b5ce98af8f5caf645f5f
Author: Taylor Robie <taylorrobie@google.com>
Date:   Wed Jul 31 16:40:12 2019 -0700

    Make traceback collection more efficient by decreasing the number of thread local lookups.
    
    PiperOrigin-RevId: 261023893

commit 5883d3a4dc0034172d38cffbb116af8810d9fedb
Author: Allen Lavoie <allenl@google.com>
Date:   Thu Jul 18 17:40:05 2019 -0700

    Fix forwardprop of a function inside a function
    
    We were recording the function call twice when graph building, and the first recording didn't have a gradient function registered. This change explicitly pauses the tape so we record once with record_operation, like we do executing eagerly.
    
    Adds a benchmark for forwardprop of a function call wrapped in a function. I'm planning to do exactly this to make it efficient from eager, so it's worth tracking (looks like it's fine at the moment, inlining/pruning makes it fast).
    
    (All reported in examples/second)
    Function benchmarks:
    forwardprop_in_defun_matmul_100_by_784_CPU           2846
    forwardprop_in_defun_matmul_256_by_2096_CPU           486
    forwardprop_in_defun_of_defun_matmul_100_by_784_CPU  2950
    forwardprop_in_defun_of_defun_matmul_256_by_2096_CPU  483
    
    Eager benchmarks:
    forwardprop_matmul_100_by_784_CPU                    1548
    forwardprop_matmul_256_by_2096_CPU                    430
    forwardprop_of_defun_matmul_100_by_784_CPU           1006
    forwardprop_of_defun_matmul_256_by_2096_CPU           183
    
    PiperOrigin-RevId: 258879462

commit 4499d732014ef7fec5fd1fb418dc46751b4fb60d
Author: Derek Murray <mrry@google.com>
Date:   Wed Jul 17 11:14:10 2019 -0700

    Avoid copying a FunctionDef in CapturedFunction.
    
    The recently added `FunctionLibraryDefinition::CopyFunctionDefFrom()` method performs a more efficient shallow copy.
    
    PiperOrigin-RevId: 258604819

commit 498df5d8c4a48eca93e74d43fbaaad8004d4a04a
Author: Taylor Robie <taylorrobie@google.com>
Date:   Wed Jul 17 09:20:58 2019 -0700

    Update keras v2 optimizers to reuse coefficients which are shared across all updates, which reduces the total number of ops created by between 5% (for simple optimizers such as SGD and Adagrad) and 25% (for complicated optimizers such as Adam and NAdam). Separate copies are made for each device and dtype.
    
    The effect of this change on run time is fairly minimal since Grappler is expected to consolidate most of these ops; however it does improve graph construction time.
    
    PiperOrigin-RevId: 258581998

commit 4acec889a11490b3d6903bba0604f013b1f60e1c
Author: Alex Zinenko <zinenko@google.com>
Date:   Mon Jul 15 06:40:07 2019 -0700

    Introduce loop coalescing utility and a simple pass
    
    Multiple (perfectly) nested loops with independent bounds can be combined into
    a single loop and than subdivided into blocks of arbitrary size for load
    balancing or more efficient parallelism exploitation.  However, MLIR wants to
    preserve the multi-dimensional multi-loop structure at higher levels of
    abstraction. Introduce a transformation that coalesces nested loops with
    independent bounds so that they can be further subdivided by tiling.
    
    PiperOrigin-RevId: 258151016

commit acab6a20512f29be48b8252db00a388d2c57c29c
Author: Allen Lavoie <allenl@google.com>
Date:   Fri Jul 12 11:02:54 2019 -0700

    Use a tf.function to more efficiently compute op jvps
    
    Allows the unused backward computation to be pruned out.
    
    Does not change custom_gradient or function forward-mode computations.
    
    Some fiddling with the memory checking on the unit tests, since tf.function creates persistent symbolic Tensors the first time it's called. This means we need to do warmup runs and ignore Tensors allocated there.
    
    Forward gradients still need some followups after this:
      - Functions should have a special-cased forward function so that they're efficient when executing eagerly.
      - Watching variables on an accumulator should be possible
    
    After those the remaining case is custom gradients, which are probably fine to leave as they are for now (they work, they're just a bit less efficient than they could be if the user provided a jvp or told us the code was safe to wrap in a tf.function).
    
    From //tensorflow/python/eager:benchmarks_test:
    
    benchmark_forwardprop_in_defun_matmul_256_by_2096_CPU 487 examples/second no change
    benchmark_forwardprop_matmul_256_by_2096_CPU          406 examples/second 1.6x speedup
    benchmark_forwardprop_of_defun_matmul_256_by_2096_CPU 176 examples/second no change
    
    benchmark_forwardprop_in_defun_matmul_100_by_784_CPU 2872 examples/second no change
    benchmark_forwardprop_matmul_100_by_784_CPU          1766 examples/second 1.4x speedup
    benchmark_forwardprop_of_defun_matmul_100_by_784_CPU  909 examples/second no change
    
    PiperOrigin-RevId: 257832992

commit 572db7bf76c4f5caea9db1136bfc6532c22b0b38
Author: George Karpenkov <cheshire@google.com>
Date:   Fri Jun 21 10:10:18 2019 -0700

    [XLA] Provide a more generic infrastructure to pass may-alias hints
    
    Currently, may-alias hints can be passed from the compiler to the buffer assignment
    through the FusionCanShareFunction callback.
    This has a number of disadvantages:
    
     - Only aliasing inside fusion is supported. It's often desirable to alias
       inside custom calls, which have efficient inout parameter implementations.
    
     - FusionCanShareFunction returns a boolean, which requires an all-or-nothing
       approach: either the function returns whether aliasing is permitted,
       or the function is not passed at all.
    
    This change replaces FusionCanShareFunction with MayAliasHint callback,
    which solves these problems:
    
     - MayAliasHint returns absl::optional<bool>, which allows the callback to say
       "I don't know", delegating to the default behavior.
    
     - The callback is called outside of fusion, allowing aliasing inside non-fused
       instructions.
    
    PiperOrigin-RevId: 254418380

commit 8efc1dadae1f4b4472122a2923abafb075483204
Author: Mehdi Amini <aminim@google.com>
Date:   Tue Jun 18 09:38:16 2019 -0700

    Add a setAttrList() method on mlir::Operation
    
    This is an efficient method to copy attributes from one operation to
    another.
    
    PiperOrigin-RevId: 253806004

commit fd6e9f41c073511cedafe323619d1e0c71348aa4
Author: River Riddle <riverriddle@google.com>
Date:   Thu Jun 13 13:22:32 2019 -0700

    Add several utility 'getValues<T>' functions to DenseElementsAttr that return ranges as opposed to filling a SmallVector. This is much more efficient for the general case and allows for avoiding constructing APInt/APFloat/Attribute when possible.
    
    PiperOrigin-RevId: 253092550

commit 222df6844f8621706049bfd9f7e16cbadf72e8ed
Author: Anudhyan Boral <anudhyan@google.com>
Date:   Tue Jun 18 00:13:59 2019 -0700

    Register GPU kernels for EinsumOp.
    
    This change is mostly boilerplate to make the nvcc compiler happy. Most of the heavy lifting in EinsumOp is done by BatchMatmul/Reduction functors and Eigen Tensor Ops which already have GPU kernels defined for them. This lets us easily obtain an efficient implementation on the GPU.
    
    PiperOrigin-RevId: 253736113

commit b9fefcb525fd25629f8909c4c8efc24da84a52c9
Author: Stella Laurenzo <laurenzo@google.com>
Date:   Wed May 15 15:04:20 2019 -0700

        Upstreaming Quantizer tool (part 2).
    
        This adds some additional core types and utilities, notably the constraint analysis graph (CAG) structures, associated metadata and configuration policy object base class.
    
        The CAG is not particularly memory efficient as it stands now. I had started some work to turn it into a form that could be better managed by a bump pointer allocator but abandoned that for now in favor of having something that does semantically what I was going for as a starting point.
    
    --
    
    PiperOrigin-RevId: 248413133

commit a0077aff0be3aeec9df388677a07f988155a2f50
Author: Bixia Zheng <bixia@google.com>
Date:   Wed May 15 13:45:00 2019 -0700

    [XLA:CPU/GPU] Adjust fusion heuristic to allow the fusing of Philox HLO
    expansion.
    
    Change the factor used by IsFusedIrEmitterInefficient from 8 to 15.
    
    PiperOrigin-RevId: 248396671

commit 802aa09f9eef556b3cfa943bdc7afe4b24694ff3
Author: Eugene Zhulenev <ezhulenev@google.com>
Date:   Fri May 3 15:55:22 2019 -0700

    Use partial packets for coefficient finalization in gemm_pack_colmajor
    
    PiperOrigin-RevId: 246588955

commit eb03daf8c03cc5c7737afaa1123347976cc1eb35
Author: Sergei Lebedev <slebedev@google.com>
Date:   Thu Apr 25 13:40:54 2019 -0700

    Removed _keras_mask from EagerTensor
    
    It does not have to always be part of an EagerTensor and could instead
    be stored in a __dict__.
    
    Note that as a side-effect
    * an EagerTensor with a _keras_mask always has a materialized __dict__ and
      consumes ~280 bytes more;
    * EagerTensor._keras_mask lookup is slightly less efficient.
    
    PiperOrigin-RevId: 245298840

commit 144412e1d2236d873dea271a96cc507c79959c07
Author: Benoit Jacob <benoitjacob@google.com>
Date:   Wed Apr 24 07:20:10 2019 -0700

    Introduce a CpuBackendContext class, to be passed to any runtime
    op implementation on CPU that may need a context object (e.g.
    for an optimized GEMM implementation, or to get a thread-pool).
    So far we had been either passing backend-specific objects
    (such as gemmlowp::GemmContext) entrenching usage of specific
    libraries (gemmlowp), or we had been omitting to pass any such
    object, which was also in a way entrenching usage of specific
    libraries not using such context objects (e.g. Eigen for GEMM).
    
    This CL migrates for now only some GEMM-using ops to taking a
    CpuBackendContext, that is: FullyConnected, LstmCell, Conv.
    A subsequent CL will migrate other ops.
    
    Once all ops take a CpuBackendContext, we will be able to switch
    their implementation backends much more easily and incrementally.
    In particular, this is one of the main steps to enable the
    integration of ruy as one implementation path.
    
    The main difficulty in this CL was how to perform this change
    of signature of the runtime ops functions, without breaking
    dependents. Indeed, these runtime ops are directly used by much
    more code than just the TFLite interpreter, whence the existing
    legacy_* ops files where we have been conserving legacy signatures
    as we changed the signatures used by TFLite.
    
    To limit this difficulty to only the optimized ops functions, this
    CL changes reference ops to no longer take any context argument.
    They didn't use it anyway. Dropping that now removes the hassle
    of doing the gemmlowp -> cpu_backend transition in reference code.
    
    In optimized ops, we make such a gemmlowp -> cpu_backend wholesale
    transition for the aforementioned op types (other ops to follow),
    and for compatibility we keep old gemmlowp signatures in
    legacy_* files. This results in a substantial amount of lines added
    in legacy_*, as we choose to keep that old code around as an independent
    implementation rather than as just calling into the new signatures,
    as we have done in the past for other legacy functions. The rationale
    is that there is no alternative that will be regression-free for all
    legacy users (so even if we tolerate incurring some regression,
    alternatives are at a minimum difficult to pass through regression
    tests). Indeed:
     - for legacy float paths not taking any context argument, making
       such paths call into new paths using a cpu_backend_context would
       have required either:  creating short-lived cpu_backend_context
       objects, which would be inefficient with new implementations
       strongly relying on such context objects being reused (like ruy);
       using a global singleton guarded by a lock, which would be
       inefficient in multithreaded use cases (most Android JNI users
       implicitly are; some applications use explicit multithreading
       around NN inference); or using a thread-local cpu_backend_context
       which would have surprising overhead/footprint/behavior in
       implicitly-multithreaded use cases such as again Android JNI.
     - for legacy 8bit paths taking a gemmlowp context argument, the
       situation was more tractable, we could have allowed constructing
       cpu_backend_context objects wrapping around an existing
       gemmlowp_context. However, that would still have had the disadvantage
       of forcing to keep gemmlowp code in the new cpu_backend_context
       code, negating some of the binary-size improvements that we
       are otherwise hoping to get from a ruy transition.
    
    PiperOrigin-RevId: 245039802

commit 44bed497bd1c4e1c7cd09147a394b2ab4ef85ec8
Author: Youlong Cheng <ylc@google.com>
Date:   Mon Apr 22 18:17:14 2019 -0700

    Build efficient all-reduce ring for TPU model parallelism.
    
    For non-model parallelism case, TF2XLA bridge reorders the replica ids to build an efficient all-reduce ring.
    For model parallelism case, we disabled this reordering for correctness since we don't want to mess up user's input pipeline or weight shading, etc (see go/tpu-ids).
    
    This guarantees the correctness but it sacrifices the performance, advanced model parallelism users(for example MeshTensorFlow) chose to build the efficient ring by themselves.
    
    With this cl, we are going to move the ring build logic to device_assignment. This usually happens at the beginning of TF graph constructing, user will know how we map the replica id to physical coordinate and build the input-pipeline accordingly.
    
    PiperOrigin-RevId: 244773805

commit 4c540c1b8eec589f370f10c0fa197fcb31554fde
Author: James Keeling <jtkeeling@google.com>
Date:   Fri Apr 12 09:33:23 2019 -0700

    Stop using tf_stack.convert_stack in registry.py
    
    Stack details can be found by calling tf_stack.extract_stack. Additional info (for example the line contents) can then be found by calling tf_stack.convert_stack on the output. However the code in registry.py was doing this despite only needing the output from extract_stack. This is inefficient, as convert_stack requires calling stat on source code files, which can be very expensive.
    
    This change:
    1) Just uses extract_stack, without convert_stack
    2) Adds a limit to the extract_stack call
    3) Fixes a rare bug where the location tag was set to a string rather than a tuple when the stack is not available. Note that this never happens in normal Python usage, but can happen when other languages bind to Python (for example, in external R bindings).
    
    PiperOrigin-RevId: 243278293

commit 8bfa8d2c1227162f9dbaf5135c8c9d5a80e6eed2
Author: James Keeling <jtkeeling@google.com>
Date:   Fri Apr 12 09:25:57 2019 -0700

    Use tf_stack in tf_decorator.py.
    
    The implementation of extract_stack in tf_stack is more efficient than the built-in one, because it does not stat source code files unnecessarily. I add a limit argument to allow the callsite to remain the same.
    
    Here are some timings in a colab. Note that the real-world performance improvement will probably be larger, as more files will need to be accessed and they are likely to be on a slower filesystem.
    BEFORE: 10000 loops, best of 3: 117 ?s per loop
    AFTER: 100000 loops, best of 3: 4.43 ?s per loop
    PiperOrigin-RevId: 243277147

commit 34fd2a5e9bcdb40957ece90fec46a37e6e9248b2
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Apr 8 04:54:48 2019 -0700

    Fix crash when forget_layer_norm_coefficients tensor has no value
    
    PiperOrigin-RevId: 242442133

commit 6bd6a4d057dbf2f40f11f12c57b2107184f9b2b8
Author: Lei Zhang <antiagainst@google.com>
Date:   Wed Mar 20 09:01:58 2019 -0700

    Change getBroadcastedShape() to return result shape via parameter
    
    This is a more efficient way than returning SmallVector directly.
    
    PiperOrigin-RevId: 239407024

commit 889b8aae707d1322f44fc77284704d22687b02aa
Author: Uday Bondhugula <bondhugula@google.com>
Date:   Tue Jan 22 13:58:52 2019 -0800

    Allocate private/local buffers for slices accurately during fusion
    
    - the size of the private memref created for the slice should be based on
      the memref region accessed at the depth at which the slice is being
      materialized, i.e., symbolic in the outer IVs up until that depth, as opposed
      to the region accessed based on the entire domain.
    
    - leads to a significant contraction of the temporary / intermediate memref
      whenever the memref isn't reduced to a single scalar (through store fwd'ing).
    
    Other changes
    
    - update to promoteIfSingleIteration - avoid introducing unnecessary identity
      map affine_apply from IV; makes it much easier to write and read test cases
      and pass output for all passes that use promoteIfSingleIteration; loop-fusion
      test cases become much simpler
    
    - fix replaceAllMemrefUsesWith bug that was exposed by the above update -
      'domInstFilter' could be one of the ops erased due to a memref replacement in
      it.
    
    - fix getConstantBoundOnDimSize bug: a division by the coefficient of the identifier was
      missing (the latter need not always be 1); add lbFloorDivisors output argument
    
    - rename getBoundingConstantSizeAndShape -> getConstantBoundingSizeAndShape
    
    PiperOrigin-RevId: 230405218

commit 46be3eaf583feb1dfda4a2fb46ddb57b3c90dcd0
Author: Uday Bondhugula <bondhugula@google.com>
Date:   Thu Dec 13 10:47:09 2018 -0800

    FlatAffineConstraints - complete TODOs: add method to remove duplicate /
    trivially redundant constraints. Update projectOut to eliminate identifiers in
    a more efficient order. Fix b/120801118.
    
    - add method to remove duplicate / trivially redundant constraints from
      FlatAffineConstraints (use a hashing-based approach with DenseSet)
    - update projectOut to eliminate identifiers in a more efficient order
    
    (A sequence of affine_apply's like this (from a real use case) finally exposed
    the lack of the above trivial/low hanging simplifications).
    
      for %ii = 0 to 64 {
        for %jj = 0 to 9 {
          %a0 = affine_apply (d0, d1) -> (d0 * (9 * 1024) + d1 * 128) (%ii, %jj)
          %a1 = affine_apply (d0) ->
            (d0 floordiv (2 * 3 * 3 * 128 * 128),
            (d0 mod 294912) floordiv (3 * 3 * 128 * 128),
            (((d0 mod 294912) mod 147456) floordiv 1152) floordiv 8,
            (((d0 mod 294912) mod 147456) mod 1152) floordiv 384,
            ((((d0 mod 294912) mod 147456) mod 1152) mod 384) floordiv 128,
            (((((d0 mod 294912) mod 147456) mod 1152) mod 384) mod 128)
              floordiv 128) (%a0)
          %v0 = load %in[%a1#0, %a1#1, %a1#3, %a1#4, %a1#2, %a1#5]
            : memref<2x2x3x3x16x1xi32>
        }
      }
    
    - update FlatAffineConstraints::print to print number of constraints.
    
    PiperOrigin-RevId: 225397480

commit 3c6933d9bc6abc5d6066b84c35fe96092fb4cf26
Author: Uday Bondhugula <bondhugula@google.com>
Date:   Tue Oct 30 13:45:10 2018 -0700

    FlatAffineConstraints API update - additional methods
    
    - add methods addConstantLowerBound, addConstantUpperBound, setIdToConstant,
      addDimsForMap
    - update coefficient storage to use numReservedCols * rows instead of numCols *
      rows (makes the code simpler/natural; reduces movement of data when new
      columns are added, eliminates movement of data when columns are added to the
      end).
    
    (addDimsForMap is tested in the child CL on memref bound checking: cl/219000460)
    
    PiperOrigin-RevId: 219358376

commit ea5c3dc837e49032c6fd19c4e1266cc61c7ef834
Author: Chris Lattner <clattner@google.com>
Date:   Tue Aug 21 08:42:19 2018 -0700

    Finish support for function attributes, and improve lots of things:
     - Have the parser rewrite forward references to their resolved values at the
       end of parsing.
     - Implement verifier support for detecting malformed function attrs.
     - Add efficient query for (in general, recursive) attributes to tell if they
       contain a function.
    
    As part of this, improve other general infrastructure:
     - Implement support for verifying OperationStmt's in ml functions, refactoring
       and generalizing support for operations in the verifier.
     - Refactor location handling code in mlir-opt to have the non-error expecting
       form of mlir-opt invocations to report error locations precisely.
     - Fix parser to detect verifier failures and report them through errorReporter
       instead of printing the error and crashing.
    
    This regresses the location info for verifier errors in the parser that were
    previously ascribed to the function.  This will get resolved in future patches
    by adding support for function attributes, which we can use to manage location
    information.
    
    PiperOrigin-RevId: 209600980

commit c1faf6658e1b74700b159645f64f95befeaa0195
Author: Uday Bondhugula <bondhugula@google.com>
Date:   Thu Jul 19 14:08:50 2018 -0700

    Simplify affine binary op expression class hierarchy
    
    - Drop sub-classing of affine binary op expressions.
    - Drop affine expr op kind sub. Represent it as multiply by -1 and add. This
      will also be in line with the math form when we'll need to represent a system of
      linear equalities/inequalities: the negative number goes into the coefficient
      of an affine form. (For eg. x_1 + (-1)*x_2 + 3*x_3 + (-2) >= 0). The folding
      simplification will transparently deal with multiplying the -1 with any other
      constants. This also means we won't need to simplify a multiply expression
      like in x_1 + (-2)*x_2 to a subtract expression (x_1 - 2*x_2) for
      canonicalization/uniquing.
    - When we print the IR, we will still pretty print to a subtract when possible.
    
    PiperOrigin-RevId: 205298958

commit 6027fdf21d7f623286d831a88a9e32af63b215d4
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Mar 12 09:18:48 2019 -0700

    Add python wrapper for xla::Einsum in tf2xla
    
    Using xla::Einsum is more efficient in some cases as it dosn't do
    any unnecessary reshapes.
    
    PiperOrigin-RevId: 238029298

commit 517112e77fb68d164fb67a62ad57baadc6a20bc4
Author: Justin Lebar <jlebar@google.com>
Date:   Mon Feb 25 15:08:22 2019 -0800

    [XLA:GPU] Enhance for-loop analysis.
    
    XLA:GPU's "for-loop" analysis looks at `while` loops and tries to determine
    whether they run a constant number of times.  If so, the loop is emitted as a
    "ForThunk", which can be run more efficiently on the GPU than the more general
    WhileThunk.
    
    A problem with our for-loop analysis is that it runs very late in the pass
    pipeline, after fusion, layout-assignment, copy-insertion, etc.  At this point
    it's challenging to pattern-match the HLO to figure out whether or not a loop
    is a bona fide `for` loop.  We can try to brute-force the loop count, but that
    only works for relatively small loops.
    
    This patch makes two changes:
    
     - Moves `for` loop matching to a separate pass, run before layout assignment
       or fusion.  The loop count is stored in a backend-config on the while
       instruction.
    
     - Adds pattern-matching machinery for loop counts.
    
    PiperOrigin-RevId: 235610452

commit 398fce03079f77d0933f941cba3de080a3116859
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Feb 15 12:05:22 2019 -0800

    Add a Euclidean norm reduction kernel. This implements a fused sqrt(reduce_sum(x * conj(x))) kernels for CPU (using Eigen) and GPU (using CUB), which is more efficient than the composite implementation at the TF level. It will also be easier to avoid the issue of producing NaNs in the gradient at the origin.
    
    Adds tf.math.reduce_euclidian_norm() Python interface to call the fused reduction kernel directly.
    
    Gradients will be added in a followup change.
    
    PiperOrigin-RevId: 234188431

commit 7b37024e464c400dd8065373f1c3d8792ed5f42b
Author: Justin Lebar <jlebar@google.com>
Date:   Thu Jan 31 18:45:52 2019 -0800

    [XLA] Fix erf for f16 inputs, and improve precision in f32.
    
    Switch to using the f32 implementations.  Previously we were using the f64
    implementation with all constants truncated down to f32.
    
    Part of the problem is that it was computing
    
      erf(x) = x * polynomial1(x^2) / polynomial2(x^2)
    
    and assuming that we had enough precision that neither of the polynomials would
    overflow.  On f16, polynomial2 would overflow to inf even for quite reasonable
    values of x (e.g. 0.79), thus causing the result to be 0 incorrectly.
    
    This may reduce precision for f64, but that's a trade-off we're willing to make
    (especially since we were using f32 coefficients *anyway*).
    
    erf and erfc also now call each other when their inputs are out of range.
    
    Relevant to #25052.
    
    PiperOrigin-RevId: 231896451

commit 91ffe999b88e665fa4c6e6899cb8170cbd6fccb8
Author: Lukas Geiger <lukas.geiger94@gmail.com>
Date:   Mon Jan 28 14:05:00 2019 +0100

    Use efficient squared_difference instead of square(diff)

commit 9b91ea70e84501d0e264d783d54907bac8cca92b
Author: Sergio Guadarrama <sguada@google.com>
Date:   Sat Jan 26 12:43:32 2019 -0800

    Use more efficient squared_difference instead of square(diff)
    
    PiperOrigin-RevId: 231064313

commit 57d4745d24195699f93d1674171a03345269d2c1
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Jan 9 12:10:02 2019 -0800

    Compute reshape permutation statically.
    
    Aside from presumably being more efficient generally, this enables using the routines in this file inside of XLA. (which currently throws an error
    `Input 0 to InvertPermutation operator must be a compile-time constant.`)
    
    PiperOrigin-RevId: 228563007

commit 106b3c183546d2f4bde8c209ef9d01ba0fb2904b
Author: Jiri Simsa <jsimsa@google.com>
Date:   Mon Jan 7 14:55:46 2019 -0800

    [tf.data] Implementation of a "standalone" C++ API that encapsulates TensorFlow runtime internals. The new API facilitates efficient execution of a dataset input pipeline (represented as a GraphDef) in C++.
    
    PiperOrigin-RevId: 228241484

commit 9585116b80c985a790fb21fe7eb00def7916d092
Author: Eugene Zhulenev <ezhulenev@google.com>
Date:   Wed Dec 26 10:43:22 2018 -0800

    Add GraphTopologyView to efficiently traverse node-to-node connections.
    + Remove SimpleGraphView from topological sorting.
    
    PiperOrigin-RevId: 226932668

commit 350791003de42dbb17c53474a677b108f473b0ba
Author: Dan Moldovan <mdan@google.com>
Date:   Wed Dec 12 11:52:20 2018 -0800

    Reduce the cost of serializing ConversionOptions to code, by using a more efficient inspect.util.getqualifiedname, reducing its max_depth and falling back to caching the value in the namespace. The latter step makes it more difficult to run the generated code afterwards, but it should in turn speed up the conversion process. This also adds an extra check to tf_decorator to improve robustness.
    
    PiperOrigin-RevId: 225226256

commit 7fa9d6a1f2d5916f3873ea38ed59c69af4c54e70
Author: Derek Murray <mrry@google.com>
Date:   Tue Dec 4 08:52:57 2018 -0800

    Make `TensorBuffer::data()` non-virtual and move the pointer into the base class.
    
    All existing `TensorBuffer` subclasses already store a pointer to
    their buffer. Accessing that pointer by calling a virtual method is
    inefficient. We currently generate the following instruction sequence
    at the callsite (when compiling for x86_64 with a recent version of Clang):
    
    1dd002:         mov    (%rdi),%rax                 tensor.h:655
    1dd005:         callq  *0x10(%rax)                 tensor.h:655
    
    ...and the following implementation for `Buffer::data()`:
    
    236520:         mov    0x10(%rdi),%rax             tensor.h:888
    236524:         retq                               tensor.h:888
    
    With this change, we generate a single `mov` instruction inline at the call
    site, and avoid any branching.
    
    PiperOrigin-RevId: 223985477

commit 411cc508e1a63d648855014f0912663e780a0ff9
Author: Derek Murray <mrry@google.com>
Date:   Tue Nov 27 10:48:57 2018 -0800

    Optimize accesses to the "begin" and "end" tensors in `ValidateStridedSliceOp()`.
    
    1. Hoist the call to `Tensor::flat<T>()` on the begin and end tensors out of the loop over dimensions. This avoids revalidating the tensor type, shape, and alignment once per dimension.
    2. Use `Tensor::vec<T>()` instead of `Tensor::flat<T>()` because we have already checked that the corresponding input tensors are vectors, and the vector codepath is more efficient.
    
    PiperOrigin-RevId: 223017951

commit 2e902b354bcea0d7eb1319de2beeaf603c7d5ee5
Author: Igor Ganichev <iga@google.com>
Date:   Thu Nov 15 13:25:33 2018 -0800

    Support attributes on tf.function calls
    
    While FunctionDef has an OpDef that can contain attribute
    specification for each function, we don't use per function
    attribute specificaton at the moment. Instead we have just two
    attributes "executor_type" and "config" shared by all functions.
    These attributes are declared in PartitionedCall.
    
    This CL makes regular functions calls (not going through
    PartitionedCall) have the same two attributes. It is done
    by hardcoding them in eager/attr_builder. This is a bit hacky
    but quick and efficient. A cleaner alternative could be to have
    TF_GraphToFunction always add these attributes to OpDef of every
    FunctionDef it builds. Hardcoding these attributes in C API
    instead of letting users specify them from Python should be fine
    because they are consumed by FunctionLibraryRuntime.
    
    PiperOrigin-RevId: 221678560

commit ee61c166bb984935caa71053fe765ac9a9a724fc
Author: Dimitris Vardoulakis <dimvar@google.com>
Date:   Tue Nov 13 10:45:31 2018 -0800

    [TF:XLA] New HLO pass that detects a pattern of AllReduce and CrossReplicaSum and rewrites it, to use an efficient CrossReplicaSum implementation that fully utilizes the interconnect bandwidth.
    
    PiperOrigin-RevId: 221293415

commit 366dd0eabccbf5bf968c16cb985e79efffdf098a
Author: Derek Murray <mrry@google.com>
Date:   Tue Nov 6 07:36:30 2018 -0800

    Add a more efficient constructor for scalar `Tensor` objects in host memory.
    
    [tf.data] Optimize the creation of tensors in `tf.data.Dataset.range()`.
    
    This change improves the range benchmark from 148.7 ns/element to 122.4 ns/element.
    
    PiperOrigin-RevId: 220279090

commit e72c9ebe78a119715541f40ea99b1a8c89639968
Author: Todd Wang <toddwang@gmail.com>
Date:   Wed Oct 24 17:46:03 2018 -0700

    1.12.0-rc2 cherry-pick request: Various XLA scatter improvements. (#23235)
    
    * [XLA] Update Tf2Xla bridge to use Scatter HLO.
    
    PiperOrigin-RevId: 215687800
    
    * [XLA:GPU] Add an implementation of scatter for GPU
    
    This simple has a kernel that runs on every element of the updates tensor,
    figure out the right indices to perform the update, and applies it with an
    atomic operation.
    
    Currently we emit a CAS for plain (i.e. non-add) updates, which is inefficient.
    Also TuplePointsToAnalysis doesn't know that it should alias the operand and
    output buffers of a scatter, which would avoid a copy.
    
    PiperOrigin-RevId: 216412467
    
    * [XLA] Allow scatter to share the operand buffer with the output
    
    This avoids a copy.
    
    PiperOrigin-RevId: 216437329
    
    * [XLA:GPU] Elide the SequentialThunk when emitting scatter with no copy
    
    We have a 1-element thunk sequence if we're not copying. That's still two
    thunks and hlo profiling gets confused if it sees two thunks for the same
    instruction and one of them claims to be the whole instruction.
    
    PiperOrigin-RevId: 216448063
    
    * [XLA:GPU] Allow input fusion into scatter
    
    We fuse everything into the scatter now, and emit two kernels. The first kernel
    fills the output buffer with the computation fused into the scatter operand.
    The second kernel is a regular scatter, which also contains the fused
    operations from the updates and scatter_indices inputs.
    
    PiperOrigin-RevId: 216624225
    
    * [XLA:GPU] Adding a test case for Scatter where GPU implementation fails.
    
    PiperOrigin-RevId: 216798034
    
    * [XLA:GPU] Fix scatter oob check computation
    
    This was comparing the index after adding it to the window, and then comparing
    against the window dimension. This means that the bounds check was only correct
    for the first element of a window. Instead compare the scatter index, which is
    the same for all elements of a window.
    
    PiperOrigin-RevId: 216921512
    
    * [XLA:GPU] Elide tuple roots of the entry computation
    
    The tuple buffer is never read, so stop emitting code to fill it. A typical
    root tuple consists of a H2D memcpy and a host callback, both of which are
    somewhat slow.
    
    This helps tiny models and inference benchmarks, where the host/device syncs
    can be a significant part of the runtime of the entire computation.
    
    PiperOrigin-RevId: 216968475

commit 5d9a7fdf4f02c2db487a03e7ad2d520f8847c4e3
Author: Benjamin Kramer <kramerb@google.com>
Date:   Tue Oct 9 13:32:24 2018 -0700

    [XLA:GPU] Add an implementation of scatter for GPU
    
    This simple has a kernel that runs on every element of the updates tensor,
    figure out the right indices to perform the update, and applies it with an
    atomic operation.
    
    Currently we emit a CAS for plain (i.e. non-add) updates, which is inefficient.
    Also TuplePointsToAnalysis doesn't know that it should alias the operand and
    output buffers of a scatter, which would avoid a copy.
    
    PiperOrigin-RevId: 216412467

commit 23a88ec5e913ba7086a9aef57875447ccf96e4b5
Author: Blake Hechtman <blakehechtman@google.com>
Date:   Thu Sep 20 15:42:23 2018 -0700

    It is more computationally efficient to represent resize bilinear as a
    depthwise convolution instead of a full convolution now that it exists in XLA.
    
    PiperOrigin-RevId: 213896333

commit 76a5936cd283d9a32c89635577b2da9c8e46785b
Author: Derek Murray <mrry@google.com>
Date:   Thu Sep 6 14:13:56 2018 -0700

    Enable unused "_Arg" nodes to be pruned from a function body.
    
    Previously, because "_Arg" nodes are considered to be "stateful", these nodes were unconditionally included in the seed set of nodes for pruning a function body. Since an "_Arg" node has no visible side effect, we can safely prune these, which makes small projection functions (like `lambda x, y: y`) more efficient.
    
    PiperOrigin-RevId: 211867380

commit 680e1754b49362858cda8fd6cea52e1cc4c41e6b
Author: Derek Murray <mrry@google.com>
Date:   Wed Sep 5 17:25:13 2018 -0700

    Deprecate `tf.train.input_producer()` and related APIs.
    
    These APIs are based on queue runners, which have been deprecated and will be removed in TensorFlow 2.0. They have been replaced with `tf.data.Dataset`, which provides a more efficient version of the same functionality.
    
    PiperOrigin-RevId: 211727844

commit ebf6d259fd4c57114c17646e40fdcfa4a1472972
Author: Derek Murray <mrry@google.com>
Date:   Wed Sep 5 15:15:20 2018 -0700

    Deprecate `tf.ReaderBase` and related APIs.
    
    These APIs are based on queue runners, which have been deprecated and will be removed in TensorFlow 2.0. They have been replaced with `tf.data.Dataset`, which provides a more efficient version of the same functionality.
    
    PiperOrigin-RevId: 211708268

commit a3c1ccd1da64040eeb139a0c6c1fc34ae46d7290
Author: Derek Murray <mrry@google.com>
Date:   Wed Sep 5 14:33:37 2018 -0700

    Deprecate `tf.train.batch()` and related APIs.
    
    These APIs are based on queue runners, which have been deprecated and will be removed in TensorFlow 2.0. They have been replaced with `tf.data.Dataset`, which provides a more efficient version of the same functionality.
    
    PiperOrigin-RevId: 211700442

commit d29eb6d1c9d1e4b2f601864f53878674f219fe6f
Author: Allen Lavoie <allenl@google.com>
Date:   Tue Sep 4 14:03:08 2018 -0700

    Remove reference cycles when constructing distribution objects
    
    self -> _parameters -> self cycles were creating work for Python's garbage collector in training loops, where Distribution objects may be created repeatedly when executing eagerly. This CL just fixes that narrow memory issue; I'm not convinced dict(locals()) is super efficient, so we may want to follow up on that for performance.
    
    Adds a few unit tests tests with run_test_in_graph_and_eager_modes(assert_no_eager_garbage=True). It'd be nice to expand this coverage over time.
    
    Includes a small test_util simplification to support this (TFP tests don't like reset_default_graph for some reason). Testing for cycles in the TFP repo will need to wait on the Normal changes from the TF repo syncing.
    
    PiperOrigin-RevId: 211520394

commit f689cb82869b75a4bf1374fe96ef8ffa9d87acc9
Author: Derek Murray <mrry@google.com>
Date:   Thu Aug 30 13:00:08 2018 -0700

    Deprecate `tf.train.QueueRunner` and related APIs.
    
    Queue runners will be removed in TensorFlow 2.0. They have been replaced with `tf.data` input pipelines, which provide a more efficient version of the same functionality.
    
    PiperOrigin-RevId: 210964268

commit 11f1dab4fce23c73073e32cda910a2a1a87c394f
Author: Alexandre Passos <apassos@google.com>
Date:   Thu Aug 30 09:49:11 2018 -0700

    StridedSlice gradient more efficient in tfe.
    
    PiperOrigin-RevId: 210927458

commit 729e39b1a4f0f7a6b3e35a04bf8bbba5e921862b
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Aug 29 20:23:07 2018 -0700

    Improve the GPU memory use discipline of CollectiveReduce.
    
    GPU memory allocation can be done in one of two modes: efficient (but
    complex and therefore somewhat risky) or conservative (simpler, but less
    efficient).  The main difference is that 'efficient' allocation allows
    the same memory area to be allocated to mutiple independent uses
    simultaenously, when it should be the case that those uses will in
    fact be serial and thus temporally disjoint, while 'conservative'
    allocation will always obey the invarient that one piece of memory is
    allocated to at most one use at any point in time.
    
    If GPUDevice::RequiresRecordingAccessedTensors() returns false, then
    the TF runtime uses efficient memory allocation for GPU ops.  That is, GPU
    ops are nominally synchronous and their tensor Ref's are deleted
    immediately after the ops returns although really the corresponding GPU
    kernel is only guaranteed to have been enqueued on the compute stream
    and may not have yet begin execution.
    
    If RequiresRecordingAccessedTensors() returns true, then conservative
    memory allocation is used, i.e. Refs on the tensors accessed by a GPU op
    are held until the corresponding kernel is guaranteed to have completed
    execution and no part of the op will touch them again.
    
    Efficient GPU memory allocation should be safe when the following criteria
    are all met:
    
    1. All GPU kernels are executed serially on a single compute stream.
    2. All GPU kernel outputs and temp buffers are allocated by
       the GPU Op in the executor thread in which it is originally called.
    3. Any read of a GPU tensor computed by a GPU kernel that is not
       by another kernel on that same GPU first synchronizes on
       the compute stream that produced it.
    4. Any read by a GPU kernel of a value that was not produced by another
       GPU kernel first synchronizes on the entity that produced it,
       e.g. a copy stream.
    5. All direct allocations of GPU memory that are not for kernel outputs
       or temp buffers are conservative in duration.
    6. Any use of directly allocated GPU memory that is not part of a kernel
       execution first synchronizes on the compute stream to ensure that
       any prior granted uses of the same region have expired before this new use.
    
    These conditions together should be sufficient for safety, and
    correspond to established practice, though it may be possible to
    contrive other sets of rules that are also sufficient.
    
    Collective Ops for GPUs are unusual in that they are async (as TF
    Ops) and they can directly allocate GPU memory in CPU threads that are
    asynchronous to the launching executor thread.  This CL corrects a
    couple of subtle misuse errors related to conditions 2 and 6.
    
    PiperOrigin-RevId: 210841522

commit 59483d0f695436d7bda688cd2ee8f1212071e41a
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Aug 15 17:11:59 2018 -0700

    The code for splitting constants to ensure single user had a side effect of deleting constants which did not have any users. It also made an extra clone when number of users reduced to one. While it did not affect correctness, but it's cleaner and more efficient to address these issues.
    
    PiperOrigin-RevId: 208910596

commit 12eb80cb9b4b51631a7cdfc9fce476a8b2ea225b
Author: Rohan Jain <rohanj@google.com>
Date:   Fri Aug 10 17:05:25 2018 -0700

    Speeding up MultiDeviceIterator by more efficient locking. We create a background thread that tries to keep a host side buffer for each device full. When a GetNext request comes in, we return from the buffer if available or else we schedule a callback to be called when the background thread eventually fetches an element for it.
    
    PiperOrigin-RevId: 208292329

commit 4d58bfb2324dbc647b294824e29ba9e75eece3ba
Author: Sanjoy Das <sanjoy@google.com>
Date:   Fri Aug 3 15:21:37 2018 -0700

    [XLA:CPU] Migrate aot/runtine.{h,cc} to xla_compiled_cpu_function.{h,cc}
    
    As a follow-on cleanup for cl/206980796 ("Overhaul XLA:CPU's calling
    convention.") I want to introduce a BufferInfo class that encapsulates whether a
    buffer is a constant, an entry parameter or a temp without using the fragile
    "size < 0" scheme I have today.  To do this efficiently I need a place to put
    the BufferInfo class that will be visible to MallocContiguousBuffers.  Instead
    of creating (what seemed to me) an odd layering with BufferInfo in aot/runtime.h
    I decided to pull in the runtime into xla_compiled_cpu_function since that's the
    only user.
    
    PiperOrigin-RevId: 207333245

commit 2c442d26f36a0f167685fd31b9ecdb4e290c2b29
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Jul 16 17:13:42 2018 -0700

    Implement digamma for XLA
    
    Compute the Lgamma function using Lanczos' approximation from "A Precision Approximation of the Gamma Function". SIAM Journal on Numerical Analysis series B. Vol. 1:
    digamma(z + 1) = log(t(z)) + A'(z) / A(z) - kLanczosGamma / t(z)
    t(z) = z + kLanczosGamma + 1/2
    A(z) = kBaseLanczosCoeff + sigma(k = 1, n, kLanczosCoefficients[i] / (z + k))
    A'(z) = sigma(k = 1, n, kLanczosCoefficients[i] / (z + k) / (z + k))
    
    PiperOrigin-RevId: 204834091

commit 3618796b3bee7bd0eb06425d6a069d28b95e6f42
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Jul 16 15:14:39 2018 -0700

    Implement lgamma for XLA
    Add support for Real and Imag for real floating point types.
    
    Compute the Lgamma function using Lanczos' approximation from "A Precision Approximation of the Gamma Function". SIAM Journal on Numerical Analysis series B. Vol. 1:
    lgamma(z + 1) = (log(2) + log(pi)) / 2 + (z + 1/2) * log(t(z)) - t(z) + A(z)
    t(z) = z + kLanczosGamma + 1/2
    A(z) = kBaseLanczosCoeff + sigma(k = 1, n, kLanczosCoefficients[i] / (z + k))
    
    PiperOrigin-RevId: 204815805

commit cead016fe01e8f0ff732a080203b29990d475c98
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Jul 4 03:06:18 2018 -0700

    Minor performance improvement for ComputeReachability
    
    We have two versions of HloReachabilityMap::SetReachabilityToUnion where
    one of them is slightly more efficient by not returning if the
    reachability have been changed or not. This change migrates the users
    not caring about the return value to the faster variant.
    
    PiperOrigin-RevId: 203256625

commit aac398674301532d0b6cb9a767325fcd03839df5
Author: Yuanzhong Xu <yuanzx@google.com>
Date:   Wed Jun 27 15:05:11 2018 -0700

    [XLA] Use subshape pointers as map keys in BFloat16Propagation.
    
    Using simple keys is more efficient.
    
    PiperOrigin-RevId: 202377039

commit 394add116efd9839e1be5342c085e6510c265687
Author: Yuanzhong Xu <yuanzx@google.com>
Date:   Wed Jun 27 15:05:11 2018 -0700

    [XLA] Use subshape pointers as map keys in BFloat16Propagation.
    
    Using simple keys is more efficient.
    
    PiperOrigin-RevId: 202377039

commit b62d76d932f93ff324d2598cdeac792fa61135a4
Author: Benjamin Kramer <kramerb@google.com>
Date:   Fri Jun 15 11:10:03 2018 -0700

    [XLA] Switch PostOrder accessors to use std::vector instead of std::list.
    
    std::list is just hilariously inefficient and the postorder list creation has
    been rewritten not to not depend on splicing anymore so there's no need for the
    list. While there remove the old unused postorder list creation code.
    PiperOrigin-RevId: 200743677

commit e1347ba769b98e260d36e895be2963af35c88d18
Author: Kay Zhu <kayzhu@google.com>
Date:   Wed May 9 13:07:35 2018 -0700

    [XLA] First step in adding Literal slice classes, to improve interface safety
    and prepare for enabling more efficient interfacing from Tensor to Literal to
    reduce host to device latency.
    
    More specically:
    * Introducing a new LiteralBase abstract base class that contains all immutable
    methods of from the old Literal class.
    
    * Introducing a subclass LiteralSlice to replace original LiteralView class.
    LiteralSlice class is read-only and does not own Shape nor any buffer through
    the Pieces. Change a number of callers to use LiteralSlice directly.
    
    * Change Literal class to explicitly own the underlying Shape as well as owning
    the underlying buffer via Piece.
    
    * Conversion from Literal to LiteralSlice is now done via an implicit
    conversion constructor instead of inheritance.
    
    * Decouple ShapeTree from Literal classes.
    
    * Use copy-and-swap for assignment constructors.
    
    * Other minor cleanups.
    
    PiperOrigin-RevId: 196016576

commit ab02bce13e49fbd001c6db241d213dc2886a5792
Author: Alexandre Passos <apassos@google.com>
Date:   Mon Apr 30 14:34:01 2018 -0700

    Do not cast int64 to int32 in keras embedding lookups.
    
    Often when working on the GPU with tf int64s are more efficient as int32s will
    be copied back and forth to the host quite a bit.
    
    PiperOrigin-RevId: 194846629

commit f0df6701d01954073e912f24f7c983de4f091a1e
Author: joel-shor <joelshor@google.com>
Date:   Fri Apr 20 14:01:02 2018 +0300

    [tf.data] Check in a strictly faster rejection resampling
    transformation.
    
    This transformation is faster because it rejects fewer data. This
    is done by occasionally sampling from the original data distribution
    in an efficient way.
    
    Tested:
    bazel test :resample_test

commit a77dcb5e56dbbbcc3383cb0b39cd79dd88135635
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Apr 13 15:23:08 2018 -0700

    Add broadcasting to all LinearOperators.
    
    This will broadcast in cases where batch shapes are not equal (but tries to determine statically if this is the case). The broadcasting is not as efficient as doing the broadcast in C++, but makes for the API to at least be completely broadcastable.
    
    PiperOrigin-RevId: 192832919

commit 5a6d5a1b3982e59548340422f831ada6f5d5e0be
Author: Francois Chollet <fchollet@google.com>
Date:   Thu Apr 12 19:01:10 2018 -0700

    Enable efficient feeding of symbolic tensors to placeholders in the Keras backend.
    
    PiperOrigin-RevId: 192707345

commit 5d81b72b9c1a7edd1a84c13b1dc753b310545e56
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Apr 2 11:35:00 2018 -0700

    [XLA] Redesign: improve error handling:
    - For every op creation method, check whether there's any existing error, if so, don't do anything and returns an empty op. To do this efficiently, make the NoteErrorOrReturn method accept a lambda, and check first_error_ before evaluating the lambda.
    - Return error instead of TF_CHECK_RET, because the second seems to always print ERROR logs.
    
    PiperOrigin-RevId: 191322082

commit 97731cb122f53552bd15351e046a256f78cca444
Author: Skye Wanderman-Milne <skyewm@google.com>
Date:   Fri Mar 30 14:56:08 2018 -0700

    Raise exception in SWIG on bad TF_Status from C API.
    
    This change provides an alternative mechanism to
    tf.raise_exception_on_not_ok_status(), which is inefficient and
    error-prone (people often use the status multiple times in the with
    block, but it's only checked when the context manager exits). Instead,
    it uses SWIG to automatically raise an exception when a C API method
    fails. Note that this removes the status argument from affected
    methods.
    
    For now, I've only applied this typemap to C API methods. It would be
    good to expand this to all uses of raise_exception_on_not_ok_status.
    
    PiperOrigin-RevId: 191121016

commit 9a7a63aff142658db6d54027815a54a267be808a
Author: Justin Lebar <jlebar@google.com>
Date:   Thu Mar 29 10:30:31 2018 -0700

    [XLA:GPU] Assume that tuple sub-buffers are available at runtime.
    
    Previously we assumed this was not the case, and allowed front-ends to
    pass in a pointer to tuple without also passing in pointers to
    sub-buffers.
    
    This mostly worked: Whenever we wanted a tuple sub-buffer, we'd just
    chase the tuple's pointers in our emitted kernel.
    
    But this doesn't work if we ever need a pointer to that sub-buffer on
    the host.  Which we do if e.g. the sub-buffer is an input to a cudnn
    call.
    
    There are various ways to make this work, but by far the simplest and
    most efficient is simply to specify away this problem, and say that the
    front-end *must* give us all the pointers we want.  This is what the
    earlier change, "Assert that all buffers and sub-buffers passed to XLA
    have an explicit pointer" did.
    
    This change adds a testcase and lets us skip some pointer chasing when
    we have a tuple whose sub-buffers are known statically.
    
    PiperOrigin-RevId: 190949743

commit 50e1888fa89bce621e988a92ede3dc362e37b248
Author: Justin Lebar <jlebar@google.com>
Date:   Tue Mar 27 17:16:31 2018 -0700

    [XLA] Assert that all buffers and sub-buffers passed to XLA have an explicit pointer.
    
    In the past, we allowed sub-buffers to be null if the top-level tuple
    was non-null.
    
    This doesn't actually work well on the GPU: For ops that are implemented
    using cudnn or cublas, we have to have a pointer to the sub-buffer on
    the host in order to make the call.  Retrieving it from the GPU in an
    efficient manner is complicated, and the best we can come up with isn't
    all that efficient (fundamentally having to pull data down from the GPU
    blocks the ability of the CPU to "run ahead" of the GPU).
    
    Since TF wasn't making use of our flexibility *anyway*, we add the
    requirement that XLA be given non-null pointers to all sub-buffers.
    
    Changes to the XLA:GPU backend to take advantage of this will come
    separately.
    
    PiperOrigin-RevId: 190700021

commit 53b2181ea5cff054d40c583f05da942a9a56a283
Author: Jeremy Lau <lauj@google.com>
Date:   Tue Feb 27 15:32:16 2018 -0800

    Make RecentRequestIds more efficient.
    
    PiperOrigin-RevId: 187242940

commit 785ee91c0d4f9a0e8eafa082f725c25ae134c9b3
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Feb 16 14:06:26 2018 -0800

    Optimization of quantized LSTM cell for the common case of batch size 1,
    where it needs efficient matrix*vector ("GEMV") code, but it's not
    exactly the same as the case of stand-alone fully-connected layers
    as here the output activations are 16bit-quantized.
    
    PiperOrigin-RevId: 186044068

commit 428d034227c9e7b637de0194d80cac3976a37eef
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Feb 16 13:20:13 2018 -0800

    Fix pontential issue with number of blocks launched for depthwise kernels: the number of work_elements was too small, which could return a block_count that is too small to cover all elements.
    
    We also have been ignoring the suggested thread_per_block, so were potentially launching more blocks than necessary to fill the GPU (which is inefficient, but functionally correct).
    
    Changing 'assert(false && ...' to LOG(FATAL) because it shouldn't be debug only.
    
    PiperOrigin-RevId: 186037306

commit 972fa89023f8f27948321c388fa3f1f7857833c3
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Feb 15 12:50:03 2018 -0800

    Add auc_with_confidence_intervals
    
    This method computes the AUC and corresponding confidence intervals using an efficient algorithm.
    
    PiperOrigin-RevId: 185884228

commit 5dd585abb84c5d13af0017f78741e29505f7b5f7
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Feb 15 11:34:10 2018 -0800

    Make conversions from ShapedBuffer <-> ScopedShapedBuffer efficient by
    moving memory ownership instead of copying.
    
    PiperOrigin-RevId: 185871648

commit 1baac7862739525351d25202800dc04e8ec3868b
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Feb 8 10:20:26 2018 -0800

    Make MklCpuAllocator a VisitableAllocator, instead of just an Allocator.
    This allows it to work more efficiently with RDMA networking.
    
    PiperOrigin-RevId: 185013628

commit 39010bef7f72709a87a275060878baac815744c2
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Feb 2 19:40:30 2018 -0800

    A more efficient implementation of the Op using batch operations.
    
    PiperOrigin-RevId: 184367562

commit 1699bc041e624cdaf249a80136b467743357fbfc
Author: Peter Hawkins <phawkins@google.com>
Date:   Wed Jan 31 12:28:13 2018 -0800

    [TF:XLA] Implement Acos, Asin, Atan in terms of Atan2 using half-angle formulae. This may not be the most efficient implementation but it is better than no implementation.
    
    PiperOrigin-RevId: 184029858

commit 548df15375488fc06ff663670f88734f3ece4814
Author: Derek Murray <mrry@google.com>
Date:   Tue Jan 30 13:26:51 2018 -0800

    [tf.data] Add `IteratorContext::allocator()`.
    
    This enables the various iterator implementations to use the actual allocator for the device on which they are running, rather than defaulting to `cpu_allocator()` (which is typically a plain malloc). In future, this will enable allocating iterator outputs in CUDA-pinned memory (and GPU memory).
    
    PERFORMANCE NOTE: In sessions where `ConfigProto.force_gpu_compatible == True`, this change has the effect of allocating all input pipeline tensors in CUDA-pinned memory. Previous if this flag was set, only the tensors allocated during function execution would be allocated in this space, and other tensors (e.g. the result of a `Dataset.batch()` would be allocated using `cpu_allocator()` (i.e. `malloc()`). This change should lead to more efficient communication between a host-side input pipeline and GPUs, but it may also create more pressure on the CUDA host allocator (whose default maximum size is 64GB). The "TF_CUDA_HOST_MEM_LIMIT_IN_MB" environment variable can be used to override this value.
    
    This change is a starting point for working on issue #13610.
    
    PiperOrigin-RevId: 183881907

commit dcddfdf045f06c9cfc7579bb61ac23d3a3b4a44e
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Jan 11 19:23:24 2018 -0800

    Improve performance of several utility functions in TensorFlow
    
    framework/types.h defines a variety of functions on DataType enums. Some of these functions are implemented by allocating arrays in the heap.  Even though DataTypeVector is a typedef for InlinedVector, it only stores 4 elements inline. Many of the vectors used in types.h/types.cc contain more than 4 elements.
    
    To make matters worse, some of these functions are called quite frequently under load, so we're wasting time allocating and copying arrays.
    
    The set of distinct DataType values is so small, however, that we can represent a set of DataType values as a bitmask, and use bit-shifts and tests instead of sequential scans of arrays.
    
    Even the functions that do not allocate, such as DataTypeCanUseMemcpy(), are needlessly inefficient (read: they use control-flow and indirect jumps when a simple table-based load would do; they are also not inlined).  These costs were significant enough that they consumed about 1.2% of CPU cycles under heavy load.
    
    The surprising cost of DataTypeCanUseMemcpy() inspired this change. I went ahead and made the change fully general, by adding a DataTypeSet type and changing all of the utility functions in framework/types.h to use it (with the exception of DataTypeAlwaysOnHost because it uses a _REF type), for the sake of generality and performance.
    
    PiperOrigin-RevId: 181695458

commit 814e4a7830b506ed26ed22b9b1bc7233d6185467
Author: Derek Murray <mrry@google.com>
Date:   Tue Jan 9 11:56:51 2018 -0800

    Add experimental `FunctionLibraryRuntime::InstantiateOptions::overlay_lib`.
    
    This option makes it possible to instantiate functions from a library
    that has been loaded separately from the runtime's own library. We
    plan to use this as part of the `tf.data` checkpoint restore process,
    which might load an iterator whose state includes functions that
    aren't present in the original graph. (This is currently achieved by
    creating an isolated `FunctionLibraryRuntime` for each function-using
    `Dataset`, but that is inefficient and prevents using features of the
    main runtime, such as cross-device function calls.)
    
    PiperOrigin-RevId: 181352217

commit 0cce4840f561bd7f8b06603e0dc1dcbf05c46a03
Author: Benoit Steiner <bsteiner@google.com>
Date:   Tue Jan 2 21:45:51 2018 -0800

    Fixed a typo that resulted in the graph being processed in reverse topological
    order. This made shape inference far less efficient than it should be in some cases.
    
    PiperOrigin-RevId: 180629882

commit d4091eec522e41093e6e10601af79c75bee14c80
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Dec 22 15:50:19 2017 -0800

    Replaces custom _lengths_to_masks function with the official, more efficient sequence_mask function that supersedes it.
    
    PiperOrigin-RevId: 179971521

commit 96f3023b6a8b154c3840776c5feff3e028860a36
Author: Derek Murray <mrry@google.com>
Date:   Wed Dec 20 20:54:25 2017 -0800

    [tf.data] Add `tf.contrib.data.parse_single_example()`.
    
    The new op is a fused implementation of the existing
    `tf.parse_single_example()`, which is more efficient when parsing a
    single Example at a time.
    
    PiperOrigin-RevId: 179768512

commit 1b4c6096e5024119dbed898ecaad63e3afd58ef0
Author: Derek Murray <mrry@google.com>
Date:   Mon Dec 11 15:39:00 2017 -0800

    [tf.data] Use a more efficient dispatch mechanism for functions in datasets.
    
    This change adds an overload of the `FunctionLibraryRuntime::Run()` method
    that allows users to pass argument and return value containers in a
    `CallFrameInterface` object, rather than using the current (and expensive for
    large arities) default `FunctionCallFrame` implementation. It also specializes
    `CapturedFunction` to use this interface.
    
    Note that the new overload currently only supports local function execution,
    and more restructuring will be required to take advantage of it in the remote
    function execution case.
    
    This change should especially benefit datasets where each element has a large
    number of components (typically when training data have many features).
    
    PiperOrigin-RevId: 178684431

commit 5d3d7fa81b87aa3c1137366f062c4f4ab9681a09
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Nov 28 10:55:32 2017 -0800

    RevBlock: memory-efficient implementation of a series of reversible residual
    layers.
    
    PiperOrigin-RevId: 177185950

commit b7a74edb5e6e134df4d66ad66b486aafd29c4ac4
Author: codrut3 <grosu.codrut@gmail.com>
Date:   Mon Nov 20 22:58:53 2017 +0200

    Use cub::ReduceByKey to count partition indices (#14665)
    
    * Use cub::ReduceByKey to count how many times each partition index appears.
    
    This implements a suggestion by @ekelsen. It replaces the
    previously custom-made counting method and is likely more
    efficient.
    
    * Remove CubReduceAdd and use instead cub::Sum.

commit 44be285351ea465db6b4c32807fb1503c5e74531
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Nov 3 16:38:56 2017 -0700

    Don't generate ConjugateTranspose nodes for real tensors. Doing so is not an error, but makes graph rewriting optimizations slightly less efficient.
    Use dtype.is_complex instead of dtype in (dtypes.complex64, dtypes.complex128) in a few places.
    
    PiperOrigin-RevId: 174531912

commit 913a96bccee065cbd34f4d24c70e225023c1987b
Author: Igor Ganichev <iga@google.com>
Date:   Thu Nov 2 10:57:54 2017 -0700

    Optimize tf.split for eager mode
    
    In eager mode, we know the shapes of input tensors and can use
    more efficient methods for retrieving shape and rank. This change
    reduces SPINN training time by about 1%. While 1% is not a lot, it is
    a small localized change to a very commonly used op.
    
    Also:
      - Adapt split_op_test to run in both modes
      - Decrease the number of splits in _testHugeNumberOfTensorsVariable
        from 10k to 1k. I don't see much value in having such large
        operations in a unit test. This reduces the total testing time
        of split_test_op from 40 seconds to 4.6 seconds.
      - Change TensorFlowTestCase.evaluate() to create a session if no default
        session is setup.
    PiperOrigin-RevId: 174349642

commit 06a79f5af7c861e695cfc20b7778519950aac9ba
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Oct 26 17:00:14 2017 -0700

    Move EyeFunctor to a separate file, and change it to a more efficient implementation (similar to matrix_set_diag).
    
    PiperOrigin-RevId: 173611865

commit 355e25ebcab64e833dfc987638c3e6c79d838266
Author: Benoit Steiner <bsteiner@google.com>
Date:   Tue Oct 24 19:47:46 2017 -0700

    Merge changes from github.
    END_PUBLIC
    
    ---
    Commit 9f8523640 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 173145770
    
    ---
    Commit 01b6b0638 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Cut tracing memory cost
    
    PiperOrigin-RevId: 173144626
    
    ---
    Commit 5e23e0e67 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Erase cloned instructions on the fly when merging fusion nodes.
    
    This avoids the awkward situation where an RNG which is clearly eligible for fusion becomes ineligible mid-fusion because it suddenly has an extra (dead) user.
    
    PiperOrigin-RevId: 173141716
    
    ---
    Commit 1038927c0 authored by Saurabh Saxena<srbs@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add SerializeIterator op that serializes an IteratorResource into a variant tensor.
    Add DeserializeIterator op that builds IteratorResource from a variant tensor.
    Move BundleReaderWrapper and BundleWriterWrapper from dataset.h to iterator_ops.cc.
    Add generic key-value store interfaces IteratorStateReader and IteratorStateWriter for reading/writing state of iterators.
    Get rid of IteratorBundleReader and IteratorBundleWriter.
    
    PiperOrigin-RevId: 173140858
    
    ---
    Commit 57f3e529d authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Internal change
    
    PiperOrigin-RevId: 173136642
    
    ---
    Commit 0e56ffb7b authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix breakages in OSS builds
    
    See example breakages logs at:
    http://ci.tensorflow.org/job/tensorflow-cl-cpu-python3-pip/10847/console
    http://ci.tensorflow.org/job/tensorflow-cl-gpu/11008/console
    
    1. CL/172477381 added the no_oss tag to tests with oss_serial tags, which broke the logic of OSS_SERIAL tests in pip.sh and run_pip_test.sh. This CL fixes that.
    
    2. The nccl_kernels BUILD target in contrib/nccl/BUILD was missing some dependencies. This CL adds the missing ones.
    
    Fixes: #13918
    PiperOrigin-RevId: 173133914
    
    ---
    Commit 3ed049b67 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Allows calling keras layers in eager mode.
    
    PiperOrigin-RevId: 173129805
    
    ---
    Commit 4ec6f2b07 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Switching contrib.summaries API to be context-manager-centric
    
    PiperOrigin-RevId: 173129793
    
    ---
    Commit 03b02ffc9 authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Put Bazel mirror URLs first
    
    PiperOrigin-RevId: 173127955
    
    ---
    Commit 46ab25e4d authored by David Majnemer<majnemer@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Add support for convolutions with no spatial dimensions
    
    PiperOrigin-RevId: 173126950
    
    ---
    Commit fc56349b7 authored by Derek Murray<mrry@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [tf.data] Convert dataset arguments to tensors as early as possible.
    
    This change raises a `TypeError` earlier if (for example) the `batch_size`
    argument to `Dataset.batch()` has the incorrect type.
    
    PiperOrigin-RevId: 173126678
    
    ---
    Commit 4f7503a87 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    K-FAC: Support for registering multiple minibatches with register_fully_connected()
    
    PiperOrigin-RevId: 173121735
    
    ---
    Commit 2845bfcd6 authored by Tim Harley<tharley@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Avoid listing all modified Enter/RefEnter nodes on INFO, use VLOG(1) instead.
    
    Leave a single, simple, message on INFO.
    
    PiperOrigin-RevId: 173121726
    
    ---
    Commit 434695921 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    K-FAC: _check_registration() supports multiple towers.
    
    PiperOrigin-RevId: 173115870
    
    ---
    Commit 670dddf4a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Multi-minibatch support for
    tf.contrib.kfac.fisher_blocks.FullyConnectedKFACBasicFB.
    
    PiperOrigin-RevId: 173109677
    
    ---
    Commit dc13a8e2f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix import of meta graphs with partitioned variables into a scope.
    
    Saver inspects SliceInfo to decide the variable name when creating a
    checkpoint. Before this fix even if a partitioned variable ("weights")
    was imported into a scope "a" it would still be checkpointed as ("weights")
    instead of ("a/weights") since import_scoped_meta_graph was not adjusting
    the SliceInfo.
    
    WARNING: if you use import_meta_graph on graphs with partitioned_variables WITH an import_scope argument AND then create a Saver to write/read checkpoints this change
    may break your checkpoint loading.
    PiperOrigin-RevId: 173105796
    
    ---
    Commit eea089bdb authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    K-FAC: Multi-tower support for ConvDiagonalFB.
    
    PiperOrigin-RevId: 173105412
    
    ---
    Commit 9b9cbbe2a authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Add int64 Tperm type support for `Transpose` (#13909)
    
    * Add int64 Tperm type support for `Transpose`
    
    This fix adds int64 Tperm support for `Transpose`. In
    `array_ops.cc`, `Transpose` and `ConjugateTranspose`
    have been specified as accepting int32 and int64 perm
    types. However, only int32 kernels has been registered.
    
    This fix adds the int64 perm support by removing
    the constraint on Tperm, resolve the type at runtime,
    and copying the data type accordingly to correctly handle
    the int64/int32 types.
    
    Additional tests have been added as well.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add test cases for int64 of perm in Transpose.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add namespace to hide PermutationHelper
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Enable use_gpu=True for perm type test.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * extra // namespace annotation
    
    * Adding a comment about int32 casting that should be safe.
    
    Permutations only contain values that refer to dimensions, and the maximum number of dimensions we have is 254, so an int32 is always safe here.
    
    ---
    Commit ac0004e71 authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Add int64 shape support on GPU for stateless random ops. (#13908)
    
    * Add int64 shape support on GPU for stateless random ops.
    
    This fix adds int64 shape support on GPU for stateless random ops
    `StatelessRandomUniform`, `StatelessRandomNormal`, `StatelessTruncatedNormal`.
    
    The int64 shape for stateless random ops is already supported on CPU
    with int32/int64 processed properly through `MakeShape`.
    
    However, on GPU a type constraint `.TypeConstraint<int32>("T")`
    has been improperly added. Such a type constraint actually prevents
    an int64 shape type to run on GPU. (As a comparision, no type constraint
    on CPU).
    
    This fix removes the type constraint and allows int64 shape to be run on GPU.
    
    This fix also adds test cases for int64 shape support on stateless random ops.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add test cases for int64 shape support for stateless random ops.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add int32 to shape types tested.
    
    ---
    Commit 0d437c3be authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Add int64 padding support for MirrorPad (#13907)
    
    * Add int64 padding support for MirrorPad
    
    This fix adds int64 padding support for `MirrorPad`.
    In the `array_ops.cc` the `MirrorPad`/`MirrorPadGrad`
    has been specified as supporting int64 padding. The related
    kernels does not have the int64 padding registered though.
    This fix adds the int64 padding support. This fix also adds
    additional test cases for coverage.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Update template for CPU and GPU support of int64 paddings.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add int64 padding support for MirrorPad
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Put eigen header first like before, just in case.
    
    ---
    Commit 690003cc0 authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Add `int64` type `multiples` support for `tf.tile` (#13884)
    
    * Add `int64` type `multiples` support for `tf.tile`
    
    In the doc of `tf.tile` (tf.tile.__doc__) both `int32`
    and `int64` are supported for `multiples`. However, the kernel
    for `int64` is not registered yet.
    
    This fix adds the support of `int64` `multiples` so that the
    behavior matches the description of the docs.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Update functors for int64 multiples support in `tf.tile`
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Update test cases for int64 of multiples in `tf.tile`
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add GPU and non GPU tests
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * format with clang-format -i
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Move Tmultiples after T (as it is  auxilliary)
    
    And use `use_gpu=True`
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit fd8d517b9 authored by Yunxing Dai<yunxing@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add tests for convolution 1D
    RELNOTES: n/a
    
    PiperOrigin-RevId: 173060283
    
    ---
    Commit 40c475b48 authored by formath<jinpengliu@163.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    add segment_reduction_ops to tf_op_files (#13901)
    
    ---
    Commit bfa4ec194 authored by Tayo Oguntebi<10927929+tayo@users.noreply.github.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Update node_def.proto comments (#13874)
    
    The device field had outdated comments.
    
    Note: We could consider adding tpu as an example here, e.g. "gpu" | "cpu" | "tpu".  Thoughts?
    ---
    Commit c9cb5a58d authored by formath<jinpengliu@163.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    protobuf lib path bug fix for benckmark on osx (#13878)
    
    ---
    Commit 1c1dad105 authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Add int64 axis support for reduction ops. (#13891)
    
    * Add int64 axis support for reduction ops.
    
    This fix is a follow up to PR 13863. In PR 13863 the
    program crash is fixed if int64 axis is passed to reduction ops,
    e.g. reduce_sum, reduce_max, etc. However, 13863 does not
    process the case of int64 support, it merely fixes the crash.
    
    This fix adds the support for int64 axis of reduction ops.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add int64 axis support for mean, prod, sum
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add int64 axis support for min and max.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add int64 axis support for reduce_all and reduce_any
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add test cases for int64 axis support of reduce_any and reduce_all
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit 17096081e authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Improve resize_bicubic performance by reorganizing loops (#13840)
    
    * Improve resize_bicubic performance by reorganizing loops
    
    This fix tries to address the issue raised in 13693 where
    performance of `resize_bicubic` is not on par with opencv.
    
    This fix rearranges the loops so that it is the same for
    num_channel=40 and num_channel=3:
    
    Pre-fix:
    ```
    CHANNEL=40
    opencv: 145.08ms
    tf: 314.26ms
    
    CHANNEL=3
    opencv: 11.95ms
    tf: 8.95ms
    ```
    
    Post-fix:
    ```
    CHANNEL=40
    opencv: 144.25ms
    tf: 214.55ms
    
    CHANNEL=3
    opencv: 11.78ms
    tf: 14.07ms
    ```
    
    This fix fixes 13693.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Keep special handling of `num_channels=3` for `resize_bicubic`
    
    This commit keeps special handling of `num_channels=3` for
    `resize_bicubic`:
    Without special handling:
    ```
    opencv: 11.78ms
    tf: 14.07ms
    ```
    With special handling:
    ```
    opencv: 11.74ms
    tf: 9.46ms
    ```
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Expand Benchmark test for resize_bicubic
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Update from review feedback.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit b927df57f authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Update protobuf.cmake to b04e5cba356212e4e8c66c61bbe0c3a20537c5b9 (#13893)
    
    This fix tries to address the issue raised in 8187 where
    protobuf.cmake used different version as bazel.
    
    The reason for discrepancy was due to the fact that a customerized
    protobuf was needed with Windows patch. Since the patch has been
    merged in (https://github.com/google/protobuf/pull/2203),
    it makes sense to update protobuf.cmake so that the same version
    of cmake is used.
    
    This fix fixes 8187.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    ---
    Commit d1183ca6a authored by Vijay Vasudevan<vrv@google.com>
    Committed by GitHub<noreply@github.com>:
    Give each variable a unique name in accumulate_n_v2_eager_test. (#13886)
    
    ---
    Commit a69945810 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update pin for bazel-toolchains to latest version
    
    PiperOrigin-RevId: 173002530
    
    ---
    Commit 9d55c249c authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Fix doc in TF_CALL_ when invoked in mobile platform (#13881)
    
    * Fix doc in TF_CALL_ when defined(IS_MOBILE_PLATFORM) && !defined(__ANDROID_TYPES_FULL__)
    
    This is a small doc fix that includes bool as part of the types
    that is supported in mobile (IS_MOBILE_PLATFORM && !__ANDROID_TYPES_FULL__),
    as bool is clearly invoked in the following define.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Also add bool to android full version.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit ba49d8583 authored by Bjarke Hammersholt Roune<broune@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Slight change to reduce_test to avoid generating inf, which was triggering an inf detector unnecessarily.
    
    PiperOrigin-RevId: 172965466
    
    ---
    Commit 93e8f3c67 authored by Anna R<annarev@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adding Python ApiDef overrides.
    
    PiperOrigin-RevId: 172960496
    
    ---
    Commit 0d6a2e353 authored by Anna R<annarev@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Internal change.
    
    PiperOrigin-RevId: 172960439
    
    ---
    Commit 62df65c72 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add dtype argument to Mean and Accuracy object-oriented metrics.
    
    PiperOrigin-RevId: 172957714
    
    ---
    Commit d7409d32b authored by Simone Cirillo<my.accounts@gmx.se>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Fix import of spatial_softmax from tensorflow.contrib.layers (#13833)
    
    ---
    Commit df8bce63d authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Fix crash when `int64` axis is passed to `tf.reduce_sum` (#13863)
    
    * Fix crash when `int64` axis is passed to `tf.reduce_sum`
    
    This fix tries to fix the crash triggered by `int64` axis passed
    to `tf.reduce_sum`:
    ```
    ubuntu@ubuntu:~/tensorflow2$ (cd && python)
    Python 2.7.12 (default, Nov 19 2016, 06:48:10)
    [GCC 5.4.0 20160609] on linux2
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import tensorflow as tf
    >>> v = tf.reduce_sum([1,2,3], tf.constant(0, tf.int64))
    2017-10-20 15:55:06.993430: F tensorflow/core/framework/tensor.cc:601] Check failed: dtype() == expected_dtype (9 vs. 3)
    ubuntu@ubuntu:~/tensorflow2$
    ```
    
    The issue is caused by the fact that shape inference in `common_shape_fns.cc`
    only assumes int32 without proper handling of diffent types. In `math_ops.cc`
    both int32 and int64 are mentioned.
    
    NOTE that this fix does not address the issue that int64 is not supported.
    To allow int64 axis it is more than adding a template in `ReductionOp` as the type
    of the axis seems to be decided by some other ways in Eigen.
    
    This fix merely fixed the crash so that an error message will return without
    exit from the python program "No OpKernel was registered to support Op 'Sum' with these attrs".
    
    Still, I think its worth to at least allow the program to continue in case of unsupported kernel.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Update implementation with a template helper function.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit 29c7b4658 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adding the Stanford Tensorflow class to community resources.
    
    PiperOrigin-RevId: 172956049
    
    ---
    Commit f758b24a8 authored by Alexandre Passos<apassos@google.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Variable name for the eager test (#13873)
    
    ---
    Commit a5fe66b15 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Removed some unnecessary broadcasts in binary ops where only one input needs
    broadcasting (which is a fairly common case, even in the fallback path).
    
    PiperOrigin-RevId: 172950493
    
    ---
    Commit c77090a0a authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Fix issues where int64 crops could not be passed to batch_to_space. (#13862)
    
    * Fix issues where int64 crops could not be passed to batch_to_space.
    
    This fix tries to address the issue where int64 `crops` could
    not be passed to `batch_to_space` even though both int32 and
    int64 are specified as supported in the docs (tf.batch_to_space.__doc__)
    
    The reason is that BatchToSpace kernel puts a constraint of int32 to crops
    data types.
    
    This fix removed the constraint so that int64 `crops` could be supported.
    
    NOTE: Just removing the constraint should work and it is not necessary
    to add specification to the kernel class template, as `SubtleMustCopyFlat`
    called in the class already correctly handled both int32 and int64 cases.
    Besides, other data types (e.g., float or double) will not be passed to the
    kernel as they are guarded by the specification in `array_ops.cc`.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Also remove int64/int32 type constraints for SpaceToBatch kernels
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Add test cases for int64 crops of batch_to_space and space_to_batch
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Fix test failures.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit 494837936 authored by Joshua V. Dillon<jvdillon@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make `tf.contrib.distributions` quadrature family accept a `Tensor` for
    `quadrature_grid_and_probs` argument.
    
    PiperOrigin-RevId: 172950094
    
    ---
    Commit 9c825d32c authored by Jinze Bai<baijinze1994@163.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    Merge two GPU kernel launching to one in DiagOp. (#13859)
    
    ---
    Commit c0ca50a47 authored by Yan Facai (???)<facai.yan@gmail.com>
    Committed by Vijay Vasudevan<vrv@google.com>:
    ENH: add Relu6GradGrad (#13268)
    
    * ENH: add Relu6GradGrad
    
    * TST: add test case
    
    * CLN: import nn_grad
    
    * TST: add init value
    
    ---
    Commit 8ff33271e authored by Justin Lebar<jlebar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Dump the computation's SessionModule as part of the tf_compile rule.
    
    PiperOrigin-RevId: 172946149
    
    ---
    Commit ebcae4a5e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add streaming_precision_recall_at_equal_thresholds
    
    This helper method computes streaming tp, fp, tn, fp, precision, and recall for the user in a way that exhibits O(T + N) time and space complexity (instead of O(T * N)), where T is the number of thresholds and N is the size of the predictions tensor.
    
    Thanks to Frank Chu for the efficient algorithm!
    
    PiperOrigin-RevId: 172946073
    
    ---
    Commit ccfd9c1e5 authored by Sanjoy Das<sanjoy@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Log Hlo IR during AOT compilation
    
    PiperOrigin-RevId: 172944165
    
    ---
    Commit 985031a10 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Allows tfe.enable_eager_execution(device_policy=tfe.DEVICE_POLICY_WARN).
    
    PiperOrigin-RevId: 172943398
    
    ---
    Commit 703182d85 authored by Mingxing Tan<tanmingxing@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add performance guide for fused decode_and_crop_jpeg optimization.
    
    PiperOrigin-RevId: 172943116
    
    ---
    Commit 66b1f4383 authored by Francois Chollet<fchollet@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make Network compatible with eager mode. Currently it only allows to instantiate a Network in eager mode using the regular Keras API, and call it on eager tensors.
    
    PiperOrigin-RevId: 172942569
    
    ---
    Commit 41df2cec2 authored by ashankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Testing pending CL: 172939383
    
    ---
    Commit 37fd95179 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Simplifies capturing code in graph_callable to use recent function improvements.
    
    PiperOrigin-RevId: 172937003
    
    ---
    Commit d1e7382af authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    BEGIN_PUBLIC
    Automated g4 rollback of changelist 172924803
    
    PiperOrigin-RevId: 173347587

commit ebcae4a5e3bf5c840d73a0d90f1b5bf01a68f82c
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Oct 20 15:55:17 2017 -0700

    Add streaming_precision_recall_at_equal_thresholds
    
    This helper method computes streaming tp, fp, tn, fp, precision, and recall for the user in a way that exhibits O(T + N) time and space complexity (instead of O(T * N)), where T is the number of thresholds and N is the size of the predictions tensor.
    
    Thanks to Frank Chu for the efficient algorithm!
    
    PiperOrigin-RevId: 172946073

commit fafff08cbc3b952d60ee98914c234bb6af09b968
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Oct 20 11:57:40 2017 -0700

    Adds the k-MC2 algorithm for efficient seeding of mini batch k-means in TensorFlow.
    
    PiperOrigin-RevId: 172914154

commit e3be40d099e1c5da869b7dfaf8d5891a8c2af312
Author: Jeffrey A. Dean <jeff@google.com>
Date:   Tue Oct 10 15:36:59 2017 -0700

    Slightly rework tf.matmul to be more efficient (important for eager mode)
    
    PiperOrigin-RevId: 171745141

commit 244b8d6b0767c0fb63e58e56f58d03bd97c27822
Author: Andrew Myers <andru@cs.cornell.edu>
Date:   Fri Sep 29 16:44:17 2017 -0400

    Java API Generics Phase 2 (#11535)
    
    * Phase 1 of the proposed generic Java API.
    
    This adds new classes to represent each of the possible tensor types,
    and some scripting support for generating those classes. There is
    essentially no effect on existing classes, except that DataType is
    made slightly more efficient.
    
    All tests pass.
    
    * Addressed Asim's review.
    
    * Hoisted copyright into a separate declaration. Maybe it should go
    in a separate file?
    
    * Added private constructors to TF types and shortened their javadoc to be
    more standard.
    
    * Added more explanation about the enum relationship.
    
    * Used more-idiomatic import statement.
    
    * Rename zero column.
    
    * Removed the datatype code from tftypes.csv
    
    * Fix the default value for Double, add one for UInt8.
    
    * Got rid of 'boxed type' column in CSV file
    
    * Somehow I did not notice that TFType.java was not checked in.
    
    * Phase 2 : Tensor, Output and friends are now parameterized.
    
    * All tests now pass.
    
    * Cleaned up and added some Javadoc and made some static fields private.
    
    * Made Outputs more convenient to use.
    Improved Javadoc regarding this functionality.
    Added explicit type parameters to examples and tests to make them better models of expected practice.
    
    * Removed extra copy of method.
    
    * This change to the Android demo app should allow it to compile successfully
    
    * Backed out unnecessary but presumably harmless removal of calls to clear().
    
    * Change from Unicode times symbol to x, to be more consistent with
    the rest of the Javadoc.
    
    * Updated Constant and ConstantTest with generics.
    
    * Registered UInt8 like all the other data types.
    
    * Removed the UINT8 test because UINT8 doesn't seem to be fully supported in next
    layer down. That probably should be fixed but it's orthongonal to this change.
    
    * * Added some missing pieces so that uint8 seems now to be supported fully by the Java API,
    addressing #12797.
    * Resurrected the uint8 test case.
    * Allowed arrays of bytes to be used to construct both tensors of strings and tensors of uint8.
    * Simplified the computation of the number of dimensions of a Java object representing a tensor.
    
    * Get rid of tab characters that violate the Google Java style guide. My IDE
    was not configured correctly.
    
    * Fix javadoc nit.
    
    * Replace testUInt8 with the generic version.
    
    * Ran formatter on code.
    
    * Addressed some of Asim's comments.
    
    - implemented constant() methods in terms of each other to reduce code duplication
    - improved a spec regarding when types are checked
    - got rid of an unnecessary method that used wildcards
    
    * Back out change to comments in Operand.java
    
    * This is what things look like if we make Tensor run on DataType as much as
    possible. Only Tensor.expect() is still using class objects as a way to
    represent tensor datatypes. It can be moved off to class Tensors when Tensors
    exists, though it will not be as convenient as when it's a method of Tensor.
    
    * Fixed build errors. This is is being committed primarily so Asim can take a look at it conveniently.
    More work will be needed before merging.
    
    * - Changed from TF-prefixed types to regular Java classes, e.g. Integer instead of
    TFInt32. Deleted most classes in org.tensorflow.types, including TFType.
    - Made Tensor mostly work in terms of Class<T> since that is the user-facing
      interface.
    - Moved zeroValue() stuff off to the testfile where it belongs
    
    * Remove unnecessary run-time check.
    
    * Updated Android inference test to latest Java API changes.
    
    * Address Asim's comments (thanks!)
    
    - Removed now-gratuitous run-time type-check.
    - Fixed non-Google-styled if.
    - Reworded/fixed a few comments as requested.
    - Removed all uses of unsafe casts and @SuppressWarnings in test cases.
    - Cleaned up constant() implementations in LabelImage example.
    - Removed reference to Tensors class (next PR!)
    
    * Ran gformat on everything.
    
    * Fixed an old typo in a comment.
    Removed a couple of unnecessary casts from the example program.
    
    * Fixed the last suppressed warnings.

commit e55574f28257bdacd744dcdba86c839e661b1b2a
Author: drpngx <drpngx@users.noreply.github.com>
Date:   Fri Sep 15 19:38:25 2017 -0700

    Branch 168917534 (#13077)
    
    * Use HLO name, rather than pointer address, for profile counter name.
    
    This removes a source of nondeterminism in IR generation.
    
    PiperOrigin-RevId: 168779489
    
    * Eager gradient tape doesn't keep tensors alive.
    
    PiperOrigin-RevId: 168782341
    
    * Add missing back-quote
    
    PiperOrigin-RevId: 168785422
    
    * Add in a comment that I forgot to add to a previous commit; NFC.
    
    PiperOrigin-RevId: 168786760
    
    * Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 168787665
    
    * Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 168788051
    
    * Fix typo "comptuation" (computation)
    
    PiperOrigin-RevId: 168799777
    
    * Fix a bug in export GTFlow model to shared format with sparse float split
    
    PiperOrigin-RevId: 168802503
    
    * Add signature def utility functions for inspection of input and output types and shapes.
    
    PiperOrigin-RevId: 168820997
    
    * Apply const qualifiers whenever appropriate.
    
    PiperOrigin-RevId: 168824461
    
    * TFE: Clearer error message when enable_eager_execution is called more than once
    
    PiperOrigin-RevId: 168834147
    
    * [tf.contrib.data] Add colocation constraints between Iterator and Datasets.
    
    This restores the colocation behavior that was present when Dataset
    objects were passed as DT_RESOURCE tensors, and avoids the (currently
    not supported) case where TensorFlow may attempt to split the dataset
    pipeline across devices.
    
    PiperOrigin-RevId: 168841061
    
    * Optimize C++ kernels for the matrix_band_part op, which is used in various ops operating on triangular or banded matrices:
     * Add benchmark for matrix_band_part.
     * Implement simple optimized CUDA kernel instead of calling Eigen generator.
     * Parallelize CPU kernel for matrix_band_part.
     * Support on-the-fly transposition in the underlying functors (to be used for future QR op in followup).
    
    Benchmarks:
    
    First column is of the form {device}_{shape}_{num_lower,num_upper}
    
    Test case                        Before       After    Speedup
    cpu_(10,16,16)_(-1,-1)          5.6505e-05  6.2108e-05  -9.92%
    cpu_(10,16,16)_(-1,0)           0.00010848  0.00010908  -0.55%
    cpu_(10,16,16)_(0,-1)            0.0001055  0.00011396  -8.02%
    cpu_(10,16,16)_(2,2)              0.000108  0.00011706  -8.39%
    cpu_(10,101,101)_(-1,-1)        0.00013697  6.0558e-05 +55.79%
    cpu_(10,101,101)_(-1,0)         0.00054002  0.00017703 +67.22%
    cpu_(10,101,101)_(0,-1)         0.00051188  0.00017607 +65.60%
    cpu_(10,101,101)_(2,2)          0.00050449  0.00016904 +66.49%
    cpu_(10,256,256)_(-1,-1)        0.00032043  5.6028e-05 +82.51%
    cpu_(10,256,256)_(-1,0)           0.001335   0.0004015 +69.93%
    cpu_(10,256,256)_(0,-1)          0.0013521  0.00038862 +71.26%
    cpu_(10,256,256)_(2,2)            0.001269  0.00039959 +68.51%
    cpu_(10,1000,1000)_(-1,-1)       0.0090729  6.3419e-05 +99.30%
    cpu_(10,1000,1000)_(-1,0)          0.01712   0.0047594 +72.20%
    cpu_(10,1000,1000)_(0,-1)         0.016647   0.0046474 +72.08%
    cpu_(10,1000,1000)_(2,2)          0.012737   0.0041161 +67.68%
    cpu_(10,1024,1024)_(-1,-1)       0.0093709  5.8889e-05 +99.37%
    cpu_(10,1024,1024)_(-1,0)         0.017075   0.0051999 +69.55%
    cpu_(10,1024,1024)_(0,-1)         0.016867    0.004617 +72.63%
    cpu_(10,1024,1024)_(2,2)          0.013191    0.003759 +71.50%
    cpu_(10,2048,2048)_(-1,-1)        0.028427  6.2466e-05 +99.78%
    cpu_(10,2048,2048)_(-1,0)         0.048134    0.017642 +63.35%
    cpu_(10,2048,2048)_(0,-1)         0.048773    0.017558 +64.00%
    cpu_(10,2048,2048)_(2,2)          0.036153    0.015452 +57.26%
    cpu_(10,10,4,4)_(-1,-1)         5.8055e-05  5.8055e-05  +0.00%
    cpu_(10,10,4,4)_(-1,0)          0.00015557   0.0001564  -0.54%
    cpu_(10,10,4,4)_(0,-1)          0.00015855  0.00015199  +4.14%
    cpu_(10,10,4,4)_(2,2)           0.00016379  0.00018096 -10.48%
    cpu_(10,10,10,10)_(-1,-1)       6.0558e-05  6.0558e-05  +0.00%
    cpu_(10,10,10,10)_(-1,0)          0.000368  0.00038695  -5.15%
    cpu_(10,10,10,10)_(0,-1)        0.00036263  0.00038612  -6.48%
    cpu_(10,10,10,10)_(2,2)         0.00038648  0.00042963 -11.17%
    cpu_(10,10,16,16)_(-1,-1)       6.9022e-05  5.7578e-05 +16.58%
    cpu_(10,10,16,16)_(-1,0)         0.0005815   0.0001874 +67.77%
    cpu_(10,10,16,16)_(0,-1)        0.00059354   0.0001924 +67.58%
    cpu_(10,10,16,16)_(2,2)         0.00062239  0.00019097 +69.32%
    cpu_(10,10,101,101)_(-1,-1)     0.00014806  6.2823e-05 +57.57%
    cpu_(10,10,101,101)_(-1,0)       0.0039785  0.00078249 +80.33%
    cpu_(10,10,101,101)_(0,-1)       0.0040585  0.00076556 +81.14%
    cpu_(10,10,101,101)_(2,2)        0.0039514  0.00077307 +80.44%
    cpu_(10,10,256,256)_(-1,-1)      0.0026824  6.0558e-05 +97.74%
    cpu_(10,10,256,256)_(-1,0)        0.017269   0.0031619 +81.69%
    cpu_(10,10,256,256)_(0,-1)        0.020287   0.0030774 +84.83%
    cpu_(10,10,256,256)_(2,2)         0.011919   0.0026599 +77.68%
    cpu_(10,10,1000,1000)_(-1,-1)     0.065783  5.6982e-05 +99.91%
    cpu_(10,10,1000,1000)_(-1,0)        0.1361    0.054533 +59.93%
    cpu_(10,10,1000,1000)_(0,-1)        0.1397    0.053405 +61.77%
    cpu_(10,10,1000,1000)_(2,2)        0.10173    0.048561 +52.26%
    cpu_(10,10,1024,1024)_(-1,-1)     0.066231  7.5579e-05 +99.89%
    cpu_(10,10,1024,1024)_(-1,0)       0.13615    0.059931 +55.98%
    cpu_(10,10,1024,1024)_(0,-1)       0.13745    0.064931 +52.76%
    cpu_(10,10,1024,1024)_(2,2)        0.10493    0.054258 +48.29%
    cpu_(10,10,2048,2048)_(-1,-1)      0.23487  6.6042e-05 +99.97%
    cpu_(10,10,2048,2048)_(-1,0)       0.41014     0.24283 +40.79%
    cpu_(10,10,2048,2048)_(0,-1)       0.43621     0.26393 +39.49%
    cpu_(10,10,2048,2048)_(2,2)        0.29919     0.22302 +25.46%
    
    gpu_(10,16,16)_(-1,-1)          0.00010753  0.00010753  +0.00%
    gpu_(10,16,16)_(-1,0)           0.00011253  0.00012445 -10.59%
    gpu_(10,16,16)_(0,-1)           0.00012493  0.00013399  -7.25%
    gpu_(10,16,16)_(2,2)              0.000108  0.00011754  -8.83%
    gpu_(10,101,101)_(-1,-1)        0.00011849  8.7976e-05 +25.75%
    gpu_(10,101,101)_(-1,0)         0.00012743  0.00012243  +3.93%
    gpu_(10,101,101)_(0,-1)         0.00012958  0.00012362  +4.60%
    gpu_(10,101,101)_(2,2)          0.00011504  0.00011504  +0.00%
    gpu_(10,256,256)_(-1,-1)        0.00013447  9.7513e-05 +27.48%
    gpu_(10,256,256)_(-1,0)         0.00018752  0.00014746 +21.36%
    gpu_(10,256,256)_(0,-1)         0.00017798  0.00016904  +5.02%
    gpu_(10,256,256)_(2,2)           0.0001514  0.00013697  +9.53%
    gpu_(10,1000,1000)_(-1,-1)       0.0005095  9.8586e-05 +80.65%
    gpu_(10,1000,1000)_(-1,0)       0.00088501  0.00056589 +36.06%
    gpu_(10,1000,1000)_(0,-1)       0.00090456  0.00055242 +38.93%
    gpu_(10,1000,1000)_(2,2)        0.00080955  0.00049639 +38.68%
    gpu_(10,1024,1024)_(-1,-1)      0.00050902  9.7036e-05 +80.94%
    gpu_(10,1024,1024)_(-1,0)       0.00098789  0.00058246 +41.04%
    gpu_(10,1024,1024)_(0,-1)            0.001  0.00059545 +40.46%
    gpu_(10,1024,1024)_(2,2)        0.00082254  0.00049961 +39.26%
    gpu_(10,2048,2048)_(-1,-1)        0.001495  9.8944e-05 +93.38%
    gpu_(10,2048,2048)_(-1,0)         0.003535   0.0017736 +49.83%
    gpu_(10,2048,2048)_(0,-1)        0.0034965   0.0017921 +48.75%
    gpu_(10,2048,2048)_(2,2)         0.0027704   0.0015399 +44.41%
    gpu_(10,10,4,4)_(-1,-1)         0.00011086  9.1076e-05 +17.85%
    gpu_(10,10,4,4)_(-1,0)           0.0001235  0.00013411  -8.59%
    gpu_(10,10,4,4)_(0,-1)          0.00011849   0.0001204  -1.61%
    gpu_(10,10,4,4)_(2,2)           0.00010896  0.00013256 -21.66%
    gpu_(10,10,10,10)_(-1,-1)       0.00010657  9.5844e-05 +10.07%
    gpu_(10,10,10,10)_(-1,0)        0.00011754  0.00013602 -15.72%
    gpu_(10,10,10,10)_(0,-1)        0.00011909  0.00012004  -0.80%
    gpu_(10,10,10,10)_(2,2)         0.00013196  0.00011349 +14.00%
    gpu_(10,10,16,16)_(-1,-1)       0.00012898  0.00010705 +17.01%
    gpu_(10,10,16,16)_(-1,0)        0.00014353  0.00012338 +14.04%
    gpu_(10,10,16,16)_(0,-1)        0.00011599  0.00012493  -7.71%
    gpu_(10,10,16,16)_(2,2)         0.00011539  0.00011349  +1.65%
    gpu_(10,10,101,101)_(-1,-1)     0.00014699  0.00010252 +30.25%
    gpu_(10,10,101,101)_(-1,0)       0.0002141  0.00015497 +27.62%
    gpu_(10,10,101,101)_(0,-1)       0.0002017  0.00015843 +21.45%
    gpu_(10,10,101,101)_(2,2)       0.00018394  0.00015402 +16.27%
    gpu_(10,10,256,256)_(-1,-1)     0.00032747  9.0003e-05 +72.52%
    gpu_(10,10,256,256)_(-1,0)      0.00074494  0.00040746 +45.30%
    gpu_(10,10,256,256)_(0,-1)      0.00072503  0.00042391 +41.53%
    gpu_(10,10,256,256)_(2,2)       0.00061846  0.00038004 +38.55%
    gpu_(10,10,1000,1000)_(-1,-1)    0.0032645  0.00010896 +96.66%
    gpu_(10,10,1000,1000)_(-1,0)      0.007543   0.0038971 +48.34%
    gpu_(10,10,1000,1000)_(0,-1)      0.006058   0.0039405 +34.95%
    gpu_(10,10,1000,1000)_(2,2)       0.005198    0.003448 +33.67%
    gpu_(10,10,1024,1024)_(-1,-1)    0.0034155  9.1434e-05 +97.32%
    gpu_(10,10,1024,1024)_(-1,0)      0.007099    0.004158 +41.43%
    gpu_(10,10,1024,1024)_(0,-1)      0.006843    0.003849 +43.75%
    gpu_(10,10,1024,1024)_(2,2)       0.005506   0.0031376 +43.02%
    gpu_(10,10,2048,2048)_(-1,-1)     0.013119  0.00010097 +99.23%
    gpu_(10,10,2048,2048)_(-1,0)      0.028533    0.015175 +46.81%
    gpu_(10,10,2048,2048)_(0,-1)      0.028458    0.014926 +47.55%
    gpu_(10,10,2048,2048)_(2,2)       0.022175    0.011797 +46.80%
    
    PiperOrigin-RevId: 168849471
    
    * * dataset_ops.read_batch_features() now discards keys for keyed Dataset.
    * dataset_ops.read_batch_features() ignores unnecessary repeat() when num_repeat == 1.
    
    PiperOrigin-RevId: 168855155
    
    * Migrate TFGAN eval to opensource.
    
    PiperOrigin-RevId: 168855880
    
    * [XLA] Remove superfluous locking from xla::ComputationBuilder.
    
    The class is thread compatible, not thread-safe. It is illegal to call non-const methods of the class concurrently. So the mutex is pointless.
    
    Also mark a couple of accessors const.
    
    PiperOrigin-RevId: 168857132
    
    * Add ConvertGraphDefToXla to convert from GraphDef to xla::Computation.
    
    The main logic is simply refactored from tfcompile, with some minor cleanups
    along the way.
    
    PiperOrigin-RevId: 168857174
    
    * Bugfix to tf.contrib.seq2seq beam_search_ops: GPU edge case of seq_len == 0.
    
    PiperOrigin-RevId: 168862288
    
    * [tf.contrib.data] Add `batch_and_drop_remainder` transformation.
    
    This transformation, which is designed for use with `Dataset.apply()`,
    acts like the default of behavior of `tf.train.batch()`, which will
    truncate a finite input source if its number of elements is not an
    exact multiple of the batch size. A benefit of using this
    transformation is that it gives a statically known shape to the output
    elements, because they are all exactly `batch_size` in the 0th
    dimension.
    
    PiperOrigin-RevId: 168863148
    
    * Minor renaming from tfcompile.Config to tf2xla.Config in comments.
    
    PiperOrigin-RevId: 168863860
    
    * Certain ops don't need eager gradients to keep their inputs / outputs alive.
    
    PiperOrigin-RevId: 168864350
    
    * [XLA] Add S64 while loop test.
    
    PiperOrigin-RevId: 168865653
    
    * tfdbg: fix a bug in list_inputs and list_outputs
    
    wherein a tensor name like "x:1" fails to be processed because it were not converted to the node name ("x" in this example) first.
    
    Also simplify analyzer_cli_test.py a little through a new helper function.
    
    PiperOrigin-RevId: 168867948
    
    * Adds multi_label_head in tf.contrib.estimator
    
    PiperOrigin-RevId: 168873313
    
    * Script that generates __init__.py files based on tf_api_names annotations.
    
    PiperOrigin-RevId: 168878737
    
    * Fixing the build command.
    
    PiperOrigin-RevId: 168881605
    
    * Make sure all checked threads are joined before they are terminated.
    
    PiperOrigin-RevId: 168884294
    
    * Output metrics in train mode for multihead.
    
    This is to be consistent with other heads who output the metric tensors in train mode. Outputting the metric tensors allow us for example to plot the metrics on the training set (and compare them to the metircs on the eval set).
    
    PiperOrigin-RevId: 168884726
    
    * Automated g4 rollback of changelist 168458634
    
    PiperOrigin-RevId: 168887778
    
    * Adds DNNEstimator to tf.contrib.estimator.
    
    PiperOrigin-RevId: 168887825
    
    * [tf.contrib.data] Expose `tf.contrib.data.batch_and_drop_remainder()`.
    
    PiperOrigin-RevId: 168888592
    
    * disabling timeout test in opensource build
    
    PiperOrigin-RevId: 168890483
    
    * Add ops that perform color transforms (including changing value, saturation and hue) in YIQ space.
    
    PiperOrigin-RevId: 168897736
    
    * Update the minimum requirement of espsilon for batch norm.
    
    PiperOrigin-RevId: 168897907
    
    * Adding support for capture-by-value.
    
    PiperOrigin-RevId: 168903482
    
    * disabling failing tsan test
    
    PiperOrigin-RevId: 168903876
    
    * disable asan for test timeout
    
    PiperOrigin-RevId: 168903999
    
    * Internal change.
    
    PiperOrigin-RevId: 168910187
    
    * Fix broken test: tensorflow/contrib/eager/python:datasets_test
    
    PiperOrigin-RevId: 168914742
    
    * [XLA:CPU] Implement map fusion.
    
    PiperOrigin-RevId: 168915358
    
    * Merge changes from github.
    END_PUBLIC
    
    I also integrated #13073 by hand to make TAP happy.
    
    ---
    Commit 92362d0f0 authored by Skye Wanderman-Milne<skyewm@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add WhileContext class and add plumbing for creating them.
    
    This change introduces WhileContext, which stores information about a
    while loop and will be used in future changes to generate while loop
    gradient graphs. Exit nodes in a while loop now have a pointer to
    their associated WhileContext. This will be used to retrieve the
    context for a given loop.
    
    This change adds an optional parameter to BuildWhileLoop() to create a
    WhileContext for the while loop (currently this is always true, but
    gradients will generate while loops without associated contexts). This
    change also adds a as-yet-unused option to BuildWhileLoop() to return
    the predicate output.
    
    PiperOrigin-RevId: 168562303
    
    ---
    Commit a4f6e7c1a authored by RJ Ryan<rjryan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add mel-scale conversion matrix support to tf.contrib.signal.
    
    PiperOrigin-RevId: 168560255
    
    ---
    Commit b00b6d23c authored by Henry Tan<henrytan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix a segmentation fault caused by invalid log directory in InternalFlush().
    
    PiperOrigin-RevId: 168557063
    
    ---
    Commit 2bc7a155a authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Rasmus Munk Larsen<rmlarsen@google.com>:
    Add uint16 support for tf.decode_raw (#12719)
    
    * Add uint16 support for tf.decode_raw
    
    This fix tries to address the request raised in 10124 where
    uint16 support for tf.decode_raw is needed. tf.decode_raw
    already support half, float32, float64, int8, int16, int32, int64,
    uint8. And uint16 was not supported.
    
    This fix adds uint16 support for tf.decode_raw.
    
    This fix fixes 10124.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Fix test failure caused by uint16 support of decode_raw and add unit tests.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit 009285c09 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove benchmark for TensorShapeOld.
    
    PiperOrigin-RevId: 168551108
    
    ---
    Commit dc1eda8a6 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Fix CHECK-failure crash if a non-tuple was passed to GetTupleElement.
    
    PiperOrigin-RevId: 168550703
    
    ---
    Commit 010922ed9 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 168549989
    
    ---
    Commit c8a6131e9 authored by Mark Daoust<markdaoust@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    make `tf.sets` examples executable
    
    Fixes #12969
    
    PiperOrigin-RevId: 168549712
    
    ---
    Commit bece65c6f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use a map instead of a vector of Children() in the BeamEntry.
    
    The assumption is that since the entries are sparse (they are all populated, but most are never Active()), using the map will save memory and make iterating over the Children() more efficient.
    
    PiperOrigin-RevId: 168548814
    
    ---
    Commit 0d5ab82ce authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 168548642
    
    ---
    Commit 3331c574b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Implementing gradients for tf.image.resize_bicubic.
    
    PiperOrigin-RevId: 168547412
    
    ---
    Commit 4982ef0fa authored by Martin Wicke<wicke@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add the ability to warn only once if deprecated functionality is used, and make that the default.
    
    PiperOrigin-RevId: 168545655
    
    ---
    Commit 99423416a authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Make shape inference error messages for the While HLO more readable. Build the error lazily.
    
    PiperOrigin-RevId: 168531083
    
    ---
    Commit d10374e45 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Discard some unneccessary logging commands.
    
    PiperOrigin-RevId: 168500721
    
    ---
    Commit 83cbabb85 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix wrong format of logging message.
    
    PiperOrigin-RevId: 168497373
    
    ---
    Commit eec4f1b3a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 168494944
    
    ---
    Commit 69301f352 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 168494220
    
    ---
    Commit 9d56f419c authored by Mingxing Tan<tanmingxing@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add crop_and_decode_jpeg_op that combines the crop and decode for better
    performance.
    
    PiperOrigin-RevId: 168493125
    
    ---
    Commit 48ddf64d0 authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Make large params test only run in opt builds.
    
    PiperOrigin-RevId: 168491913
    
    ---
    Commit 11d3ac29d authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Add tests for large numbers of parameter / return values and while loops.
    
    PiperOrigin-RevId: 168487225
    
    ---
    Commit 3cd6bdef5 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added test cases on R4 slice.
    
    PiperOrigin-RevId: 168482049
    
    ---
    Commit 46a81b5c3 authored by Jacques Pienaar<jpienaar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add cast S64 to F32 test.
    
    PiperOrigin-RevId: 168473650
    
    ---
    Commit 59bdf598d authored by Derek Murray<mrry@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add an automatically-generated "tensorflow.python.platform.build_info" script.
    
    The motivation for this script is to provide better tools for
    diagnosing load-time errors (such as the ones that plague the Windows
    build due to DLL issues). Note that the script is intended to be
    self-contained, so that it is possible to import it without loading
    the entire TensorFlow runtime.
    
    This generated script currently contains a single symbol,
    `is_cuda_build`, which records whether the build has GPU support or not.
    
    PiperOrigin-RevId: 168471034
    
    ---
    Commit c3b86347f authored by Olivia Nordquist<nolivia@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    reenabling tests that are passing
    
    PiperOrigin-RevId: 168466361
    
    ---
    Commit c728665ec authored by Henry Tan<henrytan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add const qualifiers whenever appropriate.
    
    PiperOrigin-RevId: 168465926
    
    ---
    Commit bf96fcd13 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use the scalar cache in MeanGrad.
    
    PiperOrigin-RevId: 168462267
    
    ---
    Commit 1cada9ea2 authored by Olivia Nordquist<nolivia@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    reenabling test that passed after 100 runs w/o timing out
    
    PiperOrigin-RevId: 168458634
    
    ---
    Commit 00c865566 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Generate error (instead of segfault) when trying to copy string tensor
    to GPU in EagerTensor constructor.
    
    PiperOrigin-RevId: 168457320
    
    ---
    Commit 655f26fc7 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Resurrects autograd-free eager gradients.
    
    PiperOrigin-RevId: 168448557
    
    ---
    Commit 8f37f3002 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [TF:XLA] Cleanups to handling of arguments during XLA compilation:
    * combine resource kinds in XlaCompiler::Argument::Kind, use a separate XlaResource::Kind field to distinguish different kinds of resource.
    * merge XlaContext::HandleOrConstant and XlaExpression, which were almost identical.
    * remove XlaContext::Argument; instead, build XlaExpressions directly from XlaCompiler and add them to the XlaContext.
    
    PiperOrigin-RevId: 168439341
    
    ---
    Commit 7f5346a80 authored by Gunhan Gulsoy<gunan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Reduce cmake log mess.
    
    * Echo off for the .bat scripts.
    * TF cmake: disable warnings in some of the patched projects (gif,jpeg,lmdb).
    
    PiperOrigin-RevId: 168432070
    
    ---
    Commit 2ad85aa4d authored by Mark Heffernan<meheff@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use xla/tests:xla_internal_test_main for all tests under tf/compiler/xla
    and remove any main() definitions in tests. This enables use of flags
    in all tests.
    
    PiperOrigin-RevId: 168424796
    
    ---
    Commit cd377811d authored by Henry Tan<henrytan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Comment and error message consistency cleanup.
    
    PiperOrigin-RevId: 168422582
    
    ---
    Commit 7c19b82af authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update tf.sparse_reset_shape so that when shrinking the shape of an empty
    sparse tensor, the result has a shape of all zeros.
    
    PiperOrigin-RevId: 168419639
    
    ---
    Commit fcacb40d4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    FirstReadyManager for scheduling nodes in VirtualScheduler.
    The current FIFOManager may yield inefficient scheduling; _Recv pushed to the
    FIFO blocks other nodes that can run before _Recv due to the node order in FIFO.
    FirstReadyManager picks a node with the earliest time_ready in the queue,
    avoiding this problem.
    
    Also, fixed VirtualPlacer to properly set device when Node's device name does not
    include job name and to set GPU:0 as default device.
    
    PiperOrigin-RevId: 168418455
    
    ---
    Commit 7e47624f5 authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    eager: Initial support for iteration over tf.contrib.data.Dataset objects.
    
    TODO:
    - Support function-valued operation attributes in eager
      (Required for MapDataset, FilterDataset etc. which encode the
      per-element computation in a TensorFlow function)
    PiperOrigin-RevId: 168418250
    
    ---
    Commit b0a397fce authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    eager: Remove unnecessary TFE_Context argument to TFE_OpSetDevice.
    
    PiperOrigin-RevId: 168417999
    
    ---
    Commit 86211d554 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Graph transform to flatten atrous (dilated) convolutions (i.e., a sequence of SpaceToBatchND-Conv-BatchToSpaceND ops) to a regular Conv op with upsampled filters.
    
    PiperOrigin-RevId: 168414124
    
    ---
    Commit 3438981ca authored by David G. Andersen<dga@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Apply exported symbol filtering to the c++ API analogously to
    what is filtered for the C API.
    Fixes bug reported in comments on #1924
    
    PiperOrigin-RevId: 168413719
    
    ---
    Commit 7e023d865 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA:CPU] Remove code from parallel CPU backend outlining that was causing unnecessary copies to be inserted, and which is no longer necessary since we added co-located buffer support for kCall.
    *) All bitcast copy is no longer necessary as CopyInsertion will insert copies
    at the root of the computation for a parameter which is live-out.
    *) Copy if root does not define buffer no longer necessary because colocated
    assignment looks at points-to set of root instruction.
    
    PiperOrigin-RevId: 168412076
    
    ---
    Commit 5da4df92c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Simplify some code in grappler_item_builder.cc, no change in logic.
    
    PiperOrigin-RevId: 168409110
    
    ---
    Commit 82ec6241a authored by drpngx<drpngx@users.noreply.github.com>
    Committed by GitHub<noreply@github.com>:
    Add six and numpy imports
    ---
    Commit 9c4ce2452 authored by Mark Heffernan<meheff@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add flag parsing to more tests in xla/service specifically those which build
    HLO graphs. This enables, for example, dumping of the graphs with
    --xla_generate_hlo_graph. Also remove some superfluous tensorflow test_main
    dependencies.
    
    PiperOrigin-RevId: 168406746
    
    ---
    Commit d4efa695c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Relax the feed_nodes collection check, which triggers a false positive in some modes where the feed node collection is auto-generated. Keep it as a warning to help correct user-provided feed node lists.
    
    PiperOrigin-RevId: 168396408
    
    ---
    Commit cbc46a856 authored by Changming Sun<chasun@microsoft.com>
    Committed by gunan<gunan@google.com>:
    Add a missing template explicit instantiation of SetZeroFunctor (#12791)
    
    ---
    Commit 7bb08f5bf authored by Kevin Slagle<kjslag@gmail.com>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    fix ExponentialMovingAverage documentation so that ExponentialMovingAverage.apply is evaluated within control_dependencies (#12987)
    
    ---
    Commit e6b011763 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Extend c++ gradient_checker to complex types.
    
    PiperOrigin-RevId: 168392949
    
    ---
    Commit 4086219a4 authored by Lyndon White<oxinabox@ucc.asn.au>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    Correct minor typo in substr docs example (#12991)
    
    ---
    Commit f63aa7f49 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Migrate core TFGAN functions to opensource.
    
    PiperOrigin-RevId: 168391923
    
    ---
    Commit bc6b60f1b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix tuple_losses bug caused by Python bug.
    
    PiperOrigin-RevId: 168386341
    
    ---
    Commit 7a8c63da3 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Migrate `leaky_relu` to `nn_ops.py`. Will be used for TFGAN.
    
    PiperOrigin-RevId: 168386268
    
    ---
    Commit f7ba16fdf authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Do not export from eval on train data steps.
    
    PiperOrigin-RevId: 168374021
    
    ---
    Commit 9b9e54b34 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adding NCCL sum op, register all_sum gradient.
    Streamlining nccl test.
    
    PiperOrigin-RevId: 168347428
    
    ---
    Commit bc300318e authored by Gunhan Gulsoy<gunan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update gemmlowp hash as the commit history seems to have changed in the
    repository.
    
    PiperOrigin-RevId: 168343607
    
    ---
    Commit 1e96d54d9 authored by gunan<gunan@google.com>
    Committed by GitHub<noreply@github.com>:
    Also accept non-k8 CPU types in build pip package. (#12975)
    
    * Also accept non-k8 CPU types in build pip package.
    Fixes #12735
    
    * Make the script work with `set -e`.
    
    ---
    Commit c0a4c7ffc authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Fix bug in ShapeUtil::ShapeIs that would lead to type inference errors.
    
    PiperOrigin-RevId: 168323589
    
    ---
    Commit 4af9be964 authored by Amy<amy@infosleuth.net>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    support passing in a source url to the mnist read_data_sets function, to make it easier to use 'fashion mnist' etc. (#12983)
    
    ---
    Commit 9f848734f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Tweak layer a bit to be eager friendly.
    
    PiperOrigin-RevId: 168312865
    
    ---
    Commit 60f15462b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Change conv_input_scale and side_input_scale from attributes to inputs for improved flexibility, in fused_conv2d_bias_activation op.
    
    PiperOrigin-RevId: 168311988
    
    ---
    Commit 4b4e10f9c authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds dict support of eval metrics.
    
    PiperOrigin-RevId: 168310444
    
    ---
    Commit ab7f22de6 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Move FusedConvBiasActivationShape out of common_shape_fns.cc to a lambda inside the op.
    
    PiperOrigin-RevId: 168300911
    
    ---
    Commit 3a98035fa authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Augment metadata output with source-line info, as before.
    
    PiperOrigin-RevId: 168292527
    
    ---
    Commit 349188152 authored by Yao Zhang<yaozhang@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Enable fused batch norm, which is 15-20% faster for training and inference.
    
    PiperOrigin-RevId: 168288154
    
    ---
    Commit 08587d45b authored by Yuefeng Zhou<yuefengz@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added back persistent memory tracking in queue op. The new tracking logic has avoided the crash in previous implementation:  the queue_ passed to CreateTypedQueue may be unreffed if the resource is already created by another resource op that shares the same resource name and type.
    
    PiperOrigin-RevId: 168284509
    
    ---
    Commit 733063d55 authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Fixing awkward wording.
    
    ---
    Commit c7ad6bfef authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Removing accidental hash.
    
    ---
    Commit 53dbc761a authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Adding Windows self check script to docs.
    
    ---
    Commit ed1135994 authored by Andrew Harp<andrewharp@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add -latomic flag to benchmark_model target to fix Android x86 build.
    
    PiperOrigin-RevId: 168281337
    
    ---
    Commit c0348bb55 authored by Anna R<annarev@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update tf_export.py to take constant name as an argument instead of a constant.
    
    PiperOrigin-RevId: 168280613
    
    ---
    Commit c3d19e40a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Cleanup training_ops to reduce code redudancy.
    
    PiperOrigin-RevId: 168280069
    
    ---
    Commit 123fb01ee authored by Yao Zhang<yaozhang@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Set fused=False for batch norm, because the test assumes no bessel's
    correction. Fused=True would add bessel's correction to variance.
    
    PiperOrigin-RevId: 168274392
    
    ---
    Commit f0e8c545e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Switch resource variables from copy-on-read to copy-on-write.
    
    RELNOTES: Change the signature of (C++) GetInputTensorFromVariable in
    training_op_helpers to support new copy-on-write semenatics of resource
    variables.
    PiperOrigin-RevId: 168273249
    
    ---
    Commit 495cc8e47 authored by Yuan (Terry) Tang<terrytangyuan@users.noreply.github.com>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    Minor wording change in timeseries module's README (#12938)
    
    * Minor wording change in timeseries module's README
    
    * Address comments
    
    ---
    Commit f13b876ed authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Making the default build from source version 1.4.0dev. The whl files that are built will be 1.3.0devDDMMYYYY.
    
    ---
    Commit 2356c0ff4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Delete ScopedTFStatus to avoid leaking it for long running trainers(1+day).
    
    PiperOrigin-RevId: 168259652
    
    ---
    Commit e15f4cae2 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Don't remove all aliases from linalg namespace.
    Get rid of redundant aliases.
    
    PiperOrigin-RevId: 168257658
    
    ---
    Commit c58082642 authored by postBG<profile2697@gmail.com>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    Fix minor typo in Programmers guide (#12965)
    
    * Fix minor typo in Programmers guide
    
    * change to "this"
    
    ---
    Commit 509372c2e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a lot of operations' flops calculations
    
    PiperOrigin-RevId: 168256746
    
    ---
    Commit 80ed8afc0 authored by Francois Chollet<fchollet@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add Flatten to core layers.
    
    PiperOrigin-RevId: 168254118
    
    ---
    Commit a6223c01a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix locking of variables in SparseProximalGradientDescent,
    AdagradDA, SparseAdagradDA.
    
    PiperOrigin-RevId: 168252530
    
    ---
    Commit abde00830 authored by Olivia Nordquist<nolivia@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    adding InputTensor class for symmetry with OutputTensor
    
    PiperOrigin-RevId: 168250085
    
    ---
    Commit 0451032ca authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Fix variable naming style guide violation.
    
    PiperOrigin-RevId: 168245542
    
    ---
    Commit a202a5a94 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 168245371
    
    ---
    Commit f93e354cb authored by Derek Murray<mrry@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [tf.contrib.data] Switch backend Dataset representation to DT_VARIANT.
    
    This change introduces a new `DatasetWrapper` type that wraps a
    `DatasetBase*` and can be stored in a DT_VARIANT tensor. All Dataset
    ops now consume and produce DT_VARIANT instead of DT_RESOURCE, and the
    underlying implementation is simplified because the `DatasetWrapper`
    can be passed directly by value without using the `ResourceMgr`.
    
    PiperOrigin-RevId: 168240571
    
    ---
    Commit a4042cd2a authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Introduces the placeholder for _TrainingExecutor, which serves the implementation of tf.estimator.train_and_evaluate.
    
    PiperOrigin-RevId: 168240151
    
    ---
    Commit 10ba148f7 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Switch control_flow_ops library to use Resource variants of Stack operators, instead of deprecated Ref variants.
    
    PiperOrigin-RevId: 168234822
    
    ---
    Commit ca43fe82b authored by Ali Yahya<alive@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    TFE: Improves the interfaces of tape.watch_variable() and implicit_grad().
    
    tape.watch_variable() replaces tape.watch() and now is called on ResourceVariable objects instead of their underlying handles.
    
    implicit_grad() now returns a list of (gradient, variable) pairs to be consistent with tf.Optimizer's interface.
    
    PiperOrigin-RevId: 168232055
    
    ---
    Commit b72862dfc authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    internal change
    
    PiperOrigin-RevId: 168225993
    
    ---
    Commit da3280f4d authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Re-enable tsan for sdca_estimator_test.
    
    PiperOrigin-RevId: 168186374
    
    ---
    Commit c936c1155 authored by Yifei Feng<yifeif@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix pip tests for contrib/gan.
    - Add *_impl.py so tests can still access removed symbols.
    - Add /python directory layer to make *_impy.py and __init__.py not in the same dir.
    
    PiperOrigin-RevId: 168161722
    
    ---
    Commit ce9a2b00f authored by Toby Boyd<tobyboyd@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Performance guide update
    
    PiperOrigin-RevId: 168159289
    
    ---
    Commit 3bce4f9a0 authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    TFE: expose tfe.num_gpus()
    
    PiperOrigin-RevId: 168154345
    
    ---
    Commit 67a7cbc28 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Changed the default eval throttle secs from 2 min to 10 mins.
    
    PiperOrigin-RevId: 168120323
    
    ---
    Commit 92bed178f authored by Eugene Brevdo<ebrevdo@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Reduce cmake log mess.
    
    * Echo off for the .bat scripts.
    * TF cmake: disable warnings in some of the patched projects (gif,jpeg,lmdb).
    
    PiperOrigin-RevId: 168119914
    
    ---
    Commit 702d59582 authored by joshkyh<joshkyh@users.noreply.github.com>
    Committed by Yifei Feng<fengyifei2026@gmail.com>:
    Corrected hyperlink for audio training tutorial (#12923)
    
    ---
    Commit 877c9deca authored by Frank Chen<frankchn@gmail.com>
    Committed by Yifei Feng<fengyifei2026@gmail.com>:
    Reverse change eb75ded6 so that internal tests will pass. (#12933)
    
    As support for int64 global steps is not ready in TPUs, I am reversing this change so that our internal performance and regression tests will pass.
    ---
    Commit 665966438 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Re-enable grpc_session_test.
    
    PiperOrigin-RevId: 168078694
    
    ---
    Commit 405def792 authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Switch CallInliner to use CallGraph::VisitNodes.
    
    PiperOrigin-RevId: 168078645
    
    ---
    Commit aba3466f1 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Exposes Head and factory methods in tf.contrib.estimator.
    
    PiperOrigin-RevId: 168071246
    
    ---
    Commit b76565b39 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Some profiler fixes and cleanup.
    
    PiperOrigin-RevId: 168069346
    
    ---
    Commit 32ffc5a81 authored by Jonas<sauercrowd@users.noreply.github.com>
    Committed by Yifei Feng<fengyifei2026@gmail.com>:
    Just a dot in order to be consistent (#12919)
    
    added a dot to the `7` to make clear it's a float (like every other number)
    ---
    Commit 0753b0c79 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Scope the scalar cache in the context.
    
    PiperOrigin-RevId: 168065417
    
    ---
    Commit 48deb206b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Migrate TFGAN features to third_party.
    
    PiperOrigin-RevId: 168060880
    
    ---
    Commit d2ae1311f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixing an issue in the BUILD file of the LSH ops.
    
    PiperOrigin-RevId: 168056645
    
    ---
    Commit 2f440eda4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Expose NumpyReader for reading timeseries data.
    
    PiperOrigin-RevId: 168055838
    
    ---
    Commit be1916ce7 authored by Daniel Grazian<dgr@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added functionality to allow `SqlDataset` to interpret a database column as various numeric types, including several integer types and `dtypes.float64`.
    
    PiperOrigin-RevId: 168055827
    
    ---
    Commit fa2000a0b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Supporting nightly windows pip packages.
    
    PiperOrigin-RevId: 168054959
    
    ---
    Commit a263ea626 authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    eager: Treat eager tensors as constants during graph construction.
    
    Unless capturing is explicitly enabled.
    
    PiperOrigin-RevId: 168052675
    
    ---
    Commit 6e402d0d2 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make TODO a bit more specific.
    
    PiperOrigin-RevId: 168051381
    
    ---
    Commit c779384bc authored by Daniel Grazian<dgr@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added code example to the doc string for `SqlDataset`.
    
    PiperOrigin-RevId: 168049037
    
    ---
    Commit ff6dd474a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use self._in_graph_mode consistently in ResourceVariable
    instead of sometimes getting it from the context.
    
    Also: fix formatting of a comment and use a more precise test to detect
    if initial_value is set.
    PiperOrigin-RevId: 168047258
    
    ---
    Commit f331f528b authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Removes "fast paths" which are not fast in eager mode.
    
    PiperOrigin-RevId: 168046278
    
    ---
    Commit 86f1713e5 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Introduces TrainSpec and EvalSpec.
    
    PiperOrigin-RevId: 168040435
    
    ---
    Commit c8b9e92f0 authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    eager: Move "register_function" to context.py
    
    This will allow function registration from other
    modules without having to import "function.py".
    (And besides, the function really does belong on the context).
    
    PiperOrigin-RevId: 168040411
    
    ---
    Commit 74137f994 authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix signed int overflow issue in tensor_id.cc
    
    When a node name has a long numeric suffix, e.g.,
    "foo/y_0/gradient_debug_09684b60f2184c67b744721915034528" (as has happened with tfdbg GradientsDebugger),
    
    the parsing algorithm in ParseTensorName() may experience signed int overflow. Replacing the types with "unsigned int" resolves the issue.
    
    PiperOrigin-RevId: 168039195
    
    ---
    Commit 450c3b562 authored by Rohan Jain<rohanj@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Using rendezvous manager to pass args / rets between devices during function remote execution. This enables CPU->GPU remote device executions now.
    
    PiperOrigin-RevId: 168038285
    
    ---
    Commit 82cc6529f authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixes the wording about StopIteration.
    
    PiperOrigin-RevId: 168034451
    
    ---
    Commit fb5588002 authored by Gunhan Gulsoy<gunan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a statement on install/index.md on what os are supported.
    
    PiperOrigin-RevId: 168032996
    
    ---
    Commit f83f6b9ef authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Handle higher-order HLOs (e.g. While) in CallInliner and test.
    
    PiperOrigin-RevId: 168029345
    
    ---
    Commit 8988ae365 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    BEGIN_PUBLIC
    Automated g4 rollback of changelist 167916124
    
    PiperOrigin-RevId: 168916710
    
    * Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 168917157
    
    * Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 168917534

commit a373b1f74215e44920bf9362a51bece530edf88a
Author: Patrick Nguyen <drpng@google.com>
Date:   Fri Sep 15 18:14:40 2017 -0700

    Merge changes from github.
    END_PUBLIC
    
    I also integrated #13073 by hand to make TAP happy.
    
    ---
    Commit 92362d0f0 authored by Skye Wanderman-Milne<skyewm@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add WhileContext class and add plumbing for creating them.
    
    This change introduces WhileContext, which stores information about a
    while loop and will be used in future changes to generate while loop
    gradient graphs. Exit nodes in a while loop now have a pointer to
    their associated WhileContext. This will be used to retrieve the
    context for a given loop.
    
    This change adds an optional parameter to BuildWhileLoop() to create a
    WhileContext for the while loop (currently this is always true, but
    gradients will generate while loops without associated contexts). This
    change also adds a as-yet-unused option to BuildWhileLoop() to return
    the predicate output.
    
    PiperOrigin-RevId: 168562303
    
    ---
    Commit a4f6e7c1a authored by RJ Ryan<rjryan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add mel-scale conversion matrix support to tf.contrib.signal.
    
    PiperOrigin-RevId: 168560255
    
    ---
    Commit b00b6d23c authored by Henry Tan<henrytan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix a segmentation fault caused by invalid log directory in InternalFlush().
    
    PiperOrigin-RevId: 168557063
    
    ---
    Commit 2bc7a155a authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Rasmus Munk Larsen<rmlarsen@google.com>:
    Add uint16 support for tf.decode_raw (#12719)
    
    * Add uint16 support for tf.decode_raw
    
    This fix tries to address the request raised in 10124 where
    uint16 support for tf.decode_raw is needed. tf.decode_raw
    already support half, float32, float64, int8, int16, int32, int64,
    uint8. And uint16 was not supported.
    
    This fix adds uint16 support for tf.decode_raw.
    
    This fix fixes 10124.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    * Fix test failure caused by uint16 support of decode_raw and add unit tests.
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit 009285c09 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove benchmark for TensorShapeOld.
    
    PiperOrigin-RevId: 168551108
    
    ---
    Commit dc1eda8a6 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Fix CHECK-failure crash if a non-tuple was passed to GetTupleElement.
    
    PiperOrigin-RevId: 168550703
    
    ---
    Commit 010922ed9 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 168549989
    
    ---
    Commit c8a6131e9 authored by Mark Daoust<markdaoust@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    make `tf.sets` examples executable
    
    Fixes #12969
    
    PiperOrigin-RevId: 168549712
    
    ---
    Commit bece65c6f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use a map instead of a vector of Children() in the BeamEntry.
    
    The assumption is that since the entries are sparse (they are all populated, but most are never Active()), using the map will save memory and make iterating over the Children() more efficient.
    
    PiperOrigin-RevId: 168548814
    
    ---
    Commit 0d5ab82ce authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 168548642
    
    ---
    Commit 3331c574b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Implementing gradients for tf.image.resize_bicubic.
    
    PiperOrigin-RevId: 168547412
    
    ---
    Commit 4982ef0fa authored by Martin Wicke<wicke@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add the ability to warn only once if deprecated functionality is used, and make that the default.
    
    PiperOrigin-RevId: 168545655
    
    ---
    Commit 99423416a authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Make shape inference error messages for the While HLO more readable. Build the error lazily.
    
    PiperOrigin-RevId: 168531083
    
    ---
    Commit d10374e45 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Discard some unneccessary logging commands.
    
    PiperOrigin-RevId: 168500721
    
    ---
    Commit 83cbabb85 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix wrong format of logging message.
    
    PiperOrigin-RevId: 168497373
    
    ---
    Commit eec4f1b3a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 168494944
    
    ---
    Commit 69301f352 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 168494220
    
    ---
    Commit 9d56f419c authored by Mingxing Tan<tanmingxing@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add crop_and_decode_jpeg_op that combines the crop and decode for better
    performance.
    
    PiperOrigin-RevId: 168493125
    
    ---
    Commit 48ddf64d0 authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Make large params test only run in opt builds.
    
    PiperOrigin-RevId: 168491913
    
    ---
    Commit 11d3ac29d authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Add tests for large numbers of parameter / return values and while loops.
    
    PiperOrigin-RevId: 168487225
    
    ---
    Commit 3cd6bdef5 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added test cases on R4 slice.
    
    PiperOrigin-RevId: 168482049
    
    ---
    Commit 46a81b5c3 authored by Jacques Pienaar<jpienaar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add cast S64 to F32 test.
    
    PiperOrigin-RevId: 168473650
    
    ---
    Commit 59bdf598d authored by Derek Murray<mrry@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add an automatically-generated "tensorflow.python.platform.build_info" script.
    
    The motivation for this script is to provide better tools for
    diagnosing load-time errors (such as the ones that plague the Windows
    build due to DLL issues). Note that the script is intended to be
    self-contained, so that it is possible to import it without loading
    the entire TensorFlow runtime.
    
    This generated script currently contains a single symbol,
    `is_cuda_build`, which records whether the build has GPU support or not.
    
    PiperOrigin-RevId: 168471034
    
    ---
    Commit c3b86347f authored by Olivia Nordquist<nolivia@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    reenabling tests that are passing
    
    PiperOrigin-RevId: 168466361
    
    ---
    Commit c728665ec authored by Henry Tan<henrytan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add const qualifiers whenever appropriate.
    
    PiperOrigin-RevId: 168465926
    
    ---
    Commit bf96fcd13 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use the scalar cache in MeanGrad.
    
    PiperOrigin-RevId: 168462267
    
    ---
    Commit 1cada9ea2 authored by Olivia Nordquist<nolivia@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    reenabling test that passed after 100 runs w/o timing out
    
    PiperOrigin-RevId: 168458634
    
    ---
    Commit 00c865566 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Generate error (instead of segfault) when trying to copy string tensor
    to GPU in EagerTensor constructor.
    
    PiperOrigin-RevId: 168457320
    
    ---
    Commit 655f26fc7 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Resurrects autograd-free eager gradients.
    
    PiperOrigin-RevId: 168448557
    
    ---
    Commit 8f37f3002 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [TF:XLA] Cleanups to handling of arguments during XLA compilation:
    * combine resource kinds in XlaCompiler::Argument::Kind, use a separate XlaResource::Kind field to distinguish different kinds of resource.
    * merge XlaContext::HandleOrConstant and XlaExpression, which were almost identical.
    * remove XlaContext::Argument; instead, build XlaExpressions directly from XlaCompiler and add them to the XlaContext.
    
    PiperOrigin-RevId: 168439341
    
    ---
    Commit 7f5346a80 authored by Gunhan Gulsoy<gunan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Reduce cmake log mess.
    
    * Echo off for the .bat scripts.
    * TF cmake: disable warnings in some of the patched projects (gif,jpeg,lmdb).
    
    PiperOrigin-RevId: 168432070
    
    ---
    Commit 2ad85aa4d authored by Mark Heffernan<meheff@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use xla/tests:xla_internal_test_main for all tests under tf/compiler/xla
    and remove any main() definitions in tests. This enables use of flags
    in all tests.
    
    PiperOrigin-RevId: 168424796
    
    ---
    Commit cd377811d authored by Henry Tan<henrytan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Comment and error message consistency cleanup.
    
    PiperOrigin-RevId: 168422582
    
    ---
    Commit 7c19b82af authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update tf.sparse_reset_shape so that when shrinking the shape of an empty
    sparse tensor, the result has a shape of all zeros.
    
    PiperOrigin-RevId: 168419639
    
    ---
    Commit fcacb40d4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    FirstReadyManager for scheduling nodes in VirtualScheduler.
    The current FIFOManager may yield inefficient scheduling; _Recv pushed to the
    FIFO blocks other nodes that can run before _Recv due to the node order in FIFO.
    FirstReadyManager picks a node with the earliest time_ready in the queue,
    avoiding this problem.
    
    Also, fixed VirtualPlacer to properly set device when Node's device name does not
    include job name and to set GPU:0 as default device.
    
    PiperOrigin-RevId: 168418455
    
    ---
    Commit 7e47624f5 authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    eager: Initial support for iteration over tf.contrib.data.Dataset objects.
    
    TODO:
    - Support function-valued operation attributes in eager
      (Required for MapDataset, FilterDataset etc. which encode the
      per-element computation in a TensorFlow function)
    PiperOrigin-RevId: 168418250
    
    ---
    Commit b0a397fce authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    eager: Remove unnecessary TFE_Context argument to TFE_OpSetDevice.
    
    PiperOrigin-RevId: 168417999
    
    ---
    Commit 86211d554 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Graph transform to flatten atrous (dilated) convolutions (i.e., a sequence of SpaceToBatchND-Conv-BatchToSpaceND ops) to a regular Conv op with upsampled filters.
    
    PiperOrigin-RevId: 168414124
    
    ---
    Commit 3438981ca authored by David G. Andersen<dga@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Apply exported symbol filtering to the c++ API analogously to
    what is filtered for the C API.
    Fixes bug reported in comments on #1924
    
    PiperOrigin-RevId: 168413719
    
    ---
    Commit 7e023d865 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA:CPU] Remove code from parallel CPU backend outlining that was causing unnecessary copies to be inserted, and which is no longer necessary since we added co-located buffer support for kCall.
    *) All bitcast copy is no longer necessary as CopyInsertion will insert copies
    at the root of the computation for a parameter which is live-out.
    *) Copy if root does not define buffer no longer necessary because colocated
    assignment looks at points-to set of root instruction.
    
    PiperOrigin-RevId: 168412076
    
    ---
    Commit 5da4df92c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Simplify some code in grappler_item_builder.cc, no change in logic.
    
    PiperOrigin-RevId: 168409110
    
    ---
    Commit 82ec6241a authored by drpngx<drpngx@users.noreply.github.com>
    Committed by GitHub<noreply@github.com>:
    Add six and numpy imports
    ---
    Commit 9c4ce2452 authored by Mark Heffernan<meheff@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add flag parsing to more tests in xla/service specifically those which build
    HLO graphs. This enables, for example, dumping of the graphs with
    --xla_generate_hlo_graph. Also remove some superfluous tensorflow test_main
    dependencies.
    
    PiperOrigin-RevId: 168406746
    
    ---
    Commit d4efa695c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Relax the feed_nodes collection check, which triggers a false positive in some modes where the feed node collection is auto-generated. Keep it as a warning to help correct user-provided feed node lists.
    
    PiperOrigin-RevId: 168396408
    
    ---
    Commit cbc46a856 authored by Changming Sun<chasun@microsoft.com>
    Committed by gunan<gunan@google.com>:
    Add a missing template explicit instantiation of SetZeroFunctor (#12791)
    
    ---
    Commit 7bb08f5bf authored by Kevin Slagle<kjslag@gmail.com>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    fix ExponentialMovingAverage documentation so that ExponentialMovingAverage.apply is evaluated within control_dependencies (#12987)
    
    ---
    Commit e6b011763 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Extend c++ gradient_checker to complex types.
    
    PiperOrigin-RevId: 168392949
    
    ---
    Commit 4086219a4 authored by Lyndon White<oxinabox@ucc.asn.au>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    Correct minor typo in substr docs example (#12991)
    
    ---
    Commit f63aa7f49 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Migrate core TFGAN functions to opensource.
    
    PiperOrigin-RevId: 168391923
    
    ---
    Commit bc6b60f1b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix tuple_losses bug caused by Python bug.
    
    PiperOrigin-RevId: 168386341
    
    ---
    Commit 7a8c63da3 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Migrate `leaky_relu` to `nn_ops.py`. Will be used for TFGAN.
    
    PiperOrigin-RevId: 168386268
    
    ---
    Commit f7ba16fdf authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Do not export from eval on train data steps.
    
    PiperOrigin-RevId: 168374021
    
    ---
    Commit 9b9e54b34 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adding NCCL sum op, register all_sum gradient.
    Streamlining nccl test.
    
    PiperOrigin-RevId: 168347428
    
    ---
    Commit bc300318e authored by Gunhan Gulsoy<gunan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update gemmlowp hash as the commit history seems to have changed in the
    repository.
    
    PiperOrigin-RevId: 168343607
    
    ---
    Commit 1e96d54d9 authored by gunan<gunan@google.com>
    Committed by GitHub<noreply@github.com>:
    Also accept non-k8 CPU types in build pip package. (#12975)
    
    * Also accept non-k8 CPU types in build pip package.
    Fixes #12735
    
    * Make the script work with `set -e`.
    
    ---
    Commit c0a4c7ffc authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Fix bug in ShapeUtil::ShapeIs that would lead to type inference errors.
    
    PiperOrigin-RevId: 168323589
    
    ---
    Commit 4af9be964 authored by Amy<amy@infosleuth.net>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    support passing in a source url to the mnist read_data_sets function, to make it easier to use 'fashion mnist' etc. (#12983)
    
    ---
    Commit 9f848734f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Tweak layer a bit to be eager friendly.
    
    PiperOrigin-RevId: 168312865
    
    ---
    Commit 60f15462b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Change conv_input_scale and side_input_scale from attributes to inputs for improved flexibility, in fused_conv2d_bias_activation op.
    
    PiperOrigin-RevId: 168311988
    
    ---
    Commit 4b4e10f9c authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds dict support of eval metrics.
    
    PiperOrigin-RevId: 168310444
    
    ---
    Commit ab7f22de6 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Move FusedConvBiasActivationShape out of common_shape_fns.cc to a lambda inside the op.
    
    PiperOrigin-RevId: 168300911
    
    ---
    Commit 3a98035fa authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Augment metadata output with source-line info, as before.
    
    PiperOrigin-RevId: 168292527
    
    ---
    Commit 349188152 authored by Yao Zhang<yaozhang@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Enable fused batch norm, which is 15-20% faster for training and inference.
    
    PiperOrigin-RevId: 168288154
    
    ---
    Commit 08587d45b authored by Yuefeng Zhou<yuefengz@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added back persistent memory tracking in queue op. The new tracking logic has avoided the crash in previous implementation:  the queue_ passed to CreateTypedQueue may be unreffed if the resource is already created by another resource op that shares the same resource name and type.
    
    PiperOrigin-RevId: 168284509
    
    ---
    Commit 733063d55 authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Fixing awkward wording.
    
    ---
    Commit c7ad6bfef authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Removing accidental hash.
    
    ---
    Commit 53dbc761a authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Adding Windows self check script to docs.
    
    ---
    Commit ed1135994 authored by Andrew Harp<andrewharp@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add -latomic flag to benchmark_model target to fix Android x86 build.
    
    PiperOrigin-RevId: 168281337
    
    ---
    Commit c0348bb55 authored by Anna R<annarev@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update tf_export.py to take constant name as an argument instead of a constant.
    
    PiperOrigin-RevId: 168280613
    
    ---
    Commit c3d19e40a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Cleanup training_ops to reduce code redudancy.
    
    PiperOrigin-RevId: 168280069
    
    ---
    Commit 123fb01ee authored by Yao Zhang<yaozhang@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Set fused=False for batch norm, because the test assumes no bessel's
    correction. Fused=True would add bessel's correction to variance.
    
    PiperOrigin-RevId: 168274392
    
    ---
    Commit f0e8c545e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Switch resource variables from copy-on-read to copy-on-write.
    
    RELNOTES: Change the signature of (C++) GetInputTensorFromVariable in
    training_op_helpers to support new copy-on-write semenatics of resource
    variables.
    PiperOrigin-RevId: 168273249
    
    ---
    Commit 495cc8e47 authored by Yuan (Terry) Tang<terrytangyuan@users.noreply.github.com>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    Minor wording change in timeseries module's README (#12938)
    
    * Minor wording change in timeseries module's README
    
    * Address comments
    
    ---
    Commit f13b876ed authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Making the default build from source version 1.4.0dev. The whl files that are built will be 1.3.0devDDMMYYYY.
    
    ---
    Commit 2356c0ff4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Delete ScopedTFStatus to avoid leaking it for long running trainers(1+day).
    
    PiperOrigin-RevId: 168259652
    
    ---
    Commit e15f4cae2 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Don't remove all aliases from linalg namespace.
    Get rid of redundant aliases.
    
    PiperOrigin-RevId: 168257658
    
    ---
    Commit c58082642 authored by postBG<profile2697@gmail.com>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    Fix minor typo in Programmers guide (#12965)
    
    * Fix minor typo in Programmers guide
    
    * change to "this"
    
    ---
    Commit 509372c2e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a lot of operations' flops calculations
    
    PiperOrigin-RevId: 168256746
    
    ---
    Commit 80ed8afc0 authored by Francois Chollet<fchollet@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add Flatten to core layers.
    
    PiperOrigin-RevId: 168254118
    
    ---
    Commit a6223c01a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix locking of variables in SparseProximalGradientDescent,
    AdagradDA, SparseAdagradDA.
    
    PiperOrigin-RevId: 168252530
    
    ---
    Commit abde00830 authored by Olivia Nordquist<nolivia@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    adding InputTensor class for symmetry with OutputTensor
    
    PiperOrigin-RevId: 168250085
    
    ---
    Commit 0451032ca authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Fix variable naming style guide violation.
    
    PiperOrigin-RevId: 168245542
    
    ---
    Commit a202a5a94 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 168245371
    
    ---
    Commit f93e354cb authored by Derek Murray<mrry@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [tf.contrib.data] Switch backend Dataset representation to DT_VARIANT.
    
    This change introduces a new `DatasetWrapper` type that wraps a
    `DatasetBase*` and can be stored in a DT_VARIANT tensor. All Dataset
    ops now consume and produce DT_VARIANT instead of DT_RESOURCE, and the
    underlying implementation is simplified because the `DatasetWrapper`
    can be passed directly by value without using the `ResourceMgr`.
    
    PiperOrigin-RevId: 168240571
    
    ---
    Commit a4042cd2a authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Introduces the placeholder for _TrainingExecutor, which serves the implementation of tf.estimator.train_and_evaluate.
    
    PiperOrigin-RevId: 168240151
    
    ---
    Commit 10ba148f7 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Switch control_flow_ops library to use Resource variants of Stack operators, instead of deprecated Ref variants.
    
    PiperOrigin-RevId: 168234822
    
    ---
    Commit ca43fe82b authored by Ali Yahya<alive@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    TFE: Improves the interfaces of tape.watch_variable() and implicit_grad().
    
    tape.watch_variable() replaces tape.watch() and now is called on ResourceVariable objects instead of their underlying handles.
    
    implicit_grad() now returns a list of (gradient, variable) pairs to be consistent with tf.Optimizer's interface.
    
    PiperOrigin-RevId: 168232055
    
    ---
    Commit b72862dfc authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    internal change
    
    PiperOrigin-RevId: 168225993
    
    ---
    Commit da3280f4d authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Re-enable tsan for sdca_estimator_test.
    
    PiperOrigin-RevId: 168186374
    
    ---
    Commit c936c1155 authored by Yifei Feng<yifeif@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix pip tests for contrib/gan.
    - Add *_impl.py so tests can still access removed symbols.
    - Add /python directory layer to make *_impy.py and __init__.py not in the same dir.
    
    PiperOrigin-RevId: 168161722
    
    ---
    Commit ce9a2b00f authored by Toby Boyd<tobyboyd@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Performance guide update
    
    PiperOrigin-RevId: 168159289
    
    ---
    Commit 3bce4f9a0 authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    TFE: expose tfe.num_gpus()
    
    PiperOrigin-RevId: 168154345
    
    ---
    Commit 67a7cbc28 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Changed the default eval throttle secs from 2 min to 10 mins.
    
    PiperOrigin-RevId: 168120323
    
    ---
    Commit 92bed178f authored by Eugene Brevdo<ebrevdo@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Reduce cmake log mess.
    
    * Echo off for the .bat scripts.
    * TF cmake: disable warnings in some of the patched projects (gif,jpeg,lmdb).
    
    PiperOrigin-RevId: 168119914
    
    ---
    Commit 702d59582 authored by joshkyh<joshkyh@users.noreply.github.com>
    Committed by Yifei Feng<fengyifei2026@gmail.com>:
    Corrected hyperlink for audio training tutorial (#12923)
    
    ---
    Commit 877c9deca authored by Frank Chen<frankchn@gmail.com>
    Committed by Yifei Feng<fengyifei2026@gmail.com>:
    Reverse change eb75ded6 so that internal tests will pass. (#12933)
    
    As support for int64 global steps is not ready in TPUs, I am reversing this change so that our internal performance and regression tests will pass.
    ---
    Commit 665966438 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Re-enable grpc_session_test.
    
    PiperOrigin-RevId: 168078694
    
    ---
    Commit 405def792 authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Switch CallInliner to use CallGraph::VisitNodes.
    
    PiperOrigin-RevId: 168078645
    
    ---
    Commit aba3466f1 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Exposes Head and factory methods in tf.contrib.estimator.
    
    PiperOrigin-RevId: 168071246
    
    ---
    Commit b76565b39 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Some profiler fixes and cleanup.
    
    PiperOrigin-RevId: 168069346
    
    ---
    Commit 32ffc5a81 authored by Jonas<sauercrowd@users.noreply.github.com>
    Committed by Yifei Feng<fengyifei2026@gmail.com>:
    Just a dot in order to be consistent (#12919)
    
    added a dot to the `7` to make clear it's a float (like every other number)
    ---
    Commit 0753b0c79 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Scope the scalar cache in the context.
    
    PiperOrigin-RevId: 168065417
    
    ---
    Commit 48deb206b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Migrate TFGAN features to third_party.
    
    PiperOrigin-RevId: 168060880
    
    ---
    Commit d2ae1311f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixing an issue in the BUILD file of the LSH ops.
    
    PiperOrigin-RevId: 168056645
    
    ---
    Commit 2f440eda4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Expose NumpyReader for reading timeseries data.
    
    PiperOrigin-RevId: 168055838
    
    ---
    Commit be1916ce7 authored by Daniel Grazian<dgr@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added functionality to allow `SqlDataset` to interpret a database column as various numeric types, including several integer types and `dtypes.float64`.
    
    PiperOrigin-RevId: 168055827
    
    ---
    Commit fa2000a0b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Supporting nightly windows pip packages.
    
    PiperOrigin-RevId: 168054959
    
    ---
    Commit a263ea626 authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    eager: Treat eager tensors as constants during graph construction.
    
    Unless capturing is explicitly enabled.
    
    PiperOrigin-RevId: 168052675
    
    ---
    Commit 6e402d0d2 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make TODO a bit more specific.
    
    PiperOrigin-RevId: 168051381
    
    ---
    Commit c779384bc authored by Daniel Grazian<dgr@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added code example to the doc string for `SqlDataset`.
    
    PiperOrigin-RevId: 168049037
    
    ---
    Commit ff6dd474a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use self._in_graph_mode consistently in ResourceVariable
    instead of sometimes getting it from the context.
    
    Also: fix formatting of a comment and use a more precise test to detect
    if initial_value is set.
    PiperOrigin-RevId: 168047258
    
    ---
    Commit f331f528b authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Removes "fast paths" which are not fast in eager mode.
    
    PiperOrigin-RevId: 168046278
    
    ---
    Commit 86f1713e5 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Introduces TrainSpec and EvalSpec.
    
    PiperOrigin-RevId: 168040435
    
    ---
    Commit c8b9e92f0 authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    eager: Move "register_function" to context.py
    
    This will allow function registration from other
    modules without having to import "function.py".
    (And besides, the function really does belong on the context).
    
    PiperOrigin-RevId: 168040411
    
    ---
    Commit 74137f994 authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix signed int overflow issue in tensor_id.cc
    
    When a node name has a long numeric suffix, e.g.,
    "foo/y_0/gradient_debug_09684b60f2184c67b744721915034528" (as has happened with tfdbg GradientsDebugger),
    
    the parsing algorithm in ParseTensorName() may experience signed int overflow. Replacing the types with "unsigned int" resolves the issue.
    
    PiperOrigin-RevId: 168039195
    
    ---
    Commit 450c3b562 authored by Rohan Jain<rohanj@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Using rendezvous manager to pass args / rets between devices during function remote execution. This enables CPU->GPU remote device executions now.
    
    PiperOrigin-RevId: 168038285
    
    ---
    Commit 82cc6529f authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixes the wording about StopIteration.
    
    PiperOrigin-RevId: 168034451
    
    ---
    Commit fb5588002 authored by Gunhan Gulsoy<gunan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a statement on install/index.md on what os are supported.
    
    PiperOrigin-RevId: 168032996
    
    ---
    Commit f83f6b9ef authored by Chris Leary<leary@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Handle higher-order HLOs (e.g. While) in CallInliner and test.
    
    PiperOrigin-RevId: 168029345
    
    ---
    Commit 8988ae365 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    BEGIN_PUBLIC
    Automated g4 rollback of changelist 167916124
    
    PiperOrigin-RevId: 168916710

commit c82a933f449e637ee83244d2c40162e24cdde0e1
Author: Sanjoy Das <sanjoy@google.com>
Date:   Thu Sep 14 16:06:55 2017 -0700

    Lower vector-matrix dot to LLVM IR if the RHS of the dot can be made
    column major.
    
    The naive dot lowering to LLVM IR (already present in XLA today) is
    cache efficient if the dot has LHS of shape [1,K]{1,0} and RHS of
    shape [K x N]{0,1}.  This change teaches the layout assignment pass to
    exploit this property by converting a constant RHS matrix to a column
    major layout when possible.
    
    Couple of related things I had to touch in this change:
    
     - In LayoutAssignmentTest.TupleLayout we used to generate a kCopy to satisfy
       the conflicting constraints between the result and the constant shapes, but
       with this change we change the layout of the constants themselves.  So the
       EXPECT_FALSE is now an EXPECT_TRUE.
    
     - The extra instruction layout constraints added at the end of
       CpuLayoutAssignment::AddBackendConstraints seemed redundant.  The layout
       assignment pass already tries to make all unconstrained buffers have the
       default row-major layout.  Moreover, they were blocking this optimization in
       some cases by introducing conflicting constraints.
    
     - The changes to literal_util.h have to be made to deal with the
       Literal::Relayout calls we now get on literals of various types.
    
    PiperOrigin-RevId: 168761204

commit bece65c6f3605f00a72e4163e7e6d5ccda10cd81
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Sep 13 09:19:44 2017 -0700

    Use a map instead of a vector of Children() in the BeamEntry.
    
    The assumption is that since the entries are sparse (they are all populated, but most are never Active()), using the map will save memory and make iterating over the Children() more efficient.
    
    PiperOrigin-RevId: 168548814

commit fcacb40d4c5e2874f176b27ca75e7a1ce31fd87c
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Sep 12 12:18:54 2017 -0700

    FirstReadyManager for scheduling nodes in VirtualScheduler.
    The current FIFOManager may yield inefficient scheduling; _Recv pushed to the
    FIFO blocks other nodes that can run before _Recv due to the node order in FIFO.
    FirstReadyManager picks a node with the earliest time_ready in the queue,
    avoiding this problem.
    
    Also, fixed VirtualPlacer to properly set device when Node's device name does not
    include job name and to set GPU:0 as default device.
    
    PiperOrigin-RevId: 168418455

commit f911e7b055988699f5facf7efe33363726a71183
Author: Brennan Saeta <saeta@google.com>
Date:   Mon Aug 14 18:13:55 2017 -0700

    Add shard_dimensions to TPUConfig
    
    Most CPU-based input pipeline image operations operate in the tensor layout NHWC. While this is reasonable for CPUs, may devices operate most efficiently using alternate layouts. (e.g. NVidia GPUs opt for NCHW). This change allows us to perform the layout reshapes on the CPU ahead of feeding them into the device. This change allows us to adopt non-batch-major image layouts while supporting automatic sharding. This is useful both for image inputs, as well as RNNs which sometimes would like to be time-major ordered.
    
    PiperOrigin-RevId: 165257772

commit a614eed8f8d76aee17fcbc80c9b44a3538131845
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Aug 9 18:46:43 2017 -0700

    Changed formula for FTRL L1 normalization to be simpler and more efficient.
    
    PiperOrigin-RevId: 164804532

commit aa28b80bb56adeee31ad77167a923f2485ec9df6
Author: Derek Murray <mrry@google.com>
Date:   Wed Aug 9 15:23:22 2017 -0700

    [tf.contrib.data] Add `Dataset.prefetch()` transformation.
    
    This transformation is a simpler (and potentially more efficient)
    replacement for `Dataset.map(lambda x: x, num_threads=1,
    output_buffer_size=N)`, avoiding the overhead of function invocation
    and simplifying the synchronization slightly.
    PiperOrigin-RevId: 164781954

commit b55c0523f5d2a17fdd43a11a590dc59e63365ab6
Author: Derek Murray <mrry@google.com>
Date:   Wed Aug 9 15:23:22 2017 -0700

    [tf.contrib.data] Add `Dataset.prefetch()` transformation.
    
    This transformation is a simpler (and potentially more efficient)
    replacement for `Dataset.map(lambda x: x, num_threads=1,
    output_buffer_size=N)`, avoiding the overhead of function invocation
    and simplifying the synchronization slightly.
    PiperOrigin-RevId: 164781954

commit 85d4102862c781af2346b4aa568054522e8946ea
Author: Andrew Myers <andru@cs.cornell.edu>
Date:   Sat Jul 15 20:08:11 2017 -0400

    Adding generics to the Java API - Phase 1 (#11251)
    
    * Phase 1 of the proposed generic Java API.
    
    This adds new classes to represent each of the possible tensor types,
    and some scripting support for generating those classes. There is
    essentially no effect on existing classes, except that DataType is
    made slightly more efficient.
    
    All tests pass.
    
    * Addressed Asim's review.
    
    * Hoisted copyright into a separate declaration. Maybe it should go
    in a separate file?
    
    * Added private constructors to TF types and shortened their javadoc to be
    more standard.
    
    * Added more explanation about the enum relationship.
    
    * Used more-idiomatic import statement.
    
    * Rename zero column.
    
    * Removed the datatype code from tftypes.csv
    
    * Fix the default value for Double, add one for UInt8.
    
    * Got rid of 'boxed type' column in CSV file
    
    * Somehow I did not notice that TFType.java was not checked in.

commit b1f9e2c89eb007cb4b9483d08dcace1e45e84164
Author: RJ Ryan <rjryan@google.com>
Date:   Tue Jul 11 09:51:54 2017 -0700

    Add an axis parameter to tf.gather. Fixes GitHub issue #11223.
    
    This brings tf.gather closer to compatibility with numpy.take.
    
    To emulate gathering over an axis generally requires inefficient workarounds, e.g. transpose/gather/transpose. This technique is gaining popularity (hundreds of uses inside and outside of Google), so it is worth supporting efficiently.
    
    For an `[a_0, ..., a_i, ..., a_n]` tensor, gathering `N` elements from axis `i` requires `(a_0*...*a_i-1) * N` copies of `(a_i+1 * ... * a_n)` elements each. The CPU kernel does this with memcpy which is far more efficient than transpose/gather/transpose since it requires no intermediate allocations and copies. The GPU kernel does the same number of copies but in parallel across multiple hardware threads.
    
    Since this is a backwards incompatible change, this adds a "GatherV2" op with an axis input, and simultaneously supports backwards compatibility with "Gather" ops by defaulting to axis 0 if a 3rd input is not present.
    
    PiperOrigin-RevId: 161541416

commit a6773e98e97956b7adf3aa51eb3548261f51d6f7
Author: RJ Ryan <rjryan@google.com>
Date:   Mon Jul 10 16:41:33 2017 -0700

    Add a PadV2 op with support for specifying a pad value.
    
    Added a `constant_values` keyword argument to the tf.pad Python API for compatibility with numpy.pad. For now, only scalar values are supported. To efficiently support specifying a `[D, 2]` tensor for `constant_values` to pick per-dimension pre/post constant values will require adding Eigen and XLA support first.
    
    PiperOrigin-RevId: 161460091

commit dd951bb50d454f026083097b6d6122dae696259c
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Jul 5 16:56:41 2017 -0700

    This cl is a more efficient reimplementation of non max suppression making it significantly faster in practice by reducing the number of unnecessary IOU computations.
    
    PiperOrigin-RevId: 161023977

commit 38faead386d9d19b9af10150ac9d0cddd7b788e8
Author: Mark Heffernan <meheff@google.com>
Date:   Thu Jun 29 16:51:03 2017 -0700

    [XLA] Move HLO reachability into its own file and make update-able.
    As part of the CL, change the underlying representation in the reachability map to BitVectors which allows efficient update by OR'ing the vectors together.
    
    PiperOrigin-RevId: 160591849

commit cf7c008ab150ac8e5edb3ed053d38b2919699796
Author: Yifei Feng <fengyifei2026@gmail.com>
Date:   Wed Jun 28 11:07:44 2017 -0700

    Branch 160346151 (#11094)
    
    * Properly handle ops that don't have a CPU kernel
    
    PiperOrigin-RevId: 159655906
    
    * Selected BUILD cleanup in tensorflow/contrib/...
    
    PiperOrigin-RevId: 159673079
    
    * Remove redundant `get` calls on smart pointers
    
    PiperOrigin-RevId: 159675809
    
    * PiperOrigin-RevId: 159698321
    
    * Migrate kernels to boosted_trees.
    
    PiperOrigin-RevId: 159698656
    
    * Fix a bug in the memory optimizer when two inputs to a node are both recomputed
    
    PiperOrigin-RevId: 159700457
    
    * Fixed memory leak that can be triggered by a failed node evaluation
    
    PiperOrigin-RevId: 159707380
    
    * Updates get_started tutorial.
    
    PiperOrigin-RevId: 159709158
    
    * [XLA] Remove unused factory in local_service
    
    PiperOrigin-RevId: 159712806
    
    * Fix typo in docstring
    
    PiperOrigin-RevId: 159714414
    
    * Migrate ops for new version of TensorForest.
    
    PiperOrigin-RevId: 159718610
    
    * Added parameterized tests to reduce window tests.
    
    PiperOrigin-RevId: 159721784
    
    * Use C API to implement Operation.device property
    
    PiperOrigin-RevId: 159723490
    
    * Several Estimator changes:
    - support configurable input_fn calling in Estimator subclasses.
    - pass params and config to the input_fn.
    - allow callables for model_fn and input_fn.
    
    PiperOrigin-RevId: 159725554
    
    * Fixed the scalar output for shard api when outputs_from_all_shards=True.
    
    PiperOrigin-RevId: 159726444
    
    * Automated g4 rollback of changelist 159718610
    
    PiperOrigin-RevId: 159728380
    
    * Adding missing deps to targets in llvm.BUILD. This was only working in non-sandboxed builds.
    
    PiperOrigin-RevId: 159729295
    
    * [XLA:HLO] Move sequence functions from hlo_ordering.h to hlo_scheduling.h.
    
    This is required for upcoming changes to convert the sequence creation functions
    (and HeapSimulator and BufferAssignment) over to using the new
    Hlo{Dataflow,Alias}Analysis.
    
    It's required because otherwise there's a dependency cycle:
    
    Hlo{Dataflow,Alias}Analysis depends on HloOrdering
    CreateMemoryMinimizingSequence will depend on Hlo{Dataflow,Alias}Analysis
    
    There's already a cycle here, if both HloOrdering and
    CreateMemoryMinimizingSequence are in the same file.  Also note that:
    
    MinimumMemoryForSequence depends on HeapSimulator
    HeapSimulator will depend on Hlo{Dataflow,Alias}Analysis
    Hlo{Dataflow,Alias}Analysis depends on HloOrdering
    
    Splitting out the sequence functions resolves the cycle.
    
    Refactoring only; no functional changes.
    
    PiperOrigin-RevId: 159731836
    
    * [XLA:HLO] Split Hlo{Value,Buffer} out of Hlo{Dataflow,Alias}Analysis.
    
    This will make dependencies cleaner for upcoming CLs that will convert
    HeapSimulator and HloOrdering to use the new analyses.
    
    No change in functionality.
    
    PiperOrigin-RevId: 159737265
    
    * Internal change
    
    PiperOrigin-RevId: 159738215
    
    * Suggest people need to do some build environment ./configur'ing.
    
    Fixes #4279
    
    PiperOrigin-RevId: 159738412
    
    * Rewrite SameDefinedShape function in ShapeRefiner
    
    PiperOrigin-RevId: 159745894
    
    * [XLA] Remove xla_cpu_*_eigen flags from CPU backends.
    
    These flags are currently de-facto unused; parallelism should be controlled
    through the cpu_parallel backend. For configuring Eigen, if needed, the options
    should be piped more directly to the code.
    
    PiperOrigin-RevId: 159746509
    
    * Updates layers tutorial and corresponding example.
    
    PiperOrigin-RevId: 159749528
    
    * Further BUILD cleanup
    
    PiperOrigin-RevId: 159749869
    
    * Use more efficient squared_difference
    
    PiperOrigin-RevId: 159751209
    
    * Add log_step_count_steps to RunConfig and allow it to flow to the MonitoredSession.
    
    PiperOrigin-RevId: 159753935
    
    * [XLA] Remove xla_hlo_test_generate_hlo_graph, which is now redundant.
    
    PiperOrigin-RevId: 159755688
    
    * Do not use SSE4.1 instructions on Android builds.
    
    PiperOrigin-RevId: 159756104
    
    * Add nonpublic helper `tf.distributions.util.tridiag` op.
    
    PiperOrigin-RevId: 159757904
    
    * [XLA] Remove dead "in-client" code.
    Remove Service::runs_in_client_process_ field and it's dead user. This was
    previously used by the "InProcess" methods which have been replaced with
    the LocalClient API.
    
    PiperOrigin-RevId: 159759455
    
    * [tf contrib seq2seq] Add monotonic attention mechanisms
    
    * Add monotonic_attention and safe_cumprod helper functions.
    * Add _BaseMonotonicAttentionMechanism base class.
    * Add BahdanauMonotonicAttention and LuongMonotonicAttention classes.
    
    These attention mechanisms are proposed in
    Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,
    "Online and Linear-Time Attention by Enforcing Monotonic Alignments."
    ICML 2017.  https://arxiv.org/abs/1704.00784
    
    PiperOrigin-RevId: 159760073
    
    * Add ability for argmax to output int32 indices.  Default remains int64.
    
    Change is made in a backwards and forward compatible manner, since
    we add a new attribute with a default that remains the same, and
    simply register a few new kernels.
    
    PiperOrigin-RevId: 159761347
    
    * Automated g4 rollback of changelist 159746509
    
    PiperOrigin-RevId: 159763112
    
    * Raise ValueError if invalid dtype for random_uniform.
    
    PiperOrigin-RevId: 159764956
    
    * Internal change.
    
    PiperOrigin-RevId: 159769520
    
    * Support zero shapes for random_poisson. This matches random_uniform.
    
    PiperOrigin-RevId: 159771215
    
    * Blacklist the quantized ops since they have too many issues (incorrect shape
    functions, memory corruptions, ...)
    
    PiperOrigin-RevId: 159772801
    
    * Fixed the shape functions of the QuantizedAdd and QuantizedMul ops
    
    PiperOrigin-RevId: 159772841
    
    * Switch from assigning namedtuple.__new__.__defaults__ to overwriting __new__.
    
    Assigning __defaults__ relies on an implementation detail of CPython, confuses
    type checkers (and developers :)), and is error-prone since it doesn't make the
    relationship between parameter names and default values explicit.
    This CL switches to overloading __new__ instead.
    
    PiperOrigin-RevId: 159773922
    
    * Made sure that we can call the constant folding code twice safely.
    
    PiperOrigin-RevId: 159781607
    
    * Added batch_matmul op dependence to android_extended_ops
    
    PiperOrigin-RevId: 159787178
    
    * Fixes a TODO in head_test.
    
    PiperOrigin-RevId: 159789178
    
    * When configuring per-session thread pools, allow
    a pool to be a global pool. This allows a division
    between large and small pools, without needing to make
    new pool for each session.
    
    PiperOrigin-RevId: 159789678
    
    * Add a multi-head TensorForest estimator.
    
    PiperOrigin-RevId: 159820487
    
    * Have RestoreV2's shape fn set all outputs to unknown shape.
    
    PiperOrigin-RevId: 159835723
    
    * VectorExponential added to distributions.
    
    PiperOrigin-RevId: 159840822
    
    * Fold as many nodes as possible instead of giving up if there is any error.
    
    PiperOrigin-RevId: 159841935
    
    * Removed deprecated summary usage from estimators.
    Made name_space usage consistent.
    
    PiperOrigin-RevId: 159846928
    
    * Adding missing license notice to toolchain build files
    
    PiperOrigin-RevId: 159847551
    
    * [XLA] Remove unused flags and move debugging flag to debug options.
    
    PiperOrigin-RevId: 159849759
    
    * Fixes some docstrings in feature_column.
    
    PiperOrigin-RevId: 159850619
    
    * TpuEstimator: Replicate the input_fn to the worker CPU for each shard.
    
    The batch size is configured as follows:
    The user may specify a global batch size in their hyperparameters. If the 'batch_size' field is set, then we convert the global batch size into a per-shard batch size by dividing by num_shards before running their input_fn.
    
    PiperOrigin-RevId: 159851773
    
    * Modify beam search decoder to use symbolic shape for vocab size if the static shape is not present.
    
    PiperOrigin-RevId: 159852297
    
    * Generalize cluster initialization to span multiple mini-batches if necessary.
    
    PiperOrigin-RevId: 159852557
    
    * Use a single threaded session for SDCALinearRegressorTest to
    avoid incorrect threading test failures (tsan).
    
    PiperOrigin-RevId: 159852818
    
    * Migrate ops for new version of TensorForest.
    
    PiperOrigin-RevId: 159852889
    
    * Replaced constant inputs with variables to ensure most of the graph doesn't get
    optimized away
    
    PiperOrigin-RevId: 159853171
    
    * For candidate sampling, add facility to colocate the logit computation with the sharded embeddings.
    
    PiperOrigin-RevId: 159854706
    
    * Added a utility to create parsing spec for regressors (canned estimator)
    
    PiperOrigin-RevId: 159855254
    
    * Fix cuda_kernel_helper_test. std::numeric_limits<int32>::max() doesn't pass, so
    I didn't use that.
    
    PiperOrigin-RevId: 159869169
    
    * In tfcompile, prune nodes that are not reachable from the fetches before
    building the Graph. This allows loading a graph that contains ops not
    needed for the compiled binary.
    
    PiperOrigin-RevId: 159869692
    
    * Fix bugs related to distributions over integers.
    
    - Ensure that the max number of categories does not exceed largest integer-form float.
    - Make dtype inference consistent between Categorical and Multinomial
    distributions.
    - Improve documentation to better reflect that the Categorical
    distribution is analogous to `argmax{OneHotCategorical}` (itself being
    identical to `argmax{Multinomial(p,n=1)}` but not Multinomial.
    - Fix validation_args Heisenberg uncertainty: only validation logic should live under self.validate_args. E.g., validate_args=True would sometimes imply `x=floor(x)` which changes behavior thus making debugging impossible because enabling validation *changes* values.
    - Corrected `Geometric` swapping of validate_args` and `allow_nan_stats` default-values.
    
    Fixes #10149
    
    PiperOrigin-RevId: 159872532
    
    * Make HloModule clonable
    
    This CL makes HloModule clonable, which is necessary when we want to run the same compilation twice with the same input.
    
    PiperOrigin-RevId: 159874256
    
    * Internal change.
    
    PiperOrigin-RevId: 159876942
    
    * Implement alternative `monte_carlo.expectation_v2`. This function implements
    the reparameterization and score-gradient tricks and does not depend on
    tf.Distribution like inputs.
    
    PiperOrigin-RevId: 159877923
    
    * In SE_ASSIGN_OR_RETURN change ConsumeValueOrDie to the preferred std::move ValueOrDie.
    
    PiperOrigin-RevId: 159879754
    
    * If rank is unknown, do not add output shapes to transpose nodes.
    
    PiperOrigin-RevId: 159879840
    
    * Move sparse_fill_empty_rows to new, *significantly* faster, C++ kernel for everyone.
    
    Also fix a bug in the C++ op when the input ST has 0 elements.
    
    PiperOrigin-RevId: 159880044
    
    * Add support of label_keys to DebugClassifier
    
    PiperOrigin-RevId: 159883986
    
    * Register devices under their legacy names
    
    Because some higher level APIs continue to use the legacy name format,
    when using ClusterSpec propagation, we need to ensure that we register
    the devices under their legacy names as well as their canonical names.
    
    PiperOrigin-RevId: 159885777
    
    * [BatchNorm] Minor fixes to TF doc
    
    PiperOrigin-RevId: 159886125
    
    * Generating TBAA metadata causes the LLVM to miscompile after
    https://reviews.llvm.org/rL305938).  Disable TBAA (to stop the miscompiles)
    while we fix the root issue.
    
    PiperOrigin-RevId: 159895736
    
    * Improve score-trick to be a valid Csiszar f-Divergence yet numerically stable.
    
    PiperOrigin-RevId: 159896013
    
    * Support advisor in all places (Command line, APIs)
    Add expensive operation checker
    
    PiperOrigin-RevId: 159897279
    
    * Added canned estimators to Tensorflow library. List of added estimators:
    * DNNClassifier
    * DNNRegressor
    * LinearClassifer
    * LinearRegressor
    * DNNLinearCombinedClassifier
    * DNNLinearCombinedRegressor
    
    PiperOrigin-RevId: 159898954
    
    * Alligned how model-fns handled params among linear/dnn/combined estimators.
    
    PiperOrigin-RevId: 159899925
    
    * Fixed cmake tests.
    
    PiperOrigin-RevId: 159901417
    
    * [XLA:CPU] Add VLOGs to cpu_compiler.cc
    
    PiperOrigin-RevId: 159902919
    
    * Make occurence (op run times and op definition) selectable
    in all views to address the loop problem.
    
    When a node is in loop, its execution times are accumulated, its run times
    will increase.
    
    PiperOrigin-RevId: 159912429
    
    * [XLA] Small error message improvement in binop shape inference.
    
    PiperOrigin-RevId: 159920109
    
    * Follow upstream API change from r306058.
    
    PiperOrigin-RevId: 159938416
    
    * [TF:XLA] Update LLVM to upstream revision r306085.
    
    PiperOrigin-RevId: 159946562
    
    * [XLA] Remove unused xla_cpu flag and move another to DebugOptions.
    
    PiperOrigin-RevId: 159952124
    
    * Updates linear.md tutorial
    
    PiperOrigin-RevId: 159956867
    
    * Add TraceMe instrumentation of RunStep in GRPC distributed runtime.
    A unique ID is added to each RunStep call that allows the client and server
    events to be correlated.
    
    PiperOrigin-RevId: 159956950
    
    * [XLA] Add general F32 implementation for ReducePrecision operation.
    
    This only tests with parameter inputs (which is needed to ensure we actually test on GPUs as well as CPUs); there's no point in separately testing with constants.
    
    PiperOrigin-RevId: 159961430
    
    * Java: NativeLibrary: Fix URL in error message.
    
    And add some detail.
    Inspired by #11015
    
    PiperOrigin-RevId: 159962478
    
    * Increase rtol for util_test.
    
    PiperOrigin-RevId: 159971136
    
    * Re-enable IR dumping for the sequential CPU backend.
    
    PiperOrigin-RevId: 159974126
    
    * tfdbg: a few minor fixes and improvements
    
    * Let DumpingDebugWrapperSession and DumpingDebugHook create session_root if it doesn't exist
    * Add README.md to tensorflow/python/debug
    * Add section "Debugging Keras Models with TFDBG" in debugger.md
    
    PiperOrigin-RevId: 159976070
    
    * Add None check for save_path when restoring checkpoints as if something is wrong in tf.train.latest_checkpoint, it will often return None and it's nice to have a common sense check in restore for this. This way log.error says what has happened.
    
    PiperOrigin-RevId: 159979481
    
    * Don't crash if a metagraph fails to load.
    
    PiperOrigin-RevId: 159981628
    
    * Prepare to not include node_def.proto.h in node_def_util.h
    
    The goal is to make kernels mostly independent of proto headers, which will let
    us lock down our .so imports.  This CL makes a bunch of .cc files
    either include node_def.proto.h themselves or not need the definition of
    NodeDef; a second CL will make node_def_util.h not include node_def.proto.h.
    
    RELNOTES: n/a
    PiperOrigin-RevId: 159982117
    
    * Add a few diagnostic flags to help narrow down issues with the LLVM
    backends.
    
    PiperOrigin-RevId: 159982441
    
    * Updated wide-n-deep tutorial code to use core version of estimators and feature-columns.
    
    PiperOrigin-RevId: 159984663
    
    * Modify ControlFlowContext to also respect import_scope in 'values_' and keys of 'external_values_'
    
    PiperOrigin-RevId: 159985290
    
    * Add item's graph to partition_graphs in virtual cluster's run method.
    Put node op name in timeline_label instead of node_name.
    
    PiperOrigin-RevId: 159986583
    
    * Use short-proto for logging purposes.
    
    A short proto will be output on a single log line, making it
    easier for certain automated tools to handle.
    
    PiperOrigin-RevId: 159994005
    
    * Sinh, ArcSinh, Cosh, LogCosh functions added to distributions/python/ops/trig.
    Care is taken to ensure a fair bit of stability.
    
    PiperOrigin-RevId: 159995514
    
    * Updates some examples in examples/learn.
    
    PiperOrigin-RevId: 159996397
    
    * Add kernel tests for boosted_trees.
    
    PiperOrigin-RevId: 160002696
    
    * Avoid doing unecessary work in the OptimizeGraph() function whenever possible
    
    PiperOrigin-RevId: 160003173
    
    * Use std::shared_ptr instead of core::RefCounted for Node::Properties
    
    Also changes Node::Properties to a struct and removes underscores from public member variables. This change should make it easier to work with Properties moving forward as the refcount will be automatically updated.
    
    PiperOrigin-RevId: 160003281
    
    * Make the CPU compiler dump optimized IR along with the unoptimized IR.
    
    PiperOrigin-RevId: 160005257
    
    * Disable flaky run_metadata_test.
    
    PiperOrigin-RevId: 160015399
    
    * BUILD cleanup in tensorflow/tools/...
    
    PiperOrigin-RevId: 160018623
    
    * SinhArcSinh bijector added.
    
    This two-parameter diffeomorphism from R --> R allows for skewness and fatter
    or thinner tails.  See docstring and also
    http://oro.open.ac.uk/22510/1/sinhasinh.pdf
    
    PiperOrigin-RevId: 160019380
    
    * Avoid hardcoded names for temporary files in tests.
    
    These tests (and examples that are run as tests) were using hardcoded names for
    temporary files.  This failed when multiple copies of these tests were run in
    parallel, or even successively by different users, where the second run could
    not overwrite files left by the first.
    
    This change uses the TEST_TMPDIR environment variable used by bazel's test
    runner to choose a temporary directory.   If that directory is not set,
    /tmp is used, as before.
    
    PiperOrigin-RevId: 160026924
    
    * Fix multinomial doc-string, input arg logits expects to log-probabilities and not log-odds.
    
    PiperOrigin-RevId: 160036709
    
    * Made TensorFlow documentation on LSTMs slightly more accurate.
    
    PiperOrigin-RevId: 160047054
    
    * Follow LLVM/ORC upstream API change in r306166.
    
    PiperOrigin-RevId: 160108102
    
    * Move resampler from sonnet to contrib.
    
    PiperOrigin-RevId: 160134565
    
    * [TPUEstimator] Make input_fn invoked properly with eval on CPU.
    
    PiperOrigin-RevId: 160151890
    
    * Deletes iris_val_based_early_stopping example, which uses deprecated ValidationMonitor.
    
    PiperOrigin-RevId: 160154863
    
    * [XLA] Move HLO dumping flags from service_flags to debug_options_flags
    
    This also removes the duplication in the xla_generate_hlo_graph flag.
    
    This CL also moves the actual dumping logic from Executable to the
    hlo_graph_dumper namespace, where it belongs; this is in preparation for
    removing the hlo_dumper callback altogether, since it isn't serving any role
    beyond what a direct call to hlo_graph_dumper would have (b/62872831 has more
    details).
    
    PiperOrigin-RevId: 160154869
    
    * Fix missing variable unref
    
    Direct leak of 56 byte(s) in 1 object(s) allocated from:
        #0 0xf5ee272 in operator new(unsigned long) (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0xf5ee272)
        #1 0x1b51394c in tensorflow::AssignVariableOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*)::'lambda'(tensorflow::Var**)::operator()(tensorflow::Var**) const (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0x1b51394c)
        #2 0x1b5136c0 in std::_Function_handler<tensorflow::Status (tensorflow::Var**), tensorflow::AssignVariableOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*)::'lambda'(tensorflow::Var**)>::_M_invoke(std::_Any_data const&, tensorflow::Var**) (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0x1b5136c0)
        #3 0x1b50b289 in std::function<tensorflow::Status (tensorflow::Var**)>::operator()(tensorflow::Var**) const (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0x1b50b289)
        #4 0x1b50af88 in tensorflow::Status tensorflow::ResourceMgr::LookupOrCreate<tensorflow::Var>(basic_string<char, std::char_traits<char>, std::allocator<char> > const&, basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::Var**, std::function<tensorflow::Status (tensorflow::Var**)>) (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0x1b50af88)
        #5 0x1b50ac10 in tensorflow::Status tensorflow::LookupOrCreateResource<tensorflow::Var>(tensorflow::OpKernelContext*, tensorflow::ResourceHandle const&, tensorflow::Var**, std::function<tensorflow::Status (tensorflow::Var**)>) (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0x1b50ac10)
        #6 0x1b512f1e in tensorflow::AssignVariableOp<Eigen::ThreadPoolDevice, float>::Compute(tensorflow::OpKernelContext*) (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0x1b512f1e)
        #7 0x1d1881c7 in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0x1d1881c7)
        #8 0xf96e0fe in tensorflow::KernelAndDevice::Run(std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0xf96e0fe)
        #9 0xf94f9c8 in TFE_Execute (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0xf94f9c8)
        #10 0xf94356d in TFE_Py_Execute(TFE_Context*, int, char const*, tensorflow::gtl::InlinedVector<TFE_TensorHandle*, 4>*, _object*, tensorflow::gtl::InlinedVector<TFE_TensorHandle*, 2>*, TF_Status*) (/build/cas/5d2/5d2be3b530580573ff7269adcab7cbac+0xf94356d)
    
    PiperOrigin-RevId: 160160101
    
    * Simplify strided_slice's shape handling
    
    Now that TensorShape and PartialTensorShape share memory representations, there's no need for an abstract class that makes TensorShape and TensorShapeProto look the same.
    
    RELNOTES: n/a
    PiperOrigin-RevId: 160161618
    
    * Added a tool to report the static information that can be extracted from a TF model.
    
    PiperOrigin-RevId: 160162256
    
    * Properly handle RefEnter, RefExit and RefNextIteration nodes.
    
    PiperOrigin-RevId: 160162338
    
    * Switch tfprof to use proto3
    
    PiperOrigin-RevId: 160163483
    
    * Fixes to cuda_config.h.
    
    PiperOrigin-RevId: 160168545
    
    * Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 160171187
    
    * Adds notes to prevent overfitting for Experiment continous_train_and_eval.
    
    PiperOrigin-RevId: 160172692
    
    * Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 160172985
    
    * Merge changes from github.
    END_PUBLIC
    
    Note: this CL will break builds.  cl/159887762 to follow to fix all the breakages.
    
    ---
    Commit 2336cdf7f authored by Maxwell Paul Brickner<mbrickn@users.noreply.github.com>
    Committed by gunan<gunan@google.com>:
    Updated link to use HTTPS (#10998)
    
    Howdy!
    
    I just updated a link to use https instead of http.
    
    Thanks!
    ---
    Commit ad0892df1 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Luke Iwanski<luke@codeplay.com>:
    [OpenCL] Fixes run_metadata_test for SYCL
    
     This test is designed to test CUDA specific behavior
    
    ---
    Commit 6b37a0725 authored by Todd Wang<toddwang@gmail.com>
    Committed by GitHub<noreply@github.com>:
    Update comments
    ---
    Commit 1699d904a authored by John Lawson<john@codeplay.com>
    Committed by Luke Iwanski<luke@codeplay.com>:
    [OpenCL] Fixes CUDA specific test run on SYCL (#56)
    
    The testBadParentValuesOnGPU should only be run on CUDA devices, as the
    test checks for particular CUDA behaviour. We don't actually provide a
    SYCL kernel for GatherTree and so it's not a problem that the tests
    don't target SYCL.
    ---
    Commit 3c1946230 authored by myPrecious<Moriadry@users.noreply.github.com>
    Committed by Shanqing Cai<cais@google.com>:
    Java API to get the size of specified input list of operations. (#10865)
    
    * Java API to get the size of specified input list of operations
    
    * remove unnecessary explain to avoid bring a new term to users.
    
    ---
    Commit e911c7480 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Luke Iwanski<luke@codeplay.com>:
    [OpenCL] REGISTER -> REGISTER6
    
    ---
    Commit fbf6c4cec authored by superryanguo<superryanguo@gmail.com>
    Committed by superryanguo<superryanguo@gmail.com>:
    Simplify the Quickstart section with the weblink is better
    
    ---
    Commit 72e2918cc authored by Taehoon Lee<taehoonlee@snu.ac.kr>
    Committed by Taehoon Lee<taehoonlee@snu.ac.kr>:
    Fix typos
    
    ---
    Commit 90c4406b7 authored by Rishabh Patel<patelrishabh@users.noreply.github.com>
    Committed by GitHub<noreply@github.com>:
    Correct the learning rate as per the code snippet
    ---
    Commit 03da61134 authored by Todd Wang<toddwang@gmail.com>
    Committed by GitHub<noreply@github.com>:
    Update ir_array.cc
    ---
    Commit 2df6cd3ac authored by Todd Wang<toddwang@gmail.com>
    Committed by GitHub<noreply@github.com>:
    Another try
    ---
    Commit af0cbace1 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Transpose to go through Eigen (#10321)
    
    ---
    Commit fc7361081 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Registers RGBToHSV and HSVToRGB (#91) (#10848)
    
    * [OpenCL] Added RGBToHSV and HSVToRGB
    
    * Aligning '\'
    ---
    Commit 832894ef8 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Registers AdjustContrastv2 (#10949)
    
    * [OpenCL] Registers AdjustContrastv2 (#93)
    
    * [OpenCL] Extended adjust_contrast_op_benchmark_test for OpenCL (#96)
    
    * [OpenCL] Extended adjust_contrast_op_benchmark_test for OpenCL
    
    * simplified to #ifndef
    
    * Changed to "#if GOOGLE_CUDA"
    
    * Update adjust_contrast_op_benchmark_test.cc
    
    * Added comments
    
    ---
    Commit cb4c2f8d1 authored by Yifei Feng<yifeif@google.com>
    Committed by Yifei Feng<yifeif@google.com>:
    Make TransferBufferToInFeed not virual so it compiles.
    
    ---
    Commit e89f04d80 authored by Yifei Feng<yifeif@google.com>
    Committed by Yifei Feng<yifeif@google.com>:
    Fix calling Literal member functions.
    
    ---
    Commit 15a8df724 authored by Yifei Feng<yifeif@google.com>
    Committed by Yifei Feng<yifeif@google.com>:
    Fix mac build
    clone from meheff's change:
    [XLA] Change return type of DeviceAssignment::Deserialize to fix build
    breakage on mac.
    The mac build had the following error:
    
    error: incomplete type 'xla::DeviceAssignment' used in type trait
    expression
    
    This was due to a static method returning a StatusOr<DeviceAssignment>
    inside of the definition of DeviceAssignment.
    
    ---
    Commit a54d43fa4 authored by Yifei Feng<yifeif@google.com>
    Committed by Yifei Feng<yifeif@google.com>:
    Replace LiteralUtil to Literal in compiler/plugin/executor
    
    ---
    Commit 88a6bb80c authored by Guenther Schmuelling<guschmue@microsoft.com>
    Committed by Guenther Schmuelling<guschmue@microsoft.com>:
    expand inline for debug builds to limit number of symbols
    
    ---
    Commit 62fb49d31 authored by Yifei Feng<yifeif@google.com>
    Committed by Yifei Feng<yifeif@google.com>:
    Fix visibility error for contrib/remote_fused_graph/pylib/BUILD.
    
    ---
    Commit 4c75252f2 authored by Mark Neumann<markn@allenai.org>
    Committed by Mark Neumann<markn@allenai.org>:
    fix initial test values to avoid numerical instability
    
    ---
    Commit b58d98353 authored by sj6077<epik03sj@gmail.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    Fixes of AutoParallel bug (#10368)
    
    * Fix the bug that auto_parallel could replicate variable snapshot name
    
    * Use NodeName in grappler:utils instead of substr, convert variables->variable_def of grappler item
    
    * remove variable_def from grappler item, exclude snapshot nodes from dont_replicate_nodes in auto_parallel
    
    ---
    Commit a286b7db8 authored by Yifei Feng<yifeif@google.com>
    Committed by Yifei Feng<yifeif@google.com>:
    Make debug_test slice integer.
    
    ---
    Commit 97fcfdfa6 authored by Toby Boyd<tobyboyd@google.com>
    Committed by GitHub<noreply@github.com>:
    Fixed path to seq2seq.py and minor formatting
    ---
    Commit 63c1befb8 authored by Anish Shah<shah.anish07@gmail.com>
    Committed by Anish Shah<shah.anish07@gmail.com>:
    Improve docs for tf.nn.depthwise_conv2d_native
    
    ---
    Commit 8d42202b2 authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Yong Tang<yong.tang.github@outlook.com>:
    Fix mismatched delete in mkl_tfconv_op.cc
    
    This fix fixes mismatched new[]-delete in mkl_tfconv_op.cc
    
    (the file went through clang-format so there are some additional
    changes)
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    
    ---
    Commit 26301bd55 authored by Danny Goodman<goodman.danny@gmail.com>
    Committed by Danny Goodman<goodman.danny@gmail.com>:
    fix error format
    
    ---
    Commit b3f33ad46 authored by Yao Zhang<yaozhang@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make changes to prepare for the fused option of batch norm to be set to None (None means using fused batch norm if possible).
    
    PiperOrigin-RevId: 159649743
    
    ---
    Commit a4a469832 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Add tests for select ops and while loops that produce tuples that contain predicates.
    
    PiperOrigin-RevId: 159645900
    
    ---
    Commit 980d3f2be authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use C API to implement Operation.name property
    
    This name property is used in many existing tests including those that
    already run with C API enabled (math_ops_test, framework_ops_test,
    session_test, session_partial_run_test, math_ops_test_gpu, etc).
    
    PiperOrigin-RevId: 159645767
    
    ---
    Commit 26239c706 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Previously we didn't have an implementation of BatchNormInference and BatchNormTraining, which gives a linker error if anyone ever tries to call that. A dummy implementation is friendlier than a linker error.
    
    PiperOrigin-RevId: 159645612
    
    ---
    Commit f671c5caa authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    BEGIN_PUBLIC
    Automated g4 rollback of changelist 159570549
    
    PiperOrigin-RevId: 160182040
    
    * Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 160183349
    
    * Merge changes from github followup.
    
    PiperOrigin-RevId: 160183498
    
    * Automated g4 rollback of changelist 160183498
    
    PiperOrigin-RevId: 160189134
    
    * Automated g4 rollback of changelist 160182040
    
    PiperOrigin-RevId: 160190881
    
    * [XLA] Disallow fuse X into Y if there are paths from X to Y which don't fuse
    
    Just because X can fuse into all of its consumers does not mean that those
    consumers can fuse into anything. Depending on the structure of the graph, this
    can either result in no performance win at all or, in the case of recurrent
    networks, a big performance deficit.
    
    PiperOrigin-RevId: 160194058
    
    * First draft of Tensors segment of the programmer's guide.
    
    PiperOrigin-RevId: 160196550
    
    * First draft of variables unit of programmer's guide.
    
    PiperOrigin-RevId: 160196566
    
    * Make xla::Literal moveable.
    
    PiperOrigin-RevId: 160197273
    
    * Automated g4 rollback of changelist 159897279
    
    PiperOrigin-RevId: 160198598
    
    * Updates text_classification example.
    
    PiperOrigin-RevId: 160200457
    
    * Fix backward compatibility test broken by rollback.
    
    PiperOrigin-RevId: 160222187
    
    * Support advisor in all places (Command line, APIs)
    Add expensive operation checker
    
    PiperOrigin-RevId: 160222348
    
    * [XLA] Simplify the fusion heuristic
    
    We had two different aspects of the fusion heuristic:
    - Don't fuse a producer into a consumer if there exists a path from the
      producer to the consumer which cannot be fused.
    - Don't fuse a producer into a consumer if any consumer of the producer cannot
      fuse.
    
    These can be combined into one, simpler, heuristic.
    
    PiperOrigin-RevId: 160222771
    
    * Automated g4 rollback of changelist 160196566
    
    PiperOrigin-RevId: 160222930
    
    * Automated g4 rollback of changelist 160196550
    
    PiperOrigin-RevId: 160222942
    
    * Lets the HParam parser also accept True and False as inputs, since that's how python prints booleans.
    
    PiperOrigin-RevId: 160234658
    
    * Automated g4 rollback of changelist 155070869
    
    PiperOrigin-RevId: 160249526
    
    * [TF:XLA] Inline the sigmoid operation instead of mapping it elementwise.
    
    PiperOrigin-RevId: 160274436
    
    * Make sure all convolution tests are testing non-trivial cases, i.e. where not all inputs are 0, leading to an all-0 output, which masks most possible bugs.
    We do not check-fail on 0-sized dimensions as tests for these special cases
    exist.
    
    PiperOrigin-RevId: 160274593
    
    * Explicitly use "dns" URI scheme when using DNS names or literal IP
    addresses with gRPC.  This avoids problems in environments in which the
    default URI scheme is something other than "dns".
    
    PiperOrigin-RevId: 160276862
    
    * Add RWSE (root weighted squared error) to the WALS estimator.
    
    PiperOrigin-RevId: 160276937
    
    * Don't include node_def.proto.h in node_def_util.h
    
    The goal is to make kernels mostly independent of proto headers, which will let us lock down our .so imports.
    
    RELNOTES: n/a
    PiperOrigin-RevId: 160278032
    
    * [XLA] Add tuple support to Literal::CreateFromShape.
    
    PiperOrigin-RevId: 160278561
    
    * Updates some more examples in examples/learn.
    
    PiperOrigin-RevId: 160278757
    
    * Automated g4 rollback of changelist 160278032
    
    PiperOrigin-RevId: 160280961
    
    * Fixed the bug that Estimator does not make deepcopy of params in constructor
    
    PiperOrigin-RevId: 160281247
    
    * Clean out the config and params in TPUEstimator.
    
    PiperOrigin-RevId: 160281507
    
    * [XLA] Remove the "hlo dumper" parameter of xla::Compiler and its piping.
    
    This dumper is no longer necessary since the restructuring of HLO dumping and
    the addition of MaybeDumpHloModule which heeds to the right flags. The
    remaining bits didn't have additional functionality, but constituted a lot of
    boilerplate that has to be propagated throughout the backends.
    
    PiperOrigin-RevId: 160281798
    
    * [TF:XLA] Refactor the sigmoid op as a rescaled tanh.
    
    PiperOrigin-RevId: 160282472
    
    * Fix uninitialized values in TensorForest code.
    
    PiperOrigin-RevId: 160284420
    
    * [TF:XLA] Update Tensorflow LLVM release to upstream r306370.
    
    Fix broken XLA build.
    
    PiperOrigin-RevId: 160284588
    
    * tfdbg example: fix --tensor_size issue in debug_fibonacci
    
    PiperOrigin-RevId: 160290541
    
    * [SE] ThenConvolveWithAlgorithm vlogs algorithm configs.
    
    PiperOrigin-RevId: 160292762
    
    * Fix documentation of Estimator class (invalid quotes).
    
    PiperOrigin-RevId: 160292803
    
    * Shrink the test size to avoid OOM error on old GPUs.
    
    PiperOrigin-RevId: 160292834
    
    * [TF:XLA] Reject operators with resource outputs on CPU and GPU devices.
    
    We were checking for resource inputs but not resource outputs, which led to accidental fusion of some TensorArray ops on CPU and GPU.
    
    PiperOrigin-RevId: 160294302
    
    * Add a functionality of remote fused graph transformation to fuse graphs by op type
    
    PiperOrigin-RevId: 160300039
    
    * Cudnn compatible LSTMCell and LSTMBlockCell
    
    PiperOrigin-RevId: 160300668
    
    * [XLA] Remove "operand" argument from HandleReducePrecision.
    
    PiperOrigin-RevId: 160301461
    
    * Added more reduce window tests.
    
    PiperOrigin-RevId: 160301509
    
    * Updates more text classification examples in examples/learn.
    
    PiperOrigin-RevId: 160305131
    
    * Use C API to implement Operation._output_types
    
    This change first converts the _output_types member to a property and
    then implements it using C API if it is enabled.
    
    PiperOrigin-RevId: 160306227
    
    * Add more tests for BatchNormTraining.
    RELNOTES: n/a
    
    PiperOrigin-RevId: 160307959
    
    * Update path to print_selective_registration_header.py in comment
    
    PiperOrigin-RevId: 160308173
    
    * Migrate TensorForest v4 python to contrib.
    
    PiperOrigin-RevId: 160308805
    
    * Automated g4 rollback of changelist 159454657
    
    PiperOrigin-RevId: 160314706
    
    * TESTFIX:  distributions:trig_test wasn't passing in ASAN mode.
    
    PiperOrigin-RevId: 160315597
    
    * tfdbg doc: fixes and improvements
    
    PiperOrigin-RevId: 160318411
    
    * Add a time estimation to HloCostAnalysis and represent properties as a map so that adding more properties will be easier, e.g. in a sub-class.
    
    PiperOrigin-RevId: 160318494
    
    * tfdbg: revert dns:/// prefix in gRPC mode
    
    PiperOrigin-RevId: 160319348
    
    * Moves TensorCApi from c_api.cc to c_api_internal.h, where it can be used
    by other code that require access to the underlying TensorBuffers.
    
    PiperOrigin-RevId: 160323362
    
    * Readd the new tensors and variables documents, with tests passing.
    
    PiperOrigin-RevId: 160324191
    
    * Make ResourceHandle not be a proto
    
    I'm trying to make core/kernels independent of protos.  Currently the dtype ResourceHandle is itself a proto.  After this CL, ResourceHandle is a normal C++ type which gets converted to/from ResourceHandleProto at (de)serialization time.
    
    RELNOTES: n/a
    PiperOrigin-RevId: 160329002
    
    * Minor cleanup: remove unused dependencies and inclusions
    
    PiperOrigin-RevId: 160334030
    
    * Add name_scopes to mnist_deep.py for a cleaner graph layout.
    
    PiperOrigin-RevId: 160338775
    
    * Add note about `tf.test.mock` to docs for `tf.test`
    
    PiperOrigin-RevId: 160338811
    
    * Internal change.
    
    PiperOrigin-RevId: 160339087
    
    * Fix bugs in ScatterNd and add ScatterNdNonAliasingAdd.
    
    tf.scatter_nd_non_aliasing_add acts similarly to tf.scatter_nd_add but
    works on non-ref objects (i.e., Tensors -- not Variables).  This means
    it has a gradient with respect to the primary input as well as the
    updates.  It does its best to avoid making extra copies of the input.
    
    PiperOrigin-RevId: 160339328
    
    * Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 160340888
    
    * Add checkpoint conversion for models that use the attention mechanism implemented in tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py.
    
    PiperOrigin-RevId: 160340994
    
    * Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 160341769
    
    * Merge changes from github.
    
    PiperOrigin-RevId: 160344052
    
    * Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 160346151
    
    * Load py_test in tensorflow/contrib/boosted_trees/BUILD to fix pip test
    visibility failures.
    
    * Disable boosted_trees tests on mac while they are being debugged.

commit 2d44813882ea2307c029223e6ab50ea847e411a1
Author: Sergio Guadarrama <sguada@google.com>
Date:   Wed Jun 21 15:14:26 2017 -0700

    Use more efficient squared_difference
    
    PiperOrigin-RevId: 159751209

commit fa75f26351f42e4fd3fc89b553d7919a6f147e41
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Jun 15 12:53:23 2017 -0700

    Introduce Predictor, an interface for efficient, repeated inference.
    
    PiperOrigin-RevId: 159141010

commit 1b5235fd897f7ea5cffc715300f67b4dc852fa27
Author: Jonathan Hseu <jhseu@google.com>
Date:   Fri Jun 9 10:37:18 2017 -0700

    Merge changes from github.
    END_PUBLIC
    
    ---
    Commit f0e185d1f authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Better handle nodes with a variable number of outputs
    
    PiperOrigin-RevId: 158435028
    
    ---
    Commit bc3e20807 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unused BUILD dependencies
    
    PiperOrigin-RevId: 158431059
    
    ---
    Commit a0c80e4d5 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Delete unnecessary (mistakenly duplicated) logging message.
    
    PiperOrigin-RevId: 158428506
    
    ---
    Commit b6ad1d747 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds DNN-only tests for DNNLinearCombinedClassifier.
    
    PiperOrigin-RevId: 158423119
    
    ---
    Commit ddbb58034 authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unnecessary pylint disable
    
    PiperOrigin-RevId: 158416140
    
    ---
    Commit fcaa724e2 authored by Luke Iwanski<luke@codeplay.com>
    Committed by gunan<gunan@google.com>:
    [OpenCL] Cleans pack and unpack ops (#10336)
    
    * [OpenCL] Cleans pack op
    
    * [OpenCL] Cleans unpack op
    
    ---
    Commit 2f53cacb2 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix a test failure of quantization_utils_test on ASAN
    
    PiperOrigin-RevId: 158414538
    
    ---
    Commit 50b2f951c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 158413455
    
    ---
    Commit 1e90b78e9 authored by Brennan Saeta<saeta@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add CacheDataset ops.
    
    Some input pipelines may pull down data from remote webservers or perform
    expensive processing. In order to avoid extraneous work, we now support
    caching the dataset (e.g. on disk).
    
    PiperOrigin-RevId: 158411901
    
    ---
    Commit e16cd2ede authored by Taehoon Lee<taehoonlee@snu.ac.kr>
    Committed by gunan<gunan@google.com>:
    Fix typos (#10533)
    
    ---
    Commit 50d80ddf9 authored by Jonathan Hseu<jhseu@google.com>
    Committed by Jonathan Hseu<jhseu@google.com>:
    Fix fft_ops_test.py for CPU
    
    ---
    Commit d35cbbb44 authored by Mustafa Ispir<ispir@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add weight-column support to the heads.
    
    PiperOrigin-RevId: 158409180
    
    ---
    Commit 7fb52cd54 authored by Justin Lebar<jlebar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Don't crash when displaying XLA metrics if they happen to be negative.
    
    PiperOrigin-RevId: 158407664
    
    ---
    Commit 12a7a752a authored by Jianfei Wang<me@thinxer.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Add a tip for tf.train.LoggingTensorHook (#10237)
    
    `INFO` logs are not printed by default unless in IPython. Add a friendly tip for newcomers.
    ---
    Commit 216dcbf1e authored by Luke Iwanski<luke@codeplay.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    [OpenCL] Cleans reduction ops (#10340)
    
    * [OpenCL] Cleans reduction_ops_max.cc
    
    * [OpenCL] Cleans reduction_ops_mean.cc
    
    * [OpenCL] Cleans reduction_ops_min.cc
    
    * [OpenCL] Cleans reduction_ops_prod.cc
    
    * [OpenCL] Cleans reduction_ops_sum.cc
    
    ---
    Commit 2b351062a authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Improve docs for selective registration headers (#10351)
    
    * Improve docs for selective registration headers
    
    progressing #10299
    
    * Update print_selective_registration_header.py
    
    * Mention both flags
    
    -DSELECTIVE_REGISTRATION and -DSUPPORT_SELECTIVE_REGISTRATION
    
    ---
    Commit ee919510f authored by Yun Peng<pcloudy@google.com>
    Committed by gunan<gunan@google.com>:
    Re-enable some python tests in Windows Bazel build (#10526)
    
    ---
    Commit b0e881457 authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by gunan<gunan@google.com>:
    [Bash] Declare and assign separately (#10509)
    
    As proposed by static analysis tool:
    https://github.com/koalaman/shellcheck/wiki/SC2155
    ---
    Commit 284901b08 authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by gunan<gunan@google.com>:
    [Bash] Remove unquoting quotes (#10506)
    
    As proposed by static analysis tool:
    https://github.com/koalaman/shellcheck/wiki/SC2027
    ---
    Commit 2a1f11556 authored by ksellesk<zhengdachuan200305@gmail.com>
    Committed by ksellesk<zhengdachuan200305@gmail.com>:
    Fix AttributeError in resnet.py
    
    There is no function tf.softmax() in Tensorflow 1.x.
    
    When running the old code, Python interpreter complains:
    
    File "resnet.py", line 152, in res_net_model
    prediction, loss = res_net(x, y)
    File "resnet.py", line 148, in res_net
    return tf.softmax(logits), loss
    AttributeError: 'module' object has no attribute 'softmax'
    
    ---
    Commit 1d68f729b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unneeded BUILD dependency
    
    PiperOrigin-RevId: 158391996
    
    ---
    Commit 08ed32dbb authored by Yun Peng<pcloudy@google.com>
    Committed by gunan<gunan@google.com>:
    Windows: Make TensorFlow build without --cpu=x64_windows_msvc (#10466)
    
    * Windows: Make TensorFlow build without --cpu=x64_windows_msvc
    
    Since from Bazel 0.5.0, MSVC toolchain became the default toolchain on
    Windows. So --cpu=x64_windows_msvc is not required as long as we adjust
    the BUILD files in TensorFlow.
    
    --cpu=x64_windows_msvc is also supported for now, but is depracated.
    The configuration for cpu value x64_windows_msvc is a duplicate of
    x64_windows, which should be removed in the future.
    
    * Fix breakage on macOS
    
    ---
    Commit 02dbe153a authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by gunan<gunan@google.com>:
    [Bash] Simplify Conditional (#10503)
    
    ---
    Commit c07bc581f authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by gunan<gunan@google.com>:
    [Bash] Prefer read -a to split path (#10508)
    
    As proposed by static analysis tool:
    https://github.com/koalaman/shellcheck/wiki/SC2207
    ---
    Commit 0a389674d authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by gunan<gunan@google.com>:
    [Bash] Prefer [ p ] && [ q ] over [ p -a q ] (#10507)
    
    As proposed by static analysis tool:
    https://github.com/koalaman/shellcheck/wiki/SC2166
    ---
    Commit 87a008ec3 authored by Jonathan Hseu<vomjom@vomjom.net>
    Committed by gunan<gunan@google.com>:
    Delete non-deterministic testEmpty() test (#10512)
    
    ---
    Commit 3a2971bd8 authored by Frank Chen<frankchn@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds the base for ClusterResolvers, a new way of communicating with and retrieving cluster information for running distributed TensorFlow.
    
    Implementations of this class would eventually allow users to simply point TensorFlow at a cluster management endpoint, and TensorFlow will automatically retrieve the host names/IPs and port numbers of TensorFlow workers from the cluster management service.
    
    PiperOrigin-RevId: 158358761
    
    ---
    Commit 28b4e7f04 authored by Jonathan Hseu<vomjom@vomjom.net>
    Committed by gunan<gunan@google.com>:
    Disable stage_op_test and map_stage_op_test (#10516)
    
    ---
    Commit 390e57a75 authored by Yan (Asta) Li<yanastali@users.noreply.github.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    Check EIGEN_MAX_ALIGN_BYTES to prevent mod-by-0 (#10380)
    
    * Check EIGEN_MAX_ALIGN_BYTES to prevent mod-by-0
    
    If EIGEN_MAX_ALIGN_BYTES is set to 0, alignment checks that mod by EIGEN_MAX_ALIGN_BYTES fail at runtime.
    
    * Returns true, as in tensorflow/core/framework/tensor.h
    * Update unit tests
    
    * Enable tests only if EIGEN_MAX_ALIGN_BYTES > 0
    
    ---
    Commit cd5ac40b3 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Update LLVM to upstream revision r304927.
    Add LLVM build rules for the LLVM AMDGPU backend, commented out by default. Fixes issue #10437.
    
    PiperOrigin-RevId: 158351480
    
    ---
    Commit 91cb809bd authored by David Norman<DavidNorman@users.noreply.github.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    [XLA] Add ability to run the XLA unit tests against a different device (#9759)
    
    * Add ability to run the XLA unit tests against a different device
    
    * Allow for multiple extra backend devices
    
    * Correct merge error
    
    * Include options for additional tags
    
    ---
    Commit aff4d124b authored by Yuxin Wu<ppwwyyxxc@gmail.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Compare base_dtype instead of dtype in piecewise_constant (#10280)
    
    * Compare base_dtype instead of dtype in piecewise_constant
    
    Compare base_dtype instead of dtype in piecewise_constant. Fix #10086
    
    * add unit test
    
    * Small lint fix and comment
    
    ---
    Commit 845539f98 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add evaluation test for linear classifier (n==2 or n >2).
    
    PiperOrigin-RevId: 158340296
    
    ---
    Commit 7c46214ab authored by Jonathan Hseu<vomjom@vomjom.net>
    Committed by GitHub<noreply@github.com>:
    Fix numpy 1.13 incompatibilities (#10501)
    
    * Fix numpy 1.13 incompatibilities
    
    * Skip tests with numpy 1.13.0
    
    ---
    Commit 4572c41df authored by gunan<gunan@google.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    A few changes to kernel_tests. (#10502)
    
    * Disable reader_ops_test on windows.
    
    * Run buildifier on kernel_tests/BUILD
    
    * Mark map_stage_op_test as large.
    
    * Set the size of stage_op_test to large
    
    ---
    Commit 892293d98 authored by Brennan Saeta<saeta@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Set a default for datasets end_of_sequence.
    
    While all datasets carefully set the end_of_sequence to true at the
    appropriate time, some datasets might forget to set it to false in the normal
    case. In order to avoid potential undefined behavior, we set the
    end_of_sequence variable to be false by default.
    
    PiperOrigin-RevId: 158337799
    
    ---
    Commit 187404eac authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Setup the env to since ops such as MatchFileOp rely on it.
    
    PiperOrigin-RevId: 158336344
    
    ---
    Commit 2741561c8 authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix up vz_projector script structure
    
    We now make sure scripts and HTML imports are declared in the correct
    places. In the future, pedantically listing script tags should not be
    necessary.
    
    PiperOrigin-RevId: 158334306
    
    ---
    Commit beeaade46 authored by Kay Zhu<kayzhu@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Resubmit a reverted change. Original description:
    
    [XLA] Enable HloEvaluator for constant folding, also merged a few operations
    from hlo_constant_folding to hlo_evaluator.
    
    Additionally:
    - In ShapeUtil::ForEachIndex:
        * fix a bug where visitor is called when the shape has zero elements (e.g., F32{1,0})
        * added test case for ForEachIndex.
    
    - In HloEvaluator:
        * Instead of copying and caching a Constant instruction, return the literal directly if the instruction is constant.
        * Fix an issue where TUPLE and OPAQUE primitives are not keyed in the templated typed_visitor.
        * Use (fixed) LiteralUtil::Populate to populate resulting literal, fixes the preexisting bug in the evaluator where R0 and shape with zero size dimensions are not handled.
        * Refactor ElementWiseUnaryOp and HandleCompare to be templatized on the operand's type.
        * Refactor IsFinite to be top level since it is only applicable to floats and the return type is always boolean.
        * Change from std::remainder to std::fmod for kRemainder to be compliant with existing XLA behavior.
        * Change from std::max and std::min to std::fmax and std::fmin to handle NaNs.
        * Minor comments fix.
    
    PiperOrigin-RevId: 158330052
    
    ---
    Commit b94540e6f authored by Toby Boyd<tobyboyd@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    tf.layers.conv2d use_bias=True to use nn.bias_add
    
    PiperOrigin-RevId: 158326493
    
    ---
    Commit 379aa9911 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 158325855
    
    ---
    Commit 4e529f0f1 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 158325293
    
    ---
    Commit 0a9d2dac0 authored by Yuefeng Zhou<yuefengz@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a util function in virtual placer to return canonicalized device string, which can be used to fix the node's device field before passing them to the maxcut algorithm.
    
    PiperOrigin-RevId: 158322753
    
    ---
    Commit 2d8da1d9b authored by Daniel Ylitalo<daniel@blodan.se>
    Committed by gunan<gunan@google.com>:
    Recognize CPU core count in FreeBSD (#10490)
    
    ---
    Commit c19e6cac0 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [TF:XLA] Initial implementation of TensorArray ops.
    
    The XLA implementation of TensorArrays is more restrictive than regular TensorArrays:
    * XLA TensorArrays must have dynamic_size=False.
    * all elements in an XLA TensorArray must have the same shape.
    * writes always add their values to any existing values; neither reads nor writes ever issue errors. Out-of-bounds writes currently wrap.
    
    Refactor Variable handling in the TF/XLA bridge. Use a XlaVariable* to refer to variables inside compilation rather than a numerical ID. Allow for variables that don't correspond to variables known to the user. Also use XlaVariable to handle TensorArrays.
    
    PiperOrigin-RevId: 158322041
    
    ---
    Commit b5e8d3086 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [TF:XLA] Refactor randomized tests to allow testing of larger inputs without running out of memory.
    
    PiperOrigin-RevId: 158321431
    
    ---
    Commit 5d90bbaac authored by Kay Zhu<kayzhu@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Disable constant_folding in test base, so that intended test code paths
    would not be elided by constant_folding pass.
    
    PiperOrigin-RevId: 158317641
    
    ---
    Commit 036ce8ba6 authored by Luke Iwanski<luke@codeplay.com>
    Committed by gunan<gunan@google.com>:
    [OpenCL] Cleans dense_update_ops (#10335)
    
    * [OpenCL] Cleans dense_update_ops
    
    * Acts on feedback from: #10335#discussion_r120536460
    
    ---
    Commit 85f968125 authored by Luke Iwanski<luke@codeplay.com>
    Committed by gunan<gunan@google.com>:
    [OpenCL] Cleans cast operation (#10330)
    
    * [OpenCL] Removes not needed typedef for SYCLDevice
    
    * [OpenCL] Fixes formatting
    
    * [OpenCL] use SYCLDevice for int32 cast case
    
    ---
    Commit bff5e72da authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix typo.
    
    PiperOrigin-RevId: 158310742
    
    ---
    Commit 38249d6be authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Swap the order of NanTensorHook and custom hooks
    
    to ensure that when the training encounteres NaN's in the loss function, user-supplied hooks such as tf_debug.LocalCLIDebugHook can still be used to debug the root cause of the numeric issues.
    
    PiperOrigin-RevId: 158310249
    
    ---
    Commit 599727c65 authored by Eli Bendersky<eliben@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Propagate debug option flags to hlo_test_base.
    
    Specific HLO tests have to replace the generic test_main target with a manual
    main() that invokes RUN_ALL_TESTS.
    
    To get access to a module with debug options set up, a new convenience method
    is created on HloTestBase.
    
    Initially algebraic_simplifier_test is modified as a canary; in a followup
    we'll convert all HLO tests to this approach.
    
    PiperOrigin-RevId: 158309488
    
    ---
    Commit 0770393e9 authored by Eric Liu<ioeric@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [Tensorboard] Add a trace viewer component to TensorBoard.
    
    We make the trace viewer a separate app; otherwise, there would be dependency
    conflicts (e.g. Polymer) between the trace viewer app and the tensorboard app.
    The trace viewer app would be served by a plugin, and Tensorboard dashboard will integrate trace viewer app using iframe in the
    future.
    
    This CL also added "mominify" support for link import HTML tags in the
    tensorboard home-grown java vulnizer; otherwise, the vulcanized trace viewer code
    would crash the java vulcanizer.
    
    For open-source build, we add a denpendency on the Catapult github repository
    (https://github.com/catapult-project/catapult/tree/master/tracing). We use a bazel genrule to vulcanize a trace viewer binary which is then used in the
    tf-trace-viewer component.
    
    PiperOrigin-RevId: 158309408
    
    ---
    Commit 85e832201 authored by RJ Ryan<rjryan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Support unknown emit shapes in tf.nn.raw_rnn.
    
    PiperOrigin-RevId: 158308002
    
    ---
    Commit edb5fed7f authored by Mustafa Ispir<ispir@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add label-vocab support to binary logistic head.
    Add assertion that binary classifier label is in range [0., 1.]
    Fixed Classifier Integration tests.
    
    PiperOrigin-RevId: 158307521
    
    ---
    Commit f8e1cf8fa authored by Justine Tunney<jart@google.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Open up visibility of tf_imports (#10500)
    
    This also fixes the definition of Clutz.
    ---
    Commit 9fd7cf054 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    [OpenCL] Cleans relu ops (#10343)
    
    * [OpenCL] register relu ops to gpu types (no half)
    
    * [OpenCL] Removes #undef EIGEN_USE_SYCL
    
    ---
    Commit 09c1455e3 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    [OpenCL] Cleans reverse_op.cc (#10346)
    
    ---
    Commit b7892a30f authored by orome<royl@aldaron.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Clarify tf.matmul documentation (#10381)
    
    * Update math_ops.py
    
    * Fix non-ascii character
    
    ---
    Commit 9786b7062 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Cleans StridedSlice Op (#10314)
    
    * [OpenCL] Cleans StridedSlice Op
    
    * [OpenCL] Removes half from registred types
    
    ---
    Commit f105df047 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    In the CUDA path of depthwise_conv2d, optimize backward filter convolution for images 2 or 4 times smaller than 16x16. Also initialize in_cols from blockDim, to fix the regression caused in CL 157906773.
    
    PiperOrigin-RevId: 158296136
    
    ---
    Commit 492afc2e3 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 158295169
    
    ---
    Commit abe0877ef authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add bazel version check to .configure
    
    PiperOrigin-RevId: 158294569
    
    ---
    Commit b702e7e79 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 158294289
    
    ---
    Commit 94085bee7 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Replace std::function object with regular function.
    
    The function is called recursively, and the std::function object had only existed to allow recursion from within a lambda expression. A regular function should be cheaper than a polymorphic function wrapper.
    
    PiperOrigin-RevId: 158292415
    
    ---
    Commit ba656b261 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use template specialization instead of overloaded methods. This is a more appropriate tool here. NFC
    
    PiperOrigin-RevId: 158292035
    
    ---
    Commit 55f987692 authored by Yutaka Leon<yleon@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
      Make tf.contrib.lookup  python functions use the kernels v2 that uses the resource tensor as handler.
    
    PiperOrigin-RevId: 158291836
    
    ---
    Commit ebae3deba authored by Wei Ho<weiho@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Switch back to max_num_rows_to_load instead of reading slice by slice due to performance regression from network overhead.
    
    Add check when using initializing values to avoid seg fault
    
    PiperOrigin-RevId: 158291218
    
    ---
    Commit 7b4c01794 authored by RJ Ryan<rjryan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Support numpy-style padding and slicing of tf.spectral.rfft/irfft to match the desired FFT length.
    
    Fixes incorrect RFFT/IRFFT results when fft_length does not match the input dimension.
    
    PiperOrigin-RevId: 158289991
    
    ---
    Commit fdb8e2935 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update iOS examples to use CocoaPods, and moved to tensorflow/examples/ios
    
    PiperOrigin-RevId: 158289285
    
    ---
    Commit d86167b5f authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Merging rc2 back into master.
    
    ---
    Commit dffea202a authored by Eli Bendersky<eliben@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Clean up some code after previous CL
    
    PiperOrigin-RevId: 158282834
    
    ---
    Commit 7b5302af0 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds ability to set a "family" attribute in Tensorflow summaries, which
    controls the "tab name" of the summary that is displayed.
    
    This solution keeps using name_scope to keep names unique, but then prefixes the tag with the family name if provided.
    
    PiperOrigin-RevId: 158278922
    
    ---
    Commit 611c82b5b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds integration test for DNNLinearCombined((Classifier)|(Regressor)).
    
    PiperOrigin-RevId: 158278512
    
    ---
    Commit cc6c91a9a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove a further unused proto header inclusion
    
    PiperOrigin-RevId: 158278026
    
    ---
    Commit 9f17c26ca authored by Mark Heffernan<meheff@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Add HloLocation to dataflow analysis.
    Add an HloLocation abstraction to dataflow analysis which indicates where (in the output of what instruction and at which index) an HloValue may appear. Previously only uses were stored with an HLO value where a use is an edge in the HLO graph (instruction, operand number and ShapeIndex).
    
    Also, change the handling of tuple-shaped kSelect instructions when ssa_form is true. Previously a phi value would be created. With this change the the value set instead contains the union of it's inputs identical to the ssa_form=false case.
    
    PiperOrigin-RevId: 158276598
    
    ---
    Commit b9d5e1441 authored by Eli Bendersky<eliben@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Start collecting flags for debug options in a single place.
    
    ClientLibraryTestBase will now parse command-line flags for debug options
    automatically, permitting subclasses to override certain options by using
    mutable_debug_options.
    
    main() still has to call AppendDebugOptionsFlags() explicitly before running
    the TF flag parser. In the mean-time, this CL leaves flag handling to the
    current "legacy" approach. However, this is part of a larger plan to move *all*
    debugging flags for XLA into the DebugOptions message and expose them as flags
    from a single place. The other flags (which are not controlling debugging
    options) will have to be propagated more explicitly.
    
    PiperOrigin-RevId: 158276294
    
    ---
    Commit 3b6fe94bb authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Properly handle shape nodes that have a preexisting control dependency
    
    PiperOrigin-RevId: 158274845
    
    ---
    Commit 1d67379d5 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Minor cleanup
    
    PiperOrigin-RevId: 158268933
    
    ---
    Commit 41997756c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Sort header inclusions; define EIGEN_USE_THREADS where headers depend on it.
    
    PiperOrigin-RevId: 158267803
    
    ---
    Commit 85355f015 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add missing header inclusion
    
    PiperOrigin-RevId: 158265934
    
    ---
    Commit 3cf88d390 authored by Gunhan Gulsoy<gunan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    When GPU is configured, do not require --config=cuda.
    Also fix indentation in configure.
    
    PiperOrigin-RevId: 158232959
    
    ---
    Commit f48673b50 authored by Luke Iwanski<luke@codeplay.com>
    Committed by gunan<gunan@google.com>:
    [OpenCL] Removes ReductionFunctor for SYCLDevice (#10326)
    
    We are using Eigen implementation
    ---
    Commit 1b6453bec authored by Joan Puigcerver<joapuipe@gmail.com>
    Committed by gunan<gunan@google.com>:
    Fixes issue #10258 (#10366)
    
    On CUDA versions previous to 8.0, only __shared__ variables could be declared as static in the device code.
    ---
    Commit cd56a638d authored by Beomsu Kim<123bskim@naver.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Fixed wrong range in docstring (#10272)
    
    ---
    Commit d13ae380c authored by Micha? Jastrz?bski<michal.jastrzebski@intel.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Fix CMD in Dockerfile (#10444)
    
    Currently Notebook fails execution because default user for this container is root, and unless explicitly allowed, jupyter notebook will not start.
    ---
    Commit 8118ab4ec authored by Simon Perkins<simon.perkins@gmail.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Support partial gets in MapStagingArea (#10276)
    
    * Modify map staging area tests
    
    - size from `small` to `medium`
    - introduce 2 shards
    
    * Add partial get support in MapStagingArea
    
    A partial list of tensors in a (key, value) map entry can now be
    requested. Once all tensors associated with the entry are removed,
    it is removed from the map.
    
    * Correct output/indices mismatch errors
    
    * Rename IncompleteTuple to OptionalTuple
    
    * Add partial get test with indices
    
    * Add some more index checks
    
    * Improve stage test case graph creation
    
    Test sessions (and default graphs) are reused by default.
    Create explicit, finalized graphs in each test to prevent
    possible interactions between stateful Staging Areas and
    others ops created in separate tests.
    
    * Make staging area tests small and remove shards
    
    They were originally made 'medium' to ameliorate timeouts in the test
    case, but they usually run in ~1s so they should be small.
    
    * Improve imports
    
    Avoid importing base tensorflow package
    
    * Support both python 2 and python 3 range.
    
    * Set map_stage_op_test to size=large
    
    * Convert the tests to size=medium
    
    ---
    Commit 0df102b0a authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Update `configure` script sample (#10455)
    
    The `configure` script was changed regularly since the generation of the sample.
    This PR updates the sample to reflect those changes.
    ---
    Commit f6dc1ac61 authored by Earthson Lu<Earthson.Lu@gmail.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    MKL_INSTALL_PATH should not be ignore when given (#10180)
    
    * MKL_INSTALL_PATH should not be clear when given
    
    * fix overwrite by default
    
    ---
    Commit 8ad6a036e authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Java: Update Maven release to 1.2.0-rc2
    
    PiperOrigin-RevId: 158212897
    
    ---
    Commit 15eddf035 authored by Fritz Obermeyer<fritz.obermeyer@gmail.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Export C API symbols in _pywrap_tensorflow_internal.so (#10469)
    
    * Export C API symbols
    
    * Export C API symbols under config:default
    
    ---
    Commit 754e12668 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    [OpenCL] Removes half concat op registration (#10331)
    
    ---
    Commit cfdc22dee authored by Peng Yu<yupbank@users.noreply.github.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    fix the error (#10293)
    
    ---
    Commit 58747e357 authored by Joel Hestness<jthestness@gmail.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    PhiloxRandom: Fix race in GPU fill function (#10298)
    
    * PhiloxRandom: Fix race in GPU fill function
    
    The PhiloxRandom fill kernel for the GPU had race conditions that caused the
    outputs to be non-deterministic. In particular, the code previously executed
    with N GPU threads (# thread contexts per GPU), but it would only advance the
    fill addresses by N-1 stride in each step. This incorrect stride caused the
    0th and N-1st threads to write to the same memory locations, racing for which
    was last to write their common locations. Make the stride equal to the number
    of threads to eliminate the race.
    
    BONUS: By fixing this race, PhiloxRandom constant-sized GPU initializers now
    match CPU initializers.
    
    * Update random_ops_test.py to find race conditions
    
    Increasing the size of arrays in the random_ops_test.py test to manifest
    the race conditions to be resolved.
    
    ---
    Commit 2cbcda08f authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Fixed formatting in Linux install guide (#10353)
    
    Formatting issues were introduced in PR #8825, commit f30918b3694afe844990cbddc82e27e023d88856
    ---
    Commit ab5f38560 authored by Lakshay Garg<lakshayg@outlook.in>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Fixed typos in documentation & READMEs (#10365)
    
    ---
    Commit 94dc1dbfa authored by Christos Nikolaou<cNikolaou@users.noreply.github.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Enable figures in the tfprof README.md (#10372)
    
    ---
    Commit 3018d4678 authored by Taehoon Lee<taehoonlee@snu.ac.kr>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Fix typos (#10386)
    
    ---
    Commit c5f3c6171 authored by Daniel Rasmussen<drasmuss@users.noreply.github.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Fix unbatch for Datasets with multiple elements (#10401)
    
    * Fix unbatch for datasets with multiple elements
    
    * fixup! pylint (indent two spaces instead of four)
    
    ---
    Commit 8b065bc10 authored by Yong Tang<yong.tang.github@outlook.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    Fix unaligned args in api_docs/python/tf/contrib/learn/Evaluable (#10423)
    
    This commit fixes unaligned args in api_docs/python/tf/contrib/learn/Evaluable
    
    Signed-off-by: Yong Tang <yong.tang.github@outlook.com>
    ---
    Commit 8f89b654f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Profile memory usage in VirtualScheduler and report peak memory usage.
    To do so, NodeState now handles different output ports of a node (in case
    a node has multiple outputs).
    
    Also, VirtualScheduler code is cleaned up with more comments.
    
    PiperOrigin-RevId: 158209068
    
    ---
    Commit 0ea0bf5aa authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a frontend for viewing the first ops that exhibit bad values (NaN, +/- Inf).
    
    This helps the user identify problematic ops. Also moved the debugger data logic within tf-graph-info into a new tf-graph-debugger-data-card component.
    
    PiperOrigin-RevId: 158208679
    
    ---
    Commit ed47ecf2d authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Cleans variable op (#10333)
    
    * [OpenCL] Cleans variable op
    
    * Fixes formatting and float / double -> GPU_NUMBER_TYPES_NO_HALF
    
    ---
    Commit 9b2c1af63 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Improves device reporting (#10462)
    
    Prints: id, type, name, vendor and profile of the device
    ---
    Commit 7f5384dcc authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Making load() work for resource variables.
    
    PiperOrigin-RevId: 158205361
    
    ---
    Commit 05412bd36 authored by Mark Heffernan<meheff@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Simplify Shape traversal visitors.
    Simplify shape traversal visitors in ShapeUtil and ShapeTree. Add a non-Status form because most uses of the traversal methods do not use it, and remove is_leaf parameter from ShapeTree.ForEach* as it is not frequently used.
    
    PiperOrigin-RevId: 158201574
    
    ---
    Commit 69c9365b4 authored by Mustafa Ispir<ispir@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Extracted linear estimator testing utils to be reused by dnn-linear-combined.
    Added tests for linear part of dnn-linear-combined estimator.
    
    PiperOrigin-RevId: 158200827
    
    ---
    Commit 65ce8c723 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add arrowheads to dataflow edges.
    Make reference edges orange.
    Remove animations from tooltips in the graph documentation.
    
    Previously, arrowheads were only added to reference edges (because we assumed users knew about the convention that arrowless edges flow upwards). That decision nicely reduces clutter. However, recently, some internal and external folks have expressed confusion, and so I want to try adding arrowheads to all data flow edges. And make the reference edges starkly different.
    
    See #10428
    
    PiperOrigin-RevId: 158195388
    
    ---
    Commit bf4c3dd6b authored by gunan<gunan@google.com>
    Committed by GitHub<noreply@github.com>:
    Revert "Fix patching issue on Windows" (#10472)
    
    This reverts commit 47e6785646a1266f01a1a570bd799f8518ee2997.
    
    ---
    Commit b49515539 authored by David Soergel<soergel@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add only string constants to ASSET_FILEPATHS collection.
    
    PiperOrigin-RevId: 158192152
    
    ---
    Commit 51acad09c authored by Sergio Guadarrama<sguada@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add tests with different delta to huber_loss.
    
    PiperOrigin-RevId: 158191361
    
    ---
    Commit a4e7b7add authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixes a bug in setting default optimizers for DNNLinearCombinedClassifier.
    
    PiperOrigin-RevId: 158190192
    
    ---
    Commit ddd67e333 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    [OpenCL] Cleans reshape.cc (#10347)
    
    * [OpenCL] Cleans reshape.cc
    
    * Removes half and complex numbers.
    
     Half is extension and complex numbers needs implementation in Eigen first
    
    ---
    Commit 3ca653304 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 158186454
    
    ---
    Commit 8cda8660e authored by Luke Iwanski<luke@codeplay.com>
    Committed by gunan<gunan@google.com>:
    [OpenCL] Cleans sendrecv_ops.cc (#10345)
    
    ---
    Commit 6915bb919 authored by Luke Iwanski<luke@codeplay.com>
    Committed by gunan<gunan@google.com>:
    [OpenCL] Cleans Slice op (#10341)
    
    ---
    Commit 54998b45d authored by Michele Colombo<m-colombo@users.noreply.github.com>
    Committed by Jonathan Hseu<vomjom@vomjom.net>:
    BasicRNNCell comment fix (#10467)
    
    ---
    Commit df5906fb7 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Mark saver/restore ops that depend on filesystem as stateful to disable them
    from being folded into a constant by graph optimizer.
    
    PiperOrigin-RevId: 158182282
    
    ---
    Commit 96cb4d182 authored by Sergio Guadarrama<sguada@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add support of scale_l1 == 0. or scale_l2 == 0 to l1_l2_regularizer.
    Added tests.
    
    PiperOrigin-RevId: 158179790
    
    ---
    Commit b65eb3f9b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Speed up atrous_convolution_test by combining evaluations.
    
    To make this test run faster (and prevent it from timing out under
    certain circumstances), this change combines all evaluations for each
    test method into a single call to Session.run, to eliminate overhead.
    
    This reduces the test time from about 40 seconds to 10 seconds.
    
    RELNOTES: n/a
    PiperOrigin-RevId: 158175227
    
    ---
    Commit b440abce7 authored by Gao, Xiang<qasdfgtyuiop@gmail.com>
    Committed by Rasmus Munk Larsen<rmlarsen@google.com>:
    add Cuda{2D,3D}LaunchConfig that maximizes occupancy (#10032)
    
    * add Cuda{2D,3D}LaunchConfig that max occupancy
    
    * remove default val, check input<=0
    
    * add max size check
    
    * fix typo
    
    * tests, docs, and related changes
    
    * build the test
    
    * buildify
    
    * cudaOccupancy... call check success, and style fix
    
    ---
    Commit 81cf61fdb authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Initialize tensor in graph_properties_test, to avoid msan complaint.
    
    PiperOrigin-RevId: 158169374
    
    ---
    Commit cabc5c35c authored by Eli Bendersky<eliben@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Add xla_disable_hlo_passes to DebugOptions
    
    Also add a SetDebugOptions method to ClientLibraryTestBas; this lets us set
    debug options in tests by calling it.
    
    As an example, this CL removes the current way of passing
    xla_disable_hlo_passes programmatically in tests - it used to employ a special
    constructor parameter which is no longer required.
    
    PiperOrigin-RevId: 158169006
    
    ---
    Commit 187d23337 authored by Luke Iwanski<luke@codeplay.com>
    Committed by gunan<gunan@google.com>:
    [OpenCL] Cleans Pad op (#10339)
    
    ---
    Commit e8bc38ef6 authored by gunan<gunan@google.com>
    Committed by GitHub<noreply@github.com>:
    Fix test failures on windows. (#10470)
    
    ---
    Commit 2b3535c64 authored by David Soergel<soergel@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Minor docstring fix for build_parsing_serving_input_receiver_fn
    
    PiperOrigin-RevId: 158163615
    
    ---
    Commit e55f2e036 authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Propagates constants through switch nodes.
    
    PiperOrigin-RevId: 158163537
    
    ---
    Commit b01d4b905 authored by Jacques Pienaar<jpienaar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Remove outdated todo.
    
    PiperOrigin-RevId: 158161411
    
    ---
    Commit 7125733d7 authored by William Chargin<wchargin@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Create a set of sample data for the audio plugin
    
    This implements a simple tone generator, with sine waves, square waves,
    and triangle waves, plus two simple combinations of sine waves. The step
    value is used to control the frequency.
    
    PiperOrigin-RevId: 158160889
    
    ---
    Commit dc81a2420 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Updates to the WALSMatrixFactorization estimator:
    - Add a completed_sweeps variable to keep track of sweeps that have been completed during training.
    - Add a StopAtSweepHook, which can request a stop after completing a specified number of sweeps.
    
    PiperOrigin-RevId: 158156347
    
    ---
    Commit 74220616c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Set device cores and frequency in op_level_cost_estimator_test,
    to avoid asan error about assigning inf to int64 (this comes
    in from a divide-by-0).
    
    PiperOrigin-RevId: 158155488
    
    ---
    Commit 47e678564 authored by Yun Peng<pcloudy@google.com>
    Committed by gunan<gunan@google.com>:
    Fix patching issue on Windows (#10452)
    
    ---
    Commit 6d54f09d9 authored by Yun Peng<pcloudy@google.com>
    Committed by gunan<gunan@google.com>:
    Fix linking errors of lmdb on Windows (#10457)
    
    ---
    Commit 61c8a745b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Minor cleanup: Add braces around if statement arms; remove redundant "return" and "static".
    
    PiperOrigin-RevId: 158143418
    
    ---
    Commit e9a889c5e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Pass int parameter by value, not by const reference
    
    PiperOrigin-RevId: 158142102
    
    ---
    Commit 9184726ed authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Avoid unnecessary copying of map data during visitation
    
    PiperOrigin-RevId: 158141962
    
    ---
    Commit 2e7e1d57b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Small fix for how std::move is used in constructors
    
    PiperOrigin-RevId: 158141564
    
    ---
    Commit 2a61c1652 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    In cpu compiler's CompileAheadOfTime, pass ordering when compiling entry computation.
    
    PiperOrigin-RevId: 158140349
    
    ---
    Commit f3f53e8b3 authored by Derek Murray<mrry@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [tf.contrib.data] Add support for dicts and remove lists from nested structures.
    
    This changes the behavior of constructors like
    `tf.contrib.data.Dataset.from_tensors()` when passed a list. Previously, the
    `nest` utility would recurse into each element of such a list and create a
    separate Dataset component. Now the list will be converted to a tensor, allowing code like:
    
    ```python
    dataset = tf.contrib.data.Dataset.from_tensor_slices(([1, 2, 3], [4, 5, 6]))
    ```
    
    ...to define a dataset with two components (each of shape `()`).
    
    This change also adds support for dictionaries as nested structures, which
    simplifies integration with dictionary-returning ops like `tf.parse_example()`.
    
    Fixes #10151.
    
    RELNOTES: Breaking change to `tf.contrib.data.Dataset` APIs that expect a
    nested structure. Lists are now converted to tf.Tensor implicitly. You may need
    to change uses of lists to tuples in existing code. In addition, dicts are now
    supported as a nested structure.
    PiperOrigin-RevId: 158139467
    
    ---
    Commit b6a8848c1 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Enabling python configuration to use a remotely generated configuration that is located inside of the org_tensorflow repo (previously it *had* to be a remote repo declared in workspace file).
    
    PiperOrigin-RevId: 158138601
    
    ---
    Commit 0fe0bfcc3 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unused protobuf header inclusions
    
    PiperOrigin-RevId: 158120864
    
    ---
    Commit f0c4c6c3f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    In the CUDA path of depthwise_conv2d, add a fast NCHW backward filter convolution for images smaller than 16x16.
    
    PiperOrigin-RevId: 158111294
    
    ---
    Commit 8dcf37b47 authored by Jon Malmaud<malmaud@gmail.com>
    Committed by gunan<gunan@google.com>:
    Fix typo (#10379)
    
    ---
    Commit 3039d7da2 authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by gunan<gunan@google.com>:
    Remove "bazel clean" (#10318)
    
    Reverting #8880 (see #10236)
    unnecessary since bazelbuild/bazel#2759 was merged
    ---
    Commit ae1c16ae8 authored by Yifei Feng<fengyifei2026@gmail.com>
    Committed by gunan<gunan@google.com>:
    Update docker to cudnn6. (#10307)
    
    * Update docker to cudnn6.
    
    * Update Dockerfile.gpu
    
    * Add --expunge to bazel clean to make cuda_configure run again and update TF_CUDNN_VERSION.
    
    * Remove expunge and set CUDA and CUDNN version default in configure.
    
    * Update configure
    
    * Only set --action_env once
    
    * Update prints for default version.
    
    ---
    Commit 232e9d86d authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    tf_workspace() claims that the tf_repo_name argument is unused.
    temp_workaround_http_archive still requires it.
    This change silences the spurious message.
    
    PiperOrigin-RevId: 158089834
    
    ---
    Commit cc1a02d37 authored by Francois Chollet<fchollet@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add fp16 support to convolutional layers that support it.
    
    PiperOrigin-RevId: 158086284
    
    ---
    Commit 7d3fbba48 authored by Mustafa Ispir<ispir@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Extracted dnn estimator testing utils to be reused by dnn-linear-combined.
    Added tests for dnn part of dnn-linear-combined estimator.
    
    PiperOrigin-RevId: 158084898
    
    ---
    Commit 9d12c629c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Refactor the document and some polishment
    
    PiperOrigin-RevId: 158083952
    
    ---
    Commit 134138299 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Corrected comment: import_scoped_metagraph does not return a Saver.
    
    PiperOrigin-RevId: 158082288
    
    ---
    Commit a58553e4d authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add function in shape inference to try to infer output tensor content based on
    the input shapes of the op. In some cases (E.g: shape), knowing the shapes of
    the input is all that is necessary to infer the content of the output tensor.
    This improves shape inference.
    
    PiperOrigin-RevId: 158079306
    
    ---
    Commit 0cc851c08 authored by Yuefeng Zhou<yuefengz@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Call maxcut algorithm in the model_based_cost_estimator.
    
    PiperOrigin-RevId: 158078511
    
    ---
    Commit 7d76a90be authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add question marks next to items in the graph legend.
    
    PiperOrigin-RevId: 158076005
    
    ---
    Commit 68fdb7628 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add DNNLinearCombinedClassifier.
    
    PiperOrigin-RevId: 158075939
    
    ---
    Commit 3d52e4cb9 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix create_meta_graph to respect an empty collection_list.
    
    PiperOrigin-RevId: 158073112
    
    ---
    Commit 54ccc3e5a authored by Mark Heffernan<meheff@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Add module-scoped HLO dataflow analysis.
    This is the first step to replacing TuplePointsToAnalysis with a global, module-scoped analysis. This dataflow analysis identifies all values and their defs and uses in the XLA graph. The analysis is currently unused. Follow up CLs will add buffer alias analysis using this dataflow analysis, and incrementally switch the transformation passes (for example, CopyInsertion) to use these new module-scoped analyses.
    
    PiperOrigin-RevId: 158067910
    
    ---
    Commit 93c57c6e4 authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Handle control flow logic properly:
     * Don't fold enter/exit nodes since that can interact badly with frames
     * Create proper control dependencies on switch nodes
    
    PiperOrigin-RevId: 158066691
    
    ---
    Commit 9e6899720 authored by Jingyue Wu<jingyue@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [SE] Add cudnnTransformTensor to StreamExecutor.
    
    PiperOrigin-RevId: 158062553
    
    ---
    Commit 827874c30 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    In the CUDA path of depthwise_conv2d, add a fast NCHW backward input convolution for images smaller than 16x16.
    
    PiperOrigin-RevId: 158061669
    
    ---
    Commit bee26215c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Speed up multinomial_op on CPU by using a vectorized Eigen expression and avoiding unnecessary casts.
    
    Benchmark with AVX+FMA enabled:
    
    Run on <redacted> (12 X 3492 MHz CPUs); 2017-06-05T12:54:07.881672447-07:00
    CPU: Intel Haswell with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:15MB
    Benchmark                          Base (ns)  New (ns) Improvement
    ------------------------------------------------------------------
    BM_Multinomial_cpu_1_10000_4          250817    172953    +31.0%
    BM_Multinomial_cpu_1_10000_128        273834    187552    +31.5%
    BM_Multinomial_cpu_1_10000_10000     1174175   1130778     +3.7%
    BM_Multinomial_cpu_1_100000_4        2040741   1276761    +37.4%
    BM_Multinomial_cpu_32_10000_4       10221765   4498666    +56.0%
    BM_Multinomial_cpu_32_10000_128     10638159   4994754    +53.0%
    BM_Multinomial_cpu_32_100000_4      100790019  44193314    +56.2%
    BM_Multinomial_cpu_128_100000_1     431269640  182506078    +57.7%
    PiperOrigin-RevId: 158061480
    
    ---
    Commit 515b3ac67 authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add Clutz to TensorBoard build
    
    This is so we can get JavaScript protobufs. This CL also improves the
    web_aspect and makes some peculiar Closure Compiler errors go away
    relating to externs.
    
    PiperOrigin-RevId: 158061198
    
    ---
    Commit 0df6760fe authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added a test to make sure that graph properties for variables are properly
    reported
    
    PiperOrigin-RevId: 158053084
    
    ---
    Commit 2ccfe8e76 authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added a new method to extract the graph properties from a cost graph without
    having to run the model. This will simplify the process of creating regression
    tests
    
    PiperOrigin-RevId: 158050327
    
    ---
    Commit 27f1b80c2 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixes memory leak in py_func when functions return unwrapped strings.
    
    PiperOrigin-RevId: 158046530
    
    ---
    Commit cf238e1f2 authored by Eugene Brevdo<ebrevdo@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix memory leak in python caused by @tf_should_use.
    
    The issue is that python's GC has trouble collecting objects with __del__ methods.
    
    The solution is two pronged:
    * Keep track of usage state outside of the class, via a dict mapping
      id(object) => state
    * Remove __del__ (this was the source: python's GC couldn't collect wrapped
      objects), and instead use weakref.finalize to emit warnings just as the object
      is being garbage collected.
    * Added tests for garbage collection [they were failing before i fixed the issue]
    
    PiperOrigin-RevId: 158042388
    
    ---
    Commit e6f581863 authored by Bo Wang<david.b.wang@gmail.com>
    Committed by Rasmus Munk Larsen<rmlarsen@google.com>:
    New reader for LMDB databases (#9950)
    
    * Add LMDBReader op and test case
    
    * Add testcase to load LMDB from a folder
    
    * Add tensorflow/core/lib/lmdb/testdata/data.mdb
    
    * Add EOF test
    
    * Add license export
    
    * Blacklist the test data in pip_smoke_test.py
    
    * Address issues with respect to review
    
    * Add LICENSE to BUILD rules
    
    * Remove the prefx of LICENSE
    
    * Wrap key with compat.as_bytes()
    
    * Fixed a compilation flag
    
    * Improve BUILD rules
    
    * Support LMDB build in cmake
    
    * Fix BUILD file format with buildifier
    
    * Add fake unistd.h for lmdb to build on Windows
    
    * Avoid building lmdb tools which depends on unistd.h
    
    * Fix the string encoding issue in Python3
    
    * Update lmdb library name in CMakeList.txt
    
    ---
    Commit cc411f938 authored by Yao Zhang<yaozhang@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    When converting the layout of Conv2DBackpropInput, we need to permute one of
    its inputs, which is a constant node. We permute a copy of this node, instead of the
    original node, because the original node may be used as input to other nodes.
    This kind of sharing of const node could arise if the graph is pre-optimized by common
    subexpression elimination, which is part of the L1 optimizations in
    TensorFlow.
    
    PiperOrigin-RevId: 158037552
    
    ---
    Commit 88bdb6fca authored by Dandelion Man?<dandelion@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove all remaining references to non-public TF modules from TensorBoard.
    
    I deleted the PluginAssetUtil tests because that code is deprecated.
    I'll later add manual testing for backcompat in the text plugin.
    
    PiperOrigin-RevId: 158037466
    
    ---
    Commit 6c531eb2f authored by Francois Chollet<fchollet@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add file hash to Keras Boston Housing dataset to force cache update.
    
    PiperOrigin-RevId: 158036587
    
    ---
    Commit afdc38cd3 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove deprecated resource handle functions in InferenceContext.
    
    PiperOrigin-RevId: 158034419
    
    ---
    Commit 9f932e6ce authored by Derek Murray<mrry@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Avoid parsing a rendezvous key for Send/Recv ops outside a loop.
    
    For such ops, the rendezvous key will be constant, because
    `ctx->frame_iter()` will always evaluate to `{0, 0}`. Benchmarking
    reveals that this can save between 1 and 2 microseconds per Send or
    Recv op execution. The optimization applies to all cross-process,
    inter-device, and intra-device (host-to/from-device memory) Send/Recv
    ops.
    
    PiperOrigin-RevId: 158032522
    
    ---
    Commit cc2dd4ac8 authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    tfdbg: dump debug data from different devices in separate directories
    
    Fixes: #7051
    wherein TFDBG failed to load the data dump from a Session.run() involving multiple GPUs.
    
    The root cause of the bug was that TFDBG previously assumed that node names are unique across all partition graphs. This is however not the case when multiple GPUs exist. The Send/Recv nodes in the partition graphs of the GPUs can have duplicate names. There will potentially be other cases like this in the future due to other reasons (e.g., distributed sessions and/or graph optimization).
    
    This CL relaxes this assumption, by dumping the GraphDef and tensor data from different devices into different sub-directories under the dump root directory.
    
    PiperOrigin-RevId: 158029814
    
    ---
    Commit a5909d643 authored by Toby Boyd<tobyboyd@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixed triggering create device multiple times
    
    PiperOrigin-RevId: 158025196
    
    ---
    Commit 504a307b7 authored by Martin Wicke<wicke@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make sure that Adam colocates ops with a consistent variable across workers.
    
    PiperOrigin-RevId: 158022292
    
    ---
    Commit 69ba4d3d4 authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix #10371
    
    cpuinfo.get_cpu_info() doesn't seem to include the l2_cache_size key on some
    architectures.
    
    PiperOrigin-RevId: 158021008
    
    ---
    Commit a51a9846c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Performance-related tweaks: Don't copy loop variables; remove ineffective std::move casts.
    
    PiperOrigin-RevId: 158017670
    
    ---
    Commit 009789f74 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Allow 0-sized slices in DynamicSlice and DynamicUpdateSlice; add tests.
    
    PiperOrigin-RevId: 158015870
    
    ---
    Commit 48a4853eb authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Miscellaneous cleanups
    
    PiperOrigin-RevId: 158012131
    
    ---
    Commit 379ddde24 authored by Chris Song<sjhshy@gmail.com>
    Committed by Chris Song<sjhshy@gmail.com>:
    Fix misspells.
    
    ---
    Commit a0a76da97 authored by Lakshay Garg<lakshay.garg.1996@gmail.com>
    Committed by Lakshay Garg<lakshay.garg.1996@gmail.com>:
    Fixed typo in code
    
    ---
    Commit 7ffc35732 authored by Eugene Brevdo<ebrevdo@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add support for bools in matrix_diag, matrix_diag_part, matrix_set_diag, matrix_band_part.
    
    PiperOrigin-RevId: 157939272
    
    ---
    Commit edf3d5dbe authored by Darren Garvey<darren.garvey@gmail.com>
    Committed by Darren Garvey<darren.garvey@gmail.com>:
    configure: Fix default path when enabling MPI.
    
    Correct showing what the default path is when mpi is installed.
    
    ---
    Commit aad2e3daf authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    In the CUDA path of depthwise_conv2d, add a fast NCHW forward convolution for images smaller than 16x16.
    
    PiperOrigin-RevId: 157915637
    
    ---
    Commit 5cf08d9cb authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Drop blockDim.y for the equivalent in_cols, and slightly improve naming (use 'pixels' instead of 'size' for height*width numbers).
    
    PiperOrigin-RevId: 157906773
    
    ---
    Commit 563f05ff6 authored by Eugene Brevdo<ebrevdo@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [tf contrib seq2seq] Expand tile_batch to handle nested structures.
    
    This allows it to properly tile the initial wrapper state when using
    BeamSearchDecoder with AttentionWrapper.  Unit tests updated to show this use.
    
    PiperOrigin-RevId: 157903115
    
    ---
    Commit 1234e2dda authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix Plottable definition
    
    On Mac OS the build directory in the Node package conflicts with BUILD.
    
    PiperOrigin-RevId: 157899970
    
    ---
    Commit bb7a8d8e7 authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Don't use the _output_shape attribute in the op_level_cost_estimator since
    there is no guaranty that it will be present or accurate.
    
    PiperOrigin-RevId: 157898989
    
    ---
    Commit 6f4204c3d authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix TensorBoard SHA256 in cmake
    
    PiperOrigin-RevId: 157897958
    
    ---
    Commit c9d2f432b authored by Justine Tunney<jart@google.com>
    Committed by Justine Tunney<jart@google.com>:
    Fix TensorBoard SHA256 in cmake
    
    ---
    Commit 1c70fb686 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add training test for multi classes (n>2) linear classifier.
    
    PiperOrigin-RevId: 157896002
    
    ---
    Commit 675d36be0 authored by Yao Zhang<yaozhang@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add fused batch norm to tf.layers.
    
    PiperOrigin-RevId: 157893874
    
    ---
    Commit f37d0ea47 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Internal change -- first draft docs
    
    PiperOrigin-RevId: 157891937
    
    ---
    Commit 9b8f6113b authored by Zongheng Yang<zongheng@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    tensor_bundle: fix that the read path forgets to cache file handles.
    
    In a case where a reader is geographically far from the file, this change
    achieves a speedup of end-to-end checkpoint restore by 5.8x.
    
    PiperOrigin-RevId: 157889659
    
    ---
    Commit 0c92dada6 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use inplace Cholesky factorization and solves to speed up and reduce memory usage in matrix_solve_ls.
    Check succes before copying outputs in cholesky_op.
    
    PiperOrigin-RevId: 157887564
    
    ---
    Commit a4caeb2ea authored by William Chargin<wchargin@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Extract the graphs dashboard to a plugin
    
    This completes the great plugin migration!
    
    The graphs plugin is somewhat different from the plugins considered so
    far. First, it exposes two kinds of data: graph data and run metadata.
    We elect to put both sources of data under the domain of the graphs
    plugin for now, because it's not clear that the run metadata would be
    useful for anything else. Second, the graph data really has no use for
    "tags": a run either has an associated graph or it does not. Thus, we
    expose an endpoint /data/plugin/graphs/runs that is different in format
    from the /tags routes exposed by other plugins (it returns just a list
    instead of a run-to-tag mapping).
    
    This change removes a bunch of tests from application_test.py. The tests
    cover the compresion behavior of the graph endpoint, but the graph
    endpoint doesn't have any special logic in the way of compression. Thus,
    the tests are, apparently, testing that werkzeug (or whatever is
    relevant here) provides good compression defaults. This isn't
    necessarily a bad idea, but it shouldn't be coupled to the graph tests.
    
    To get test data that includes run metadata, you can run this script:
    
        https://raw.githubusercontent.com/tensorflow/tensorflow/326942394e69074d50d5889218a24c9371eff259/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py
    
    PiperOrigin-RevId: 157884714
    
    ---
    Commit 05a6a13f7 authored by Gunhan Gulsoy<gunan@google.com>
    Committed by gunan<gunan@google.com>:
    Make sure all writer caches are closed before deleting directories in dnn_test.
    
    ---
    Commit d0e761f8d authored by Gunhan Gulsoy<gunan@google.com>
    Committed by gunan<gunan@google.com>:
    Disable another test that uses matrix_set_diag on windows.
    
    ---
    Commit 8939b8562 authored by Derek Murray<mrry@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [tf.contrib.data] Re-implement IteratorGetNext as an AsyncOpKernel.
    
    This prevents the op from consuming an inter-op thread pool thread
    when blocked, and fixes a potential deadlock when many IteratorGetNext
    ops are blocked. Fixes #10369.
    
    PiperOrigin-RevId: 157878885
    
    ---
    Commit 9e25c68ad authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add loss_only_head to hold additional loss terms for multi_head setup
    
    PiperOrigin-RevId: 157875934
    
    ---
    Commit 7cdcd0cca authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Filter more op types that don't benefit from constant folding.
    
    PiperOrigin-RevId: 157875168
    
    ---
    Commit 366990d92 authored by Kay Zhu<kayzhu@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Fix a subtle issue in copy_insertion due the interaction between copy
    overriding logic and RecordIndicesToColocatingBuffers:
    
    - When building instructions ShapeTree to be copy overriden, it is possible
    that we create a single kCopy for two identical instructions. An example can
    be:
    
        %tuple.19 = tuple(%constant.4, %constant.1793, %constant.1793)
    
    where it is used in a while.init operand, and constant.1793 is read-only within
    the loop and also used by another while loop. The copy overriding pass will then
    create the following (logical, not finalized) tuple:
    
        %tuple.19 = tuple(%constant.4, %copy.5, %copy.5)
    
    - In the subsequent pass RecordAmbiguousOrNonDistinctIndices, to add copies to
    ensure point_to set is distinct, the duplicate %copy.5 are ignored because they
    are not yet finalized, and these indices (1 and 2 in the example) are still
    marked as to-be copied.
    
    Therefore distinctiveness is lost.
    
    This fix applies to the override building stage, to explicitly avoid creating
    shared copies for non-distinct buffers.
    
    PiperOrigin-RevId: 157872231
    
    ---
    Commit f4b8d21b8 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Change function parameters to references to avoid copying, or otherwise move from function parameters when moving reduces the amount of copying.
    
    PiperOrigin-RevId: 157867333
    
    ---
    Commit 3eee61caa authored by Drew Hintz<pushespretn@gmail.com>
    Committed by GitHub<noreply@github.com>:
    fix quotes in example code from ? to "
    ---
    Commit 4905c0eae authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove TODO - the new tolerance is okay to keep.
    
    PiperOrigin-RevId: 157861020
    
    ---
    Commit 55f6b6ff1 authored by David Soergel<soergel@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add explicit SparseTensor support to SignatureDef.
    
    PiperOrigin-RevId: 157860466
    
    ---
    Commit 79099d677 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Removes default thresholds from BinaryLogisticHead and adds predict and evaluate tests for DNNClassifier.
    
    PiperOrigin-RevId: 157856471
    
    ---
    Commit 54595f0f3 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds the training test for LinearClassifier with n_classes=2.
    
    PiperOrigin-RevId: 157855473
    
    ---
    Commit cd6c02985 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add 'streaming_curve_points' metric which returns curve [ROC, PR] approximation at specified number of points.
    
    PiperOrigin-RevId: 157851535
    
    ---
    Commit 0f2db7391 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [TF:XLA] Split union-find implementation in mark_for_compilation_pass.cc into a separate library, make it more generic.
    
    PiperOrigin-RevId: 157850985
    
    ---
    Commit d5421cf58 authored by Justin Lebar<jlebar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add additional concat test.
    
    PiperOrigin-RevId: 157844113
    
    ---
    Commit f661128db authored by Geoffrey Irving<geoffreyi@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unused overloads of SummarizeGraphDef and EqualGraphDef
    
    PiperOrigin-RevId: 157843404
    
    ---
    Commit a56d59a84 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Set flow to a value during TensorArray creation,
    Re-enable tensor_array_ops_test in msan.
    
    PiperOrigin-RevId: 157841785
    
    ---
    Commit edcc5cc13 authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add manual test runner for vz_sorting
    
    PiperOrigin-RevId: 157841098
    
    ---
    Commit 3f6404f20 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Assign a max height of 800px to images in the image dashboard.
    
    The user could always expand to actual dimensions if need be.
    
    PiperOrigin-RevId: 157838046
    
    ---
    Commit c6ea6972a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove debugging LOG(INFO) from previous change.
    
    PiperOrigin-RevId: 157837305
    
    ---
    Commit 07d39f28e authored by freedom" Koan-Sin Tan<koansin.tan@gmail.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    make gcc-5 on Ubuntu 16.04 happy (#10385)
    
    gcc-5 complains of ambiguity and refuses to go when doing something
    like 'bazel build -c opt tensorflow/...'
    ---
    Commit ac66be783 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Minor cleanup: Remove unused BUILD dependencies and unnecessary code.
    
    PiperOrigin-RevId: 157837211
    
    ---
    Commit 4161ccc8e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adjust tolerance on dirichlet_multinomial test.
    
    PiperOrigin-RevId: 157834660
    
    ---
    Commit 43c0f52f1 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix off-by-one error in BoolVector(begin, end) constructor.
    
    PiperOrigin-RevId: 157833086
    
    ---
    Commit 419d437ba authored by Lakshay Garg<lakshay.garg.1996@gmail.com>
    Committed by Lakshay Garg<lakshay.garg.1996@gmail.com>:
    Fixed typo in code comment
    
    ---
    Commit 07710014d authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix device colocation for KMeans in case of multiple parameter servers.
    
    PiperOrigin-RevId: 157795360
    
    ---
    Commit b659bc39f authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Simplify TensorBoard build
    
    - Remove tensorboard_typescript_genrule
    - Remove tensorboard_typescript_bundle
    - Introduce ts_web_library Skylark rule which supports seamless
      TypeScript compilation.
    - Use Closure Compiler in semi-advanced mode to compile JavaScript.
      This is done in a way that preserves <script> tag placement, which
      causes pages to load faster and avoid FOUC, thereby making it a
      better solution than the existing vulcanize.
    
    PiperOrigin-RevId: 157794795
    
    ---
    Commit 0503ce09c authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Wipe out previous shape inference result when importing a grappler item
    Run graph optimizations last: since they can be expensive it's best to filter invalid items first.
    
    PiperOrigin-RevId: 157792834
    
    ---
    Commit 9ae941c4a authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Turn reductions along an empty set of dimensions into identity nodes.
    
    PiperOrigin-RevId: 157792209
    
    ---
    Commit 69075f354 authored by Yangzihao Wang<yangzihao@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add functional support for cudnnConvolutionBiasActivationForward().
    
    PiperOrigin-RevId: 157788425
    
    ---
    Commit 7d7a40309 authored by William Chargin<wchargin@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Extract the distributions dashboard to a plugin
    
    This continues the great plugin migration. The distributions plugin was
    similar to the histograms plugin, but it also purported to allow CSV
    download like the scalars plugin. However, the existing implementation
    of this was flawed, and would always yield a 500 on current prod [1]
    (unless there were actually no data). This indicates that no one is
    actually using it---probably because there isn't a relevant button on
    the frontend, anyway!---so I just removed it.
    
    This also changes most frontend occurrences of "compressedHistograms"
    to "distributions" while we're at it.
    
    [1]: Due to the reference `value.rank_in_bps` in the handler
    `_serve_compressed_histograms`; this field does not exist and throws an
    `AttributeError`.
    
    PiperOrigin-RevId: 157787156
    
    ---
    Commit 23cdf96b8 authored by Brennan Saeta<saeta@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Re-enable session_test.py
    
    A number of CL's have split up session_test.py to be a bit smaller. As a
    result, this CL will re-enable the session_test to see if it remains flaky.
    
    PiperOrigin-RevId: 157786407
    
    ---
    Commit d741d81c5 authored by Dandelion Man?<dandelion@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Expose tf.test.StubOutForTesting in the tf testing api
    
    Also redirect TensorBoard usage to use that endpoint.
    
    This is part of my ongoing effort to have TensorBoard only
    depend on TensorFlow via its public api, so that it can
    be split into a project with a fast external build.
    
    PiperOrigin-RevId: 157784552
    
    ---
    Commit 40411cd5c authored by Dandelion Man?<dandelion@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Refactor projector plugin to only use tf public methods.
    
    Remove all reference to the PluginAsset system, which is deprecated.
    
    Part of an ongoing effort to have TensorBoard only consume the public
    TensorFlow api.
    
    PiperOrigin-RevId: 157784016
    
    ---
    Commit a65a70ea5 authored by Gunhan Gulsoy<gunan@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix pip tests under contrib/text
    
    PiperOrigin-RevId: 157783952
    
    ---
    Commit fb4bc806a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix flakiness in GpuMultiSessionMemoryTest.
    
    PiperOrigin-RevId: 157781368
    
    ---
    Commit f7de292df authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update placeholder nodes' shapes in the GraphDef to reflect manually specified values for incomplete placeholder shapes. Previously, these overrides were only specified in the feed nodes, which improves estimates when using dynamic shapes but not when using static shapes. With this change, static shapes also benefit.
    
    PiperOrigin-RevId: 157780800
    
    ---
    Commit eebd44123 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a frontend method for retrieving numeric alerts from the debugger plugin.
    
    This route responds with a list of alerts (occurrences of bad values) in ascending timestamp order.
    
    PiperOrigin-RevId: 157780270
    
    ---
    Commit 5bc685d7f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] If an op has a single "large" operand, we want to fuse this op into some of its consumers, even if we can't fuse into all of them.
    
    PiperOrigin-RevId: 157779106
    
    ---
    Commit 2ee09b873 authored by Mark Heffernan<meheff@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Various improvements to ShapeTree.
    Add support for holding non-copyable types, operator==, and a
    CopySubtreeFrom method for copying a subtree from one ShapeTree to
    another.
    
    PiperOrigin-RevId: 157777636
    
    ---
    Commit 4f3ae7699 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add beam_search kernels used by BeamSearchDecoder to tensorflow.contrib.
    
    PiperOrigin-RevId: 157775011
    
    ---
    Commit 6b16c33b3 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make audio-related logic use the audio plugin.
    
    Previously, fetching audio and related data from TensorBoard used handlers within application.py. We now remove those handlers in favor of routes offered by the audio plugin. ML Dash is updated as well.
    
    PiperOrigin-RevId: 157774953
    
    ---
    Commit 8032e1f75 authored by Geoffrey Irving<geoffreyi@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make function instantiation use std::vector<NodeDef> instead of GraphDef
    
    It's about to turn into std::vector<NodeInfoPtr>; this change gets us partway there.
    
    RELNOTES: n/a
    PiperOrigin-RevId: 157771141
    
    ---
    Commit 2e44be35d authored by Vinu Rajashekhar<vinuraja@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds a protected DeleteResourceMgr(...) method in Device.
    
    PiperOrigin-RevId: 157770378
    
    ---
    Commit cc346e690 authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Strip the :x suffix when generating control inputs from input names
    
    PiperOrigin-RevId: 157770257
    
    ---
    Commit d6fe47af5 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use tensorflow::StringPiece in literal_util.
    Use template for RepeatedField assignment.
    
    PiperOrigin-RevId: 157765477
    
    ---
    Commit 7866fa01b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    This change significantly reduces time and resources used to load large TensorFlow graphs.
    
    For a real-world large graph (13k nodes, 20k edges), this change:
    
    * reduces all heap allocations by 19%
    * reduces retained (final) heap allocations by 2.2%
    * reduces CPU time by 11.2%
    
    In most TF graphs, the set of unique values set to Node::assigned_device_name() is quite small.  This change adds an interning table to the Graph object, which contains all of the unique values used for Node::set_assigned_device_name(), as well as a look-up table.  This is the main source of the reduction in retained heap memory; nearly all nodes are assigned to just one or two unique devices.
    
    This change removes the "string assigned_device_name_" field from the Node class, and replaces it with "int assigned_device_name_index_".  However, because you need both the index and the name table to get the actual value, the Node::assigned_device_name() accessor needs access to the parent Graph.  This requires adding a "Graph* graph_" field to the Node class.
    
    In the future, if all users of this property are converted to use Graph::assigned_device_name(Node*), then the Node::graph_ field can be deleted, and the space reclaimed.  However, doing so is out of the scope of this CL, and even with this new pointer field, the Node class is smaller than it was before, so this is still a net win.
    
    The placement algorithm in simple_placer.cc is one of the main accessors of the Node::assigned_device_name property.  This CL contains significant changes to simple_placer.cc, which directly take advantage of the fact that the property is an index into a name table, rather than treating it simply as a string.  Many temporary allocations are also removed, which is the main source of the reduction in total heap allocations.
    
    This CL also contains a few changes that remove short-lived allocations in unrelated code, such as the changes in op.cc/h, costmodel.cc, etc.  It is extremely easy in C++ to accidentally allocate memory, especially when implicit conversions and copy constructors allocate memory.
    
    All of the changes in this CL were motivated by empirical measurement, using CPU profiling and heap profiling.
    
    PiperOrigin-RevId: 157762909
    
    ---
    Commit fdffafbc1 authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add QueueDequeueUpTo to the list of dequeue ops
    
    PiperOrigin-RevId: 157760201
    
    ---
    Commit 7ad0d0698 authored by Mustafa Ispir<ispir@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add type error to start_queue_runners if given session is not a `tf.Session`. Due to semver, we suppress the error if a MonitoredSession is provided.
    
    PiperOrigin-RevId: 157748375
    
    ---
    Commit 7106f9fac authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Implemented an initial version of virtual scheduler unit test.
    
    PiperOrigin-RevId: 157746305
    
    ---
    Commit b020db0c6 authored by Andrew Harp<andrewharp@google.com>
    Committed by Andrew Harp<andrewharp@google.com>:
    revert public visibility
    
    ---
    Commit 5b05728c2 authored by Andrew Harp<andrewharp@google.com>
    Committed by Andrew Harp<andrewharp@google.com>:
    visibility workaround 3
    
    ---
    Commit 15a740ebb authored by Mustafa Ispir<ispir@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update and Move DNNLinearCombinedRegressor to estimator/canned.
    
    PiperOrigin-RevId: 157744087
    
    ---
    Commit d29bbeca3 authored by Dandelion Man?<dandelion@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix outdated code ref in TensorBoard README, add link to SO question.
    
    PiperOrigin-RevId: 157743374
    
    ---
    Commit 9fc164225 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix index_table_from_file to allow vocabulary_file be a Tensor
    
    PiperOrigin-RevId: 157740677
    
    ---
    Commit 0aa3e0194 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Internal change
    
    PiperOrigin-RevId: 157740660
    
    ---
    Commit 02ac85399 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Introduce new class Literal to replace protobuf Literal.
    
    This renames the existing Literal message to LiteralProto and introduces a new
    C++ class named Literal to replace it.
    
    The LiteralProto is only used at RPC boundaries, or when protobuf-specific
    functionality is required.  The Literal class offers a 'ToProto' function to
    generate a new LiteralProto message when necessary.
    
    Currently, all the static functions in class LiteralUtil, just forward to their
    counterparts in class Literal.  This will change in a future CL.
    
    Class Literal implements all the buffers as std::vectors.  The only exception
    is preds(), which given the std::vector<bool> representation, makes it unusable
    for the semantics we require (it's not possible to get the address of the
    underlying vector, for instance).
    
    The CL adds a BoolVector class to work around that issue.
    
    In future CLs, the std::vector representation may be changed to something more
    efficient, if needed.
    
    PiperOrigin-RevId: 157739125
    
    ---
    Commit 207203253 authored by gunan<gunan@google.com>
    Committed by GitHub<noreply@github.com>:
    Python 3.6 support on windows. (#10356)
    
    * Python 3.6 support on windows.
    
    * Fix typo in README.md
    
    * Make environment configurable for windows gpu build.
    
    ---
    Commit 2b75a9a6e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 157734029
    
    ---
    Commit f60b6bdcb authored by Mustafa Ispir<ispir@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a warning to documentation of MonitoredSession.
    
    PiperOrigin-RevId: 157728225
    
    ---
    Commit eb10a4c49 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Preallocate vector storage when the ultimate vector size is known in advance
    
    PiperOrigin-RevId: 157724431
    
    ---
    Commit ce32228c4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add release notes for Intel MKL integration.
    
    PiperOrigin-RevId: 157722003
    
    ---
    Commit a23255bc0 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds missing group OP to benchmark
    
    PiperOrigin-RevId: 157716500
    
    ---
    Commit d3e840a6c authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Disable writing of compressed checkpoints.
    
    Snappy compression (and decompression) was enabled after the
    1.1 release (in commit 63b2f999d3f22cfe915b89103faa1b0a1b1b7617).
    This means that checkpoints produced by the 1.2.0 release candidates
    will cause TensorFlow 1.1 (and prior) binaries to crash as they
    CHECK fail when trying to load snappy-compressed tables.
    
    To ease transition, disable writing of compressed checkpoints in
    1.2.0 for now.
    
    Reconsider this in the next release.
    
    PiperOrigin-RevId: 157675189
    
    ---
    Commit 6db400bbc authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Refactoring Python op code generation.
    
    PiperOrigin-RevId: 157675126
    
    ---
    Commit d9620cab8 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add flag to determine whether to do L1 optimizations and inline functions. Default is to do them. In tf_optimizer don't inline or do l1 optimizations.
    
    PiperOrigin-RevId: 157673614
    
    ---
    Commit 25bb504cc authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make a plugin that serves data for the audio dashboard.
    
    Subsequent changes will make TensorBoard use this audio plugin instead of the previous handlers for audio-related data.
    
    PiperOrigin-RevId: 157673132
    
    ---
    Commit 24623653b authored by James Qin<jamesqin@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix graph text format serialization
    
    PiperOrigin-RevId: 157669530
    
    ---
    Commit 3aed1735c authored by Andrew Harp<andrewharp@google.com>
    Committed by Andrew Harp<andrewharp@google.com>:
    visibility workaround 2
    
    ---
    Commit fea90f89d authored by Andrew Harp<andrewharp@google.com>
    Committed by Andrew Harp<andrewharp@google.com>:
    visibility workaround
    
    ---
    Commit 732a6b1ae authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Upgrade TypeScript to v2.3.4
    
    PiperOrigin-RevId: 157667511
    
    ---
    Commit 95d90ab2e authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Fixes Split op (#10322)
    
    * [OpenCL] Fixes Split op
    
      Split should alway go through SYCL device
    
    * [OpenCL] Removes half from registred types
    
    ---
    Commit 963441400 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Extends softmax op to cover double (#10323)
    
    ---
    Commit a702863e8 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Extends tile ops to int16 and int32 (#10328)
    
    * [OpenCL] Extends tile ops to int16 and int32
    
    * [OpenCL] Extends tile_ops to cover bool, uint8, int16, int64
    
    ---
    Commit 75385814f authored by cxx<cxxgtxy@gmail.com>
    Committed by cxx<cxxgtxy@gmail.com>:
    Fix comments error in mnist_replica.py where only one ps is used with two works by default.
    
    ---
    Commit 23364e2c6 authored by Andrew Harp<andrewharp@google.com>
    Committed by Andrew Harp<andrewharp@google.com>:
    buildifier fix
    
    ---
    Commit e5088cb82 authored by Yao Zhang<yaozhang@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix discrepancy between measured and analytical cost graph. Use tf_cuda_library for utils.
    
    PiperOrigin-RevId: 157660745
    
    ---
    Commit 787381ca5 authored by Brennan Saeta<saeta@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Split up session_test.py -> session_clusterspec_prop_test.py
    
    session_test.py has gotten very large. Additionally, recently it has become
    flaky. In order to both (1) improve overall code health, and (2) to facilitate
    root-causing the test flakiness, this CL begins to split apart session_test
    into focused subsets.
    
    I've suffixed the scoping of the session_test in order to preserve filesystem
    sort-order grouping.
    
    PiperOrigin-RevId: 157658981
    
    ---
    Commit b09932d74 authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Added PlaceholderWithDefault to the list of known placeholder types
    Use PartialTensorShape instead of TensorShapes to better handle partially known
    shapes
    
    PiperOrigin-RevId: 157657664
    
    ---
    Commit 0462416f6 authored by Dandelion Man?<dandelion@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add make_ndarray, tensor_proto, and MetaGraphDef to tf api.
    
    Since TensorProtos are part of the TensorFlow API, it makes sense
    to also include the methods that generate and parse them.
    
    Similarly, we write out MetaGraphDef protos in the summary writer,
    so we should provide the proto as well.
    
    This is part of an ongoing effort to have TensorBoard only consume
    TensorFlow methods through the public api.
    
    PiperOrigin-RevId: 157657564
    
    ---
    Commit 458f94c12 authored by Wei Ho<weiho@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Open-source skip-gram ops
    
    PiperOrigin-RevId: 157655970
    
    ---
    Commit faac0331c authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Introduce tensorboard_zip_file build rule
    
    This rule can depend on web_library or tensorboard_html_binary. In
    both cases it will create a .zip file containing all the transitive
    web server paths. This can be used to deploy static assets to web
    servers.
    
    A small change was also made to Vulcanize to support path overriding.
    
    PiperOrigin-RevId: 157655047
    
    ---
    Commit 7ed44f4c9 authored by Brennan Saeta<saeta@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Split up session_test.py -> session_partial_run_test.py
    
    session_test.py has gotten very large. Additionally, recently it has become
    flaky. In order to both (1) improve overall code health, and (2) to facilitate
    root-causing the test flakiness, this CL begins to split apart session_test
    into focused subsets.
    
    I've suffixed the scoping of the session_test in order to preserve filesystem
    sort-order grouping.
    
    PiperOrigin-RevId: 157651813
    
    ---
    Commit 3c7ac46ae authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Teach Executable to do its own profiling (patch 4/4).
    
    This CL removes the xla::Service stub for ExecuteOnStreamWrapper so the users call the xla::Executable version directly, and simplifies the function API to simply accept "arguments" as a parameter (with a templated type) rather than requiring the user to capture it into a lambda around the relevant Executable::ExecuteOnStream method.
    
    PiperOrigin-RevId: 157651740
    
    ---
    Commit 626f95ab9 authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [TF:XLA] Don't enforce that all nodes in an encapsulated subgraph are on the same device.
    Use the assigned device rather than the user-requested device when converting a Graph to a FunctionDef.
    
    PiperOrigin-RevId: 157648977
    
    ---
    Commit 414470329 authored by Jacques Pienaar<jpienaar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Guard stream pool with mutex.
    
    A data race can occur while populating the map.
    
    PiperOrigin-RevId: 157647183
    
    ---
    Commit ccdb30763 authored by Eugene Brevdo<ebrevdo@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Additional colocation options and bugfixes for TensorArray
    
    * colocate_with is now set properly when a TensorArray is passed through a
      while_loop
    * added a new argument, "colocate_with_first_write" (default: True; this is
      the current behavior).  If False, the TensorArray is simply placed on the
      device from the context it's constructed in, and no colocation constraints
      are added.
    
    PiperOrigin-RevId: 157643133
    
    ---
    Commit 03fc7022b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 157642677
    
    ---
    Commit 41b87d6ce authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add a new attribute narrow_range to FakeQuant* operations.  It quantizes into range [1; 255] instead of [0; 255].
    
    PiperOrigin-RevId: 157641054
    
    ---
    Commit c048e2938 authored by Alexandre Passos<apassos@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds support to non-placeholder inputs in _graph_to_function_def.
    
    Specifically, supports input ops with more than one output tensor.
    
    PiperOrigin-RevId: 157640908
    
    ---
    Commit d310de4fa authored by Brennan Saeta<saeta@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Split up session_test.py -> session_list_devices_test.py
    
    session_test.py has gotten very large. Additionally, recently it has become
    flaky. In order to both (1) improve overall code health, and (2) to facilitate
    root-causing the test flakiness, this CL begins to split apart session_test
    into focused subsets.
    
    I've suffixed the scoping of the session_test in order to preserve filesystem
    sort-order grouping.
    
    PiperOrigin-RevId: 157640788
    
    ---
    Commit 8e868cf6a authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unused arguments to call_cpp_shape_fn.
    
    PiperOrigin-RevId: 157640125
    
    ---
    Commit 9ddbf31fe authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use unnamed namespace to effect internal linkage, replace string constructors with array-deducing helper function
    
    PiperOrigin-RevId: 157636308
    
    ---
    Commit 88ffe6276 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Increase cholesky_op_test to medium, bump shard_count 1 more.
    
    PiperOrigin-RevId: 157635774
    
    ---
    Commit bef563dc8 authored by Benjamin Kramer<kramerb@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Don't add constraints for computations we're not currently looking at.
    
    TuplePointsToAnalysis is computed globally per module, so we add all
    unconstrained buffers in that module, even if it's outside of the computation
    we're currently running on. Then we proceed to propagate default layouts to all
    those buffers and then throw the constraints away because they don't affect any
    instruction in the current computation.
    
    PiperOrigin-RevId: 157635564
    
    ---
    Commit a980aead8 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use test_adjusted_name when making the mangled_test_name in
    run_and_gather_logs_lib.py, to avoid duplicate file names when the same test is
    run on multiple GPUs.
    
    PiperOrigin-RevId: 157630193
    
    ---
    Commit 0a84cfd58 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 157629497
    
    ---
    Commit 6882effb8 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make single-parameter constructors explicit
    
    PiperOrigin-RevId: 157628970
    
    ---
    Commit 0b8070253 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Support negative axis for Split op
    
    PiperOrigin-RevId: 157628162
    
    ---
    Commit 289e7bf5b authored by gunan<gunan@google.com>
    Committed by GitHub<noreply@github.com>:
    Fixes and improvements to cmake windows build. (#10354)
    
    * Disable linalg ops tests on windows.
    
    * Do not print the full source code path for logs on windows.
    
    ---
    Commit bc236cfc3 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Passes classification head to LinearClassifier.
    
    PiperOrigin-RevId: 157624020
    
    ---
    Commit cebd7e246 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Shanqing Cai<cais@google.com>:
    [OpenCL] Cleans debug ops (#10334)
    
    * [OpenCL] Cleans debug ops
    
    * Acts on feedback from #10334#discussion_r119452513
    
    * Acts on #10334#discussion_r119459463
    
    ---
    Commit fd6c3c4f1 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixes flaky test in dnn_linear_combined_test.
    
    PiperOrigin-RevId: 157622951
    
    ---
    Commit c9cc388dc authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Avoid CHECKs in BundleReader, propagate errors instead.
    
    Motivation:
    We'd like to evolve the checkpoint format over time (e.g., enable
    different types of compression). Without this change, a TensorFlow
    version that encounters a format that it doesn't understand would CHECK fail
    with an unhelpful error message.
    
    With this, it propagates a clearer error message up, giving the user some
    hints about what could be wrong.
    
    I don't have a unittest for this - I thought about writing a bundle and
    then strategically corrupting the bytes on disk before reading it back,
    but that seems a bit much. The intention of this change is to enable
    graceful reporting of forward compatibility breakages. Ideas for an
    appropriate unittest are appreciated.
    
    PiperOrigin-RevId: 157620358
    
    ---
    Commit ee05b8b69 authored by Wei Ho<weiho@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix to remove TF op usage outside of the initializer fn (due to deferred execution of initializer fn, this prevent issues with graph mismatch).
    
    PiperOrigin-RevId: 157620177
    
    ---
    Commit e8d17ea8c authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Materialize shapes that are known at graph construction time into constants
    that can be folded
    
    PiperOrigin-RevId: 157619380
    
    ---
    Commit dc0427d48 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Directly depend on the used libraries
    
    Do not rely on transitive dependencies.
    
    PiperOrigin-RevId: 157618184
    
    ---
    Commit 964d1a509 authored by Yuan Yu<yuanbyu@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix a bug that an erroneous control edge can be introduced when loops are nested in control dependency context.
    
    PiperOrigin-RevId: 157616919
    
    ---
    Commit 2de94bbb8 authored by Eli Bendersky<eliben@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Add an option to set the "generate HLO graph" regex without a flag.
    
    Pipes the option through xla.proto ExecutionOptions, to HloModuleConfig, which
    can then be accessed throughout the compiler.
    
    PiperOrigin-RevId: 157615458
    
    ---
    Commit d3c0482e6 authored by My name is<raviqqe@gmail.com>
    Committed by gunan<gunan@google.com>:
    Fix a typo in export_output.py (#9975)
    
    ---
    Commit 0c75d9f52 authored by ddurham2<ddurham@davyandbeth.com>
    Committed by gunan<gunan@google.com>:
    Adding lost documentation to tf.abs from the old tf.complex_abs when it learned how to work on complex data. (#9954)
    
    ---
    Commit 84661fa73 authored by Benoit Steiner<bsteiner@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Propagate control dependencies during constant folding
    
    PiperOrigin-RevId: 157610040
    
    ---
    Commit a3520340e authored by gunan<gunan@google.com>
    Committed by GitHub<noreply@github.com>:
    Improve windows bazel python test suite. (#10305)
    
    * Improve windows bazel python test suite.
    
    - Create new tags, no_windows and no_windows_gpu
    - Instead of a separate maintained list, use bazel tags to exclude tests.
    - Tag all the python tests that are known to have issues in windows.
    
    * Also blacklist neon_depthwise_conv_ops_test in windows.
    
    * Only build tests in CPU windows tests.
    
    * Only build tests in GPU windows tests.
    
    * Also disable session_test on windows.
    
    * Only run py tests on windows, and only build tests that are not
    disabled.
    
    ---
    Commit a6f284ca4 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds integration tests for LinearRegressor.
    
    PiperOrigin-RevId: 157604107
    
    ---
    Commit d21bf7d75 authored by Francois Chollet<fchollet@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Backport changes from Github master.
    
    PiperOrigin-RevId: 157603238
    
    ---
    Commit 43bfc138c authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix OSS compilation error in tfprof_main.cc
    
    PiperOrigin-RevId: 157602449
    
    ---
    Commit 904a3d075 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fixing issue with cuda compilation related to missing include (exception is only thrown when running with sandboxing on)
    
    PiperOrigin-RevId: 157602401
    
    ---
    Commit f59203c98 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Shard cholesky_op_test.
    
    PiperOrigin-RevId: 157601172
    
    ---
    Commit 3fdbb5579 authored by Amit Patankar<amitpatankar@google.com>
    Committed by Amit Patankar<amitpatankar@google.com>:
    Merging rc1 back into master.
    
    ---
    Commit be5d98a8b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adds integration tests for DNNClassifier.
    
    PiperOrigin-RevId: 157592010
    
    ---
    Commit a05de6cd2 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Change reporting feature importances in RandomForestEstimator to run at the end of training, instead of part of the inference graph.
    
    PiperOrigin-RevId: 157591575
    
    ---
    Commit e96f1142f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unnecessary casts
    
    PiperOrigin-RevId: 157591439
    
    ---
    Commit 5f8571a6b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix missing namespace comments
    
    PiperOrigin-RevId: 157591364
    
    ---
    Commit eeb0b4067 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 157573997
    
    ---
    Commit 7f9674217 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 157573723
    
    ---
    Commit 473a590c9 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Allow complex valued input for Cholesky decomposition.
    
    PiperOrigin-RevId: 157572536
    
    ---
    Commit 2d1860859 authored by Blake Hechtman<blakehechtman@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix test name in array_elementwise_ops_test.
    
    PiperOrigin-RevId: 157552402
    
    ---
    Commit a7fff05e0 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    tfprof multi-step profiling.
    
    This allows users to fill in RunMetadata across different steps.
    1. It is useful for RL model which runs a subset of graph each step.
    2. It also gets averages of multi-step stats.
    
    PiperOrigin-RevId: 157552388
    
    ---
    Commit fe589d9e7 authored by Luke Iwanski<luke@codeplay.com>
    Committed by Benoit Steiner<benoitsteiner@users.noreply.github.com>:
    [OpenCL] Implementation improvements (#9117)
    
    * OpenCL Improvements
    
    * Registers Scatter and ScatterNd Ops for SYCL
    
    * Registers Stack op for SYCL
    
    * Fixes No sycl buffer found error for debug ops
    
    * Registers MatMul and Transpose Ops to SYCL device for double
    
    * Extends analyzer_cli_test.py test to cover SYCL
    
    * Fixes Transpose Op for double when on SYCL
    
    * Bumps Eigen version to fix double precision issue on SYCL
    
    * Extends SessionDebugTestBase to cover SYCL
    
    * Register SYCL implementations for random ops
    
    * Avoid functions that might not be defined on SYCL device (#51)
    
    * Avoid functions that might not be defined on SYCL device
    
    * Simplify by using Eigen math functions
    
    * OpenCL improvements
    
     - Bumps Eigen Version
     - Refactors Ops registration
     - Introduces workaround for Const Op related to the difference between
       CUDA which uses pointers and OpenCL that uses buffers/accessors
     - Extends memory types to cover DEVICE_SYCL as well
     - Introduces  GetSYCLDevice() method that returns list of supported devices
       with GPU device having the highest priority ( doesn't include blacklisted devices )
     - ::internal::Transpose -> tensorflow::internal::Transpose in order to
       avoid compilation reported error
     - re-introduces fix for bugged string replacement causing a lot of compilation
       warnings -c -> --include
     - Adds sycl_runtime to bazels ARRAY_DEPS
     - Replicates TF_CALL_GPU_PROXY_TYPES for SYCL
    
    * [OpenCL] Fixes an issue caused by switch to aligned allocator for sycl buffer (#53)
    
    * [Build] Use gcc/g++ as a host compiler to avoid #8394 (#54)
    
    * [OpenCL] Fixes Scatter Op
    
    * Fix testSimple and testConst in stack_op_test (#3)
    
    * Fix testSimple and testConst in stack_op_test
    
    * Create a specialisation of DoParallelConcatUpdate for SyclDevice and
    register it
    
    * Guard all code in TENSORFLOW_USE_SYCL
    
    * Do not use sycl device for int32
    
    * Registration of the Sycl version is now looking like the one for the GPU
    
    * Remove added empty line
    
    * Register batch normalization kernels for OpenCL (#61)
    
    * [OpenCL] RandomGamma has no GPU friendly implementation (#57)
    
    * [OpenCL] Compatibility fixes for TensorFlow 1.1.0-rc1
    
    * [OpenCL] Implements BatchMatmul Op for SYCL
    
    * Lowercase the device name when GPU or SYCL returned
    
    * [OpenCL] kernel_estimator_test.py assertEqual-> assertAlmostEqual due to floating point representation on the device
    
    * [Eigen] Version bump
    
    * GPU device name string manipulation is not needed anymore
    
    * [OpenCL] Adds SYCL to device backwards compatibility
    
    * [OpenCL] Extends core_rnn_test.py to run for SYCL device
    
    * [OpenCL] Minor optimizations for build script
    
    * [OpenCL] Enables skip folder list in build script
    
    * [OpenCL] Fixes ApplyAdamOp for Sycl device
    
    * [OpenCL] SYCL device improvements
    
    * [OpenCL] Fixes debug_ops's SEGFAULT for SYCL device
    
    * [Build] Adds hexagon to skipped folders list
    
    * [OpenCL] Removes EnterLameDuckMode from SYCL device and allocator
    
    * [OpenCL] Registers Unique Op for SYCL device
    
    * [OpenCL][Temporary] Disables tests for SYCL target due to features not being implemented yet
    
      Tests affected:
        - tensorflow/contrib/memory_stats/python/kernel_tests/memory_stats_ops_test.py
        - tensorflow/contrib/rnn/python/kernel_tests/core_rnn_test.py
        - tensorflow/python/kernel_tests/conv_ops_test.py
        - tensorflow/python/kernel_tests/depthwise_conv_op_test.py
        - tensorflow/python/kernel_tests/pooling_ops_3d_test.py
        - tensorflow/python/kernel_tests/pooling_ops_test.py
        - tensorflow/python/kernel_tests/scatter_nd_ops_test.py
        - tensorflow/python/training/adam_test.py
        - tensorflow/python/training/localhost_cluster_performance_test.py
        - tensorflow/python/training/training_ops_test.py
    
    * [OpenCL][Temporary] Disables failing tests for SYCL in order to establish regression baseline
    
      Tests affected:
        - tensorflow/python/debug/cli/analyzer_cli_test.py
        - tensorflow/python/debug/lib/session_debug_testlib.py
        - tensorflow/python/debug/lib/stepper_test.py
        - tensorflow/python/kernel_tests/unstack_op_test.py
        - tensorflow/python/ops/image_ops_test.py
    
    * [OpenCL] Take options.config.device_count() into consideration
    
    * [OpenCL] Fixes compilation warning
    
    * [OpenCL] device:SYCL:0 -> sycl:0
    
    * [OpenCL] Removes unwanted flags in building script
    
    Removes flags given to computecpp that enable SIMD instructions
    Removes duplicate flags
    
    * bool -> const bool
    
    * [OpenCL] sycl in test_util.gpu_device_name() -> is_sycl_enabled()
    
    * [OpenCL][Temporary] Disables failing tests for SYCL in order to establish regression baseline
    
      Test affected:
        - tensorflow/contrib/stateless/python/kernel_tests/stateless_random_ops_test.py
    
    * Imports test_util from tensorflow.python.framework
    
    * [OpenCL] Fixes formatting in Python code
    
    * [OpenCL] Extends session_test.py to cover SYCL device
    
    * [OpenCL] Cleans singleton class
    
    * [OpenCL] Keeping CUDA happy
    
    * [OpenCL][Temporary] Disables failing tests for SYCL in order to establish regression baseline
    
      Test affected:
       - tensorflow/contrib/rnn/python/kernel_tests/core_rnn_cell_test.py
       - tensorflow/contrib/seq2seq/python/kernel_tests/beam_search_ops_test.py
    
    * Added support for building with SYCL on ARM.
    
    * Acts on the review feedback from:
     - #9117#discussion_r113608975
     - #9117#discussion_r113609173
    
    * [OpenCL] Fixes scatter_nd_op_test
    
    * Fixes auto-merge mistake
    
    * [OpenCL] struct SyclDevice -> class SyclDevice
    
    * Revert "[OpenCL] struct SyclDevice -> class SyclDevice"
    
    This reverts commit addd43348c374a5379f67bb1e5ad084715722fc2.
    
    * [OpenCL] Reverting refactoring commit.
    
      As requested in the review #9117#issuecomment-298454466
      This change set will be re-introduced in smaller chunks.
    
    * Revert "[OpenCL] device:SYCL:0 -> sycl:0"
    
    This reverts commit cf16e60340b62d16c3764d71b716fe03d35f87a9.
    
    * Revert "[OpenCL] Adds SYCL to device backwards compatibility"
    
    This reverts commit b8401b5164199b7a169be1c1d8dea5001195c390.
    
    * Acts on the feedback from #9117#discussion_r115036905
    
    * control_flow_ops_py_test.py expects device name to be lower cased
    
    * Acts on the feedback from #9117#discussion_r115037222
    
    * Removes debug print
    
    * Removes not needed partial specialisation
    
    * [OpenCL] Registers ScatterNdFunctor for SYCL device
    
    * [OpenCL] Make it compile
    
    * [OpenCL] Follow gpu_device changes
    
    * [OpenCL] Adds cxx_builtin_include_directory for python lib
    
      Fixes bazels missing undeclared inclusions that appeared after
      merge with TensorFlow upstream
    
    * [OpenCL] Fixes Constant Op
    
    * [OpenCL] gXX-4.8 -> gXX
    
    * [OpenCL] Removes -D_GLIBCXX_USE_CXX11_ABI=0 as it breaks default compiler setup for Ubuntu 16.04
    
    * Revert "[OpenCL] kernel_estimator_test.py assertEqual-> assertAlmostEqual due to floating point representation on the device"
    
    This reverts commit 06c50c0a485f40c30a436f02c3fa7794e370c49d.
    
    * [OpenCL] CPU allocator is a singleton we should not delete it
    
    ---
    Commit 7aac2395c authored by Blake Hechtman<blakehechtman@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Merge a copies of copies.
    
    PiperOrigin-RevId: 157549434
    
    ---
    Commit 37d9d5f0e authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add some routines for managing summaries to slim.
    
    PiperOrigin-RevId: 157541902
    
    ---
    Commit d58cd2962 authored by Justine Tunney<jart@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix weblas license mirror URL
    
    PiperOrigin-RevId: 157537115
    
    ---
    Commit 5c13ee13b authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Make images-related logic use the images plugin.
    
    Previously, fetching images and related data from TensorBoard used handlers within application.py. We now remove those handlers in favor of routes offered by the images plugin. ML Dash is updated as well.
    
    PiperOrigin-RevId: 157536471
    
    ---
    Commit 60394a3d1 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Reduce size of the no-winograd tests, but still large enough that
    ShouldIncludeWinogradNonfusedAlgo returns true.
    
    PiperOrigin-RevId: 157535386
    
    ---
    Commit 9501c4104 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Replace protobuf CopyFrom with assignment
    
    PiperOrigin-RevId: 157534272
    
    ---
    Commit 96698f7fd authored by Eugene Brevdo<ebrevdo@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [tf contrib seq2seq] Improve BeamSearchDecoder's ability to handle unknown shapes.
    
    Updated unit tests to contain inputs of unknown shape (at graph build time).
    Found an issue in the gather helper that stops it from properly propagating
    the batch size of the output shape.  This caused problems with tf.while_loop.
    Fixed.
    
    PiperOrigin-RevId: 157533937
    
    ---
    Commit 5c73d0102 authored by Neal Wu<wun@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Batch norm docs fix applied to _fused_batch_norm as well
    
    PiperOrigin-RevId: 157530527
    
    ---
    Commit abd4aa49a authored by Jonathan Hseu<jhseu@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix docs for tf.abs() and tf.pow().
    
    PiperOrigin-RevId: 157528475
    
    ---
    Commit dd5ad6917 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Declarations of operators to support batch norm in xla
    
    PiperOrigin-RevId: 157527596
    
    ---
    Commit bbeaa1307 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix the expand_dim for label and weight for classifier heads.
    
    PiperOrigin-RevId: 157524909
    
    ---
    Commit 346021ab4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Cleanup: Use C++ casts, remove redundant casts, use CHECK_OK
    
    PiperOrigin-RevId: 157522142
    
    ---
    Commit e405b0f6b authored by Francois Chollet<fchollet@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Refactoring of layer name autogeneration, to remove a graph serialization warning.
    
    PiperOrigin-RevId: 157520123
    
    ---
    Commit 5784e1e35 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add HasOutputProperties to check for pruned ops; Return
    device name instead of casting it to a short name (GPU:0/CPU:0); VLOG(2) when printing op device placement since it is a lot of output.
    
    PiperOrigin-RevId: 157519077
    
    ---
    Commit 2994444bf authored by Peter Hawkins<phawkins@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Issue a more user-friendly error message if a variable's initializer is from inside a control-flow scope, such as tf.cond() or tf.while_loop().
    
    Fixes #8604.
    
    PiperOrigin-RevId: 157516279
    
    ---
    Commit da2daf068 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unused using declarations
    
    PiperOrigin-RevId: 157513772
    
    ---
    Commit 8b2e8b566 authored by Derek Murray<derek.murray@gmail.com>
    Committed by gunan<gunan@google.com>:
    Exclude Python test files from CMake PIP package. (#10302)
    
    * Exclude *_test.py files from the CMake-built PIP package.
    
    * Add stray _test.py file to the PIP package.
    
    * Nit. Convert tabs to spaces in tf_python.cmake
    
    ---
    Commit 2249a4ea8 authored by Dan Ringwalt<ringwalt@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix control reaching the end of ProjectiveGenerator.
    
    PiperOrigin-RevId: 157510013
    
    ---
    Commit 040e2e20f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unneeded check for has properties in grappler.
    
    PiperOrigin-RevId: 157507665
    
    ---
    Commit 684006955 authored by Yun Peng<pcloudy@google.com>
    Committed by gunan<gunan@google.com>:
    Windows: Remove session_test from bazel_test_lib.sh (#10274)
    
    It was disabled in 49b17146d2e4f04192d16ed67574142de167f3a1
    ---
    Commit 890a0a407 authored by Gunhan Gulsoy<gunan@google.com>
    Committed by Gunhan Gulsoy<gunan@google.com>:
    Upgrade TF ci build and docker files to use bazel 0.5.0
    
    ---
    Commit 46db634e5 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Only run the no-winograd tests once each.
    Only run the no-winograd tests on GPU; this also fixes
    timeouts in asan and msan.
    
    PiperOrigin-RevId: 157505317
    
    ---
    Commit a6cd4e735 authored by Dandelion Man?<dandelion@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove all TB build references that circumvent TF's public API.
    
    This doesn't actually remove all the code references, lots of code references continue to work despite the BUILD references being removed. I think this is because depending on the public api transitively makes all of TensorFlow's guts available too.
    
    PiperOrigin-RevId: 157502987
    
    ---
    Commit dcc3cdce8 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove redundant get() calls and string conversions
    
    PiperOrigin-RevId: 157497932
    
    ---
    Commit af2b9d875 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix the trace inputs functionality of the graph explorer.
    
    After migrating to d3 v4, the graph can no longer directly index into d3.Selections to obtain elements. Instead, we must use the nodes method of d3.Selection to generate an array of selected elements.
    
    PiperOrigin-RevId: 157493509
    
    ---
    Commit 5cf484584 authored by Jacques Pienaar<jpienaar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Small test that performs A*B+A and A*B+B.
    
    PiperOrigin-RevId: 157492992
    
    ---
    Commit b2355913b authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    remove some invalid entries (#10294)
    
    I noticed that some entries don't exist (anymore).
    This seems to be some kind of a consistency issue.
    
    More specifically:
    `tensorflow/contrib/ios_examples/camera/data`
    `tensorflow/contrib/session_bundle/testdata/saved_model_half_plus_two`
    `tensorflow/contrib/session_bundle/testdata/saved_model_half_plus_two/variables`
    
    This is the continuation of PR #10264
    ---
    Commit 367ec84f8 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add SampleEmbeddingHelper to do sampling at inference time
    
    PiperOrigin-RevId: 157487623
    
    ---
    Commit a3ba225d5 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add BatchMatMul execution cost prediction
    
    PiperOrigin-RevId: 157487507
    
    ---
    Commit 34a29fc3b authored by Eric Liu<ioeric@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [TF:XLA] preserve metadata when replacing HLO instructions.
    
    The motivation is to add metadata for HLO instructions that are created to replace existing HLO instructions during optimizations. The assumption is that the old instruction and the new instruction would perform the same function, and that they would be correlated to the same TF op. This might not always be correct since HLO optimizations can cross TF op boundaries. But still this seems to be better than nothing.
    
    Note that this still doesn't fully resolve missing OpMetadata after HLO optimizations; new instructions might be added without using ReplaceInstruction.
    
    PiperOrigin-RevId: 157484394
    
    ---
    Commit 092a7b6e6 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Disable keras lstm test in tsan.
    
    PiperOrigin-RevId: 157484268
    
    ---
    Commit 7280dafca authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use "empty" member function to test for emptiness
    
    PiperOrigin-RevId: 157483181
    
    ---
    Commit 6c3b15915 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Expands integration tests in dnn_test.
    
    PiperOrigin-RevId: 157476608
    
    ---
    Commit 727193b1f authored by Androbin<robin.richtsfeld@gmail.com>
    Committed by drpngx<drpngx@users.noreply.github.com>:
    add missing import for `signal` package (#10264)
    
    * add missing import for `signal` package
    
    * add missing dependency for `signal` package
    
    * Update tf_python.cmake
    
    ---
    Commit 21461213d authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unused BUILD dependencies
    
    PiperOrigin-RevId: 157473460
    
    ---
    Commit 4788ca2be authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix handling of Infinity/NaN in line chart domain
    
    Test Plan:
      - Use the script listed below to generate data that has enough
        infinities for these values to not be treated as outliers.
      - Load the data into TensorBoard (`--logdir /tmp/infbug`) and look at
        the scalars plot; also look at the console.
      - Before this change, the chart is completely blank, and there is a
        console warning: "QuantitativeScales cannot take NaN or Infinity as
        a domain value. Ignoring."
      - After this change, there is no console output, and the chart appears
        as intended: a reasonable domain is shown, and the infinities just
        shoot off the chart.
    
    Generating script:
    ```py
    import tensorflow as tf
    
    LOGDIR = '/tmp/infbug'
    STEPS = 134
    
    def main():
      x = tf.Variable(3.1415)
      y = x.assign_add(x)
      tf.summary.scalar('y', y)
      summ = tf.summary.merge_all()
    
      sess = tf.Session()
      writer = tf.summary.FileWriter(LOGDIR)
      writer.add_graph(sess.graph)
      sess.run(tf.global_variables_initializer())
      for step in xrange(STEPS):
        writer.add_summary(sess.run(summ), step)
      writer.close()
    
    if __name__ == '__main__':
      main()
    ```
    
    PiperOrigin-RevId: 157472340
    
    ---
    Commit 49476a62c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Remove unused namespace aliases
    
    PiperOrigin-RevId: 157468609
    
    ---
    Commit d83074847 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Use "nullptr" for null pointer values
    
    PiperOrigin-RevId: 157468186
    
    ---
    Commit b73fea6e2 authored by Tim Harley<tharley@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Refactor `tf.Operation.traceback` implementation in to methods of tf.Graph.
    
    Adds an `_extract_frame_info` method to allow derived classes to extend the
    information available in each op traceback, if desired. The default result of
    `tf.Operation.traceback` is unchanged.
    
    Also fixes a poorly scoped `pylint disable=line-too-long`, so adds the necessary
    enable/disable blocks to silence pylint for the offending docstrings.
    
    PiperOrigin-RevId: 157466174
    
    ---
    Commit f7ca8db7d authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [XLA] Improve shape inference error messages for DynamicSlice/DynamicUpdateSlice.
    
    PiperOrigin-RevId: 157461335
    
    ---
    Commit 8c2a079ec authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Adding a slot / accumulator warmstart initializer that overrides the provided partitioner at call time with one passed at construction time.  This is intended to be used for slot Variables (such as accumulators) associated with Optimizers, since these Variables are created in a fashion that relies on replicating the exact shape of the associated primary variables (see slot_creator).
    
    PiperOrigin-RevId: 157453498
    
    ---
    Commit 73d10599f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Default CUDNN_HOME to CUDA_TOOLKIT_TARGET_DIR. The cuDNN distro is most naturally installed in the same directory as the CUDA SDK, so try to find it there if the user doesn't specify any other directory.
    
    PiperOrigin-RevId: 157436253
    
    ---
    Commit eb7cf9331 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 157429266
    
    ---
    Commit 346dcc0a4 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 157429078
    
    ---
    Commit 3d5ede131 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update documentation for sparse_matmul op to reflect gradient calculation.
    
    PiperOrigin-RevId: 157428135
    
    ---
    Commit 822d64f0c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix embedding_lookup() bug where normalization did not work with ids of rank != 1.
    
    PiperOrigin-RevId: 157422220
    
    ---
    Commit 8cad6b824 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Improve the error message for live set memory check.
    
    PiperOrigin-RevId: 157415647
    
    ---
    Commit 34dcd5b49 authored by Eugene Brevdo<ebrevdo@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    [tf contrib seq2seq] Bugfixes to BeamSearchDecoder
    
    Implementation by Cinjon Resnick.  He can't push this since he's traveling.
    I just copied the fix and added some small syntax tweaks to make the unit
    tests pass.  More comprehensive unit tests will come in the near future.
    
    Fixes at least part of #9904.
    
    BeamSearchDecoder:
    1. Fix the bug where we don't pass the next cell state through.
    2. Gather the cell state (and attention if that's a part of the model
       as an AttentionWrapper on the cell) according to the next_beam_ids.
    PiperOrigin-RevId: 157415564
    
    ---
    Commit f7ae1461c authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix oversampling in the GPU version of multinomial due to an error in generating
    gumbel noise.  -log(-log(U)) gives infinity if U draws a hard 0.  Adds a tiny
    offset to U (2e-30) to avoid log(U) = -inf.
    
    The CPU sampling algorithm depends on the order of the logits which is
    undesirable and can also oversample the first logit if it is smaller than the
    smallest random float larger than 0 (~1e-7).  Switching to double precision
    internally mitigates these problems, although it doesn't fix them.  Slowdown
    is ~35% in the worst case.
    
    Also adds various tests that we would like the sampling to pass.
    
    CPU Benchmark before:
    
    32 10000 1 0.060 0.069 0.87
    32 10000 4 0.229 0.074 3.10
    32 10000 32 2.180 0.059 37.09
    32 100000 1 0.430 0.480 0.90
    32 100000 4 2.322 0.449 5.17
    32 100000 32 31.508 0.471 66.96
    128 10000 1 0.168 0.235 0.71
    128 10000 4 0.965 0.246 3.93
    128 10000 32 7.989 0.225 35.51
    128 100000 1 1.681 1.539 1.09
    128 100000 4 9.012 1.57 35.73
    128 100000 32 126.222 1.626 77.60
    
    CPU Benchmark after:
    
    32 10000 1 0.054 0.112 0.48
    32 10000 4 0.206 0.093 2.21
    32 10000 32 1.826 0.091 20.12
    32 100000 1 0.292 0.636 0.46
    32 100000 4 2.086 0.606 3.44
    32 100000 32 28.496 0.633 45.03
    128 10000 1 0.125 0.266 0.47
    128 10000 4 0.759 0.258 2.94
    128 10000 32 7.362 0.254 29.03
    128 100000 1 1.550 2.18 10.71
    128 100000 4 8.712 2.22 23.92
    128 100000 32 122.585 2.213 55.39
    
    PiperOrigin-RevId: 157414849
    
    ---
    Commit 62cf561f1 authored by Jianwei Xie<xiejw@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add numpy_input_fn integration for LinearRegressor and fix the expand_dim for label and weight.
    
    PiperOrigin-RevId: 157405237
    
    ---
    Commit 40c7e0dd7 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Go: Update generated wrapper functions for TensorFlow ops.
    
    PiperOrigin-RevId: 157402364
    
    ---
    Commit 2726c00ce authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 157402063
    
    ---
    Commit e9d2fba8f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix comment describing ignore_longer_outputs_than_inputs.
    
    PiperOrigin-RevId: 157400110
    
    ---
    Commit 5f097217f authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    An initial step of eliminating all implicit broadcast at the HLO level.
    Guard the shape inference for binary ops behind a flag.
    
    PiperOrigin-RevId: 157373647
    
    ---
    Commit e78e5ec8a authored by Yangzihao Wang<yangzihao@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Set winograd nofused flag to be true by default.
    
    Disable winograd nonfused conv for certain input params to avoid a known bug in cuDNNv5 and cuDNNv6.
    
    PiperOrigin-RevId: 157352847
    
    ---
    Commit 3f9b69a50 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    In the CUDA path of depthwise_conv2d, add a fast variant for forward convolution when the input images are smaller than 16x16.
    
    PiperOrigin-RevId: 157347823
    
    ---
    Commit 848123e61 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix incorrect condition to instantiate depthwise_ops introduced in commit 15d9f00fa. The change should have excluded depthwise_conv2d for doubles on windows debug builds, but it excluded it for all windows and all debug builds.
    
    PiperOrigin-RevId: 157345929
    
    ---
    Commit 060d67b34 authored by Taehoon Lee<taehoonlee@snu.ac.kr>
    Committed by Taehoon Lee<taehoonlee@snu.ac.kr>:
    Fix typos
    
    ---
    Commit 409419bcc authored by Mark Daoust<markdaoust@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    add closing code quotes
    
    PiperOrigin-RevId: 157339360
    
    ---
    Commit d20d0a623 authored by Jonathan Hseu<jhseu@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Fix the contrib estimator_test by updating the global step in all the appropriate spots.
    
    PiperOrigin-RevId: 157328239
    
    ---
    Commit d1144d3a9 authored by Juang, Yi-Lin<b02901026@ntu.edu.tw>
    Committed by Juang, Yi-Lin<b02901026@ntu.edu.tw>:
    Fix typos
    
    ---
    Commit fa8bb43b1 authored by lanhin<lanhin1@gmail.com>
    Committed by lanhin<lanhin1@gmail.com>:
    Fixed a comment typo in GraphView:InitializeNode(), executor.cc.
    
    ---
    Commit 9f13ae93f authored by Asim Shankar<ashankar@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Java: Update Maven release to 1.2.0-rc1
    
    PiperOrigin-RevId: 157294719
    
    ---
    Commit c8256769c authored by Gunhan Gulsoy<gunan@google.com>
    Committed by Gunhan Gulsoy<gunan@google.com>:
    Address comments and sanity check failures.
    
    ---
    Commit 344225a60 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Update ops-related pbtxt files.
    
    PiperOrigin-RevId: 157292254
    
    ---
    Commit eb2f6d041 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    VLOG(2) instead of VLOG(1) for detailed op printouts.
    
    PiperOrigin-RevId: 157291238
    
    ---
    Commit b4466279a authored by Shanqing Cai<cais@google.com>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    tfdbg: add runtime shape and dtype info to DebugNumericSummary
    
    PiperOrigin-RevId: 157291215
    
    ---
    Commit 4fb2425f8 authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    Add GraphOptimizer to Grappler item builder to do L1 optimizations and
    inlining.
    
    Op Counts Comparison (BNMT)
    Counts: Profile vs Grappler
    Op: Add, 968 vs 965
    Op: AddN, 2228 vs 2228
    Op: ApplyGradientDescent, 84 vs 84
    Op: BatchMatMul, 998 vs 998
    Op: Identity, 142 vs 105
    Op: MatMul, 63 vs 63
    Op: Mul, 10318 vs 10306
    Op: OneHot, 1 vs 1
    Op: Reshape, 8421 vs 8422
    Op: Select, 488 vs 488
    Op: Shape, 8132 vs 8131
    Op: Sigmoid, 942 vs 942
    Op: Softmax, 19 vs 19
    Op: StridedSlice, 58 vs 74
    Op: Sub, 1398 vs 1394
    Op: Tanh, 333 vs 333
    Op: Tile, 21 vs 21
    Op: Transpose, 39 vs 39
    PiperOrigin-RevId: 157288420
    
    ---
    Commit 8918fa9ef authored by A. Unique TensorFlower<gardener@tensorflow.org>
    Committed by TensorFlower Gardener<gardener@tensorflow.org>:
    BEGIN_PUBLIC
    Automated g4 rollback of changelist 157272843
    
    PiperOrigin-RevId: 158534336

commit 02ac85399d4fb35d5055ecf426632b9446a70041
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Jun 1 11:30:36 2017 -0700

    Introduce new class Literal to replace protobuf Literal.
    
    This renames the existing Literal message to LiteralProto and introduces a new
    C++ class named Literal to replace it.
    
    The LiteralProto is only used at RPC boundaries, or when protobuf-specific
    functionality is required.  The Literal class offers a 'ToProto' function to
    generate a new LiteralProto message when necessary.
    
    Currently, all the static functions in class LiteralUtil, just forward to their
    counterparts in class Literal.  This will change in a future CL.
    
    Class Literal implements all the buffers as std::vectors.  The only exception
    is preds(), which given the std::vector<bool> representation, makes it unusable
    for the semantics we require (it's not possible to get the address of the
    underlying vector, for instance).
    
    The CL adds a BoolVector class to work around that issue.
    
    In future CLs, the std::vector representation may be changed to something more
    efficient, if needed.
    
    PiperOrigin-RevId: 157739125

commit 9c495f9499199ea46fff9028774374fa0c52e018
Author: Brennan Saeta <saeta@google.com>
Date:   Fri May 26 11:04:04 2017 -0700

    Add session.list_devices() API
    
    In order to debug a TensorFlow cluster or check whether devices are available
    in a local session (e.g. GPU drivers are loaded), this change adds a
    `sess.list_devices` API to list all devices within the cluster.
    
    This CL implements the list_devices() feature via extensions to the TensorFlow
    C API, and the corresponding additions to the session.h session class and
    corresponding subclasses for both direct sessions, grpc_sessions,
    tensorflow_serving, and others.
    
    Additionally, in order to accomidate ClusterSpec propagation clusters,
    Master::ListDevices now also includes a session_handle in order to identify
    the appropriate master_session on which it should list the available
    devices. (With ClusterSpec propagation, different sessions can have different
    servers with different device capabilities.)
    
    This CL adds a ListDevices() API to MasterSession. It is most
    efficient to implement this API call there, because the MasterSession
    already has a list of devices.
    
    Additionally, this change upgrades the implementation of
    Master::ListDevices() to delegate to the MasterSession if a session
    handle is specified, and to return an error if no corresponding session
    is found.
    PiperOrigin-RevId: 157239656

commit ac9fee249c52de7abb113372f093a6bb620dee9c
Author: Eugene Brevdo <ebrevdo@google.com>
Date:   Mon May 22 17:06:20 2017 -0700

    C++ implementation of SparseFillEmptyRows
    
    Should be much faster and more efficient than the python version.  Does not
    require sorted indices.  safe_embedding_lookup_sparse should now be faster.
    
    (will take full effect in 3 weeks, on June 10th)
    
    PiperOrigin-RevId: 156806888

commit 18727ef581297437e20d6df4a08b60e8b021f284
Author: Ian Langmore <langmore@google.com>
Date:   Mon May 15 09:43:42 2017 -0700

    .assert_positive_definite and .assert_non_singular default implementations
    added to LinearOperator base class.
    
    Previously I was avoiding these because they are inefficient...however, the
    desire to have a consistent API is overriding this.
    
    PiperOrigin-RevId: 156064641

commit 9e7bf403817a3acd4e8d865b041f37609564076e
Author: drpngx <drpngx@users.noreply.github.com>
Date:   Mon Apr 10 13:55:56 2017 -0700

    Branch 152703253 (#9112)
    
    * Improve py_func error handling.
    
    Automatically translate some python errors into corresponding TF errors at runtime.
    Change: 152156821
    
    * Update interaction with libpng so that we use the public API instead of
    knowledge of the internal libpng data structures.
    Change: 152167754
    
    * TensorBoard plugins now contain their own name/route prefix.
    Change: 152167807
    
    * Passes trainable flag to separable_conv2d biases.
    Change: 152170239
    
    * Saving resource variables with a caching device.
    Change: 152171539
    
    * Drop loss from estimator_spec.eval_metric_ops, as required by core Estimator.
    Change: 152179924
    
    * sample_stats.percentile DOCFIX.
    Change: 152182295
    
    * Added a memory optimizer to grappler.
    Change: 152184170
    
    * Change default behavior of the tf runs selector:
    
    - If there are fewer than 41 runs, enable them all by default
    - If there are 41 runs or more, disable them all by default
    
    This is in response to user complaints that having it enable only the first ten runs by default was confusing, because it was not obvious to users that some runs had been disabled.
    However, it still solves the initial user complaint that having very many runs simultaneously enabled would lag the UI.
    
    I also changed the "toggle all runs" button to try to turn everything off before turning everything on.
    Also, I improved the logic for detecting when the runs selection is back in the default state, so that we can avoid generating long URI strings wherever possible.
    Change: 152188948
    
    * Autogenerated Change: Change TensorBoard TAG to 52
    Change: 152189000
    
    * Remove warning that only happening with config cuda.
    Change: 152189205
    
    * Make resource variable shared name consistent with non-resource variables.
    
    Remove colocation constraint from resource variable cached value with the
    variable itself.
    Change: 152192203
    
    * Add a way to specify the optimization order; refactor and add constant folding to meta optimizer.
    Change: 152193646
    
    * Backport fixes and improvements from external Keras.
    Change: 152198296
    
    * Merge changes from github.
    Change: 152200430
    
    * Go: Update generated wrapper functions for TensorFlow ops.
    Change: 152200754
    
    * Update ops-related pbtxt files.
    Change: 152203174
    
    * Make ImportGraphDef() work with functions.
    
    In addition to modify graph_constructor.cc, this patch adds some other
    functionality to enable importing fucntions:
    * Ability to add FunctionDefLibraries to Graphs and
      FunctionLibraryDefinitions (in addition to existing functions)
    * FunctionDefsEqual() utility function
    Change: 152205258
    
    * Expand contrib test to more than just test targets.
    Change: 152206822
    
    * Preserve graph version during optimization
    Change: 152213262
    
    * Exclude enter and exit nodes from shape refiner's constant folding.
    Change: 152213637
    
    * Allow reshape_mover and algebraic_simplifier to make multiple mutations, by avoiding the short-circuit
    std::any_of.
    Change: 152232810
    
    * Fix dynamic_rnn transpose bug (can input/output non-3d tensors).
    
    Also a few cleanups to RNN code.
    Change: 152267628
    
    * Fix flaky tests
    Change: 152272801
    
    * Add an auto parallelization grappler optimization pass.
    Change: 152276787
    
    * Change json.decode.JSONDecodeError to ValueError.  JSONDecodeError seems to be
    the exception used in the simplejson module, not the json module.
    Change: 152278012
    
    * Internal change.
    Change: 152281471
    
    * [XLA] Force buffer sharing of separate while instructions.
    Change: 152288540
    
    * replica_device_setter should work for resource variables
    Change: 152289915
    
    * Fix ./configure script
    1. Add %workspace% in .bazelrc file when using import statement
    2. Write action_env into bazelrc file for required environment variables for OpenCL support
    Change: 152290700
    
    * Pointing a number of Tensorboard graph visualization-related help links to the new locations for the correspondent API documentation.
    Change: 152293459
    
    * Restore most of pull request #8606
    
    Pull request #8606 added str(Label(...)) for most dependencies in
    tensorflow.bzl, allowing most functions to be used from repositories which
    include TensorFlow as a submodule.  Unfortunately, it broke when pulled into
    Google and was removed in cl/152200430.  This CL restores the change, except
    for two Android-only functions; these were the only problematic bits.
    Change: 152297413
    
    * Removed dead code in Estimator.
    Change: 152297597
    
    * Assert rank is at least equal to new_rank for `_sparse_inner_flatten`.
    Change: 152303319
    
    * Extend quantization ranges to include 0.0f.
    Change: 152304380
    
    * Remove Keras config file saving.
    Change: 152306552
    
    * API backwards compatibility tests.
    Change: 152310869
    
    * [TF:XLA] Add a test for an R3 -> R4 broadcast.
    Change: 152313967
    
    * Fix the problem that no enough placeholders for persistent tensor
    batch delete
    
    The deleter_key is always a device_name, hence there is only one
    of it. Hence, we cannot delete >1 handles at one time.
    
    In the fix, it creates delete placeholder on demand, the max
    number of placeholders is _DEAD_HANDLES_THRESHOLD.
    Change: 152322770
    
    * [XLA] Add several reduction tests.
    Change: 152323510
    
    * Added the memory optimizer to the meta optimizer.
    Change: 152323689
    
    * Started a set of utilities to categorize op types
    Change: 152329057
    
    * Add AudioSpectrogram op to TensorFlow for audio feature generation
    Change: 152332221
    
    * Update ops-related pbtxt files.
    Change: 152332812
    
    * Automated rollback of change 152332221
    Change: 152333917
    
    * Call Py_CLEAR on dead fields during TF_RESOURCE-to-ndarray conversion
    Change: 152338333
    
    * [TF contrib seq2seq] Initial, incomplete implementation of beam search decoder.
    
    **DOES NOT WORK, pushed for collaboration only**
    Change: 152343927
    
    * [XLA] Change HloPassPipeline to disallow Add* calls after Run.
    Change: 152345578
    
    * Automated rollback of change 152332812
    Change: 152349057
    
    * Remove all 64/32 bit compiler warnings from core/ops.
    Change: 152353506
    
    * libtensorflow.so: Don't export private symbols.
    
    With this change, libtensorflow.so will only export
    functions defined in c_api.h. This also results in
    a decreased binary size of libtensorflow.so.
    
    On Linux the decrease was from roughly 150MB to 67MB.
    On OS X it was from roughly 101MB to 82MB.
    
    Also fixes #8923
    Change: 152366053
    
    * Add Elu ops in XLA.
    Change: 152383201
    
    * Fixed test. ('broadcast_dims' has size 1)
    Change: 152383633
    
    * Add more detailed error message for rank assertion in _sparse_inner_flatten.
    Change: 152397909
    
    * tensor_bundle: propagrates errors related to directory creation.
    Change: 152401909
    
    * matrix_adjoint added to contrib/linalg/linear_operator_util
    Change: 152404828
    
    * Add an is_active method to plugins
    
    This method determines whether a plugin is active. A plugin may be inactive if say it lacks data. This new is_active method allows us to add a route to TensorBoard noting which plugins are active. The frontend could then avoid querying routes of inactive plugins.
    Change: 152406232
    
    * Replace a gather op for shapes by a stack op so dilated convolutions can be
    placed on GPU even with strict placing (before the gather went to CPU).
    Change: 152411159
    
    * [TF:XLA] Implement BatchToSpace, BatchToSpaceND, SpaceToBatch, SpaceToBatchND.
    Fix crashes in core implementations of the same operators for zero-sized blocks.
    Change: 152416903
    
    * Estimator saves relative paths in checkpoint.
    Change: 152420211
    
    * Fix layers_test exception regex matching.
    Change: 152422855
    
    * Unhide bijectors. Correct TransformedDistribution docstring.
    Change: 152424418
    
    * Choosing a saner default for min_eval_frequency in the constructor for Experiment for the GCS file system, because the default of 1 causes performance problems.
    Change: 152439984
    
    * Inherit use_resource from scope for partitioned variables.
    Change: 152442103
    
    * Support quantized reshape in hexagon runtime
    Change: 152445539
    
    * tfdbg CLI: add command list_source (ls) + UI fixes and improvements
    
    The new list_source (shorthand: ls) command lists Python source files responsible for constructing the nodes and tensors encountered in the run() call.
    
    It divides the source files into two categories and list them separately.
    1) files that are not part of the TensorFlow Python library, and
    2) files that are a part of it.
    
    The list contains information about how many nodes, tensors and dumps of tensors the files is responsible for. The file paths contain clickable links to the existing print_source/ps command.
    
    The list_source/ls command supports filtering by file-path and node-name regex patterns.
    
    UI fixes:
    * Fixed inconsistent black vs. transparent background color that made the layout look messy on some terminal types. Now using the transparent color for default font color consistently.
    * In the print_source command output, add clickable links to expand source lines and graph elements.
    Change: 152446002
    
    * tfcompile: Be a little more verbose about missing required flags.
    
    Fixes #9014
    Change: 152446338
    
    * Disable failing test cases in pooling_ops_test.
    Change: 152447322
    
    * Register more types for tf.image_crop_and_resize(). Resolves #9020.
    Change: 152448160
    
    * Automated rollback of change 152439984
    Change: 152450929
    
    * Add a route to TensorBoard for fetching plugin names
    
    Specifically, we add a /data/plugins_listing route to the TensorBoard application. This route responds with an object mapping the name of each initialized plugin to whether it is active.
    
    This route could help the frontend avoid issuing requests to inactive plugins.
    
    Ordered the listing of routes within application.py so there is a little more organization.
    
    Refactored the test for application to use a fake plugin.
    Change: 152451390
    
    * Added the ability to retrieve the amount of usable gpu memory
    Change: 152453470
    
    * Allow to set session ConfigProto in RunConfig and use it in Estimator.
    Change: 152454548
    
    * Colocate ResourceVariable reads with their handles.
    Change: 152455939
    
    * tfdbg: update doc for new command list_source/ls
    Change: 152456128
    
    * Make rnn directions slightly easier to follow.
    Change: 152456296
    
    * Internal change
    Change: 152458104
    
    * Adds batch renormalization.
    
    NOTE: if you use renormalization, you might want to use faster moving average updates, i.e. lower `decay` values.
    Change: 152458872
    
    * When using ImportGraphDef with a passed in ShapeRefiner, use the
    producer version of the GraphDef when importing; the ShapeRefiner
    may be initialized with a different graph_def_version, so we need
    to be able to override it.
    
    The test failed without the change to graph_constructor and passes with it.
    The test uses a legacy graph that is supported (reduction shape).
    Change: 152459169
    
    * Allow any iterable for `export_strategies` arg.
    Change: 152461826
    
    * Log steps/sec every 100 steps in MonitoredSession, as before.
    Change: 152465320
    
    * Fixes documentation to note that the in case of ties the identity of the return value of ArgMin and ArgMaxis not guaranteed .
    Change: 152465346
    
    * Automated rollback of change 152465346
    Change: 152465844
    
    * Fix shape inference fn on _ParallelConcatStart.
    Change: 152466076
    
    * Fix getting started guide
    
    Explain numerical differences in loss
    fix one example to print
    Change: 152466119
    
    * Remove superfluous mode argument.
    Change: 152467334
    
    * Add a tool that converts HLO computations to tensorflow GraphDef which can be visualized on Tensorboard.
    
    This CL defines basic tensorflow::OpDef for each HLO instruction/node. More attributes (e.g. shapes, colors) will be added in the future.
    Change: 152477918
    
    * [TF:XLA] Increase shard count of //third_party/tensorflow/compiler/tests:spacetobatch_test to reduce flakiness when built under ASAN.
    Change: 152496244
    
    * Make projector plugin backend read assets saved via the PluginAssets API.
    
    At the same time, keep backwards compatibility with the old way of looking up assets.
    Change: 152504793
    
    * Move MNIST pointers to mirror hosted by the CVDF on Google Cloud.
    Fixes: #9031
    Change: 152504901
    
    * Merge changes from github.
    Change: 152508170
    
    * Update API after changing default step couter frequency before.
    Change: 152517535
    
    * Move a few random op helper functions to header files
    
    1. shape_inference::RandomShape
    2. OpKernel::MakeShape(Tensor, TensorShape*)
    Change: 152522156
    
    * addresses the divide by zero bug
    Change: 152522488
    
    * Clarify doc on tf.assign.
    Change: 152523909
    
    * Sparse adam for resource variables.
    Change: 152525327
    
    * Automated rollback of change 152310869
    Change: 152528732
    
    * Add an env_var tf_sync_on_finish_bool that block until device has finished all queued operations in a step if true.
    Change: 152533676
    
    * Add more node attributes for HloInstruction on Tensorboard e.g. shape and layout etc.
    Change: 152534472
    
    * Add tf.complex64 GPU support to tf.gather.
    
    Also add ldg specializations for std::complex.
    Change: 152537848
    
    * Formatting changes
    Change: 152544842
    
    * Upgrade TensorBoard TypeScript to 2.2.1
    
    See also: #8326
    Change: 152545950
    
    * TEST:  Getting reasonable test sizes on linalg library, removing need for
    sharding.
    Change: 152546409
    
    * Disabling _testSourceUtilModuleReturnsTrue as its causing opensource issues.
    Change: 152548721
    
    * Fix race due to unsafe buffer forwarding in maxpooling second order gradients added in #6664.
    Re-enable previously flaky tests.
    Clean up a few minor things in maxpooling_op_gpu.cu.cc
    Change: 152550050
    
    * LinearOperator:  adjoint_arg kwarg added to all operators.  Now,
    operator.apply(x, adjoint_arg=True) means that the adjoint of 'x' is taken
    before application of operator.  Sometimes this is done more efficiently than
    simply taking adjoint.
    Change: 152560471
    
    * Adds weighted_average_loss metric key.
    Change: 152560999
    
    * Documentation: Fix bug in manual device placement example
    Change: 152563392
    
    * Change for internal compatibility.
    
    * Use std::vector for storage instead of map.
    Do the sorting inplace and return the same vector to avoid any copies.
    On larger streams it is about 50% faster.
    Change: 152576112
    
    * Add tf.add_n GPU support for complex64/complex128.
    
    Also adds a unit test for tf.add_n.
    Change: 152577190
    
    * - Adds support for nested types in tf.case and tf.cond.
    - Adds a "strict" mode which disables silent unpacking of singleton lists.
    - Adds shape inference to tf.case.
    - Adds a lot of unit tests.
    Change: 152581097
    
    * [XLA] Add support for folding transpose into convolution
    Change: 152581336
    
    * Add a smoke test to ensure that the doc generator runs.
    Change: 152592164
    
    * Add tensorboard to the _do_not_descend_map of the PublicAPIVisitor.
    Change: 152592268
    
    * Add auto parallelization to meta optimizer. Enable MetaOptimizer if any one of the optimizers is on.
    Change: 152598517
    
    * Update ops-related pbtxt files.
    Change: 152629248
    
    * Prevent the renorm_weight from being updated too early.
    Change: 152631776
    
    * Automated rollback of change 152528732
    Change: 152652473
    
    * Construct TensorBoard dashboards in a JS list
    
    Previously, adding a dashboard to TensorBoard involved changing logic in several places.
    
    As part of this effort, added constructors to dashboards. Tweaked logic in various dashboards to preserve original behavior. For instance, the graph dashboard can only perform fitting after the dashboard is attached to the DOM.
    Change: 152658532
    
    * Make CheckpointSaverListener visible next to CheckpointSaverHook.
    Change: 152662945
    
    * tfdbg CLI: minor bug fixes
    
    1: The calculation of the scroll command in the scroll bar didn't take into account that the y-coordinate of the scroll block is in the ScrollBar coordinate system, while the mouse click y-coordinate is in the screen coordinate system.
    
    2: The y position of the ScrollBar was off by one.
    
    3: The command box is not re-created after mouse-triggered commands, leading to strange-looking cursor position.
    Change: 152684294
    
    * Remove obsolete use of validate_indices from embedding_ops.py
    
    validate_indices is ignored, so it shouldn't appear in new code.
    Change: 152691948
    
    * Preparation of using GMock matchers in XLA tests.
    Change: 152691970
    
    * Replace RuntimeException by RuntimeError in coordinator documentation.
    Change: 152697758
    
    * Move the TensorBoard debugger plugin to be internal.
    
    This feature is currently not open-source anyway.
    Change: 152700267
    
    * Add a single-machine tf.learn Estimator implementation for the WALS solver.
    Change: 152700915
    
    * Add tf.contrib.training.python_input -- making it easy to feed data into
    TensorFlow from python coroutines.
    Change: 152701623
    
    * Show that QuantizeToFloat consistently introduces a small error. The
    error is equal to
      range_min - round(range_min / range_scale) * range_scale
    Change: 152702015
    
    * Internal Changes
    Change: 152703253
    
    * Remove tensorflow/tensorboard/plugins/debugger, as part of merge resolution.

commit 7ec0b6864d80f86d919bf025299225571a785852
Author: Ian Langmore <langmore@google.com>
Date:   Fri Apr 7 15:54:13 2017 -0800

    LinearOperator:  adjoint_arg kwarg added to all operators.  Now,
    operator.apply(x, adjoint_arg=True) means that the adjoint of 'x' is taken
    before application of operator.  Sometimes this is done more efficiently than
    simply taking adjoint.
    Change: 152560471

commit e05e5ed13e2e3270b61b89a1ba936f5bb1d7ecf2
Author: Jojy George Varghese <jojy.varghese@gmail.com>
Date:   Tue Mar 7 18:55:09 2017 -0800

    Use vector constructor to initialize Array3D. (#8144)
    
    * Use vector constructor to initialize Array3D.
    
    Current constructor of Array3D calls resize to initialize the inner vector of
    Array3D and follows up with a Fill call. This can be replaced with a simple
    call to vector constructor. Vector constructor might also be more efficient.
    
    * Change Array4D ctor implementation.
    
     Using vector ctor.

commit 778de47cdb257cecfac1888f88a653358e0fe021
Author: Jojy George Varghese <jojy.varghese@gmail.com>
Date:   Tue Mar 7 15:21:47 2017 -0800

    Use vector constructor to initialize Array2D. (#8115)
    
    Current constructor of Array2D calls resize to initialize the inner vector of
      Array2D and follows up with a Fill call. This can be replaced with a simple
      call to vector constructor. Vector constructor might also be more efficient.

commit 4891c01b1cadf085a915a3eac5dd1b8d8cdee203
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Feb 21 17:31:57 2017 -0800

    Allow (safe) in-place computation in TensorFlow C++ ops. When at least one input tensor has the same size and type as the output, and the underlying buffer is owned by the op, i.e. when its refcount is 1 at the time the op's Compute method executes, the computation can be performed in place and allocation of the output buffer avoided.
    
    I updated the following ops to perform in-place computation automatically when possible:
       * All standard coefficient-wise unary and binary operators (including with broadcasting) inheriting from base classes in kernels/cwise_ops_common.h.
       * unary and binary operators inheriting from base classes in framework/numeric_op.h. This is mostly old code for the Relu family and associated gradients.
       * All linear algebra ops inheriting from linalg_common.
       * Misc individual files/ops: softmax, select, bias, aggregate ops, batch_norm & fused_batch_norm, adjust_hue, constant, depthwise_conv_grad, fractional_avg_pool, misc. pooling ops, matrix_set_diag, xent & sparse_xent, unique_op.
    Change: 148166936

commit df5d3cd42335e31bccb6c796169d000d73c747d3
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Feb 15 15:03:07 2017 -0800

    Add ops for efficient WALS loss computation in factorization_ops.
    Add unit tests for loss computation.
    Improve performance of unit tests.
    Update trainer to add an option for loss computation (turned off by default).
    Change: 147649717

commit ff6c40e7fca121c869f6308b3a0a9fad7625d527
Author: Jeffrey A. Dean <jeff@google.com>
Date:   Thu Feb 2 08:54:46 2017 -0800

    Add GraphView internal helper class, which is initialized from the Graph
    but holds a more efficient and cache-friendly representation of the data
    structures needed during graph traversal.
    
    Also made faster path for main critical section for "normal" nodes not
    related to control flow.
    
    RELNOTES: Performance tuning of core graph execution module to be more
    cache friendly, and to have faster paths through critical sections.
    
    Co-author=sanjay
    Change: 146368628

commit 6ced72274b62be7f697d045eb2e7efa4f7f521a0
Author: Jeffrey A. Dean <jeff@google.com>
Date:   Thu Feb 2 08:54:46 2017 -0800

    Add GraphView internal helper class, which is initialized from the Graph
    but holds a more efficient and cache-friendly representation of the data
    structures needed during graph traversal.
    
    Also made faster path for main critical section for "normal" nodes not
    related to control flow.
    
    RELNOTES: Performance tuning of core graph execution module to be more
    cache friendly, and to have faster paths through critical sections.
    
    Co-author=sanjay
    Change: 146368628

commit 3699dfecc5942cab5f3c488442c07bccfd64261d
Author: Derek Murray <mrry@google.com>
Date:   Fri Jan 13 13:53:23 2017 -0800

    Provide multiple implementations of RPC responses on the fetch path.
    
    This CL includes wrapper classes for the protocol buffer messages
    `tensorflow::RunStepResponse` and `tensorflow::RunGraphResponse` (to
    complement the corresponding request message wrappers that were added recently).
    
    This change makes the backend code deal with abstract
    `tensorflow::MutableRunStepResponseWrapper` and
    `tensorflow::MutableRunGraphResponseWrapper` interfaces and adds three
    concrete implementations of each interface:
    
    * A mutable in-memory wrapper, which maintains the tensor data in
      `tensorflow::Tensor` objects, and provides the most efficient
      implementation when the client and master (or master and worker) or
      in the same address space.
    
    * A mutable, owned protobuf wrapper, which has a similar implementation
      to today's client code.
    
    * A mutable, non-owned protobuf wrapper, which has a similar
      implementation to today's server code (where the protobuf message is
      owned by the RPC subsystem).
    
    This is another improvement for issue #6256.
    Change: 144481118

commit bf00bcc5fc75d9bd1d61c67cc6c2fc55708a26ea
Author: Derek Murray <mrry@google.com>
Date:   Wed Jan 4 18:34:21 2017 -0800

    Provide multiple implementations of RPC requests on the feed path.
    
    This CL includes wrapper classes for the protocol buffer messages
    `tensorflow::RunStepRequest` and `tensorflow::RunGraphRequest`.
    
    Previously the service arguments were always protocol buffer messages,
    which can entail copying large tensor values into and out of the
    request message. This change makes the backend code deal with abstract
    `tensorflow::RunStepRequestWrapper` and
    `tensorflow::RunGraphRequestWrapper` interfaces and adds three
    concrete implementations of each interface:
    
    * An mutable in-memory wrapper, which maintains the tensor data in
      `tensorflow::Tensor` objects, and provides the most efficient
      implementation when the client and master are in the same address
      space.
    
    * A mutable protobuf wrapper, which has a similar implementation to
      today's client code.
    
    * A const wrapper around a const protobuf, which has a similar
      implementation to today's server code.
    
    This is another improvement for issue #6256.
    Change: 143620823

commit 70d4f7e40648e6f1aa942503bc746a4459ab4925
Author: Ian Langmore <langmore@google.com>
Date:   Wed Jan 4 12:31:23 2017 -0800

    LinearOperatorIdentity added to tensorflow/contrib/linalg/
    
    This operator is initialized with a num_rows and batch_shape parameter.  An alternative would be a single shape parameter.  The reasons for this are:
    
    1.  Compatibility with tf.eye and np.eye
    2.  If "shape" was used, and ndims was not known statically, then operator.apply(x) and operator.solve(x) need to force a broadcast by adding an array of zeros.  This is unnecessarily inefficient.  Having an explicit "batch_shape=None" option allows (in the event that batch_shape is None) operator.apply(x) and operator.solve(x) to simply return x.
    Change: 143582824

commit 8e83097d53c2095782736db55c0d174f1c1b51dd
Author: Derek Murray <mrry@google.com>
Date:   Thu Dec 22 17:34:54 2016 -0800

    Combine NamedTensorProto and NamedTensor into a single proto.
    
    Also use `Tensor::AsProtoTensorContent()` when populating the fetched
    values from a gRPC worker service, as this is more efficient for larger
    values. This should improve #6256 slightly.
    Change: 142813084

commit 694c52608f599992e80b3cd5a5161259c8f18831
Author: Ian Langmore <langmore@google.com>
Date:   Tue Nov 29 13:34:29 2016 -0800

    LinearOperator updates
    
    SquareLinearOperatorDerivedClassTest.  New base test class that reduces boilerplate in derived class tests for square operators.
    
    _tests_to_skip property.  Provides a way to skip tests.
    
    add_to_tensor() method added.  Some operators, like LinearOperatorDiag, can easily be added to a Tensor.
    
    Generic _to_dense() method now implemented in the base class.  This only relies on .apply.  This is somewhat efficient when .apply() is efficient.  This is useful for some cases, such as a Circulant operator where .apply() is fast, but forming the dense matrix would be cumbersome.
    Change: 140519839

commit f9c693b58d1e398783aaf44c8e6d0f1d6d763487
Author: Xingdong Zuo <zuoxingdong@users.noreply.github.com>
Date:   Sun Nov 27 02:04:28 2016 +0100

    [TF-Slim]The latest version has `write_version=saver_pb2.SaverDef.V2` as default set… (#5875)
    
    * The latest TF has `write_version=tf.train.SaverDef.V2` as default setting. And V2 is more efficient than V1.
    
    * Update evaluation.py
    
    * Update evaluation.py

commit b978323e37d1a3651294ba39c1273639e8dcffd6
Author: Benoit Steiner <benoit.steiner.goog@gmail.com>
Date:   Fri Nov 18 18:56:57 2016 -0800

    Added basic tests to cover doubles as well as coefficient wise operations with broadcasts

commit f803bd7c5338d522d262314bd1e0eb4021367c3d
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Nov 10 16:47:07 2016 -0800

    Add a new op split_v that can handle variable size splits.
    
    Aside from being useful on its own, this op also makes the implementation
    of the gradient of concat much more efficient.  Previously a new slice op was
    created in the graph for every input tensor to concat.  This op moves that
    logic inside of one op.  The overhead could be quite significant in cases
    with 100+ input Tensors to concat.
    Change: 138822942

commit a21ca0536674e40c762951004eade753f1e6fb88
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Nov 9 15:30:03 2016 -0800

    More efficient implementation of tf.einsum().
    
    The current implementation generates intermediate tensors whose size grows
    exponentially as a function of the sum of the ranks of the input tensors.  The
    new implementation reduces to batch matrix multiplication, which limits the
    size of intermediate tensors to the size of the intermediate products.  This
    also allows the function to benefit from GPU acceleration.
    
    Benchmarking:
    
    The following einsum() multiplies two 1000 x 1000 matrices:
    
      m = tf.random_normal((1000, 1000))
      x = tf.einsum('ij,jk->ik', m, m)
    
    Timing results on a Z440 workstation:
    
      Before: 7s, 4GB of ram
      After: 0.04s, 80MB of ram
    Change: 138693729

commit 80aec93166dadb2dc30250e1251ab3eb006c2d53
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Oct 27 08:10:54 2016 -0800

    Added new tensorflow::gtl::FlatMap and tensorflow::gtl::FlatSet classes.
    Mostly drop-in replacements for std::unordered_map and std::unordered_set,
    but much faster (does not do an allocation per entry, and represents
    entries in groups of 8 in a flat array, which is much more cache efficient).
    
    Benchmarks not included in this cl show about 3X to 5X performance
    improvements over the std::unordered_{set,map} for many kinds of
    common maps e.g. std::unordered_mapmap<int64, int64> or
    std::unordered_map<string, int64>.
    Change: 137401863

commit de7aaad2c29820ee8e4d9341834add5b32044f0a
Author: Yuan Yu <yuanbyu@google.com>
Date:   Tue Oct 25 20:55:26 2016 -0800

    Introduced per-loop PendingCount and Entry[]. This could significantly reduce the cost of iteration creation if the partition graph is large but the loop body is small, as explained in this old todo:
    
      // TODO(yuanbyu): We current use O(# of nodes in partition) space
      // even for nested iterations where only a small fraction of the
      // nodes are involved.  This is not efficient if the subgraph for
      // the frame is only a small subset of the partition. We should make
      // the vector size to be only the size of the frame subgraph.
    Change: 137238722

commit ffdd64b6aae6f2f70f126fcbfc82ce3bfa8ac79c
Author: Alexey Surkov <surkov@google.com>
Date:   Mon Oct 17 17:18:52 2016 -0800

    Implements DeleteRecursively for GcsFileSystem.
    
    GcsFileSystem uses a more efficient way to list all ojbects in a folder than the default FileSystem implementation.
    Change: 136423015

commit fdea17d8b449cbee9719ab4022a24e2d9918c25f
Author: Eugene Brevdo <ebrevdo@google.com>
Date:   Tue Oct 11 08:42:35 2016 -0800

    Store SparseTensors in a Map inside a container for Queue round-trip.
    
    This is much more efficient than serializing the underlying Tensors to strings
    and dserializing them on the other side.  Instead we pass through the keys
    to the SparseTensors inside the Map.
    
    Methods are kept private for use by queueing wrappers.
    
    Includes benchmarks that show wall-time is almost 50% of the wall-time of using the sparse serialization/deserialization wrappers:
    
    I1003 17:24:34.355306   18675 benchmark.py:77] Benchmark [BenchmarkSparseTensorsMapVsSerialization.benchmark_very_large_2d_float_st_tensor_maps] iters: 2000, wall_time: 0.00260997, cpu_time: -1,throughput: -1
    
    I1003 17:24:42.735983   18675 benchmark.py:77] Benchmark [BenchmarkSparseTensorsMapVsSerialization.benchmark_vey_large_2d_float_st_serialization] iters: 2000, wall_time: 0.00415492, cpu_time: -1,throughput: -1
    
    *** Update:
    
    After updates to sparse_tensor.h's concat code (pushed in a sister PR), there's a speedup in both benchmarks:
    
    I1004 09:39:30.630354   24400 benchmark.py:77] Benchmark [BenchmarkSparseTensorsMapVsSerialization.benchmark_very_large_2d_float_st_tensor_maps] iters: 2000, wall_time: 0.0022105
    
    I1004 09:39:38.125391   24400 benchmark.py:77] Benchmark [BenchmarkSparseTensorsMapVsSerialization.benchmark_very_large_2d_float_st_serialization] iters: 2000, wall_time: 0.00372696
    
    *** Update 2:
    
    After properly placed std::moves in the sparse_tensors_map code, that benchmark is now faster:
    
    Benchmark [BenchmarkSparseTensorsMapVsSerialization.benchmark_very_large_2d_float_st_tensor_maps] iters: 2000, wall_time: 0.00187492
    
    Total speedup is now: 0.00415492 / 0.00187492 = 2.2x
    Change: 135805924

commit 0907fbaf8eae931c4039beec407c4ec191323161
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Oct 10 17:14:21 2016 -0800

    Add initial version of MutableDenseHashTable that is implemented using
    tensors to store the key and value buckets.
    The implementation was inspired by the C++ dense_hash_map from
    https://github.com/sparsehash/sparsehash
    
    Compared to the existing MutableHashTable it can be more memory efficient
    when using small keys and values because it avoids the padding added by the
    memory allocator and does not need to store pointers in the buckets.
    For checkpointed tables it also avoids creating additional tensors during
    checkpoint and restore operations.
    
    Note that this is an initial implementation that is missing many features.
    They will be added shortly.
    
    Also update LookupInterface and existing implementations to pass down
    OpKernelContext, which is required for MutableDenseHashTable.
    Change: 135746705

commit ecdee38a534133ecd7ba18e58527cc4120277190
Author: Jianmin Chen <jmchen@google.com>
Date:   Fri Oct 7 12:53:06 2016 -0800

    Switch to the new accumulators in the sync_rep optimizer (currently called V2). Please note that the gradients from replicas are now averaged instead of summed (as in the old sync_replicas_optimizer) so you need to increase the learning rate according to the number of replicas. This change is introduced to be consistent with how gradients are aggregated (averaged) within a batch in a replica.
    
    As shown in the code change, the switch results in:
    1. much cleaner and simpler code.
    2. much more efficient and reliable staleness check. It is now 100% strict with
    no extra contention to PS servers.
    3. no need for clean_up op so we can get rid of the abort_op which can confuse users.
    4. number of replicas can be changed without complaints from checkpoint as the
    local_step is now just a local variable instead of a global vector variable.
    
    This has been tried with manual restarts of workers (chief or non chief) and
    ps and seems to be quite robust.
    Change: 135513399

commit 52bd6c4297fe684ffbafa93daa92a3802eceffb5
Author: Alexey Surkov <surkov@google.com>
Date:   Mon Sep 26 14:30:51 2016 -0800

    More efficient implementation of GetMatchingPaths for GCS.
    Change: 134335319

commit 8c784c51555db944fbe8c6cff702f7201a5a6463
Author: Alexey Surkov <surkov@google.com>
Date:   Mon Sep 26 14:30:51 2016 -0800

    More efficient implementation of GetMatchingPaths for GCS.
    Change: 134335319

commit b517f16a02965712d8902c5da699898c577a84f9
Author: Zongheng Yang <zongheng@google.com>
Date:   Thu Sep 22 17:48:42 2016 -0800

    Tensor bundle: a module to efficiently serialize/deserialize tensors.
    
    This module will serve as the backing store of TensorFlow's V2 checkpoint
    format.
    
    Highlights of the benefits:
    
    * Peak memory usage of both write and read paths are just 1x (plus a small
      constant-sized buffer).
    * Stores tensors in a non-proto format, circumventing the size limit on proto
      messages.
    * Flexibility: potentially allows by-row loading of a large tensor, or mmap.
    Change: 134028119

commit db15be82ce8c9e9e83168da80543e5a4ad1e0918
Author: Benoit Steiner <bsteiner@google.com>
Date:   Fri Aug 5 14:05:26 2016 -0800

    Encode the size of the input tensors in the placeholders. This helps generate
    a more efficient translation model
    Change: 129487167

commit 6e6d0dcde7c59f306c6f276ed153128fe8bdd8bf
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Jul 19 12:58:51 2016 -0800

    Rewrite to make SDCA optimizer distributed.
    
    Changes are:
    * Modifies the update equation to include the aspect of distribution, supports logistic loss, other loss functions in later CL.
    * Speeds up BM_SDCA_LARGE_SPARSE by 20x-34x.
    * Refactors the interface for the following:
       - Removes the need of the sparse-tensor wrappers
       - Makes the code efficient for dense features.
       - Removes the need of sparse_merge
       - Allows sparse features with weights, or without weights.
    Change: 127870447

commit 873889f856581167bc286cbb6e8ae7b94a506f08
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Jul 11 10:02:48 2016 -0800

    Replace stratified_sample implementation to a strictly more efficient implementaiton. The new implementation discards fewer data, and scales better based on the number of classes. However, it requires knowing the class distribution of the data.
    Change: 127104811

commit 379df09118ddfdbad19375d6d853254312ccf1ae
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Jun 30 14:00:39 2016 -0800

    Fix initialization issues with Variables whose shape contains a zero.
    
    Fixes #2099.
    
    Tries to give Variables the same behavior as non-Variable tensors in
    this respect. Useful for not having to special case e.g. coefficients
    of a feature vector which may sometimes not have any features.
    Change: 126347791

commit 8bf25a491b60d223bba11233de9e62f4b0db17e8
Author: Alexey Surkov <surkov@google.com>
Date:   Thu Jun 23 09:29:53 2016 -0800

    Add a read-ahead cache to the GCS implementation of RandomAccessFile.
    
    In some cases TensorFlow reads the data via RandomAccessFile in really small
    chunks, which doesn't work very efficiently with HTTP requests. Adding a
    read-ahead cache significantly boosts the performance.
    Change: 125691397

commit 13b2a36828269725ea18a86488de4e803c2d0d97
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Wed May 4 11:35:38 2016 -0800

    Make tf.pack use Const op when input is a constant list (or ndarray).
    This facilitates shape induction when shape is produced via tf.pack() and is
    generally more efficient.
    Change: 121508615

commit a4b475cb320a69fd787803095aec1c514375a136
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Mon May 2 20:51:19 2016 -0800

    Fix dynamic_rnn documentation for time_major.
    
    From reading the code, it looks like time_major = True is more efficient.
    Change: 121342997

commit 098f930de4ef044021f3ef1d3cdd6848c23eddb0
Author: Yuan Yu <yuanbyu@google.com>
Date:   Sun Apr 10 08:46:36 2016 -0800

    This is another step to make TensorFlow more interactive and flexible to users. It allows a tensor produced by a run call to stay "in-place" so that a future run call can use it in-place. To achieve this, a run call can now return a handle of a tensor to the client, which can then be fed to a subsequent run call. This feature is complimentary to partial run, though there are some overlaps.
    
    Here are a few properties of the current implementation:
    
    1. Tensors are stored in the state of a session. The tensors are garbage collected if the client doesn't have a reference to the tensor or the session is closed.
    
    2. There is no change to the current session API. We introduced two ops to manage the conversions between tensors and its handles. (There is a third op to garbage collect a tensor.) See the example below.
    
    3. It fits quite well into the current feed-fetch design/implementation. It tries to reuse the graph (and caches) as much as possible so to make things efficient.
    
    Below is a simple example. More examples can be found in sessopn_ops_test.py.
    
    # Return a handle.
    a = tf.constant(10)
    b = tf.constant(5)
    c = tf.mul(a, b)
    h = tf.get_session_handle(c).eval()
    
    # Feed a tensor handle.
    f, x = tf.get_session_tensor(dtypes.int32)
    y = tf.mul(x, 10)
    result = sess.run(y, feed_dict={f: h.handle})
    # result == 500
    Change: 119481352

commit 264bac93b1c8cc367d7519b13a6e1d11177c0227
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Fri Apr 8 16:03:23 2016 -0800

    Use new adjoint attribute for solvers to make gradients more efficient.
    Consolidate linalg shape inference functions.
    Change: 119423897

commit 2d691fe77da04492146b65f6d700bdf843902e4a
Author: Eugene Brevdo <ebrevdo@gmail.com>
Date:   Fri Apr 8 12:34:56 2016 -0800

    Re-enable write-once, read-many semantics for TensorArray.
    
    This implementation is a bit more efficient than the previous one because
    the first write just performs a shallow copy.  Only on an aggregation is
    any new memory allocated.
    
    For read-many semantics, the operations read, pack, and concat must be called
    with parameter  clear_after_read=False.  By default, the flag is set True; this
    means a read will remove the reference to the underlying Tensor in
    the TensorArray to reclaim memory in the runtime.
    Change: 119404140

commit 769aa524c4bdc99c61df468257c95c2f7daeb16e
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Wed Mar 16 13:18:49 2016 -0800

    Because TensorFlow knows the entire graph of your computations, it
    can automatically use the [backpropagation
    algorithm](http://colah.github.io/posts/2015-08-Backprop/)
    to efficiently determine how your variables affect the cost you ask it to
    minimize.
    Change: 117382393

commit 9a8c5ad18c61cb0695d31e2ce969008c82999c7c
Author: Derek Murray <mrry@google.com>
Date:   Tue Mar 15 11:02:22 2016 -0800

    Prevent the feeding of tensors whose values are used to calculate shapes.
    
    This change prevents feeding a tensor if its constant value has been
    accessed. For example, the constant value is often used in shape
    functions (e.g. in `tf.reshape(data, indices)`, `indices` is a tensor,
    but it is often constant) in order to infer more precise shapes. It is
    also used in the `tf.image.*` to generate simpler, more efficient
    graphs. However, doing so is potentially unsafe, because the tensor
    can be fed with a different value, which invalidates the previously
    obtained constant value, and can lead to subtle bugs.
    
    IF THIS BREAKS YOU
    ------------------
    
    Replace the tensor that you are feeding with a `tf.placeholder()` of
    the appropriate dtype and shape.
    Change: 117263031

commit febfec7256026a38bda30c78c2768b32e64c9a2c
Author: Geoffrey Irving <geoffreyi@google.com>
Date:   Tue Mar 8 11:08:47 2016 -0800

    Partially revert a change to zeros_like which broke the Mandelbrot example
    
    An earlier change made zeros_like inefficient if an output dtype different than
    the input was desired: it first made zeros of the input dtype and then cast.
    In addition to not being efficient, this broke the Mandelbrot example, which
    used zeros_like on a complex tensor with float output.  Cast is not allowed from
    complex to float, so this failed.
    
    The new code used tf.zeros and tf.shape if the dtypes differ.
    
    Fixes #1427.
    Change: 116675900

commit 5f7683ea100c06bba66536fd97b5c141f576e0d7
Author: Jianmin Chen <goog.jmchen@gmail.com>
Date:   Fri Mar 4 11:38:02 2016 -0800

    Add native depthwise_convolution op (backward pass) with GPU kernels. Now the
    depthwise convolution op is completed for GPU runs. There are still space for
    more optimization which will be done in the future CLs.
    
    Note that the current CPU kernels with this CL are just reference kernels and
    not optimized at all.
    
    The old depthwise_conv is very inefficient by calling slice() on each
    input channel on input and filters, followed by a conv() on each input channel,
    after which is a concat().
    Change: 116383911

commit 0c69be4e12fe5355cfdba931780a83d666e7abea
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Mon Feb 29 17:31:08 2016 -0800

    Lazily initialize the static coeffcients table once, on first use,
    rather than doing it every time ResizeBicubicOp<...>::Compute is
    invoked.
    
    Also refactored the way this is handled so that there is at most
    one coefficients table in the process, rather than one per TensorFlow
    numeric type.
    
    Saves about 60K bytes / binary.
    Change: 115927183

commit b51ef0cd06e1bfb529b272e55010790ff3ead31f
Author: Jianmin Chen <goog.jmchen@gmail.com>
Date:   Thu Feb 25 16:27:58 2016 -0800

    Rollback of: Add native depthwise_convolution op (forward pass).
    
    The current depthwise_conv is very inefficient by calling slice() on each
    input channel on input and filters, followed by a conv() on each input channel,
    after which is a concat().
    Change: 115617901

commit 90cf3e2eeaddd480cec8a587b8b20b3b562427ef
Author: Vijay Vasudevan <vrv@google.com>
Date:   Thu Feb 25 14:04:25 2016 -0800

    Rollback of: Add native depthwise_convolution op (forward pass).
    
    The current depthwise_conv is very inefficient by calling slice() on each
    input channel on input and filters, followed by a conv() on each input channel,
    after which is a concat().
    Change: 115601904

commit d1245c3c87160760c0fb66f19ddbd7fa48989e81
Merge: d16868d4bd4 590b8c6c398
Author: Illia Polosukhin <ilblackdragon@gmail.com>
Date:   Thu Feb 25 13:14:21 2016 -0800

    Merge pull request #115 from elqursh/optimize-predict
    
    Make estimator.predict memory efficient for large n_classes

commit 590b8c6c3986f0e494657d85848e5ae0d59d550b
Author: Ali Elqursh <elqursh@google.com>
Date:   Thu Feb 25 15:22:20 2016 -0500

    Make estimator.predict more memory efficient
    
    For multi-class classification estimator.predict currently
    appends all the predictions and then argmax the resulting
    tensor. For a large number of classes this tensor is huge
    and may not fit in memory.
    
    This CL changes the behavior to argmax before appending
    the results for the batch.

commit 7b47c8b4a362556bdbd604c81f804d76893737e4
Author: Jianmin Chen <goog.jmchen@gmail.com>
Date:   Thu Feb 25 11:19:05 2016 -0800

    Add native depthwise_convolution op (forward pass).
    
    The current depthwise_conv is very inefficient by calling slice() on each
    input channel on input and filters, followed by a conv() on each input channel,
    after which is a concat().
    Change: 115583330

commit 18cbcdfaabf58588675cf9c29e2b9ec84602422e
Author: Lukasz Kaiser <lukaszkaiser@gmail.com>
Date:   Wed Feb 10 15:39:46 2016 -0800

    Add an optional argument to model_with_buckets to get per-example loss and
    correct the use of conditionals in seq2seq functions to make them efficient.
    Change: 114377454

commit fea55e1e05ffbdaa4ae4369d1c35e689a7dc48a2
Author: Eugene Brevdo <ebrevdo@gmail.com>
Date:   Wed Jan 27 14:54:54 2016 -0800

    Breaking change in TF RNN python api: Return the final state instead of the
    list of states when calling tf.nn.rnn() and tf.nn.state_saving_rnn()
    
    This is necessary for further cleanup of RNN state propagation code
    (currently dynamic RNN calculations when passing sequence_length do not return
    the proper final state, this is a necessary fix to make that fix efficient).
    Change: 113203893

commit 72bf16f937c827050ee71ea4cfa7a25e44303246
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Fri Jan 8 07:26:00 2016 -0800

    Added support for unsorted top-k output, which makes it much more efficient.
    
    This is especially true if k=1 or k is large.
    Change: 111680805

commit 827163e960e8cb86d3dc3f70434c22713ac9f41c
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Thu Jan 7 18:38:32 2016 -0800

    Change to Relu gradient computation, to allow it to use the output of the Relu instead of its input. The reason is memory consumption: usually the Relu is followed by a Conv layer, which will keep its inputs until the backward pass. The Relu, which produces these inputs, can use them for its own backprop, and doing so will be more memory efficient than having Relu keep its inputs around.
    Change: 111649040
commit cf6a88aa42a9490892be6c5e040b7b8cdf3e3ba8
Author: Michael Gester <mgester@google.com>
Date:   Wed Oct 6 10:51:44 2021 -0700

    Rewrite side effect analysis for TF dialect
    
    Some advantages of the new code:
    - support op-based side effects efficiently which saves control dependencies and
      results in better performance; previously they were treated like unknown side
      effects (see new tests which failed before)
    - support value-based side effects for non-resource operands/results
      efficiently; those are not supported by resource alias analysis and are now
      treated like op-based side effects; previously they were treated like unknown
      side effects (see new tests which failed before)
    - simplified/removed many special cases, unified behavior for value-based and
      op-based side effects
    - improved code efficiency and readability
    - added function for querying all resource IDs that an op potentially accesses
    
    PiperOrigin-RevId: 401286244
    Change-Id: I33e782dfd83a1ab7f7876e2e96b90b2b738cd819

diff --git a/tensorflow/compiler/mlir/tensorflow/BUILD b/tensorflow/compiler/mlir/tensorflow/BUILD
index f6547671940..767b48ed416 100644
--- a/tensorflow/compiler/mlir/tensorflow/BUILD
+++ b/tensorflow/compiler/mlir/tensorflow/BUILD
@@ -992,7 +992,7 @@ cc_library(
         "@llvm-project//mlir:Analysis",
         "@llvm-project//mlir:IR",
         "@llvm-project//mlir:Pass",
-        "@llvm-project//mlir:SideEffectInterfaces",
+        "@llvm-project//mlir:StandardOps",
         "@llvm-project//mlir:Support",
     ],
 )
diff --git a/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.cc b/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.cc
index 6fa510f0595..a57713fadf3 100644
--- a/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.cc
+++ b/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.cc
@@ -15,31 +15,24 @@ limitations under the License.
 
 #include "tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h"
 
-#include <cstdint>
-#include <initializer_list>
+#include <bitset>
 
 #include "llvm/ADT/DenseMap.h"
 #include "llvm/ADT/DenseSet.h"
-#include "llvm/ADT/Optional.h"
 #include "llvm/ADT/STLExtras.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/iterator_range.h"
 #include "llvm/Support/Casting.h"
-#include "llvm/Support/Debug.h"
-#include "llvm/Support/ErrorHandling.h"
+#include "mlir/Dialect/StandardOps/IR/Ops.h"  // from @llvm-project
 #include "mlir/IR/Attributes.h"  // from @llvm-project
 #include "mlir/IR/Block.h"  // from @llvm-project
 #include "mlir/IR/Builders.h"  // from @llvm-project
 #include "mlir/IR/BuiltinOps.h"  // from @llvm-project
-#include "mlir/IR/BuiltinTypes.h"  // from @llvm-project
-#include "mlir/IR/Location.h"  // from @llvm-project
 #include "mlir/IR/Operation.h"  // from @llvm-project
-#include "mlir/IR/OperationSupport.h"  // from @llvm-project
 #include "mlir/IR/Value.h"  // from @llvm-project
 #include "mlir/Interfaces/SideEffectInterfaces.h"  // from @llvm-project
 #include "mlir/Support/DebugStringHelper.h"  // from @llvm-project
 #include "mlir/Support/LLVM.h"  // from @llvm-project
-#include "mlir/Support/LogicalResult.h"  // from @llvm-project
 #include "tensorflow/compiler/mlir/tensorflow/ir/tf_device.h"
 #include "tensorflow/compiler/mlir/tensorflow/ir/tf_executor.h"
 #include "tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h"
@@ -50,286 +43,435 @@ namespace mlir {
 namespace TF {
 namespace {
 
-constexpr auto kUnknownResourceId =
+constexpr ResourceId kUnknownResourceId =
     ResourceAliasAnalysis::Info::kUnknownResourceId;
+static_assert(kUnknownResourceId < 0, "kUnknownResourceId must be < 0");
+
+// A collection of Resource IDs. Note that `kUnknownResourceId` is smaller than
+// all other resource IDs which are nonnegative (see check above) so it will
+// always be the first element of a `ResourceIdSet` (we make use of this).
+using ResourceIdSet = llvm::SmallSet<ResourceId, 8>;
+
+// Note that we cannot simply define a `static const llvm::SmallSet` here
+// because of missing `initializer_list` support for `llvm::SmallSet`.
+const ResourceIdSet& UnknownResourceSet() {
+  // clang-format off
+  static auto* id_set = new ResourceIdSet();
+  id_set->insert(kUnknownResourceId);
+  return *id_set;
+}
 
-//===----------------------------------------------------------------------===//
-// SideEffectAnalysisInfo helper functions.
-//===----------------------------------------------------------------------===//
-
-// Returns a set that contains only kUnknownResourceId.
-llvm::SmallDenseSet<int64_t, 8> UnknownResourceSet() {
-  llvm::SmallDenseSet<int64_t, 8> unknown_set;
-  unknown_set.insert(kUnknownResourceId);
-  return unknown_set;
+// Helper function to avoid frequent checks for unknown IDs.
+const ResourceIdSet& GetResourceUniqueIdsOrUnknown(
+    Value value,
+    const ResourceAliasAnalysis::Info& alias_analysis) {
+  if (!getElementTypeOrSelf(value.getType()).isa<TF::ResourceType>() ||
+      alias_analysis.IsUnknownResource(value)) return UnknownResourceSet();
+  return alias_analysis.GetResourceUniqueIds(value);
 }
 
-// Returns all resources that could be accessed by op, or UnknownResourceSet()
-// if we cannot find all of them.
-llvm::SmallDenseSet<int64_t, 8> FindAccessedResources(
-    Operation* op, const ResourceAliasAnalysis::Info& alias_analysis) {
-  VLOG(1) << "Find accessed resources for: " << debugString(*op);
-  llvm::SmallDenseSet<int64_t, 8> resources;
-
-  for (auto operand : filter_resources(op->getOperands())) {
-    if (alias_analysis.IsUnknownResource(operand)) {
-      VLOG(1) << "\tunknown resource in operand";
-      return UnknownResourceSet();
-    }
-    const auto& ids = alias_analysis.GetResourceUniqueIds(operand);
-    resources.insert(ids.begin(), ids.end());
+// Helper class for a collection of side effects for one resource.
+class SideEffects {
+  enum Type {
+    kAlloc = 0,
+    kFree = 1,
+    kRead = 2,
+    kWrite = 3
+  };
+
+ public:
+  bool IsAlloc() const { return effects_.test(kAlloc); }
+  bool IsFree() const { return effects_.test(kFree); }
+  bool IsRead() const { return effects_.test(kRead); }
+  bool IsWrite() const { return effects_.test(kWrite); }
+  bool IsAllocOnly() const { return IsAlloc() && effects_.count() == 1; }
+  bool IsReadOnly() const { return IsRead() && effects_.count() == 1; }
+  ResourceId GetResourceId() const { return resource_id_; }
+
+  void SetAlloc() { effects_.set(kAlloc); }
+  void SetFree() { effects_.set(kFree); }
+  void SetRead() { effects_.set(kRead); }
+  void SetWrite() { effects_.set(kWrite); }
+  void SetUnknownEffect() { effects_.set(); }
+  void SetResourceId(ResourceId resource_id) { resource_id_ = resource_id; }
+  void AddEffects(const SideEffects& other_effects) {
+    effects_ |= other_effects.effects_;
   }
-  for (auto result : filter_resources(op->getResults())) {
-    if (alias_analysis.IsUnknownResource(result)) {
-      VLOG(1) << "\tunknown resource in result";
-      return UnknownResourceSet();
-    }
-    const auto& ids = alias_analysis.GetResourceUniqueIds(result);
-    resources.insert(ids.begin(), ids.end());
+
+ private:
+  std::bitset<4> effects_ = 0;
+  ResourceId resource_id_ = kUnknownResourceId;
+};
+
+// We use `std::map` here because we rely on the order of elements.
+using SideEffectsByResourceId = std::map<ResourceId, SideEffects>;
+
+// We use `std::unordered_map` here for pointer stability reasons.
+// Note: If memory usage ever becomes a bottleneck here (not expected) we could
+// use a Trie-like data structure to avoid storing side effects in both parent
+// op and all its child ops (recursively), at the expense of lookup time.
+using OpSideEffectMap = std::unordered_map<Operation*, SideEffectsByResourceId>;
+
+// Update `side_effects_by_resource_id` with `side_effects`.
+void UpdateSideEffectsByResourceId(
+    const SideEffects& side_effects,
+    SideEffectsByResourceId& side_effects_by_resource_id) {
+  ResourceId id = side_effects.GetResourceId();
+  auto iter = side_effects_by_resource_id.find(id);
+  if (iter == side_effects_by_resource_id.end()) {
+    side_effects_by_resource_id[id] = side_effects;
+  } else {
+    iter->second.AddEffects(side_effects);
   }
-  return resources;
 }
 
-// Helper struct defining what memory effects are present for a resource.
-struct SideEffects {
-  bool alloc = false;
-  bool free = false;
-  bool read = false;
-  bool write = false;
-
-  bool IsAllocOnly() const { return alloc && !free && !read && !write; }
-  bool IsReadOnly() const { return !alloc && !free && read && !write; }
-};
+bool MayHaveSideEffect(Operation* op) {
+  // For op's in the Tensorflow dialect, query the dialect.
+  if (isa_and_nonnull<TF::TensorFlowDialect>(op->getDialect()))
+    return TensorFlowDialect::CanHaveSideEffects(op);
 
-using SideEffectsByValue = llvm::SmallDenseMap<Value, SideEffects>;
+  // Otherwise, conservatively assume that there can be side effects.
+  // TODO(b/196885000) First call `MemoryEffectOpInterface::hasNoEffect` here to
+  // reduce conservatism. This requires several modifications in unit tests that
+  // rely on the current behavior and will be done in a subsequent CL.
+  return true;
+}
 
-bool MustExecute(const MemoryEffects::EffectInstance& effect) {
-  VLOG(1) << "MustExecute check with: "
-          << std::string(effect.getResource()->getName());
-  if (llvm::isa<ResourceEffects::TPUEmbedding>(effect.getResource())) {
-    assert(!effect.getValue() && !effect.getParameters() &&
-           isa<MemoryEffects::Write>(effect.getEffect()));
+bool ShouldUseResourceAliasAnalysis(
+    const MemoryEffects::EffectInstance& effect) {
+  Value value = effect.getValue();
+  if (value && getElementTypeOrSelf(value.getType()).isa<ResourceType>()) {
+    // For value-based effects on resource values we can use resource alias
+    // analysis.
     return true;
   }
+  // For all other effects don't rely on resource alias analysis. Note that
+  // non-resource values are not processed in resource alias analysis.
   return false;
 }
 
-// Collects memory side effects for an operation by value (operands and
-// results).
-void GetSideEffectsByValue(Operation* op,
-                           SideEffectsByValue& side_effects_by_value,
-                           bool& must_execute) {
-  VLOG(1) << "Querying for " << mlir::debugString(*op);
-  auto interface = dyn_cast<MemoryEffectOpInterface>(op);
-  if (!interface) return;
+//===----------------------------------------------------------------------===//
+// SideEffectAnalysisInfo helper functions.
+//===----------------------------------------------------------------------===//
 
-  llvm::SmallVector<MemoryEffects::EffectInstance, 4> effects;
-  interface.getEffects(effects);
+SideEffects GetSideEffectsFromEffectInstance(
+    const MemoryEffects::EffectInstance& effect_instance, Operation* op) {
+  mlir::SideEffects::Effect* effect = effect_instance.getEffect();
+  SideEffects side_effects;
+  if (llvm::isa<ResourceEffects::TPUEmbedding>(effect_instance.getResource())) {
+    // TODO(mgester) This hack can be removed once b/196857154 is fixed.
+    // See definition of `TF_TPUEmbeddingSideEffect` for more details.
+    side_effects.SetRead();
+  } else if (isa<MemoryEffects::Allocate>(effect)) {
+    side_effects.SetAlloc();
+  } else if (isa<MemoryEffects::Free>(effect)) {
+    side_effects.SetFree();
+  } else if (isa<MemoryEffects::Read>(effect)) {
+    side_effects.SetRead();
+  } else if (isa<MemoryEffects::Write>(effect)) {
+    side_effects.SetWrite();
+  } else {
+    LOG(WARNING) << "Unsupported effect for op "
+                 << op->getName().getStringRef().str();
+    side_effects.SetUnknownEffect();
+  }
+  return side_effects;
+}
 
-  for (auto& effect : effects) {
-    if (MustExecute(effect)) {
-      VLOG(1) << "\tmust execute";
-      must_execute = true;
-      continue;
-    }
+// Collects all op-based and value-based side effects for `op` per resource ID.
+SideEffectsByResourceId CollectSideEffectsByResourceId(
+    Operation* op,
+    const SideEffectsByResourceId& op_side_effects,
+    const TF::ResourceAliasAnalysis::Info& alias_analysis) {
+  SideEffectsByResourceId side_effects_by_resource_id;
+  if (!MayHaveSideEffect(op)) return side_effects_by_resource_id;
 
-    // TODO(lyandy): Support effects with no value defined.
-    if (!effect.getValue()) {
-      VLOG(1) << "\teffect with no value, skipping";
-      side_effects_by_value.clear();
-      must_execute = false;
-      return;
+  // Copy op-based side effects.
+  bool found_any_effect = !op_side_effects.empty();
+  side_effects_by_resource_id = op_side_effects;
+
+  // Collect value-based side effects from op interface.
+  llvm::SmallVector<MemoryEffects::EffectInstance, 4> effects;
+  auto interface = dyn_cast<MemoryEffectOpInterface>(op);
+  if (interface) interface.getEffects(effects);
+
+  llvm::SmallDenseSet<Value, 8> processed_values;
+  for (const auto& effect : effects) {
+    Value value = effect.getValue();
+    found_any_effect = true;
+    if (value) processed_values.insert(value);
+
+    // We only collect value-based side effects here for which we can use
+    // resource alias analysis. Other side effects are treated as op-based
+    // side effects.
+    if (!ShouldUseResourceAliasAnalysis(effect)) continue;
+
+    // Add side effects for every potentially accessed resource ID.
+    SideEffects side_effects(GetSideEffectsFromEffectInstance(effect, op));
+    const auto& ids = GetResourceUniqueIdsOrUnknown(value, alias_analysis);
+    for (ResourceId id : ids) {
+      side_effects.SetResourceId(id);
+      UpdateSideEffectsByResourceId(side_effects, side_effects_by_resource_id);
     }
-    auto it = side_effects_by_value.try_emplace(effect.getValue());
-    auto& side_effect = it.first->getSecond();
-    auto* resource_effect = effect.getEffect();
-    if (isa<MemoryEffects::Allocate>(resource_effect)) {
-      VLOG(1) << "\tallocate effect";
-      side_effect.alloc = true;
-    } else if (isa<MemoryEffects::Free>(resource_effect)) {
-      VLOG(1) << "\tfree effect";
-      side_effect.free = true;
-    } else if (isa<MemoryEffects::Read>(resource_effect)) {
-      VLOG(1) << "\tread effect";
-      side_effect.read = true;
-    } else if (isa<MemoryEffects::Write>(resource_effect)) {
-      VLOG(1) << "\twrite effect";
-      side_effect.write = true;
-    } else {
-      VLOG(1) << "\tunknown effect, skipping";
-      side_effects_by_value.clear();
-      must_execute = false;
-      return;
+  }
+
+  auto add_remaining_effects = [&](auto resource_values) {
+    for (Value resource_value : resource_values) {
+      // If we already processed this value before, skip it.
+      if (processed_values.count(resource_value) > 0) continue;
+      found_any_effect = true;
+
+      // Conservatively set unknown effect.
+      SideEffects unknown_effect;
+      unknown_effect.SetUnknownEffect();
+
+      // Add side effects for every potentially accessed resource ID.
+      const auto& ids =
+          GetResourceUniqueIdsOrUnknown(resource_value, alias_analysis);
+      for (ResourceId id : ids) {
+        unknown_effect.SetResourceId(id);
+        UpdateSideEffectsByResourceId(unknown_effect,
+                                      side_effects_by_resource_id);
+      }
     }
+  };
+  // Add value-based side effects for resource values which are not covered by
+  // any side effect so far, for example, resource values being passed to
+  // `tf.While` or `tf.If` ops which are not part of the op definition but
+  // appear in a variadic input list.
+  add_remaining_effects(filter_resources(op->getOperands()));
+  add_remaining_effects(filter_resources(op->getResults()));
+
+  if (!found_any_effect) {
+    // We haven't collected any side effect but the op is potentially
+    // side-effecting (otherwise we would have returned), therefore we have an
+    // unknown side effect for an unknown resource.
+    SideEffects unknown_effect;
+    unknown_effect.SetUnknownEffect();
+    unknown_effect.SetResourceId(kUnknownResourceId);
+    UpdateSideEffectsByResourceId(unknown_effect,
+                                  side_effects_by_resource_id);
   }
+  return side_effects_by_resource_id;
 }
 
-// Checks if a value is a result of `op`.
-bool IsOperationResult(Operation* op, Value value) {
-  return value.getDefiningOp() == op;
-}
+}  // namespace
 
-// Checks if an operation's resource operands are read only. Operation results
-// are ignored.
-bool IsResourceOpReadOnly(Operation* op,
-                          const SideEffectsByValue& side_effects_by_value) {
-  if (side_effects_by_value.empty()) return false;
-
-  for (const auto& value_side_effect : side_effects_by_value) {
-    Value value = value_side_effect.getFirst();
-    if (IsOperationResult(op, value)) continue;
-    const SideEffects& side_effects = value_side_effect.getSecond();
-    if (!side_effects.IsReadOnly()) return false;
-  }
+namespace detail {
 
-  return true;
-}
+// Class for propagating op-based side effects bottom-up and collecting them
+// per op, by resource ID.
+class OpSideEffectCollector {
+ public:
+  // Recursively collects op-based side effects for all ops in module and
+  // populates `op_side_effect_map_`.
+  explicit OpSideEffectCollector(ModuleOp module) {
+    symbol_table_collection_.getSymbolTable(module);
+    for (auto func : module.getOps<FuncOp>()) {
+      CollectOpSideEffects(func);
+    }
+  }
 
-// Checks if an operation's resource results are alloc only and no side effects
-// are present for its operands.
-bool IsResourceOpAllocOnly(Operation* op,
-                           const SideEffectsByValue& side_effects_by_value) {
-  if (side_effects_by_value.empty()) return false;
-
-  for (const auto& value_side_effect : side_effects_by_value) {
-    // Operand with side effect.
-    Value value = value_side_effect.getFirst();
-    if (!IsOperationResult(op, value)) return false;
-    const SideEffects& side_effects = value_side_effect.getSecond();
-    if (!side_effects.IsAllocOnly()) return false;
+  // Returns op-based side effects by resource ID for `op`.
+  const SideEffectsByResourceId& GetSideEffectsForOp(Operation* op) const {
+    auto iter = op_side_effect_map_.find(op);
+    if (iter != op_side_effect_map_.end()) return iter->second;
+    return empty_side_effects_map_;
   }
 
-  return true;
-}
+ private:
+  // Adds op-based side effects from all ops in `region` to `op` side effects.
+  // Collects side effects for ops that weren't visited before.
+  void AddRegionSideEffectsForOp(Region& region, Operation* op) {
+    for (Block& block : region) {
+      for (Operation& curr_op : block) {
+        if (op_side_effect_map_.count(&curr_op) == 0) {
+          CollectOpSideEffects(&curr_op);
+        }
+        for (const auto& entry : op_side_effect_map_[&curr_op]) {
+          UpdateSideEffectsByResourceId(entry.second, op_side_effect_map_[op]);
+        }
+      }
+    }
+  }
 
-// Returns if `op` is a resource declaration.
-bool OpIsDeclaration(Operation* op,
-                     const ResourceAliasAnalysis::Info& alias_analysis) {
-  return llvm::isa<TF::IdentityNOp, TF::IdentityOp>(op) &&
-         !FindAccessedResources(op, alias_analysis).empty();
-}
+  // Collects op-based side effects for `op` in `op_side_effect_map_[op]`.
+  void CollectOpSideEffects(Operation* op) {
+    if (!MayHaveSideEffect(op)) return;
+    // Skip following ops to avoid that every island, graph and function is
+    // classified as unknown side-effecting.
+    if (isa<tf_executor::YieldOp, tf_executor::FetchOp, mlir::ReturnOp>(op))
+      return;
 
-// A vector of resource variable id's with their associated resource value.
-using ResourceIdsByValue =
-    llvm::SmallVector<std::pair<Value, const llvm::SmallSet<int64_t, 8>*>, 4>;
-
-// Collects resource id's by resource value. If operation resource side effects
-// are unknown or a resource is unknown, an empty optional is returned.
-llvm::Optional<ResourceIdsByValue> GetResourceIdsByValue(
-    Operation* op, const ResourceAliasAnalysis::Info& alias_analysis,
-    const SideEffectsByValue& side_effects_by_value) {
-  ResourceIdsByValue resource_ids_by_value;
-  if (side_effects_by_value.empty()) return llvm::None;
-
-  // Returns true iff all side-effect-related values are known to
-  // `alias_analysis`.
-  auto collect_ids = [&](ValueRange values) {
-    for (auto value : values) {
-      // Value is not related to any side-effect, skip.
-      if (side_effects_by_value.count(value) == 0) continue;
-      // Value is not a resource variable, thus not known to `alias_analysis`.
-      if (!getElementTypeOrSelf(value.getType()).isa<TF::ResourceType>())
-        return false;
-      // Value is a resource variable not known to `alias_analysis`.
-      if (alias_analysis.IsUnknownResource(value)) return false;
-      // Value is a resource variable known to `alias_analysis`.
-      const auto& ids = alias_analysis.GetResourceUniqueIds(value);
-      resource_ids_by_value.push_back({value, &ids});
+    // Propagate side effects from regions or functions attached to `op` for
+    // some special cases.
+    if (auto func = llvm::dyn_cast<FuncOp>(op)) {
+      AddRegionSideEffectsForOp(func.getBody(), op);
+    } else if (auto call = llvm::dyn_cast<CallOpInterface>(op)) {
+      FuncOp func_op =
+          dyn_cast<FuncOp>(call.resolveCallable(&symbol_table_collection_));
+      if (func_op) {
+        AddRegionSideEffectsForOp(func_op.getBody(), op);
+      }
+    } else if (auto if_op = llvm::dyn_cast<IfOp>(op)) {
+      AddRegionSideEffectsForOp(if_op.then_function().getBody(), op);
+      AddRegionSideEffectsForOp(if_op.else_function().getBody(), op);
+    } else if (auto while_op = dyn_cast<WhileOp>(op)) {
+      AddRegionSideEffectsForOp(while_op.body_function().getBody(), op);
+    } else if (auto while_region_op = dyn_cast<WhileRegionOp>(op)) {
+      AddRegionSideEffectsForOp(while_region_op.body(), op);
+    } else if (auto case_op = dyn_cast<CaseOp>(op)) {
+      llvm::SmallVector<FuncOp, 4> branch_funcs;
+      case_op.get_branch_functions(branch_funcs);
+      for (auto branch_func : branch_funcs) {
+        AddRegionSideEffectsForOp(branch_func.getBody(), op);
+      }
+    } else if (isa<tf_device::LaunchOp, tf_device::ClusterOp,
+                   tf_executor::IslandOp, tf_executor::GraphOp, IfRegionOp,
+                   CaseRegionOp>(op)) {
+      for (Region& region : op->getRegions()) {
+        AddRegionSideEffectsForOp(region, op);
+      }
+    } else {
+      // Now handle all other ops.
+      auto& side_effects_by_resource_id = op_side_effect_map_[op];
+      llvm::SmallVector<MemoryEffects::EffectInstance, 4> effects;
+      auto interface = dyn_cast<MemoryEffectOpInterface>(op);
+      if (interface) interface.getEffects(effects);
+      if (effects.empty()) {
+        // The op is potentially side-effecting and doesn't have any effect
+        // assigned, treat it as unknown side effect.
+        SideEffects side_effects;
+        side_effects.SetResourceId(kUnknownResourceId);
+        side_effects.SetUnknownEffect();
+        UpdateSideEffectsByResourceId(side_effects,
+                                      side_effects_by_resource_id);
+        // An unknown side effect dominates other side effects so we don't have
+        // to add them and can return here.
+        return;
+      }
+      // Add op-based side effects from regions (if any).
+      for (Region& region : op->getRegions()) {
+        AddRegionSideEffectsForOp(region, op);
+      }
+      // Add op-based side effects for the op itself.
+      for (const auto& effect : effects) {
+        // We handle value-based side effects for which we can use resource
+        // alias analysis at a different place, skip here.
+        if (ShouldUseResourceAliasAnalysis(effect)) continue;
+
+        // Add side effects for op resource ID.
+        SideEffects side_effects(GetSideEffectsFromEffectInstance(effect, op));
+        ResourceId resource_id =
+            GetOpResourceId(effect.getResource()->getResourceID());
+        side_effects.SetResourceId(resource_id);
+        UpdateSideEffectsByResourceId(side_effects,
+                                      side_effects_by_resource_id);
+      }
     }
-    return true;
-  };
-
-  if (collect_ids(op->getOperands()) && collect_ids(op->getResults()))
-    // No unknown side-effect-related values.
-    return resource_ids_by_value;
-  else
-    return llvm::None;
-}
+  }
 
-// Returns true if `op` is known to not have any side effect.
-bool OpIsKnownToHaveNoSideEffect(Operation* op) {
-  // For op's in the Tensorflow dialect, query the dialect.
-  if (isa_and_nonnull<TF::TensorFlowDialect>(op->getDialect()))
-    return !TensorFlowDialect::CanHaveSideEffects(op);
+  // Get internal op resource ID from MLIR type ID.
+  ResourceId GetOpResourceId(TypeID type_id) {
+    auto emplace_result =
+        type_id_to_op_resource_id_.try_emplace(type_id, next_op_resource_id_);
+    // Increment type ID if we have encountered a new resource type.
+    if (emplace_result.second) ++next_op_resource_id_;
+    return emplace_result.first->getSecond();
+  }
 
-  // Otherwise, conservatively assume that there can be side effects.
-  return false;
-}
+  // We use [0, kMaxResourceId] for resource IDs returned by resource alias
+  // analysis and [kMaxResourceId + 1, ...] for resource IDs which we generate
+  // for op-based side effects.
+  const ResourceId kMaxResourceId =
+      std::numeric_limits<ResourceId>::max() / 2;
+  // Next available ID for op-based resources (resources not handled by resource
+  // alias analysis).
+  ResourceId next_op_resource_id_ = kMaxResourceId + 1;
+  // Maps MLIR type IDs for resource types to internal IDs for op-based
+  // resources. Also see comment above.
+  llvm::SmallDenseMap<TypeID, ResourceId> type_id_to_op_resource_id_;
+  // Used for faster callable resolution.
+  SymbolTableCollection symbol_table_collection_;
+  // Collect all op-based side effects here.
+  OpSideEffectMap op_side_effect_map_;
+  const SideEffectsByResourceId empty_side_effects_map_;
+};
 
-}  // namespace
 
-namespace detail {
 //===----------------------------------------------------------------------===//
 // SideEffectAnalysisInfo
 //===----------------------------------------------------------------------===//
 
-void SideEffectAnalysisInfo::TrackAccess(int64_t resource_id, Operation* op,
-                                         bool read_only) {
-  VLOG(1) << "TrackAccess for " << debugString(*op);
+void SideEffectAnalysisInfo::AddPredecessorsForAccess(ResourceId resource_id,
+                                                      Operation* op,
+                                                      bool read_only) {
+  VLOG(2) << "    Adding predecessors for resource " << resource_id;
+  auto it = per_resource_access_info_.find(resource_id);
+  if (it == per_resource_access_info_.end()) return;
+  const auto& access_info = it->getSecond();
+
+  auto& control_predecessors = control_predecessors_[op];
+  bool is_last_write_indirectly_tracked = false;
+  if (!read_only) {
+    // Add reads after last write as predecessors.
+    control_predecessors.insert(access_info.reads_since_last_write.begin(),
+                                access_info.reads_since_last_write.end());
+    // Last write is indirectly tracked by any read predecessor we added.
+    is_last_write_indirectly_tracked =
+        !access_info.reads_since_last_write.empty();
+  }
+  if (access_info.last_write && !is_last_write_indirectly_tracked) {
+    // Add last write as predecessor.
+    control_predecessors.insert(access_info.last_write);
+  }
+}
+
+void SideEffectAnalysisInfo::UpdateAccess(ResourceId resource_id,
+                                          Operation* op,
+                                          bool read_only) {
+  VLOG(2) << "    Updating access for resource " << resource_id;
+  op_to_resource_ids_[op].push_back({resource_id, read_only});
   if (resource_id == kUnknownResourceId) {
-    VLOG(1) << "\tunknown resource id";
     if (read_only) {
       // New unknown read is not tracked by any known resource access.
       for (auto& entry : per_resource_access_info_) {
-        entry.getSecond().tracked_last_unknown_read = false;
+        entry.getSecond().are_last_unknown_reads_tracked = false;
       }
     } else {
       // Unknown write can clear all other tracked information, since it acts
       // like a barrier.
-      VLOG(1) << "\tclearing per resource access info";
       per_resource_access_info_.clear();
     }
   }
-  VLOG(1) << "\tinfo for " << resource_id;
-  auto& info = per_resource_access_info_[resource_id];
+  auto& access_info = per_resource_access_info_[resource_id];
   if (read_only) {
-    info.reads_since_last_write.push_back(op);
-    // Resource read must have carried control dependencies of unknown write. It
-    // can only avoid adding control edges (from uknown accesses) for a later
-    // write, but not for a later read, because this read can be reordered with
-    // a later read.
-    info.tracked_last_unknown_write_for_write = true;
+    access_info.reads_since_last_write.push_back(op);
+    // Last unknown write is indirectly tracked by this read (we have added the
+    // write as a predecessor for `op` before).
+    access_info.is_last_unknown_write_tracked = true;
   } else {
-    // Resource write must have carried control dependencies of unknown access.
-    info.tracked_last_unknown_write_for_read = true;
-    info.tracked_last_unknown_write_for_write = true;
-    info.tracked_last_unknown_read = true;
-    info.last_write = op;
-    info.reads_since_last_write.clear();
+    access_info.last_write = op;
+    access_info.reads_since_last_write.clear();
+    // Last unknown read(s) and write are indirectly tracked by this write (we
+    // have added the read(s) and write as predecessors for `op` before).
+    access_info.are_last_unknown_reads_tracked = true;
+    access_info.is_last_unknown_write_tracked = true;
+    access_info.is_last_unknown_write_tracked_by_write = true;
   }
 }
 
-void SideEffectAnalysisInfo::AddPredecessorsForAccess(int64_t resource_id,
-                                                      Operation* op,
-                                                      bool read_only) {
-  VLOG(1) << "Adding predecessors for resource " << resource_id << " and op "
-          << debugString(*op);
-  auto it = per_resource_access_info_.find(resource_id);
-  if (it == per_resource_access_info_.end()) return;
-  const auto& access_info = it->getSecond();
-  auto& control_predecessors = control_predecessors_[op];
-  bool read_tracked = false;
-  if (!read_only) {
-    control_predecessors.insert(access_info.reads_since_last_write.begin(),
-                                access_info.reads_since_last_write.end());
-    read_tracked = !access_info.reads_since_last_write.empty();
-  }
-  if (access_info.last_write && !read_tracked) {
-    control_predecessors.insert(access_info.last_write);
-  }
-}
-
-void SideEffectAnalysisInfo::AnalyzeFunction(
-    FuncOp func_op, const TF::ResourceAliasAnalysis::Info& alias_analysis) {
+void SideEffectAnalysisInfo::AnalyzeFunction(FuncOp func_op) {
   // AnalyzeRegion() recursively analyzes the function body, and only populates
   // control_predecessors_.
-  AnalyzeRegion(&func_op.getBody(), alias_analysis);
+  AnalyzeRegion(&func_op.getBody());
   // Populate sorted_control_predecessors_ and sorted_control_successors_ based
   // on control_predecessors.
   for (auto& entry : control_predecessors_) {
     auto op = entry.getFirst();
+    auto& predecessors = entry.getSecond();
     auto& sorted_predecessors = sorted_control_predecessors_[op];
-    for (auto predecessor : entry.getSecond()) {
+    for (Operation* predecessor : predecessors) {
       sorted_predecessors.push_back(predecessor);
       sorted_control_successors_[predecessor].push_back(op);
     }
@@ -347,152 +489,111 @@ void SideEffectAnalysisInfo::AnalyzeFunction(
   }
 }
 
-void SideEffectAnalysisInfo::AnalyzeRegion(
-    Region* region, const TF::ResourceAliasAnalysis::Info& alias_analysis) {
-  // This function populates control_predecessors_ by walking through the
-  // region, and tracking resource accesses in per_resource_access_info_.
-
-  // Returns whether an access to `resource` can skip control edges from
-  // previous accesses to unknown resources, due to that earlier accesses to
-  // `resource` already indirectly tracked previous accesses to unknown
-  // resources. `read_only` specifies the type of access of the current op being
-  // considered.
-  auto unknown_access_indirectly_tracked_by_resource = [&](int64_t resource,
-                                                           bool read_only) {
-    VLOG(1) << "\tunknown access indirectly tracked by resource " << resource;
-    auto it = per_resource_access_info_.find(resource);
-    if (it == per_resource_access_info_.end()) {
-      VLOG(1) << "\t\tnot found";
-      return false;
-    }
-    auto unknown_it = per_resource_access_info_.find(kUnknownResourceId);
-    const bool no_unknown_read =
-        unknown_it == per_resource_access_info_.end() ||
-        unknown_it->getSecond().reads_since_last_write.empty();
-    bool ret = read_only ? it->second.tracked_last_unknown_write_for_read
-                         : it->second.tracked_last_unknown_write_for_write &&
-                               (it->second.tracked_last_unknown_read ||
-                                no_unknown_read);
-    VLOG(1) << "\t\tunknown access inderictly tracked by resource: " << ret;
-    return ret;
-  };
-
-  // We explicitly iterates through the regions and blocks, in order to handle
+void SideEffectAnalysisInfo::AnalyzeRegion(Region* region) {
+  // We explicitly iterate through the regions and blocks in order to handle
   // different nested regions separately.
-  for (auto& block : *region) {
-    llvm::SmallPtrSet<Operation*, 8> non_resource_control_predecessors;
-    for (auto& op : block) {
-      for (Region& child : op.getRegions()) {
-        SideEffectAnalysisInfo child_analysis(&child, alias_analysis);
-        // Moves the control_predecessors_ fields in child region to current
-        // region
+  for (Block& block : *region) {
+    for (Operation& op : block) {
+      for (Region& child_region : op.getRegions()) {
+        SideEffectAnalysisInfo child_analysis(
+            &child_region, op_side_effect_collector_, alias_analysis_);
+        // Move data from `child_analysis` to current region.
         for (auto& entry : child_analysis.control_predecessors_)
           control_predecessors_[entry.first] = std::move(entry.second);
+        for (auto& entry : child_analysis.op_to_resource_ids_)
+          op_to_resource_ids_[entry.first] = std::move(entry.second);
       }
+      AnalyzeOp(&op);
+    }
+  }
+}
 
-      // We do not need explicit control edges for declaration ops.
-      if (OpIsDeclaration(&op, alias_analysis)) continue;
-
-      SideEffectsByValue side_effects_by_value;
-      bool must_execute = false;
-      GetSideEffectsByValue(&op, side_effects_by_value, must_execute);
-
-      if (side_effects_by_value.empty() && OpIsKnownToHaveNoSideEffect(&op))
-        continue;
-
-      // TODO(jpienaar): This only currently uses unknown when not per value
-      // resource is used.
-      if (side_effects_by_value.empty() && must_execute) {
-        VLOG(1) << "No resources & must execute: " << debugString(op);
-        // Add unknown resource ops as predecessors of the op that must execute,
-        // to guarantee ordering between unknown resource ops.
-        AddPredecessorsForAccess(kUnknownResourceId, &op, /*read_only=*/false);
-        non_resource_control_predecessors.insert(&op);
-        continue;
-      }
-
-      if (IsResourceOpAllocOnly(&op, side_effects_by_value)) {
-        VLOG(1) << "Resource alloc only: " << debugString(op);
-        continue;
-      }
-
-      auto resource_ids_by_value =
-          GetResourceIdsByValue(&op, alias_analysis, side_effects_by_value);
-      const bool read_only = IsResourceOpReadOnly(&op, side_effects_by_value);
-      bool indirectly_tracked_unknown_access = false;
-      // First add edges from known resources.
-      if (!resource_ids_by_value.hasValue()) {
-        VLOG(1) << "Resource not by value: " << debugString(op);
-        for (auto& entry : per_resource_access_info_) {
-          if (entry.getFirst() == kUnknownResourceId) {
-            VLOG(1) << "\tskipping over unknown resource id";
-            continue;
-          }
-          AddPredecessorsForAccess(entry.getFirst(), &op, read_only);
-          indirectly_tracked_unknown_access |=
-              unknown_access_indirectly_tracked_by_resource(entry.getFirst(),
-                                                            read_only);
-        }
-      } else {
-        // Collect all resource id's and whether their side effect is read only.
-        llvm::SmallDenseMap<int64_t, bool> read_only_by_resource_id;
-        for (const auto& resource_ids : *resource_ids_by_value) {
-          const bool is_result = resource_ids.first.getDefiningOp() == &op;
-          auto value_side_effect =
-              side_effects_by_value.find(resource_ids.first);
-          bool resource_read_only = false;
-          if (value_side_effect != side_effects_by_value.end()) {
-            if (is_result && value_side_effect->getSecond().IsAllocOnly())
-              continue;
-            resource_read_only = value_side_effect->getSecond().IsReadOnly();
-          }
-
-          for (const auto& id : *resource_ids.second) {
-            auto it =
-                read_only_by_resource_id.try_emplace(id, resource_read_only);
-            if (!it.second && !resource_read_only)
-              it.first->getSecond() = resource_read_only;
-          }
-        }
-
-        for (const auto& resource : read_only_by_resource_id) {
-          const auto& resource_id = resource.getFirst();
-          const auto& resource_read_only = resource.getSecond();
-          AddPredecessorsForAccess(resource_id, &op, resource_read_only);
-          indirectly_tracked_unknown_access |=
-              unknown_access_indirectly_tracked_by_resource(resource_id,
-                                                            resource_read_only);
-          // Update access info for known resources.
-          TrackAccess(resource_id, &op, resource_read_only);
-        }
-      }
-
-      // If not indirectly tracked, add edges from the resource.
-      if (!indirectly_tracked_unknown_access) {
-        VLOG(1) << "Not indirectly tracked with unknown access: "
-                << debugString(op);
-        if (auto interface = dyn_cast<MemoryEffectOpInterface>(op)) {
-          llvm::SmallVector<MemoryEffects::EffectInstance, 4> effects;
-          interface.getEffects(effects);
-        }
-        AddPredecessorsForAccess(kUnknownResourceId, &op, read_only);
-      }
-      if (!resource_ids_by_value.hasValue()) {
-        VLOG(1) << "Indirectly tracked with no value: " << debugString(op);
-
-        // Update access info for unknown resource.
-        TrackAccess(kUnknownResourceId, &op, read_only);
-        // Add ops that must execute to unknown resource op predecessors.
-        auto& control_predecessors = control_predecessors_[&op];
-        control_predecessors.insert(non_resource_control_predecessors.begin(),
-                                    non_resource_control_predecessors.end());
-        // Ops that must execute currently tracked are cleared as transitively
-        // unknown resource ops will allow for such ops to be transitively
-        // reachable.
-        non_resource_control_predecessors.clear();
+void SideEffectAnalysisInfo::AnalyzeOp(Operation* op) {
+  VLOG(2) << "Processing op " << mlir::debugString(*op);
+  SideEffectsByResourceId side_effects_by_resource_id =
+      CollectSideEffectsByResourceId(
+          op,
+          op_side_effect_collector_.GetSideEffectsForOp(op),
+          alias_analysis_);
+
+  // Traverse all resource IDs and their associated side effects.
+  bool had_unknown_resource_read = false;
+  for (auto pair : side_effects_by_resource_id) {
+    ResourceId resource_id = pair.first;
+    const SideEffects& side_effects = pair.second;
+    const bool read_only = side_effects.IsReadOnly();
+    VLOG(2) << "  Processing resource ID: " << resource_id
+            << ", read-only effect: " << read_only;
+    // An op that only allocates a resource is expected to return a handle that
+    // is used by all other accesses of the same resource. That means, other ops
+    // that access the same resource already have a data dependency on the
+    // allocating op so it doesn't need any control predecessors or successors.
+    if (side_effects.IsAllocOnly()) continue;
+    // Effect is dominated by previous unknown resource read effect.
+    if (read_only && had_unknown_resource_read) continue;
+
+    // We collect all conflicting IDs except unknown resource ID which is
+    // handled later.
+    ResourceIdSet conflicting_ids;
+    bool is_unknown_access_indirectly_tracked = false;
+    if (resource_id == kUnknownResourceId) {
+      for (auto& entry : per_resource_access_info_) {
+        ResourceId other_id = entry.getFirst();
+        if (other_id != kUnknownResourceId) conflicting_ids.insert(other_id);
       }
+    } else {
+      conflicting_ids.insert(resource_id);
+    }
+    // Add predecessors for conflicting IDs.
+    for (ResourceId id : conflicting_ids) {
+      AddPredecessorsForAccess(id, op, read_only);
+      is_unknown_access_indirectly_tracked |=
+          IsUnknownAccessIndirectlyTrackedByResource(id, read_only);
     }
+    // Add predecessors for unknown resource if not already tracked.
+    if (!is_unknown_access_indirectly_tracked)
+      AddPredecessorsForAccess(kUnknownResourceId, op, read_only);
+    // Update resource access.
+    UpdateAccess(resource_id, op, read_only);
+
+    // If this effect dominates all other possible effects, return here. Note
+    // that if there is any effect for an unknown resource, then we encounter it
+    // in the first iteration since `kUnknownResourceId` is smaller than all
+    // other resource IDs.
+    if (resource_id == kUnknownResourceId && !read_only) return;
+    if (resource_id == kUnknownResourceId && read_only) {
+      had_unknown_resource_read = true;
+    }
+  }
+}
+
+bool SideEffectAnalysisInfo::IsUnknownAccessIndirectlyTrackedByResource(
+    ResourceId resource_id, bool read_only) {
+  auto it = per_resource_access_info_.find(resource_id);
+  if (it == per_resource_access_info_.end()) return false;
+  auto access_info = it->getSecond();
+
+  auto unknown_it = per_resource_access_info_.find(kUnknownResourceId);
+  if (unknown_it == per_resource_access_info_.end()) return true;
+  auto unknown_access_info = unknown_it->getSecond();
+
+  bool no_unknown_read = unknown_access_info.reads_since_last_write.empty();
+  bool no_unknown_write = (unknown_access_info.last_write == nullptr);
+
+  // For the read-only case we only need that the last unknown write is already
+  // tracked by the last `resource` write since we don't have dependencies to
+  // any other read accesses.
+  // Otherwise, we need that the last unknown read(s) and write are already
+  // tracked by any read or write accesses of `resource`.
+  bool is_tracked = read_only ?
+      no_unknown_write || access_info.is_last_unknown_write_tracked_by_write :
+      (no_unknown_write || access_info.is_last_unknown_write_tracked) &&
+      (no_unknown_read || access_info.are_last_unknown_reads_tracked);
+  if (is_tracked) {
+    VLOG(2) << "      Unknown access indirectly tracked by resource "
+            << resource_id;
   }
+  return is_tracked;
 }
 
 llvm::SmallVector<Operation*, 4>
@@ -520,15 +621,25 @@ SideEffectAnalysisInfo::DirectControlSuccessors(
   }
   return result;
 }
+
+const llvm::SmallVector<std::pair<ResourceId, bool>>&
+SideEffectAnalysisInfo::GetResourceIds(Operation* op) const {
+  auto it = op_to_resource_ids_.find(op);
+  if (it == op_to_resource_ids_.end()) return empty_resource_ids_;
+  return it->getSecond();
+}
+
 }  // namespace detail
 
 SideEffectAnalysis::SideEffectAnalysis(ModuleOp module) {
   // Analyze entire module for alias analysis info.
   ResourceAliasAnalysis alias_analysis(module);
+  detail::OpSideEffectCollector op_side_effect_collector(module);
 
   // Analyze all functions.
   for (auto func : module.getOps<FuncOp>())
     this->info_map_.try_emplace(func, func,
+                                op_side_effect_collector,
                                 alias_analysis.GetAnalysisForFunc(func));
 }
 
diff --git a/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h b/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h
index a75f7eb7dee..cb60e57411e 100644
--- a/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h
+++ b/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h
@@ -31,23 +31,47 @@ limitations under the License.
 
 namespace mlir {
 namespace TF {
+using ResourceId = int64_t;
+
 namespace detail {
 
+class OpSideEffectCollector;
+
 // Side effect analysis info for a single function.
+//
+// This class provides an interface for querying control predecessors and
+// successors for ops of the given function. This information is computed from
+// side effects, using resource alias analysis where possible.
+// Remarks:
+// - Control dependencies model execution order constraints for side-effecting
+//   ops. For example, two ops writing to the same resource cannot switch their
+//   order and cannot be executed in parallel.
+// - A control dependency (A,B) means that op A has to be executed before op B.
+//   A is a control predecessor of B, and B is a control successor of A.
+// - The control dependencies provided by side effect analysis are guaranteed to
+//   be sufficient for correct execution but they are not guaranteed to be
+//   minimal (that means, some control dependencies might not be required for
+//   correct execution).
 class SideEffectAnalysisInfo {
  public:
   SideEffectAnalysisInfo() = default;
 
   // Constructs analysis info by analyzing the given function.
-  SideEffectAnalysisInfo(
-      FuncOp func_op, const TF::ResourceAliasAnalysis::Info& alias_analysis) {
-    AnalyzeFunction(func_op, alias_analysis);
+  SideEffectAnalysisInfo(FuncOp func_op,
+                         const OpSideEffectCollector& op_side_effect_collector,
+                         const TF::ResourceAliasAnalysis::Info& alias_analysis)
+      : op_side_effect_collector_(op_side_effect_collector),
+        alias_analysis_(alias_analysis) {
+    AnalyzeFunction(func_op);
   }
 
   // Constructs analysis info by analyzing the given region.
-  SideEffectAnalysisInfo(
-      Region* region, const TF::ResourceAliasAnalysis::Info& alias_analysis) {
-    AnalyzeRegion(region, alias_analysis);
+  SideEffectAnalysisInfo(Region* region,
+                         const OpSideEffectCollector& op_side_effect_collector,
+                         const TF::ResourceAliasAnalysis::Info& alias_analysis)
+      : op_side_effect_collector_(op_side_effect_collector),
+        alias_analysis_(alias_analysis) {
+    AnalyzeRegion(region);
   }
 
   SideEffectAnalysisInfo(SideEffectAnalysisInfo&&) = default;
@@ -66,23 +90,36 @@ class SideEffectAnalysisInfo {
       Operation* op,
       llvm::function_ref<bool(Operation*)> filter = nullptr) const;
 
+  // Returns a vector with IDs of all resources that might be accessed by `op`.
+  // This includes both op-based and value-based resources. The bool indicates
+  // whether a resource is accessed read-only.
+  const llvm::SmallVector<std::pair<ResourceId, bool>>& GetResourceIds(
+      Operation* op) const;
+
  private:
-  // Runs the analysis on `func_op` and populates sorted_control_predecessors_
-  // and sorted_control_successors_.
-  void AnalyzeFunction(FuncOp func_op,
-                       const TF::ResourceAliasAnalysis::Info& alias_analysis);
-
-  // Runs the analysis on `region` and populates control_predecessors_.
-  void AnalyzeRegion(Region* region,
-                     const TF::ResourceAliasAnalysis::Info& alias_analysis);
-
-  // Updates control_predecessors_ for `op` that is being visited, on the given
-  // `resource_id`.
-  void AddPredecessorsForAccess(int64_t resource_id, Operation* op,
+  // Runs the analysis and populates `sorted_control_predecessors_` and
+  // `sorted_control_successors_` for `func_op`. Clears `control_predecessors_`.
+  void AnalyzeFunction(FuncOp func_op);
+
+  // Runs the analysis and populates `control_predecessors_` for `region`.
+  void AnalyzeRegion(Region* region);
+
+  // Runs the analysis and populates `control_predecessors_` for `op`.
+  void AnalyzeOp(Operation* op);
+
+  // Updates `control_predecessors_` for given `resource_id` and `op`.
+  void AddPredecessorsForAccess(ResourceId resource_id, Operation* op,
                                 bool read_only);
 
-  // Adds op's access to per_resource_access_info_.
-  void TrackAccess(int64_t resource_id, Operation* op, bool read_only);
+  // Updates resource access for given `resource_id` and `op` in
+  // `per_resource_access_info_` and `op_to_resource_ids_`.
+  void UpdateAccess(ResourceId resource_id, Operation* op, bool read_only);
+
+  // Returns true iff the last unknown resource access is already indirectly
+  // tracked by a previous `resource` access. `read_only` specifies the type of
+  // access considered.
+  bool IsUnknownAccessIndirectlyTrackedByResource(ResourceId resource,
+                                                  bool read_only);
 
   // Maps from an op to its control predecessors.
   llvm::SmallDenseMap<Operation*, llvm::SmallPtrSet<Operation*, 4>, 8>
@@ -93,32 +130,42 @@ class SideEffectAnalysisInfo {
   // Maps from an op to its control successors sorted in program order.
   llvm::SmallDenseMap<Operation*, llvm::SmallVector<Operation*, 4>, 8>
       sorted_control_successors_;
-
-  // Internal per-resource data structure when we build the dependencies.
+  // Maps from an op to its resource IDs along with a bool indicating if the
+  // resource is accessed `read-only`.
+  llvm::SmallDenseMap<Operation*,
+                      llvm::SmallVector<std::pair<ResourceId, bool>>>
+      op_to_resource_ids_;
+  llvm::SmallVector<std::pair<ResourceId, bool>> empty_resource_ids_;
+
+  // Internal per-resource data structure for building the dependencies.
   struct PerResourceAccessInfo {
-    // Last op that writes the resource before the current op being analyzed.
+    // Last op that writes to resource before the current op is being analyzed.
     Operation* last_write = nullptr;
-    // Read ops since last_write before the current op being analyzed.
+    // Read ops since `last_write` before the current op is being analyzed.
     llvm::SmallVector<Operation*, 8> reads_since_last_write;
-    // Whether previous accesses of this resource already tracked last unknown
-    // read for the current access being analyzed.
-    bool tracked_last_unknown_read = false;
-    // Whether previous accesses of this resource already tracked last unknown
-    // write for a the current read being analyzed.
-    bool tracked_last_unknown_write_for_read = false;
-    // Whether previous accesses of this resource already tracked last unknown
-    // write for a the current write being analyzed.
-    bool tracked_last_unknown_write_for_write = false;
+    // Whether a previous access of this resource already tracks the last
+    // unknown read(s).
+    bool are_last_unknown_reads_tracked = false;
+    // Whether a previous write access of this resource already tracks the last
+    // unknown write.
+    bool is_last_unknown_write_tracked_by_write = false;
+    // Whether a previous read or write access of this resource already tracks
+    // the last unknown write.
+    bool is_last_unknown_write_tracked = false;
   };
 
-  llvm::SmallDenseMap<int64_t, PerResourceAccessInfo, 8>
+  // Resource access info per resource ID.
+  llvm::SmallDenseMap<ResourceId, PerResourceAccessInfo, 8>
       per_resource_access_info_;
+
+  const OpSideEffectCollector& op_side_effect_collector_;
+  const TF::ResourceAliasAnalysis::Info& alias_analysis_;
 };
 
 }  // namespace detail
 
 // An analysis that runs on a function and infers the control predecessors and
-// successors for each op, based on side-effects on known and unknown resources.
+// successors for each op, based on side effects on known and unknown resources.
 // Side-effecting ops on unknown resources are conservatively treated as
 // interfering with all known resource op accesses. It distinguishes accesses
 // based on whether they are read-only, and read-only ops do not interfere with
diff --git a/tensorflow/compiler/mlir/tensorflow/tests/side-effect-analysis-test.mlir b/tensorflow/compiler/mlir/tensorflow/tests/side-effect-analysis-test.mlir
index 3deba01cfa9..aa3ce8a1922 100644
--- a/tensorflow/compiler/mlir/tensorflow/tests/side-effect-analysis-test.mlir
+++ b/tensorflow/compiler/mlir/tensorflow/tests/side-effect-analysis-test.mlir
@@ -1265,8 +1265,9 @@ func @arguments_with_unique_ids(
 
 // -----
 
-// Tests value-based side-effects for non-resource values.
-func @value_based_side_effect_non_resource(
+// Tests interplay of value-based side-effects for non-resource values and
+// unknown side effects.
+func @value_based_side_effect_non_resource_to_unknown(
   // expected-remark@above {{ID: 8}}
   %arg0: tensor<!tf_type.string>) {
   tf_executor.graph {
@@ -1301,6 +1302,42 @@ func @value_based_side_effect_non_resource(
 
 // -----
 
+// Tests interplay of value-based side-effects for non-resource values and
+// other known side effects.
+func @value_based_side_effect_non_resource_to_known(
+  // expected-remark@above {{ID: 8}}
+  %arg0: tensor<!tf_type.string>) {
+  tf_executor.graph {
+    // expected-remark@above {{ID: 6}}
+    // expected-remark@above {{Successors: {7}}}
+    %island = tf_executor.island {
+        // expected-remark@above {{ID: 4}}
+        // expected-remark@above {{Successors: {5}}}
+        %0 = "tf.GeneratorDataset"(%arg0, %arg0, %arg0) {device = "/job:tpu_host_worker/replica:0/task:0/device:CPU:0", finalize_func = @__func_a, init_func = @__func_b, next_func = @__func_c, next_func.experimental_ints_on_device = true, operand_segment_sizes = dense<[1, 1, 1]> : vector<3xi32>, output_shapes = [#tf_type.shape<>], output_types = [!tf_type.string]} : (tensor<!tf_type.string>, tensor<!tf_type.string>, tensor<!tf_type.string>) -> tensor<!tf_type.variant>
+        // expected-remark@above {{ID: 0}}
+        // expected-remark@above {{Successors: {2}}}
+        "tf._InternalTestNonResourceValueSideEffects_"(%arg0) : (tensor<!tf_type.string>) -> ()
+        // expected-remark@above {{ID: 1}}
+        // expected-remark@above {{Successors: {3}}}
+        %1 = "tf.GeneratorDataset"(%arg0, %arg0, %arg0) {device = "/job:tpu_host_worker/replica:0/task:0/device:CPU:0", finalize_func = @__func_a, init_func = @__func_b, next_func = @__func_c, next_func.experimental_ints_on_device = true, operand_segment_sizes = dense<[1, 1, 1]> : vector<3xi32>, output_shapes = [#tf_type.shape<>], output_types = [!tf_type.string]} : (tensor<!tf_type.string>, tensor<!tf_type.string>, tensor<!tf_type.string>) -> tensor<!tf_type.variant>
+        // expected-remark@above {{ID: 2}}
+        // expected-remark@above {{Predecessors: {0}}}
+        // expected-remark@above {{Successors: {3}}}
+        tf_executor.yield
+        // expected-remark@above {{ID: 3}}
+        // expected-remark@above {{Predecessors: {1,2}}}
+    }
+    tf_executor.fetch %island : !tf_executor.control
+    // expected-remark@above {{ID: 5}}
+    // expected-remark@above {{Predecessors: {4}}}
+  }
+  return
+  // expected-remark@above {{ID: 7}}
+  // expected-remark@above {{Predecessors: {6}}}
+}
+
+// -----
+
 // Tests that the analysis correctly handles a sequence of ops using the
 // `DatasetIterator` resource.
 func @dataset_op_sequence(
@@ -1487,3 +1524,178 @@ func @side_effecting_ops_with_different_resources_and_allocations(
   // expected-remark@above {{ID: 8}}
   // expected-remark@above {{Predecessors: {7}}}
 }
+
+// -----
+
+// Tests that we treat different op instances with `TPUEmbeddingSideEffect` as
+// independent.
+func @embedding_effect_ops(
+  // expected-remark@above {{ID: 7}}
+  %arg0: tensor<!tf_type.string>) {
+  tf_executor.graph {
+    // expected-remark@above {{ID: 5}}
+    // expected-remark@above {{Successors: {6}}}
+    %island = tf_executor.island {
+        // expected-remark@above {{ID: 3}}
+        // expected-remark@above {{Successors: {4}}}
+        "tf.EnqueueTPUEmbeddingRaggedTensorBatch"(%arg0){table_ids = [1, 2]} : (tensor<!tf_type.string>) -> ()
+        // expected-remark@above {{ID: 0}}
+        // expected-remark@above {{Successors: {2}}}
+        "tf.EnqueueTPUEmbeddingRaggedTensorBatch"(%arg0){table_ids = [1, 2]} : (tensor<!tf_type.string>) -> ()
+        // expected-remark@above {{ID: 1}}
+        // expected-remark@above {{Successors: {2}}}
+        tf_executor.yield
+        // expected-remark@above {{ID: 2}}
+        // expected-remark@above {{Predecessors: {0,1}}}
+    }
+    tf_executor.fetch %island : !tf_executor.control
+    // expected-remark@above {{ID: 4}}
+    // expected-remark@above {{Predecessors: {3}}}
+  }
+  return
+  // expected-remark@above {{ID: 6}}
+  // expected-remark@above {{Predecessors: {5}}}
+}
+
+// -----
+
+// Tests that we create dependencies between ops with `TPUEmbeddingSideEffect`
+// and unknown side-effecting ops.
+func @mixed_embedding_and_unknown_effects(
+  // expected-remark@above {{ID: 8}}
+  %arg0: tensor<!tf_type.string>) {
+  tf_executor.graph {
+    // expected-remark@above {{ID: 6}}
+    // expected-remark@above {{Successors: {7}}}
+    %island = tf_executor.island {
+        // expected-remark@above {{ID: 4}}
+        // expected-remark@above {{Successors: {5}}}
+        "tf.EnqueueTPUEmbeddingRaggedTensorBatch"(%arg0){table_ids = [1, 2]} : (tensor<!tf_type.string>) -> ()
+        // expected-remark@above {{ID: 0}}
+        // expected-remark@above {{Successors: {1}}}
+        "tf._UnknownSideEffectingOp_"() : () -> ()
+        // expected-remark@above {{ID: 1}}
+        // expected-remark@above {{Predecessors: {0}}}
+        // expected-remark@above {{Successors: {2}}}
+        "tf.EnqueueTPUEmbeddingRaggedTensorBatch"(%arg0){table_ids = [1, 2]} : (tensor<!tf_type.string>) -> ()
+        // expected-remark@above {{ID: 2}}
+        // expected-remark@above {{Predecessors: {1}}}
+        // expected-remark@above {{Successors: {3}}}
+        tf_executor.yield
+        // expected-remark@above {{ID: 3}}
+        // expected-remark@above {{Predecessors: {2}}}
+    }
+    tf_executor.fetch %island : !tf_executor.control
+    // expected-remark@above {{ID: 5}}
+    // expected-remark@above {{Predecessors: {4}}}
+  }
+  return
+  // expected-remark@above {{ID: 7}}
+  // expected-remark@above {{Predecessors: {6}}}
+}
+
+// -----
+
+// Tests that we create a dependency between two ops with the same op-based
+// write effect.
+func @same_op_based_write_effect(
+  // expected-remark@above {{ID: 7}}
+  %arg0: tensor<!tf_type.string>) {
+  tf_executor.graph {
+    // expected-remark@above {{ID: 5}}
+    // expected-remark@above {{Successors: {6}}}
+    %island = tf_executor.island {
+        // expected-remark@above {{ID: 3}}
+        // expected-remark@above {{Successors: {4}}}
+        %0 = "tf.GeneratorDataset"(%arg0, %arg0, %arg0) {device = "/job:tpu_host_worker/replica:0/task:0/device:CPU:0", finalize_func = @__func_a, init_func = @__func_b, next_func = @__func_c, next_func.experimental_ints_on_device = true, operand_segment_sizes = dense<[1, 1, 1]> : vector<3xi32>, output_shapes = [#tf_type.shape<>], output_types = [!tf_type.string]} : (tensor<!tf_type.string>, tensor<!tf_type.string>, tensor<!tf_type.string>) -> tensor<!tf_type.variant>
+        // expected-remark@above {{ID: 0}}
+        // expected-remark@above {{Successors: {1}}}
+        %1 = "tf.GeneratorDataset"(%arg0, %arg0, %arg0) {device = "/job:tpu_host_worker/replica:0/task:0/device:CPU:0", finalize_func = @__func_a, init_func = @__func_b, next_func = @__func_c, next_func.experimental_ints_on_device = true, operand_segment_sizes = dense<[1, 1, 1]> : vector<3xi32>, output_shapes = [#tf_type.shape<>], output_types = [!tf_type.string]} : (tensor<!tf_type.string>, tensor<!tf_type.string>, tensor<!tf_type.string>) -> tensor<!tf_type.variant>
+        // expected-remark@above {{ID: 1}}
+        // expected-remark@above {{Predecessors: {0}}}
+        // expected-remark@above {{Successors: {2}}}
+        tf_executor.yield
+        // expected-remark@above {{ID: 2}}
+        // expected-remark@above {{Predecessors: {1}}}
+    }
+    tf_executor.fetch %island : !tf_executor.control
+    // expected-remark@above {{ID: 4}}
+    // expected-remark@above {{Predecessors: {3}}}
+  }
+  return
+  // expected-remark@above {{ID: 6}}
+  // expected-remark@above {{Predecessors: {5}}}
+}
+
+// -----
+
+// Tests that we treat ops with different op-based side effects as independent.
+func @different_op_based_side_effects(
+  // expected-remark@above {{ID: 8}}
+  %arg0: tensor<!tf_type.string>) {
+  tf_executor.graph {
+    // expected-remark@above {{ID: 6}}
+    // expected-remark@above {{Successors: {7}}}
+    %island = tf_executor.island {
+        // expected-remark@above {{ID: 4}}
+        // expected-remark@above {{Successors: {5}}}
+        "tf.EnqueueTPUEmbeddingRaggedTensorBatch"(%arg0){table_ids = [1, 2]} : (tensor<!tf_type.string>) -> ()
+        // expected-remark@above {{ID: 0}}
+        // expected-remark@above {{Successors: {3}}}
+        %0 = "tf.GeneratorDataset"(%arg0, %arg0, %arg0) {device = "/job:tpu_host_worker/replica:0/task:0/device:CPU:0", finalize_func = @__func_a, init_func = @__func_b, next_func = @__func_c, next_func.experimental_ints_on_device = true, operand_segment_sizes = dense<[1, 1, 1]> : vector<3xi32>, output_shapes = [#tf_type.shape<>], output_types = [!tf_type.string]} : (tensor<!tf_type.string>, tensor<!tf_type.string>, tensor<!tf_type.string>) -> tensor<!tf_type.variant>
+        // expected-remark@above {{ID: 1}}
+        // expected-remark@above {{Successors: {3}}}
+        "tf.EnqueueTPUEmbeddingRaggedTensorBatch"(%arg0){table_ids = [1, 2]} : (tensor<!tf_type.string>) -> ()
+        // expected-remark@above {{ID: 2}}
+        // expected-remark@above {{Successors: {3}}}
+        tf_executor.yield
+        // expected-remark@above {{ID: 3}}
+        // expected-remark@above {{Predecessors: {0,1,2}}}
+    }
+    tf_executor.fetch %island : !tf_executor.control
+    // expected-remark@above {{ID: 5}}
+    // expected-remark@above {{Predecessors: {4}}}
+  }
+  return
+  // expected-remark@above {{ID: 7}}
+  // expected-remark@above {{Predecessors: {6}}}
+}
+
+// -----
+
+// Tests that we don't create dependencies between ops with different op-based
+// and value-based side effects.
+!tf_res = type tensor<*x!tf_type.resource<tensor<f32>>>
+func @mixed_op_based_value_based_side_effects(
+  // expected-remark@above {{ID: 8}}
+  %arg0: tensor<!tf_type.string>,
+  %arg1: !tf_res,
+  %arg2: tensor<f32>) {
+  tf_executor.graph {
+    // expected-remark@above {{ID: 6}}
+    // expected-remark@above {{Successors: {7}}}
+    %island = tf_executor.island {
+        // expected-remark@above {{ID: 4}}
+        // expected-remark@above {{Successors: {5}}}
+        "tf.AssignVariableOp"(%arg1, %arg2) : (!tf_res, tensor<f32>) -> ()
+        // expected-remark@above {{ID: 0}}
+        // expected-remark@above {{Successors: {2}}}
+        "tf.EnqueueTPUEmbeddingRaggedTensorBatch"(%arg0){table_ids = [1, 2]} : (tensor<!tf_type.string>) -> ()
+        // expected-remark@above {{ID: 1}}
+        // expected-remark@above {{Successors: {3}}}
+        "tf.ReadVariableOp"(%arg1) : (!tf_res) -> tensor<f32>
+        // expected-remark@above {{ID: 2}}
+        // expected-remark@above {{Predecessors: {0}}}
+        // expected-remark@above {{Successors: {3}}}
+        tf_executor.yield
+        // expected-remark@above {{ID: 3}}
+        // expected-remark@above {{Predecessors: {1,2}}}
+    }
+    tf_executor.fetch %island : !tf_executor.control
+    // expected-remark@above {{ID: 5}}
+    // expected-remark@above {{Predecessors: {4}}}
+  }
+  return
+  // expected-remark@above {{ID: 7}}
+  // expected-remark@above {{Predecessors: {6}}}
+}

commit d36e9732e6ee5a73b8621e2a393b6e54228cc1f6
Author: Edward Loper <edloper@google.com>
Date:   Tue Sep 14 08:31:36 2021 -0700

    Improve efficiency of dispatch v2: update InstanceChecker to support checking multiple base classes.  This helps with cases like `tf.types.experimental.TensorLike`, which is a union containing 11 different types.
    
    PiperOrigin-RevId: 396601669
    Change-Id: I361c65e5ce5076cee840010f51004b8eb2f9794c

diff --git a/tensorflow/python/framework/python_api_dispatcher.cc b/tensorflow/python/framework/python_api_dispatcher.cc
index e179d465574..0be392d099e 100644
--- a/tensorflow/python/framework/python_api_dispatcher.cc
+++ b/tensorflow/python/framework/python_api_dispatcher.cc
@@ -19,6 +19,7 @@ limitations under the License.
 
 #include "absl/strings/str_join.h"
 #include "tensorflow/core/platform/logging.h"
+#include "tensorflow/core/platform/macros.h"
 #include "tensorflow/python/lib/core/py_util.h"
 #include "tensorflow/python/lib/core/safe_pyobject_ptr.h"
 #include "tensorflow/python/util/util.h"
@@ -205,12 +206,13 @@ std::string PySignatureChecker::DebugString() const {
                        });
 }
 
-PyInstanceChecker::PyInstanceChecker(PyObject* py_class) : py_class_(py_class) {
+PyInstanceChecker::PyInstanceChecker(const std::vector<PyObject*>& py_classes) {
   DCheckPyGilState();
-  Py_INCREF(py_class);
-  match_type_ = IsRegisteredDispatchableType(py_class)
-                    ? MatchType::MATCH_DISPATCHABLE
-                    : MatchType::MATCH;
+  py_classes_.reserve(py_classes.size());
+  for (PyObject* py_class : py_classes) {
+    py_classes_.emplace_back(py_class);
+    Py_INCREF(py_class);
+  }
 }
 
 PyInstanceChecker::~PyInstanceChecker() {
@@ -225,13 +227,23 @@ PyTypeChecker::MatchType PyInstanceChecker::Check(PyObject* value) {
   auto* type = Py_TYPE(value);
   auto it = py_class_cache_.find(type);
   if (it != py_class_cache_.end()) {
-    return it->second ? match_type_ : MatchType::NO_MATCH;
+    return it->second;
   }
 
-  int result = PyObject_IsInstance(value, py_class_.get());
-  if (result < 0) {
-    PyErr_Clear();
-    return MatchType::NO_MATCH;
+  MatchType result = MatchType::NO_MATCH;
+  for (const auto& py_class : py_classes_) {
+    int is_instance = PyObject_IsInstance(value, py_class.get());
+    if (is_instance == 1) {
+      if (IsRegisteredDispatchableType(py_class.get())) {
+        result = MatchType::MATCH_DISPATCHABLE;
+        break;
+      } else {
+        result = MatchType::MATCH;
+      }
+    } else if (is_instance < 0) {
+      PyErr_Clear();
+      return MatchType::NO_MATCH;
+    }
   }
 
   if (py_class_cache_.size() < kMaxItemsInCache) {
@@ -239,12 +251,22 @@ PyTypeChecker::MatchType PyInstanceChecker::Check(PyObject* value) {
     auto insert_result = py_class_cache_.insert({type, result});
     DCHECK(insert_result.second);
   }
-  return result ? match_type_ : MatchType::NO_MATCH;
+  return result;
 }
 
+int PyInstanceChecker::cost() const { return py_classes_.size(); }
+
 std::string PyInstanceChecker::DebugString() const {
   DCheckPyGilState();
-  return reinterpret_cast<PyTypeObject*>(py_class_.get())->tp_name;
+  std::vector<const char*> type_names;
+  for (const auto& py_class : py_classes_) {
+    type_names.push_back(
+        reinterpret_cast<PyTypeObject*>(py_class.get())->tp_name);
+  }
+  return absl::StrJoin(
+      py_classes_, ", ", [](std::string* out, const Safe_PyObjectPtr& v) {
+        out->append(reinterpret_cast<PyTypeObject*>(v.get())->tp_name);
+      });
 }
 
 PyTypeChecker::MatchType PyListChecker::Check(PyObject* value) {
@@ -274,7 +296,7 @@ PyTypeChecker::MatchType PyListChecker::Check(PyObject* value) {
   return result;
 }
 
-int PyListChecker::cost() { return 10 * element_type_->cost(); }
+int PyListChecker::cost() const { return 10 * element_type_->cost(); }
 
 std::string PyListChecker::DebugString() const {
   return absl::StrCat("List[", element_type_->DebugString(), "]");
@@ -296,7 +318,7 @@ PyTypeChecker::MatchType PyUnionChecker::Check(PyObject* value) {
   return result;
 }
 
-int PyUnionChecker::cost() {
+int PyUnionChecker::cost() const {
   int cost = 1;
   for (auto& type_option : options_) {
     cost += type_option->cost();
diff --git a/tensorflow/python/framework/python_api_dispatcher.h b/tensorflow/python/framework/python_api_dispatcher.h
index d94251d9889..519c347e76e 100644
--- a/tensorflow/python/framework/python_api_dispatcher.h
+++ b/tensorflow/python/framework/python_api_dispatcher.h
@@ -198,7 +198,7 @@ class PyTypeChecker {
   // Approximate cost of calling this type checker, so we can perform less
   // expensive checks first.  (E.g., checking if every element in a list has a
   // given type is more costly than checking a single value.)
-  virtual int cost() = 0;
+  virtual int cost() const = 0;
 
   virtual std::string DebugString() const = 0;
 };
@@ -206,10 +206,10 @@ class PyTypeChecker {
 // PyTypeChecker that checks if a value is an instance of a given Python type.
 class PyInstanceChecker : public PyTypeChecker {
  public:
-  explicit PyInstanceChecker(PyObject* py_class);
+  explicit PyInstanceChecker(const std::vector<PyObject*>& py_classes);
   ~PyInstanceChecker() override;
   MatchType Check(PyObject* value) override;
-  int cost() override { return 1; }
+  int cost() const override;
   std::string DebugString() const override;
 
   // Size of the cache (for regression testing).
@@ -217,17 +217,12 @@ class PyInstanceChecker : public PyTypeChecker {
 
  private:
   // Python class to check values against.
-  Safe_PyObjectPtr py_class_;
-
-  // Value to return if the value is a subclass of `py_type_`.  Either MATCH or
-  // MATCH_DISPATCHABLE (depending on whether py_class_ has been registered
-  // for dispatch).
-  MatchType match_type_;
+  std::vector<Safe_PyObjectPtr> py_classes_;
 
   // Cache to avoid having to call PyObject_IsInstance.  Note: we rely on the
   // Python GIL (global interpreter lock) to avoid concurrent writes to this
   // cache, since `Check()` is always called from Python (via pybind11).
-  absl::flat_hash_map<PyTypeObject*, bool> py_class_cache_;
+  absl::flat_hash_map<PyTypeObject*, MatchType> py_class_cache_;
 
   // Maximum cache size.  In typical user programs, the cache will never become
   // full, but we use a maximum size in case the user creates types dynamically,
@@ -243,7 +238,7 @@ class PyListChecker : public PyTypeChecker {
   explicit PyListChecker(PyTypeChecker_ptr element_type)
       : element_type_(element_type) {}
   MatchType Check(PyObject* value) override;
-  int cost() override;
+  int cost() const override;
   std::string DebugString() const override;
 
  private:
@@ -257,7 +252,7 @@ class PyUnionChecker : public PyTypeChecker {
   explicit PyUnionChecker(std::vector<PyTypeChecker_ptr> options)
       : options_(options) {}
   MatchType Check(PyObject* value) override;
-  int cost() override;
+  int cost() const override;
   std::string DebugString() const override;
 
  private:
diff --git a/tensorflow/python/framework/python_api_dispatcher_test.py b/tensorflow/python/framework/python_api_dispatcher_test.py
index 9fed6c31d8c..607f1b10cf6 100644
--- a/tensorflow/python/framework/python_api_dispatcher_test.py
+++ b/tensorflow/python/framework/python_api_dispatcher_test.py
@@ -66,6 +66,14 @@ class PythonTypeCheckerTest(test_util.TensorFlowTestCase):
       self.assertEqual(ragged_checker.cost(), 1)
       self.assertEqual(repr(ragged_checker), '<PyTypeChecker RaggedTensor>')
 
+    with self.subTest('int or float checker'):
+      int_checker = dispatch.MakeInstanceChecker(int, float)
+      self.assertEqual(int_checker.Check(3), MATCH)
+      self.assertEqual(int_checker.Check(3.0), MATCH)
+      self.assertEqual(int_checker.Check(t), NO_MATCH)
+      self.assertEqual(int_checker.cost(), 2)
+      self.assertEqual(repr(int_checker), '<PyTypeChecker int, float>')
+
     with self.subTest('subclasses'):
 
       class A(object):
diff --git a/tensorflow/python/framework/python_api_dispatcher_wrapper.cc b/tensorflow/python/framework/python_api_dispatcher_wrapper.cc
index d67f2279c28..921c8717a93 100644
--- a/tensorflow/python/framework/python_api_dispatcher_wrapper.cc
+++ b/tensorflow/python/framework/python_api_dispatcher_wrapper.cc
@@ -114,9 +114,17 @@ PYBIND11_MODULE(_pywrap_python_api_dispatcher, m) {
            })
       .def("__repr__", &PythonAPIDispatcher::DebugString);
 
-  m.def("MakeInstanceChecker", [](py::handle py_class) {
+  m.def("MakeInstanceChecker", [](py::args py_classes) {
+    std::vector<PyObject*> py_classes_vector;
+    py_classes_vector.reserve(py_classes.size());
+    for (auto& cls : py_classes) {
+      if (!PyType_Check(cls.ptr())) {
+        throw py::type_error("`*py_classes` must be a tuple of types.");
+      }
+      py_classes_vector.push_back(cls.ptr());
+    }
     return std::shared_ptr<PyTypeChecker>(
-        std::make_shared<PyInstanceChecker>(py_class.ptr()));
+        std::make_shared<PyInstanceChecker>(py_classes_vector));
   });
   m.def("MakeListChecker", [](std::shared_ptr<PyTypeChecker> elt_type) {
     return std::shared_ptr<PyTypeChecker>(
diff --git a/tensorflow/python/util/dispatch.py b/tensorflow/python/util/dispatch.py
index 2df75686d20..b6b42328d85 100644
--- a/tensorflow/python/util/dispatch.py
+++ b/tensorflow/python/util/dispatch.py
@@ -612,6 +612,20 @@ def make_type_checker(annotation):
   """Builds a PyTypeChecker for the given type annotation."""
   if type_annotations.is_generic_union(annotation):
     type_args = type_annotations.get_generic_type_args(annotation)
+
+    # If the union contains two or more simple types, then use a single
+    # InstanceChecker to check them.
+    simple_types = [t for t in type_args if isinstance(t, type)]
+    simple_types = tuple(sorted(simple_types, key=id))
+    if len(simple_types) > 1:
+      if simple_types not in _is_instance_checker_cache:
+        checker = _api_dispatcher.MakeInstanceChecker(*simple_types)
+        _is_instance_checker_cache[simple_types] = checker
+      options = ([_is_instance_checker_cache[simple_types]] +
+                 [make_type_checker(t) for t in type_args
+                  if not isinstance(t, type)])
+      return _api_dispatcher.MakeUnionChecker(options)
+
     options = [make_type_checker(t) for t in type_args]
     return _api_dispatcher.MakeUnionChecker(options)
 
diff --git a/tensorflow/python/util/dispatch_test.py b/tensorflow/python/util/dispatch_test.py
index db305302262..ace749e2170 100644
--- a/tensorflow/python/util/dispatch_test.py
+++ b/tensorflow/python/util/dispatch_test.py
@@ -20,6 +20,7 @@ from __future__ import print_function
 
 import collections
 import typing
+import numpy as np
 
 from tensorflow.python.eager import context
 from tensorflow.python.framework import constant_op
@@ -36,6 +37,7 @@ from tensorflow.python.ops.proto_ops import decode_proto
 from tensorflow.python.platform import googletest
 from tensorflow.python.platform import test
 from tensorflow.python.platform import tf_logging
+from tensorflow.python.types import core as core_tf_types
 from tensorflow.python.util import deprecation
 from tensorflow.python.util import dispatch
 from tensorflow.python.util import nest
@@ -465,6 +467,32 @@ class DispatchV2Test(test_util.TensorFlowTestCase):
       # Clean up dispatch table.
       dispatch.unregister_dispatch_target(math_ops.add, masked_add)
 
+  def testDispatchForTensorLike(self):
+    MaskedOrTensorLike = typing.Union[MaskedTensor, core_tf_types.TensorLike]
+
+    @dispatch.dispatch_for_api(math_ops.add)
+    def masked_add(x: MaskedOrTensorLike, y: MaskedOrTensorLike, name=None):
+      with ops.name_scope(name):
+        x_values = x.values if isinstance(x, MaskedTensor) else x
+        x_mask = x.mask if isinstance(x, MaskedTensor) else True
+        y_values = y.values if isinstance(y, MaskedTensor) else y
+        y_mask = y.mask if isinstance(y, MaskedTensor) else True
+        return MaskedTensor(x_values + y_values, x_mask & y_mask)
+
+    try:
+      x = MaskedTensor([1, 2, 3, 4, 5], [1, 0, 1, 1, 1])
+      y1 = [10, 20, 30, 40, 50]
+      y2 = np.array([10, 20, 30, 40, 50])
+      y3 = constant_op.constant([10, 20, 30, 40, 50])
+      for y in [y1, y2, y3]:
+        z = math_ops.add(x, y)
+        self.assertAllEqual(z.values, x.values + y)
+        self.assertAllEqual(z.mask, x.mask)
+
+    finally:
+      # Clean up dispatch table.
+      dispatch.unregister_dispatch_target(math_ops.add, masked_add)
+
   def testDispatchForOptional(self):
     # Note: typing.Optional[X] == typing.Union[X, NoneType].
 

commit 0f755ec44ec56855d77e025bac8e02a7b5099181
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Sep 13 10:28:14 2021 -0700

    Improve efficiency in some code in c_api where we are calling `vector::push_back` in a loop without calling `vector::reserve` before the loop. Added those `reserve` calls.
    
    PiperOrigin-RevId: 396387348
    Change-Id: Idc10c8758969cdacbd6c4dcf365576434a326c1a

diff --git a/tensorflow/c/eager/c_api.cc b/tensorflow/c/eager/c_api.cc
index 4d9cd814836..b7dca6fb944 100644
--- a/tensorflow/c/eager/c_api.cc
+++ b/tensorflow/c/eager/c_api.cc
@@ -1043,7 +1043,9 @@ void SetOpAttrValueScalar(TFE_Context* ctx, TFE_Op* op,
       // String
       if (const int s_size = default_value.list().s_size()) {
         absl::InlinedVector<const void*, 4> values_vector;
+        values_vector.reserve(s_size);
         absl::InlinedVector<size_t, 4> lengths_vector;
+        lengths_vector.reserve(s_size);
         for (int i = 0; i < s_size; ++i) {
           const string& v = default_value.list().s(i);
           values_vector.push_back(v.data());
@@ -1056,6 +1058,7 @@ void SetOpAttrValueScalar(TFE_Context* ctx, TFE_Op* op,
       // Int
       if (const int i_size = default_value.list().i_size()) {
         absl::InlinedVector<int64_t, 4> i_vector;
+        i_vector.reserve(i_size);
         for (int i = 0; i < i_size; ++i) {
           i_vector.push_back(default_value.list().i(i));
         }
@@ -1064,6 +1067,7 @@ void SetOpAttrValueScalar(TFE_Context* ctx, TFE_Op* op,
       // Float
       if (const int f_size = default_value.list().f_size()) {
         absl::InlinedVector<float, 4> f_vector;
+        f_vector.reserve(f_size);
         for (int i = 0; i < f_size; ++i) {
           f_vector.push_back(default_value.list().f(i));
         }
@@ -1072,6 +1076,7 @@ void SetOpAttrValueScalar(TFE_Context* ctx, TFE_Op* op,
       // Bool
       if (const int b_size = default_value.list().b_size()) {
         absl::InlinedVector<unsigned char, 4> b_vector;
+        b_vector.reserve(b_size);
         for (int i = 0; i < b_size; i++) {
           b_vector.push_back(default_value.list().b(i));
         }
@@ -1080,6 +1085,7 @@ void SetOpAttrValueScalar(TFE_Context* ctx, TFE_Op* op,
       // Type
       if (const int type_size = default_value.list().type_size()) {
         absl::InlinedVector<unsigned int, 4> type_vector;
+        type_vector.reserve(type_size);
         for (int i = 0; i < type_size; ++i) {
           type_vector.push_back(default_value.list().type(i));
         }

commit eca90a0f89f8a8fe3835a84942b03899deb358b9
Author: Mehdi Amini <aminim@google.com>
Date:   Thu Sep 9 10:57:41 2021 -0700

    Disable MLIR multi-threading in MlirXlaOpKernel
    
    Since this kernel is implementing lowering for a single TF operation, we
    disable MLIR threading for efficiency purpose (avoid starting a large number
    of threads eagerly).
    
    This also caches the MLIRContext in the kernel so that repeated invocation won't lead to creating/destroying an MLIRContext.
    
    PiperOrigin-RevId: 395740833
    Change-Id: I4d98591ccb7640808d1aaae2101031796168b67c

diff --git a/tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc b/tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc
index 83274862eea..58ffed17320 100644
--- a/tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc
+++ b/tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.cc
@@ -704,20 +704,17 @@ xla::StatusOr<mlir::OwningModuleRef> GraphToModule(
   return ConvertGraphToMlir(graph, debug_info, flib_def, config, context);
 }
 
-Status BuildHloFromGraph(const Graph& graph, xla::XlaBuilder& builder,
-                         llvm::ArrayRef<xla::XlaOp> xla_params,
-                         std::vector<xla::XlaOp>& returns,
-                         llvm::ArrayRef<XlaArgument> args,
-                         llvm::ArrayRef<std::string> control_rets,
-                         llvm::StringRef device_type,
-                         const FunctionLibraryDefinition& flib_def,
-                         const GraphDebugInfo& debug_info,
-                         llvm::MutableArrayRef<std::unique_ptr<mlir::Pass>>
-                             custom_legalization_passes) {
-  mlir::MLIRContext context;
+Status BuildHloFromGraph(
+    const Graph& graph, xla::XlaBuilder& builder,
+    mlir::MLIRContext& mlir_context, llvm::ArrayRef<xla::XlaOp> xla_params,
+    std::vector<xla::XlaOp>& returns, llvm::ArrayRef<XlaArgument> args,
+    llvm::ArrayRef<std::string> control_rets, llvm::StringRef device_type,
+    const FunctionLibraryDefinition& flib_def, const GraphDebugInfo& debug_info,
+    llvm::MutableArrayRef<std::unique_ptr<mlir::Pass>>
+        custom_legalization_passes) {
   TF_ASSIGN_OR_RETURN(
       mlir::OwningModuleRef module,
-      GraphToModule(graph, control_rets, flib_def, debug_info, &context));
+      GraphToModule(graph, control_rets, flib_def, debug_info, &mlir_context));
   return BuildHloFromModule(module.get(), builder, xla_params, returns, args,
                             device_type, custom_legalization_passes);
 }
diff --git a/tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.h b/tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.h
index ec01d4fb7df..46c8097b6b7 100644
--- a/tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.h
+++ b/tensorflow/compiler/mlir/tensorflow/utils/compile_mlir_util.h
@@ -163,16 +163,14 @@ Status CompileGraphToXlaHlo(
 // XlaBuilder. This function adds HLO to a larger HLO computation, so
 // HLO-level inputs are supplied, and HLO-level outputs are produced.
 // xla_params is the HLO-level inputs and returns is the HLO-level outputs.
-Status BuildHloFromGraph(const Graph& graph, xla::XlaBuilder& builder,
-                         llvm::ArrayRef<xla::XlaOp> xla_params,
-                         std::vector<xla::XlaOp>& returns,
-                         llvm::ArrayRef<XlaArgument> args,
-                         llvm::ArrayRef<std::string> control_rets,
-                         llvm::StringRef device_type,
-                         const FunctionLibraryDefinition& flib_def,
-                         const GraphDebugInfo& debug_info,
-                         llvm::MutableArrayRef<std::unique_ptr<mlir::Pass>>
-                             custom_legalization_passes = {});
+Status BuildHloFromGraph(
+    const Graph& graph, xla::XlaBuilder& builder,
+    mlir::MLIRContext& mlir_context, llvm::ArrayRef<xla::XlaOp> xla_params,
+    std::vector<xla::XlaOp>& returns, llvm::ArrayRef<XlaArgument> args,
+    llvm::ArrayRef<std::string> control_rets, llvm::StringRef device_type,
+    const FunctionLibraryDefinition& flib_def, const GraphDebugInfo& debug_info,
+    llvm::MutableArrayRef<std::unique_ptr<mlir::Pass>>
+        custom_legalization_passes = {});
 
 static inline Status CompileToHloGraphAnalysisFailedError() {
   return errors::Internal("disabled after graph analysis");
diff --git a/tensorflow/compiler/tf2xla/BUILD b/tensorflow/compiler/tf2xla/BUILD
index 1947bd79c04..f4f66914502 100644
--- a/tensorflow/compiler/tf2xla/BUILD
+++ b/tensorflow/compiler/tf2xla/BUILD
@@ -1155,6 +1155,7 @@ cc_library(
         "//tensorflow/compiler/jit:xla_compilation_cache",
         "//tensorflow/compiler/mlir:array_container_utils",
         "//tensorflow/compiler/mlir/tensorflow:compile_mlir_util_no_tf_dialect_passes",
+        "@llvm-project//mlir:IR",
     ],
 )
 
diff --git a/tensorflow/compiler/tf2xla/mlir_xla_op_kernel.cc b/tensorflow/compiler/tf2xla/mlir_xla_op_kernel.cc
index a56e75ab27a..b20646b4b4b 100644
--- a/tensorflow/compiler/tf2xla/mlir_xla_op_kernel.cc
+++ b/tensorflow/compiler/tf2xla/mlir_xla_op_kernel.cc
@@ -49,6 +49,13 @@ Status ContextToXlaArgs(XlaOpKernelContext* ctx,
 
 }  // namespace
 
+MlirXlaOpKernel::MlirXlaOpKernel(OpKernelConstruction* ctx)
+    : XlaOpKernel(ctx),
+      // Since this kernel implements lowering for a single TF operation, we
+      // disable MLIR threading for efficiency purpose (avoid starting a large
+      // number of threads eagerly).
+      mlir_ctx_(mlir::MLIRContext::Threading::DISABLED) {}
+
 Status MlirXlaOpKernel::ConstructXlaOp(XlaOpKernelContext* ctx) {
   // Create input XlaArguments.
   std::vector<XlaCompiler::Argument> xla_args;
@@ -88,7 +95,7 @@ Status MlirXlaOpKernel::ConstructXlaOp(XlaOpKernelContext* ctx) {
   GraphDebugInfo debug_info;
   std::vector<xla::XlaOp> returns(1);
   TF_RETURN_IF_ERROR(BuildHloFromGraph(
-      *graph, *ctx->builder(), xla_params, returns,
+      *graph, *ctx->builder(), mlir_ctx_, xla_params, returns,
       mlir::SpanToArrayRef<XlaCompiler::Argument>(xla_args), control_rets,
       device->device_type(),
       *ctx->function_library()->GetFunctionLibraryDefinition(), debug_info,
diff --git a/tensorflow/compiler/tf2xla/mlir_xla_op_kernel.h b/tensorflow/compiler/tf2xla/mlir_xla_op_kernel.h
index 278cc53446c..e319b286f5d 100644
--- a/tensorflow/compiler/tf2xla/mlir_xla_op_kernel.h
+++ b/tensorflow/compiler/tf2xla/mlir_xla_op_kernel.h
@@ -16,6 +16,7 @@ limitations under the License.
 #ifndef TENSORFLOW_COMPILER_TF2XLA_MLIR_XLA_OP_KERNEL_H_
 #define TENSORFLOW_COMPILER_TF2XLA_MLIR_XLA_OP_KERNEL_H_
 
+#include "mlir/IR/MLIRContext.h"  // from @llvm-project
 #include "tensorflow/compiler/tf2xla/xla_op_kernel.h"
 
 namespace tensorflow {
@@ -24,11 +25,12 @@ namespace tensorflow {
 // legalization.
 class MlirXlaOpKernel : public XlaOpKernel {
  public:
-  explicit MlirXlaOpKernel(OpKernelConstruction* ctx) : XlaOpKernel(ctx) {}
+  explicit MlirXlaOpKernel(OpKernelConstruction* ctx);
 
  private:
   void Compile(XlaOpKernelContext* ctx) override;
   Status ConstructXlaOp(XlaOpKernelContext* ctx);
+  mlir::MLIRContext mlir_ctx_;
 };
 
 }  // namespace tensorflow

commit bf16d44a6cb089d3d8aa86746130f54c9308dcc2
Author: CJ Carey <cjcarey@google.com>
Date:   Wed Aug 25 15:34:44 2021 -0700

    Improve the efficiency of AppendFeatureValues when passing a container that supports a `size()` method.
    
    Instead of reusing the iterator-based overload when a container is passed, we now attempt to explicitly Reserve() the necessary space before copying/moving elements.
    
    This will save on allocations and total RAM usage, as the new feature's repeated field capacity is no longer grown dynamically.
    
    PiperOrigin-RevId: 392994752
    Change-Id: Icf3918716e2989d9980bdda5b233780bb2f97900

diff --git a/tensorflow/core/example/feature_util.h b/tensorflow/core/example/feature_util.h
index f3c6bec84bc..6f6e0e7820c 100644
--- a/tensorflow/core/example/feature_util.h
+++ b/tensorflow/core/example/feature_util.h
@@ -114,8 +114,10 @@ limitations under the License.
 #ifndef TENSORFLOW_CORE_EXAMPLE_FEATURE_UTIL_H_
 #define TENSORFLOW_CORE_EXAMPLE_FEATURE_UTIL_H_
 
+#include <algorithm>
 #include <iterator>
 #include <type_traits>
+#include <utility>
 
 #include "absl/base/macros.h"
 #include "tensorflow/core/example/example.pb.h"
@@ -292,14 +294,49 @@ void AppendFeatureValues(IteratorType first, IteratorType last,
 template <typename ValueType>
 void AppendFeatureValues(std::initializer_list<ValueType> container,
                          Feature* feature) {
-  AppendFeatureValues(container.begin(), container.end(), feature);
+  using FeatureType = typename internal::FeatureTrait<ValueType>::Type;
+  auto* values = GetFeatureValues<FeatureType>(feature);
+  values->Reserve(container.size());
+  std::move(container.begin(), container.end(),
+            protobuf::RepeatedFieldBackInserter(values));
 }
 
+namespace internal {
+// HasSize<T>(0) returns true_type if T has a size() member.
+template <typename T>
+constexpr auto HasSize(int)
+    -> decltype((std::declval<T>().size(), std::true_type{})) {
+  return {};
+}
+template <typename>
+constexpr std::false_type HasSize(...) {
+  return {};
+}
+
+// Reserves the container's size, if a container.size() method exists.
+template <typename ContainerType, typename RepeatedFieldType,
+          typename std::enable_if_t<HasSize<ContainerType>(0), int> = 0>
+void ReserveIfSizeAvailable(const ContainerType& container,
+                            RepeatedFieldType& values) {
+  values.Reserve(container.size());
+}
+
+template <typename ContainerType, typename RepeatedFieldType,
+          typename std::enable_if_t<!HasSize<ContainerType>(0), int> = 0>
+void ReserveIfSizeAvailable(const ContainerType& container,
+                            RepeatedFieldType& values) {}
+
+}  // namespace internal
+
 template <typename ContainerType>
 void AppendFeatureValues(const ContainerType& container, Feature* feature) {
   using IteratorType = typename ContainerType::const_iterator;
-  AppendFeatureValues<IteratorType>(container.begin(), container.end(),
-                                    feature);
+  using FeatureType = typename internal::FeatureTrait<
+      typename std::iterator_traits<IteratorType>::value_type>::Type;
+  auto* values = GetFeatureValues<FeatureType>(feature);
+  internal::ReserveIfSizeAvailable(container, *values);
+  std::copy(container.begin(), container.end(),
+            protobuf::RepeatedFieldBackInserter(values));
 }
 
 // Copies elements from the range, defined by [first, last) into the feature
@@ -314,9 +351,8 @@ void AppendFeatureValues(IteratorType first, IteratorType last,
 template <typename ContainerType, typename ProtoType>
 void AppendFeatureValues(const ContainerType& container, const std::string& key,
                          ProtoType* proto) {
-  using IteratorType = typename ContainerType::const_iterator;
-  AppendFeatureValues<IteratorType>(container.begin(), container.end(), key,
-                                    proto);
+  AppendFeatureValues<ContainerType>(container,
+                                     GetFeature(key, GetFeatures(proto)));
 }
 
 // Copies all elements from the initializer list into a Feature contained by
@@ -324,10 +360,8 @@ void AppendFeatureValues(const ContainerType& container, const std::string& key,
 template <typename ValueType, typename ProtoType>
 void AppendFeatureValues(std::initializer_list<ValueType> container,
                          const std::string& key, ProtoType* proto) {
-  using IteratorType =
-      typename std::initializer_list<ValueType>::const_iterator;
-  AppendFeatureValues<IteratorType>(container.begin(), container.end(), key,
-                                    proto);
+  AppendFeatureValues<ValueType>(container,
+                                 GetFeature(key, GetFeatures(proto)));
 }
 
 // Clears the feature's repeated field (int64, float, or string).
@@ -350,7 +384,9 @@ void SetFeatureValues(IteratorType first, IteratorType last, Feature* feature) {
 template <typename ValueType>
 void SetFeatureValues(std::initializer_list<ValueType> container,
                       Feature* feature) {
-  SetFeatureValues(container.begin(), container.end(), feature);
+  using FeatureType = typename internal::FeatureTrait<ValueType>::Type;
+  ClearFeatureValues<FeatureType>(feature);
+  AppendFeatureValues(container, feature);
 }
 
 // Clears the feature's repeated field (int64, float, or string). Copies all
@@ -358,7 +394,10 @@ void SetFeatureValues(std::initializer_list<ValueType> container,
 template <typename ContainerType>
 void SetFeatureValues(const ContainerType& container, Feature* feature) {
   using IteratorType = typename ContainerType::const_iterator;
-  SetFeatureValues<IteratorType>(container.begin(), container.end(), feature);
+  using FeatureType = typename internal::FeatureTrait<
+      typename std::iterator_traits<IteratorType>::value_type>::Type;
+  ClearFeatureValues<FeatureType>(feature);
+  AppendFeatureValues(container, feature);
 }
 
 // Clears the feature's repeated field (int64, float, or string). Copies
@@ -375,9 +414,8 @@ void SetFeatureValues(IteratorType first, IteratorType last,
 template <typename ContainerType, typename ProtoType>
 void SetFeatureValues(const ContainerType& container, const std::string& key,
                       ProtoType* proto) {
-  using IteratorType = typename ContainerType::const_iterator;
-  SetFeatureValues<IteratorType>(container.begin(), container.end(), key,
-                                 proto);
+  SetFeatureValues<ContainerType>(container,
+                                  GetFeature(key, GetFeatures(proto)));
 }
 
 // Clears the feature's repeated field (int64, float, or string). Copies all
@@ -385,10 +423,7 @@ void SetFeatureValues(const ContainerType& container, const std::string& key,
 template <typename ValueType, typename ProtoType>
 void SetFeatureValues(std::initializer_list<ValueType> container,
                       const std::string& key, ProtoType* proto) {
-  using IteratorType =
-      typename std::initializer_list<ValueType>::const_iterator;
-  SetFeatureValues<IteratorType>(container.begin(), container.end(), key,
-                                 proto);
+  SetFeatureValues<ValueType>(container, GetFeature(key, GetFeatures(proto)));
 }
 
 // Returns true if a feature with the specified key belongs to the Features.

commit 1a67d56238ff7791a1534b749fb2b0c136b0627d
Author: Edward Loper <edloper@google.com>
Date:   Mon Aug 23 14:19:40 2021 -0700

    Add Python variable to keep track of what dispatcher signatures have been registered, for documentation & error-checking purposes.  (This variable is not used by the actual dispatch system, which is coded in c++ for efficiency.)
    
    PiperOrigin-RevId: 392517389
    Change-Id: Iea43273de1ac494eae1bc220cc305c0781d6006d

diff --git a/tensorflow/python/util/dispatch.py b/tensorflow/python/util/dispatch.py
index 40df88790de..1d01b2eb8a5 100644
--- a/tensorflow/python/util/dispatch.py
+++ b/tensorflow/python/util/dispatch.py
@@ -63,6 +63,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import collections
 import itertools
 import typing  # pylint: disable=unused-import (used in doctests)
 
@@ -355,16 +356,70 @@ def dispatch_for(api, *signatures):
 
     for signature_checker in signature_checkers:
       dispatcher.Register(signature_checker, dispatch_target)
+    _TYPE_BASED_DISPATCH_SIGNATURES[api][dispatch_target].extend(signatures)
+
     if not signature_checkers:
-      dispatcher.Register(
-          _signature_checker_from_annotations(api_signature, dispatch_target),
-          dispatch_target)
+      signature = _signature_from_annotations(dispatch_target)
+      checker = _make_signature_checker(api_signature, signature)
+      dispatcher.Register(checker, dispatch_target)
+      _TYPE_BASED_DISPATCH_SIGNATURES[api][dispatch_target].append(signature)
 
     return dispatch_target
 
   return decorator
 
 
+# Nested dict mapping `api_func` -> `dispatch_target` -> `List[signature]`,
+# which can be used for documentation generation and for improved error messages
+# when APIs are called with unsupported types.
+_TYPE_BASED_DISPATCH_SIGNATURES = {}
+
+
+def apis_with_type_based_dispatch():
+  """Returns a list of TensorFlow APIs that support type-based dispatch."""
+  return sorted(
+      _TYPE_BASED_DISPATCH_SIGNATURES,
+      key=lambda api: f"{api.__module__}.{api.__name__}")
+
+
+def type_based_dispatch_signatures_for(cls):
+  """Returns dispatch signatures that have been registered for a given class.
+
+  This function is intended for documentation-generation purposes.
+
+  Args:
+    cls: The class to search for.  Type signatures are searched recursively, so
+      e.g., if `cls=RaggedTensor`, then information will be returned for all
+      dispatch targets that have `RaggedTensor` anywhere in their type
+      annotations (including nested in `typing.Union` or `typing.List`.)
+
+  Returns:
+    A `dict` mapping `api` -> `signatures`, where `api` is a TensorFlow API
+    function; and `signatures` is a list of dispatch signatures for `api`
+    that include `cls`.  (Each signature is a dict mapping argument names to
+    type annotations; see `dispatch_for` for more info.)
+  """
+
+  def contains_cls(x):
+    """Returns true if `x` contains `cls`."""
+    if isinstance(x, dict):
+      return any(contains_cls(v) for v in x.values())
+    elif x is cls:
+      return True
+    elif (type_annotations.is_generic_list(x) or
+          type_annotations.is_generic_union(x)):
+      type_args = type_annotations.get_generic_type_args(x)
+      return any(contains_cls(arg) for arg in type_args)
+    else:
+      return False
+
+  result = {}
+  for api, api_signatures in _TYPE_BASED_DISPATCH_SIGNATURES.items():
+    for _, signatures in api_signatures.items():
+      result.setdefault(api, []).extend(filter(contains_cls, signatures))
+  return result
+
+
 # TODO(edloper): Consider using a mechanism like this to automatically add
 # the `name` argument to all TensorFlow APIs that are implemented in Python
 # (so each Python function doesn't need to do it manually).
@@ -418,6 +473,9 @@ def unregister_dispatch_target(api, dispatch_target):
   dispatcher = getattr(api, TYPE_BASED_DISPATCH_ATTR, None)
   if dispatcher is None:
     raise ValueError(f"{api} does not support dispatch.")
+  if dispatch_target not in _TYPE_BASED_DISPATCH_SIGNATURES[api]:
+    raise ValueError(f"{dispatch_target} was not registered for {api}")
+  del _TYPE_BASED_DISPATCH_SIGNATURES[api][dispatch_target]
   dispatcher.Unregister(dispatch_target)
 
 
@@ -458,6 +516,7 @@ def add_type_based_api_dispatcher(target):
       _api_dispatcher.PythonAPIDispatcher(unwrapped.__name__,
                                           target_argspec.args,
                                           target_argspec.defaults))
+  _TYPE_BASED_DISPATCH_SIGNATURES[target] = collections.defaultdict(list)
   return target
 
 
@@ -569,8 +628,8 @@ def make_type_checker(annotation):
                      " List[...], and Union[...]")
 
 
-def _signature_checker_from_annotations(api_signature, func):
-  """Build SignatureChecker from type annotations."""
+def _signature_from_annotations(func):
+  """Builds a dict mapping from parameter names to type annotations."""
   func_signature = tf_inspect.signature(func)
 
   signature = dict([(name, param.annotation)
@@ -580,7 +639,7 @@ def _signature_checker_from_annotations(api_signature, func):
     raise ValueError("The dispatch_for decorator must be called with at "
                      "least one signature, or applied to a function that "
                      "has type annotations on its parameters.")
-  return _make_signature_checker(api_signature, signature)
+  return signature
 
 
 ################################################################################
diff --git a/tensorflow/python/util/dispatch_test.py b/tensorflow/python/util/dispatch_test.py
index f05fc20689d..0d055f9353c 100644
--- a/tensorflow/python/util/dispatch_test.py
+++ b/tensorflow/python/util/dispatch_test.py
@@ -745,6 +745,11 @@ class DispatchV2Test(test_util.TensorFlowTestCase):
     with self.assertRaisesRegex(ValueError, ".* does not support dispatch"):
       dispatch.unregister_dispatch_target(fn, fn)
 
+  def testUnregisterDispatchTargetBadDispatchTargetError(self):
+    fn = lambda x: x + 1
+    with self.assertRaisesRegex(ValueError, ".* was not registered for .*"):
+      dispatch.unregister_dispatch_target(math_ops.add, fn)
+
   def testAddDuplicateApiDisptacherError(self):
     some_op = lambda x: x
     some_op = dispatch.add_type_based_api_dispatcher(some_op)
@@ -752,6 +757,32 @@ class DispatchV2Test(test_util.TensorFlowTestCase):
         ValueError, ".* already has a type-based API dispatcher."):
       some_op = dispatch.add_type_based_api_dispatcher(some_op)
 
+  def testGetApisWithTypeBasedDispatch(self):
+    dispatch_apis = dispatch.apis_with_type_based_dispatch()
+    self.assertIn(math_ops.add, dispatch_apis)
+    self.assertIn(array_ops.concat, dispatch_apis)
+
+  def testTypeBasedDispatchTargetsFor(self):
+    MaskedTensorList = typing.List[typing.Union[MaskedTensor, ops.Tensor]]
+    try:
+      @dispatch.dispatch_for(math_ops.add)
+      def masked_add(x: MaskedTensor, y: MaskedTensor):
+        del x, y
+
+      @dispatch.dispatch_for(array_ops.concat)
+      def masked_concat(values: MaskedTensorList, axis):
+        del values, axis
+
+      targets = dispatch.type_based_dispatch_signatures_for(MaskedTensor)
+      expected = {math_ops.add: [{"x": MaskedTensor, "y": MaskedTensor}],
+                  array_ops.concat: [{"values": MaskedTensorList}]}
+      self.assertEqual(targets, expected)
+
+    finally:
+      # Clean up dispatch table.
+      dispatch.unregister_dispatch_target(math_ops.add, masked_add)
+      dispatch.unregister_dispatch_target(array_ops.concat, masked_concat)
+
 
 if __name__ == "__main__":
   googletest.main()

commit 054b867b895fc441e816ede4e7245b60b7742f43
Author: Tatiana Shpeisman <shpeisman@google.com>
Date:   Fri May 7 09:27:27 2021 -0700

    Improve efficiency of op name comparison in ClusterOpsByPolicy pass.
    
    For union-find policy, represents a set of op names as a set of identifiers instead of a set of strings. For use-def policy, hoists out string to identifier conversion to be done once per function instead of on every op name comparison. Unlike strings, identifier comparison takes constant time.
    
    PiperOrigin-RevId: 372570592
    Change-Id: Iebcb8f4b3e81c23d8bc89fd5a0a8daa6cf132795

diff --git a/tensorflow/compiler/mlir/tensorflow/transforms/cluster_ops_by_policy_pass.cc b/tensorflow/compiler/mlir/tensorflow/transforms/cluster_ops_by_policy_pass.cc
index 7bbcf81622a..9e84299d704 100644
--- a/tensorflow/compiler/mlir/tensorflow/transforms/cluster_ops_by_policy_pass.cc
+++ b/tensorflow/compiler/mlir/tensorflow/transforms/cluster_ops_by_policy_pass.cc
@@ -152,18 +152,14 @@ static tf_device::ClusterOp ClusterMatchedOps(
 //    %2 = "tf.Neg" %a
 //    %3 = "tf.Sub" %c, %1 // the only use of %1
 // matches "tf.Add, tf.Sub".
-static bool IsOplistMatch(Operation *op, ArrayRef<std::string> oplist,
+static bool IsOplistMatch(Operation *op, ArrayRef<Identifier> oplist,
                           llvm::DenseSet<Operation *> &is_matched,
                           llvm::SmallVectorImpl<Operation *> &matched_ops) {
-  MLIRContext *ctx = op->getContext();
-
   // Skip 'op' if it's already part of another matched sequence of ops.
   if (is_matched.contains(op)) return false;
 
   // Does this operation match first element in the oplist?
-  StringRef op_name = *oplist.begin();
-  if (op->getName().getIdentifier() != Identifier::get(op_name, ctx))
-    return false;
+  if (op->getName().getIdentifier() != *oplist.begin()) return false;
 
   matched_ops.push_back(op);
 
@@ -183,9 +179,7 @@ static bool IsOplistMatch(Operation *op, ArrayRef<std::string> oplist,
     if (is_matched.contains(curr_op)) return false;
 
     // Check that the op matches the next op in the oplist.
-    op_name = *oplist_iter;
-    if (curr_op->getName().getIdentifier() != Identifier::get(op_name, ctx))
-      return false;
+    if (curr_op->getName().getIdentifier() != *oplist_iter) return false;
 
     // Don't cluster operations assigned to different devices.
     if (curr_op->getAttr(kDeviceAttr) != device) return false;
@@ -210,9 +204,16 @@ static bool IsOplistMatch(Operation *op, ArrayRef<std::string> oplist,
 // `clusters` list.
 static void FormUseDefClusters(mlir::FuncOp func, ArrayRef<std::string> oplist,
                                llvm::SmallVectorImpl<OpList> *clusters) {
+  MLIRContext *context = func.getContext();
+
   // Do not place the same operation into multiple cluster.
   llvm::DenseSet<Operation *> is_matched;
 
+  // Convert 'oplist' of strings into a list of identifiers.
+  std::vector<Identifier> op_id_list;
+  for (const auto &op : oplist)
+    op_id_list.push_back(Identifier::get(op, context));
+
   // Find matching op sequences within this function.
   func.walk([&](Operation *op) {
     llvm::SmallVector<Operation *> matched_ops;
@@ -220,8 +221,8 @@ static void FormUseDefClusters(mlir::FuncOp func, ArrayRef<std::string> oplist,
     // Skip 'op' if it's already part of another matched sequence of ops.
     if (is_matched.contains(op)) return;
 
-    // Try to match 'op' to the sequence of ops in 'oplist'.
-    if (!IsOplistMatch(op, oplist, is_matched, matched_ops)) return;
+    // Try to match 'op' to the sequence of ops in 'op_id_list'.
+    if (!IsOplistMatch(op, op_id_list, is_matched, matched_ops)) return;
 
     // We found a matching sequence of ops. Record it.
     clusters->push_back(matched_ops);
@@ -298,16 +299,16 @@ static void Union(Members &members, unsigned a, unsigned b) {
 //
 // Although %0, %1, %2 do not form a single use-def chain, they are still
 // clustered together based on the union-find algorigthm.
-static void ClusterOpsInTheBlock(
-    Block *block, const llvm::DenseSet<llvm::StringRef> &cluster_ops,
-    llvm::SmallVectorImpl<OpList> *clusters) {
+static void ClusterOpsInTheBlock(Block *block,
+                                 const llvm::DenseSet<Identifier> &cluster_ops,
+                                 llvm::SmallVectorImpl<OpList> *clusters) {
   // Returns true if op can be clustered.
   auto can_be_clustered = [&](Operation &op) -> bool {
     // Check that op has no side effects. This guarantees that we will not
     // reorder side-effecting ops during cluster formation.
     if (!MemoryEffectOpInterface::hasNoEffect(&op)) return false;
 
-    return cluster_ops.contains(op.getName().getStringRef());
+    return cluster_ops.contains(op.getName().getIdentifier());
   };
 
   // Use an array based union-find algorithm to cluster operations together
@@ -409,8 +410,10 @@ static void ClusterOpsInTheBlock(
 static void FormUnionFindClusters(mlir::FuncOp func,
                                   ArrayRef<std::string> oplist,
                                   llvm::SmallVectorImpl<OpList> *clusters) {
-  llvm::DenseSet<llvm::StringRef> opset;
-  for (const auto &op : oplist) opset.insert(op);
+  MLIRContext *context = func->getContext();
+
+  llvm::DenseSet<Identifier> opset;
+  for (const auto &op : oplist) opset.insert(Identifier::get(op, context));
   func->walk(
       [&](Block *block) { ClusterOpsInTheBlock(block, opset, clusters); });
 }

commit 311c53372c29ab23f6ca0a3bc63f40154278caf4
Author: Haoliang Zhang <haoliang@google.com>
Date:   Thu Apr 29 14:03:24 2021 -0700

    Improve the efficiency of registering FunctionDef in the flex delegate:
    Previously we add a FunctionDef for each subgraph in the tflite model, this isn't efficient since there are subgraphs that are not intended to be invoked as functions. After the change, we will collect subgraphs used by a list of ops (MapDataset, ReduceDataset) and only register functions for those subgraphs.
    
    PiperOrigin-RevId: 371199293
    Change-Id: I3a8ca3841d018b9a57e339fd8becf0d03863b974

diff --git a/tensorflow/lite/delegates/flex/BUILD b/tensorflow/lite/delegates/flex/BUILD
index 861afb5bc32..ce8220a38d1 100644
--- a/tensorflow/lite/delegates/flex/BUILD
+++ b/tensorflow/lite/delegates/flex/BUILD
@@ -162,7 +162,10 @@ cc_library(
         ":util",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/strings",
+        "@flatbuffers",
+        "//tensorflow/lite/schema:schema_fbs",
         "//tensorflow/lite:cc_api",
+        "//tensorflow/lite:util",
     ] + select({
         "//tensorflow:android": [
             "//tensorflow/core:portable_tensorflow_lib_lite",
@@ -176,6 +179,7 @@ cc_library(
             "//tensorflow/core:core_cpu",
             "//tensorflow/core:framework",
             "//tensorflow/core:lib",
+            "//tensorflow/core:protos_all_cc",
         ],
     }),
 )
@@ -186,8 +190,10 @@ tf_cc_test(
     srcs = ["delegate_data_test.cc"],
     deps = [
         ":delegate_data",
+        "//tensorflow/core:test",
         "//tensorflow/core/common_runtime/eager:context",
         "//tensorflow/core/platform:protobuf",
+        "//tensorflow/core/platform:status",
         "//tensorflow/lite:framework",
         "//tensorflow/lite/c:common",
         "//tensorflow/lite/core/api:error_reporter",
diff --git a/tensorflow/lite/delegates/flex/delegate_data.cc b/tensorflow/lite/delegates/flex/delegate_data.cc
index 2c6f94b57bd..15d0149b890 100644
--- a/tensorflow/lite/delegates/flex/delegate_data.cc
+++ b/tensorflow/lite/delegates/flex/delegate_data.cc
@@ -14,13 +14,16 @@ limitations under the License.
 ==============================================================================*/
 #include "tensorflow/lite/delegates/flex/delegate_data.h"
 
+#include <set>
 #include <vector>
 
 #include "absl/memory/memory.h"
 #include "absl/strings/str_cat.h"
+#include "flatbuffers/flexbuffers.h"  // from @flatbuffers
 #include "tensorflow/core/common_runtime/device_factory.h"
 #include "tensorflow/core/common_runtime/eager/context.h"
 #include "tensorflow/core/framework/function.h"
+#include "tensorflow/core/framework/node_def.pb.h"
 #include "tensorflow/core/framework/op_kernel.h"
 #include "tensorflow/core/framework/resource_mgr.h"
 #include "tensorflow/core/graph/graph.h"
@@ -30,6 +33,8 @@ limitations under the License.
 #include "tensorflow/core/platform/tstring.h"
 #include "tensorflow/lite/core/subgraph.h"
 #include "tensorflow/lite/delegates/flex/util.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/util.h"
 
 namespace tflite {
 namespace flex {
@@ -97,25 +102,74 @@ void BuildFunctionDefProto(const std::string& function_name,
   fdef.mutable_node_def(1)->mutable_attr()->insert({"Tout", tout_attrs});
 }
 
-// Creates a `TFLiteSubgraphResource` for each subgraph (execpt
-// for main subgraph) in the model and adds it in the eager context's resource
-// manager. It also registers a FunctionDef for each subgraph which is used to
-// invoke the subgraph by the function library runtime.
+// Returns a list of subgraph names which have associated function attributes.
+tensorflow::Status GetSubgraphNamesForFunctionExecution(
+    const std::vector<std::unique_ptr<Subgraph>>& subgraphs,
+    std::set<std::string>* result) {
+  tensorflow::NodeDef node_def;
+  for (const auto& subgraph : subgraphs) {
+    for (const auto& node_and_reg : subgraph->nodes_and_registration()) {
+      if (node_and_reg.second.builtin_code != tflite::BuiltinOperator_CUSTOM) {
+        // If this isn't a custom op, skip.
+        continue;
+      }
+      const std::string custom_name = node_and_reg.second.custom_name;
+      if (custom_name.substr(0, strlen(tflite::kFlexCustomCodePrefix)) !=
+          tflite::kFlexCustomCodePrefix) {
+        // Skip if this is not a flex op.
+        continue;
+      }
+      // The flexbuffer contains a vector where the first elements is the
+      // op name and the second is a serialized NodeDef.
+      const flexbuffers::Vector& v =
+          flexbuffers::GetRoot(reinterpret_cast<const uint8_t*>(
+                                   node_and_reg.first.custom_initial_data),
+                               node_and_reg.first.custom_initial_data_size)
+              .AsVector();
+      // TODO(b/181352924): Use proto arena if we see performance regression.
+      if (!node_def.ParseFromString(v[1].AsString().str())) {
+        return tensorflow::Status(tensorflow::error::INTERNAL,
+                                  "could not parse NodeDef");
+      }
+      // Loop through all the attributes in this node to check if it has
+      // function attribute.
+      for (const auto& attr : node_def.attr()) {
+        if (attr.second.has_func()) {
+          result->insert(attr.second.func().name());
+        }
+      }
+    }
+  }
+  return tensorflow::Status::OK();
+}
+
+}  // namespace
+
 tensorflow::Status RegisterFunctionDefForSubgraphs(
-    Subgraph& main_subgraph, tensorflow::ResourceMgr* resource_mgr,
+    Subgraph& main_subgraph,
+    const std::function<tensorflow::Status(
+        const std::vector<std::unique_ptr<Subgraph>>&, std::set<std::string>*)>&
+        select_subgraphs_to_register,
+    tensorflow::ResourceMgr* resource_mgr,
     tensorflow::EagerContext* eager_context) {
   std::vector<std::unique_ptr<Subgraph>>* subgraphs =
       main_subgraph.GetSubgraphs();
   if (!subgraphs) {
     return tensorflow::Status(tensorflow::error::INTERNAL, "subgraphs is null");
   }
+  std::set<std::string> function_subgraphs;
+  TF_RETURN_IF_ERROR(
+      select_subgraphs_to_register(*subgraphs, &function_subgraphs));
   for (int i = 0; i < subgraphs->size(); ++i) {
     if (subgraphs->at(i)->GetName() == "main") {
       continue;
     }
-    // TODO(b/181352924): Find a way to only add function def used by dataset
-    // ops.
     const std::string subgraph_name = subgraphs->at(i)->GetName();
+    if (!function_subgraphs.count(subgraph_name)) {
+      continue;
+    }
+    // This is to ensure that we only register FunctionDefs for subgraphs that
+    // are used by TF ops to invoke functions.
     auto* subgraph_resource = new TFLiteSubgraphResource(*(subgraphs->at(i)));
     TF_RETURN_IF_ERROR(resource_mgr->Create<TFLiteSubgraphResource>(
         "flex", subgraph_name, subgraph_resource));
@@ -126,8 +180,6 @@ tensorflow::Status RegisterFunctionDefForSubgraphs(
   return tensorflow::Status::OK();
 }
 
-}  // namespace
-
 DelegateData::DelegateData() {}
 
 DelegateData::~DelegateData() {
@@ -164,8 +216,8 @@ tensorflow::Status DelegateData::Prepare(
 
   if (main_subgraph) {
     TF_RETURN_IF_ERROR(RegisterFunctionDefForSubgraphs(
-        *main_subgraph, eager_context_->HostCPU()->resource_manager(),
-        eager_context_));
+        *main_subgraph, GetSubgraphNamesForFunctionExecution,
+        eager_context_->HostCPU()->resource_manager(), eager_context_));
   }
   return tensorflow::Status();
 }
diff --git a/tensorflow/lite/delegates/flex/delegate_data.h b/tensorflow/lite/delegates/flex/delegate_data.h
index f0aef21486b..0219257df72 100644
--- a/tensorflow/lite/delegates/flex/delegate_data.h
+++ b/tensorflow/lite/delegates/flex/delegate_data.h
@@ -60,6 +60,22 @@ class DelegateData {
   std::unordered_map<const TfLiteContext*, BufferMap> buffer_map_;
 };
 
+// Creates a `TFLiteSubgraphResource` for each subgraph (execpt
+// for main subgraph) in the model and adds it in the eager context's resource
+// manager. It also registers FunctionDefs in the function library runtime for
+// subgraphs which are used by a list of flex ops.
+// TODO(b/181352924): For now, the `GetSubgraphs` API will return all the
+// subgraphs in the tflite model, so that it ensures we won't miss any subgraphs
+// during function registration. We need to revisit here when we introduce
+// multi-signature support.
+tensorflow::Status RegisterFunctionDefForSubgraphs(
+    Subgraph& main_subgraph,
+    const std::function<tensorflow::Status(
+        const std::vector<std::unique_ptr<Subgraph>>&,
+        std::set<std::string>* result)>& select_subgraphs_to_register,
+    tensorflow::ResourceMgr* resource_mgr,
+    tensorflow::EagerContext* eager_context);
+
 }  // namespace flex
 }  // namespace tflite
 
diff --git a/tensorflow/lite/delegates/flex/delegate_data_test.cc b/tensorflow/lite/delegates/flex/delegate_data_test.cc
index f17c856dcdd..501f424e115 100644
--- a/tensorflow/lite/delegates/flex/delegate_data_test.cc
+++ b/tensorflow/lite/delegates/flex/delegate_data_test.cc
@@ -23,7 +23,9 @@ limitations under the License.
 #include "absl/memory/memory.h"
 #include "absl/strings/string_view.h"
 #include "tensorflow/core/common_runtime/eager/context.h"
+#include "tensorflow/core/lib/core/status_test_util.h"
 #include "tensorflow/core/platform/protobuf.h"
+#include "tensorflow/core/platform/status.h"
 #include "tensorflow/lite/c/common.h"
 #include "tensorflow/lite/core/api/error_reporter.h"
 #include "tensorflow/lite/core/subgraph.h"
@@ -55,9 +57,21 @@ TEST(DelegateDataTest, Basic) {
 }
 
 TEST(DelegateDataTest, CheckFunctionDef) {
-  DelegateData data;
-  tensorflow::SessionOptions session_options;
-  session_options.config.set_intra_op_parallelism_threads(2);
+  tensorflow::StaticDeviceMgr device_mgr(tensorflow::DeviceFactory::NewDevice(
+      "CPU", {}, "/job:localhost/replica:0/task:0/device:CPU:0"));
+  tensorflow::EagerContext* eager_context = new tensorflow::EagerContext(
+      tensorflow::SessionOptions(),
+      tensorflow::ContextDevicePlacementPolicy::DEVICE_PLACEMENT_SILENT,
+      /*async=*/false, &device_mgr, /*device_mgr_owned*/ false, nullptr,
+      nullptr);
+
+  auto select_subgraphs_to_register =
+      [](const std::vector<std::unique_ptr<Subgraph>>& subgraphs,
+         std::set<std::string>* result) {
+        result->insert("add_subgraph");
+        result->insert("mul_subgraph");
+        return tensorflow::Status::OK();
+      };
 
   // Builds a TF Lite primary graph with two subgraphs.
   subgraph_test_util::SubgraphBuilder builder;
@@ -78,17 +92,9 @@ TEST(DelegateDataTest, CheckFunctionDef) {
   subgraphs.push_back(std::move(mul_subgraph));
   Subgraph main_subgraph(error_reporter.get(), nullptr, &subgraphs, nullptr);
   main_subgraph.SetName("main");
-
-  ASSERT_TRUE(data.Prepare(session_options, &main_subgraph).ok());
-
-  TfLiteContext dummy_context1 = {};
-  TfLiteContext dummy_context2 = {};
-  ASSERT_NE(data.GetEagerContext(), nullptr);
-  EXPECT_NE(data.GetBufferMap(&dummy_context1), nullptr);
-  EXPECT_NE(data.GetBufferMap(&dummy_context1),
-            data.GetBufferMap(&dummy_context2));
-
-  tensorflow::EagerContext* eager_context = data.GetEagerContext();
+  TF_ASSERT_OK(RegisterFunctionDefForSubgraphs(
+      main_subgraph, select_subgraphs_to_register,
+      eager_context->HostCPU()->resource_manager(), eager_context));
 
   const string add_fdef_txt = R"pb(
     signature {
@@ -186,6 +192,8 @@ TEST(DelegateDataTest, CheckFunctionDef) {
       *(eager_context->GetFunctionDef("add_subgraph")), add_fdef));
   EXPECT_TRUE(MessageDifferencer::Equals(
       *(eager_context->GetFunctionDef("mul_subgraph")), mul_fdef));
+
+  eager_context->Unref();
 }
 
 }  // namespace

commit 706350f02372cbed8ee9dae04aabbc4a5da84760
Author: Andrew Audibert <aaudibert@google.com>
Date:   Wed Jan 13 11:15:20 2021 -0800

    [tf.data service] Implement round-robin reads.
    
    This enables a new mode of reading from the tf.data service, where consumers read from tasks in a coordinated fashion, instead of the normal first-come first-served.
    
    The main use case for this is coordinated bucketization for synchronous training, where we want to ensure that at each step consumers get batches with elements of similar sizes. This mitigates the inefficiency of some consumers slowly training on large examples while others quickly train on small examples, then block waiting for the slower examples to be processed.
    
    When `consumer_index` and `num_consumers` are specified to `distribute`, each task will enforce a strict round-robin order, where its first element goes to consumer 0, second element to consumer 1, and so on. This requires that all consumers consume the same number of elements.
    
    PiperOrigin-RevId: 351625063
    Change-Id: I9b400f55ad61406cb125af8225096e7ff5dc4b0c

diff --git a/RELEASE.md b/RELEASE.md
index 650f94f3e9f..75b0090810c 100644
--- a/RELEASE.md
+++ b/RELEASE.md
@@ -25,6 +25,11 @@
     gathered at runtime to be used in embedding layer partitioning decisions.
 * `tf.keras.metrics.AUC` now support logit predictions.
 * Creating `tf.random.Generator` under `tf.distribute.Strategy` scopes is now allowed (except for `tf.distribute.experimental.CentralStorageStrategy` and `tf.distribute.experimental.ParameterServerStrategy`). Different replicas will get different random-number streams.
+* `tf.data`:
+    *   tf.data service now supports strict round-robin reads, which is useful
+        for synchronous training workloads where example sizes vary. With strict
+        round robin reads, users can guarantee that consumers get similar-sized
+        examples in the same step.
 
 ## Bug Fixes and Other Changes
 
diff --git a/tensorflow/core/api_def/base_api/api_def_DataServiceDataset.pbtxt b/tensorflow/core/api_def/base_api/api_def_DataServiceDataset.pbtxt
index 3801878cd71..a04f1e830c4 100644
--- a/tensorflow/core/api_def/base_api/api_def_DataServiceDataset.pbtxt
+++ b/tensorflow/core/api_def/base_api/api_def_DataServiceDataset.pbtxt
@@ -1,3 +1,5 @@
 op {
   graph_op_name: "DataServiceDataset"
+  visibility: HIDDEN
+  summary: "Creates a dataset that reads data from the tf.data service."
 }
diff --git a/tensorflow/core/api_def/base_api/api_def_DistributeDataset.pbtxt b/tensorflow/core/api_def/base_api/api_def_DataServiceDatasetV2.pbtxt
similarity index 71%
rename from tensorflow/core/api_def/base_api/api_def_DistributeDataset.pbtxt
rename to tensorflow/core/api_def/base_api/api_def_DataServiceDatasetV2.pbtxt
index a04f1e830c4..57aa8777939 100644
--- a/tensorflow/core/api_def/base_api/api_def_DistributeDataset.pbtxt
+++ b/tensorflow/core/api_def/base_api/api_def_DataServiceDatasetV2.pbtxt
@@ -1,5 +1,5 @@
 op {
-  graph_op_name: "DataServiceDataset"
+  graph_op_name: "DataServiceDatasetV2"
   visibility: HIDDEN
   summary: "Creates a dataset that reads data from the tf.data service."
 }
diff --git a/tensorflow/core/data/service/BUILD b/tensorflow/core/data/service/BUILD
index c1cb8589e11..eec9745b198 100644
--- a/tensorflow/core/data/service/BUILD
+++ b/tensorflow/core/data/service/BUILD
@@ -87,14 +87,10 @@ cc_library(
     deps = [
         ":credentials_factory",
         ":dispatcher_cc_grpc_proto",
-        ":dispatcher_proto_cc",
         ":grpc_util",
         ":worker_cc_grpc_proto",
-        ":worker_proto_cc",
         "//tensorflow/core:framework",
-        "//tensorflow/core:lib",
-        "//tensorflow/core:lib_internal",
-        "//tensorflow/core:protos_all_cc",
+        "@com_google_absl//absl/types:optional",
         tf_grpc_cc_dependency(),
     ],
 )
@@ -379,8 +375,11 @@ cc_library(
     hdrs = ["task_runner.h"],
     deps = [
         ":common_proto_cc",
+        "//tensorflow/core:framework",
         "//tensorflow/core:lib",
+        "//tensorflow/core/data:compression_utils",
         "//tensorflow/core/data:standalone",
+        "//tensorflow/core/kernels/data:dataset_utils",
     ],
 )
 
@@ -389,11 +388,11 @@ tf_cc_test(
     srcs = ["task_runner_test.cc"],
     deps = [
         ":task_runner",
+        "//tensorflow/core:framework",
         "//tensorflow/core:lib",
         "//tensorflow/core:test",
         "//tensorflow/core:test_main",
         "//tensorflow/core:testlib",
-        "//tensorflow/core/data:standalone",
         "@com_google_absl//absl/memory",
     ],
 )
diff --git a/tensorflow/core/data/service/data_service.cc b/tensorflow/core/data/service/data_service.cc
index 680edcce33a..78435cba422 100644
--- a/tensorflow/core/data/service/data_service.cc
+++ b/tensorflow/core/data/service/data_service.cc
@@ -17,6 +17,7 @@ limitations under the License.
 
 #include "grpcpp/create_channel.h"
 #include "grpcpp/security/credentials.h"
+#include "absl/types/optional.h"
 #include "tensorflow/core/data/service/credentials_factory.h"
 #include "tensorflow/core/data/service/dispatcher.grpc.pb.h"
 #include "tensorflow/core/data/service/grpc_util.h"
@@ -150,7 +151,8 @@ Status DataServiceDispatcherClient::RegisterDataset(GraphDef dataset,
 
 Status DataServiceDispatcherClient::GetOrCreateJob(
     int64 dataset_id, ProcessingMode processing_mode,
-    const absl::optional<JobKey>& job_key, int64& job_client_id) {
+    const absl::optional<JobKey>& job_key, absl::optional<int64> num_consumers,
+    int64& job_client_id) {
   TF_RETURN_IF_ERROR(EnsureInitialized());
   GetOrCreateJobRequest req;
   req.set_dataset_id(dataset_id);
@@ -158,6 +160,9 @@ Status DataServiceDispatcherClient::GetOrCreateJob(
   if (job_key.has_value()) {
     *req.mutable_job_key() = job_key.value();
   }
+  if (num_consumers.has_value()) {
+    req.set_num_consumers(num_consumers.value());
+  }
   GetOrCreateJobResponse resp;
   grpc::ClientContext client_ctx;
   grpc::Status status = stub_->GetOrCreateJob(&client_ctx, req, &resp);
@@ -239,11 +244,19 @@ Status DataServiceDispatcherClient::EnsureInitialized() {
 }
 
 Status DataServiceWorkerClient::GetElement(int64 task_id,
+                                           absl::optional<int64> consumer_index,
+                                           absl::optional<int64> round_index,
                                            CompressedElement& element,
                                            bool& end_of_sequence) {
   TF_RETURN_IF_ERROR(EnsureInitialized());
   GetElementRequest req;
   req.set_task_id(task_id);
+  if (consumer_index.has_value()) {
+    req.set_consumer_index(consumer_index.value());
+  }
+  if (round_index.has_value()) {
+    req.set_round_index(round_index.value());
+  }
   GetElementResponse resp;
   grpc::ClientContext ctx;
   grpc::Status s = stub_->GetElement(&ctx, req, &resp);
diff --git a/tensorflow/core/data/service/data_service.h b/tensorflow/core/data/service/data_service.h
index 935e549b303..3bd135ef729 100644
--- a/tensorflow/core/data/service/data_service.h
+++ b/tensorflow/core/data/service/data_service.h
@@ -100,11 +100,12 @@ class DataServiceDispatcherClient : public DataServiceClientBase {
   // dataset id in `dataset_id`.
   Status RegisterDataset(GraphDef dataset, int64& dataset_id);
 
-  // Gets the job id for the job represented by the tuple
-  // (job_name, job_name_index), and stores the id in `job_client_id`. If the
-  // job doesn't exist yet, it will be created.
+  // If `job_key` is set, looks up a job matching `job_key`. If `job_key` is
+  // absent or no matching job is found, creates a new job. The resulting job
+  // id is stored in `job_client_id`.
   Status GetOrCreateJob(int64 dataset_id, ProcessingMode processing_mode,
                         const absl::optional<JobKey>& job_key,
+                        absl::optional<int64> num_consumers,
                         int64& job_client_id);
 
   // Releases a job client id, indicating that the id will no longer be used to
@@ -138,11 +139,14 @@ class DataServiceWorkerClient : public DataServiceClientBase {
                           const std::string& protocol)
       : DataServiceClientBase(address, protocol) {}
 
-  // Fetches the next element for the specified task_id. The element's
-  // compressed tensors will be stored in `element`. If no element is available,
-  // `end_of_sequence` will be `true`, and `element` will be left unchanged.
-  Status GetElement(int64 task_id, CompressedElement& element,
-                    bool& end_of_sequence);
+  // Fetches the next element for the specified task_id. The optional
+  // `consumer_index` and `round_index` must be specified for tasks which use
+  // round-robin ordering. The element's compressed tensors will be stored in
+  // `element`. If no element is available, `end_of_sequence` will be `true`,
+  // and `element` will be left unchanged.
+  Status GetElement(int64 task_id, absl::optional<int64> consumer_index,
+                    absl::optional<int64> round_index,
+                    CompressedElement& element, bool& end_of_sequence);
 
  protected:
   Status EnsureInitialized() override;
diff --git a/tensorflow/core/data/service/dispatcher_impl.cc b/tensorflow/core/data/service/dispatcher_impl.cc
index 5f242438922..6cd68fe5c72 100644
--- a/tensorflow/core/data/service/dispatcher_impl.cc
+++ b/tensorflow/core/data/service/dispatcher_impl.cc
@@ -533,7 +533,11 @@ Status DataServiceDispatcherImpl::CreateTasksForWorker(
     const std::string& worker_address) TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
   std::vector<std::shared_ptr<const Job>> jobs = state_.ListJobs();
   for (const auto& job : jobs) {
-    if (job->finished) {
+    if (job->finished || job->num_consumers.has_value()) {
+      // Don't add new tasks for late-joining workers when doing round-robin
+      // reads. It would create synchronization issues where some clients might
+      // learn about the new tasks earlier than others, potentially causing
+      // deadlock.
       continue;
     }
     std::shared_ptr<const Task> task;
@@ -646,6 +650,9 @@ Status DataServiceDispatcherImpl::AssignTask(std::shared_ptr<const Task> task)
   }
   task_def->set_task_id(task->task_id);
   task_def->set_processing_mode(ProcessingModeDef(task->job->processing_mode));
+  if (task->job->num_consumers.has_value()) {
+    task_def->set_num_consumers(task->job->num_consumers.value());
+  }
   ProcessTaskResponse resp;
   WorkerService::Stub* stub;
   TF_RETURN_IF_ERROR(GetOrCreateWorkerStub(task->worker_address, stub));
diff --git a/tensorflow/core/data/service/task_runner.cc b/tensorflow/core/data/service/task_runner.cc
index 6eacc16d59c..dc99e19d232 100644
--- a/tensorflow/core/data/service/task_runner.cc
+++ b/tensorflow/core/data/service/task_runner.cc
@@ -15,12 +15,21 @@ limitations under the License.
 
 #include "tensorflow/core/data/service/task_runner.h"
 
+#include "tensorflow/core/data/compression_utils.h"
 #include "tensorflow/core/data/standalone.h"
+#include "tensorflow/core/framework/dataset.h"
+#include "tensorflow/core/framework/tensor_util.h"
 #include "tensorflow/core/lib/gtl/cleanup.h"
 #include "tensorflow/core/platform/errors.h"
 
 namespace tensorflow {
 namespace data {
+namespace {
+// How long to wait for other round-robin consumers before returning with an
+// Unavailable error. The unavailable error gives the client an opportunity to
+// either give up or retry to continue waiting.
+const int64 kDefaultTimeoutUs = 2 * 1000 * 1000;  // 2 seconds.
+}  // namespace
 
 StandaloneTaskIterator::StandaloneTaskIterator(
     std::unique_ptr<standalone::Dataset> dataset,
@@ -32,12 +41,25 @@ Status StandaloneTaskIterator::GetNext(std::vector<Tensor>& element,
   return iterator_->GetNext(&element, &end_of_sequence);
 }
 
+int64 StandaloneTaskIterator::Cardinality() const {
+  return dataset_->Get()->Cardinality();
+}
+
 Status TaskRunner::Create(const TaskDef& task_def,
                           std::unique_ptr<TaskIterator> iterator,
                           std::unique_ptr<TaskRunner>& out) {
   if (task_def.optional_num_consumers_case() == TaskDef::kNumConsumers) {
-    out = absl::make_unique<RoundRobinTaskRunner>(std::move(iterator),
-                                                  task_def.num_consumers());
+    int64 cardinality = iterator->Cardinality();
+    if (cardinality != kInfiniteCardinality &&
+        cardinality != kUnknownCardinality) {
+      return errors::FailedPrecondition(
+          "Round robin reads require that the input dataset has infinite "
+          "cardinality, but the dataset has cardinality ",
+          cardinality,
+          ". Consider adding a `.repeat()` transformation to the dataset.");
+    }
+    out = absl::make_unique<RoundRobinTaskRunner>(
+        std::move(iterator), task_def.num_consumers(), kDefaultTimeoutUs);
   } else {
     out =
         absl::make_unique<FirstComeFirstServedTaskRunner>(std::move(iterator));
@@ -56,10 +78,15 @@ Status FirstComeFirstServedTaskRunner::GetNext(const Request& request,
 }
 
 RoundRobinTaskRunner::RoundRobinTaskRunner(
-    std::unique_ptr<TaskIterator> iterator, int64 num_consumers)
+    std::unique_ptr<TaskIterator> iterator, int64 num_consumers,
+    int64 timeout_us)
     : num_consumers_(num_consumers),
+      timeout_us_(timeout_us),
       iterator_(std::move(iterator)),
-      buffer_(num_consumers_) {}
+      buffer_(num_consumers_) {
+  VLOG(1) << "Creating task runner for distributing data round-robin to "
+          << num_consumers << " consumers";
+}
 
 Status RoundRobinTaskRunner::GetNext(const Request& request,
                                      std::vector<Tensor>& element,
@@ -74,12 +101,15 @@ Status RoundRobinTaskRunner::GetNext(const Request& request,
         "Requesting data for consumer index ", request.consumer_index,
         ", but the task is configured for only ", num_consumers_, " consumers");
   }
+  VLOG(2) << "Received request from consumer index " << request.consumer_index
+          << " for round " << request.round_index;
 
   mutex_lock l(mu_);
   absl::flat_hash_set<int64>& round = requests_[request.round_index];
   first_round_ = std::min(first_round_, request.round_index);
   round.insert(request.consumer_index);
   if (current_round_ < request.round_index && round.size() == num_consumers_) {
+    VLOG(1) << "Starting normal round with round index " << request.round_index;
     // This was the last request to arrive, time to start a new round.
     TF_RETURN_IF_ERROR(FillBuffer());
     current_round_ = request.round_index;
@@ -88,39 +118,44 @@ Status RoundRobinTaskRunner::GetNext(const Request& request,
   if (current_round_ < 0 &&
       requests_[first_round_].size() + requests_[first_round_ + 1].size() ==
           num_consumers_) {
+    VLOG(1) << "Starting partial round for " << requests_[first_round_].size()
+            << " consumers";
     // Indicates that we need a partial round to get consumers back in sync.
     TF_RETURN_IF_ERROR(FillBuffer());
     current_round_ = first_round_;
     new_round_cv_.notify_all();
   }
   while (current_round_ < request.round_index) {
-    new_round_cv_.wait(l);
+    std::cv_status s =
+        new_round_cv_.wait_for(l, std::chrono::microseconds(timeout_us_));
+    if (s == std::cv_status::timeout) {
+      // Clients will retry Unavailable.
+      return errors::Unavailable(
+          "Timeout waiting for other round-robin consumers to be ready.");
+    }
   }
-  Result& result = buffer_[request.consumer_index];
-  end_of_task = result.end_of_task;
+  end_of_task = end_of_task_;
   if (!end_of_task) {
-    element = std::move(result.element);
+    element.clear();
+    for (auto& component : buffer_[request.consumer_index]) {
+      element.push_back(tensor::DeepCopy(component));
+    }
   }
+  VLOG(2) << "Returning to consumer " << request.consumer_index << " for round "
+          << request.round_index;
   return Status::OK();
 }
 
 Status RoundRobinTaskRunner::FillBuffer() TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
   for (int i = 0; i < num_consumers_; ++i) {
-    Result& result = buffer_[i];
-    result.element.clear();
-    TF_RETURN_IF_ERROR(iterator_->GetNext(result.element, result.end_of_task));
-    if (buffer_[i].end_of_task && !buffer_[0].end_of_task) {
-      std::vector<Tensor>& first_element = buffer_[0].element;
-      // Pad out the round with empty elements.
-      buffer_[i].element.clear();
-      for (int c = 0; c < first_element.size(); ++c) {
-        TensorShape shape = first_element[c].shape();
-        if (shape.dims() > 0) {
-          shape.set_dim(0, 0);
-        }
-        buffer_[i].element.push_back(Tensor(first_element[c].dtype(), shape));
-      }
-      buffer_[i].end_of_task = false;
+    buffer_[i].clear();
+    bool end_of_sequence;
+    TF_RETURN_IF_ERROR(iterator_->GetNext(buffer_[i], end_of_sequence));
+    if (end_of_sequence) {
+      return errors::FailedPrecondition(
+          "Encountered end of sequence on a round-robin read iterator. Please "
+          "ensure that the dataset used for round-robin reading has infinite "
+          "cardinality, e.g. by adding a .repeat() transformation at the end.");
     }
   }
   return Status::OK();
diff --git a/tensorflow/core/data/service/task_runner.h b/tensorflow/core/data/service/task_runner.h
index 940da113d30..2a791238949 100644
--- a/tensorflow/core/data/service/task_runner.h
+++ b/tensorflow/core/data/service/task_runner.h
@@ -31,6 +31,8 @@ class TaskIterator {
   // `end_of_sequence to `true`.
   virtual Status GetNext(std::vector<Tensor>& element,
                          bool& end_of_sequence) = 0;
+  // Reports the cardinality of the dataset that created this iterator.
+  virtual int64 Cardinality() const = 0;
 };
 
 // Implementation of TaskIterator wrapping a standalone iterator.
@@ -42,6 +44,7 @@ class StandaloneTaskIterator : public TaskIterator {
   StandaloneTaskIterator(std::unique_ptr<standalone::Dataset> dataset,
                          std::unique_ptr<standalone::Iterator> iterator);
   Status GetNext(std::vector<Tensor>& element, bool& end_of_sequence) override;
+  int64 Cardinality() const override;
 
  private:
   std::unique_ptr<standalone::Dataset> dataset_;
@@ -109,19 +112,16 @@ class FirstComeFirstServedTaskRunner : public TaskRunner {
 class RoundRobinTaskRunner : public TaskRunner {
  public:
   RoundRobinTaskRunner(std::unique_ptr<TaskIterator> iterator,
-                       int64 num_consumers);
+                       int64 num_consumers, int64 timeout_us);
   Status GetNext(const Request& request, std::vector<Tensor>& element,
                  bool& end_of_task) override;
 
  private:
-  struct Result {
-    std::vector<Tensor> element;
-    bool end_of_task = false;
-  };
   // Fills `buffer_` with `num_consumers_` elements.
   Status FillBuffer();
 
   const int64 num_consumers_;
+  const int64 timeout_us_;
   std::unique_ptr<TaskIterator> iterator_;
   mutex mu_;
   // Condition variable notified whenever we start a new round of round-robin.
@@ -134,7 +134,8 @@ class RoundRobinTaskRunner : public TaskRunner {
   int64 first_round_ TF_GUARDED_BY(mu_) = kint64max;
   int64 current_round_ TF_GUARDED_BY(mu_) = -1;
   // Buffered results for the current round.
-  std::vector<Result> buffer_ TF_GUARDED_BY(mu_);
+  std::vector<std::vector<Tensor>> buffer_ TF_GUARDED_BY(mu_);
+  bool end_of_task_ TF_GUARDED_BY(mu_) = false;
 };
 
 }  // namespace data
diff --git a/tensorflow/core/data/service/task_runner_test.cc b/tensorflow/core/data/service/task_runner_test.cc
index 558bc585b65..4e454d9306e 100644
--- a/tensorflow/core/data/service/task_runner_test.cc
+++ b/tensorflow/core/data/service/task_runner_test.cc
@@ -13,13 +13,17 @@ limitations under the License.
 #include "tensorflow/core/data/service/task_runner.h"
 
 #include "absl/memory/memory.h"
+#include "tensorflow/core/framework/dataset.h"
 #include "tensorflow/core/framework/tensor_testutil.h"
 #include "tensorflow/core/lib/core/status_test_util.h"
+#include "tensorflow/core/platform/errors.h"
 #include "tensorflow/core/platform/test.h"
 
 namespace tensorflow {
 namespace data {
 namespace {
+const int64 kNoTimeoutUs = 60ull * 60 * 1000 * 1000;  // 60 minutes.
+
 class TestTaskIterator : public TaskIterator {
  public:
   explicit TestTaskIterator(const std::vector<std::vector<Tensor>>& elements)
@@ -28,30 +32,31 @@ class TestTaskIterator : public TaskIterator {
   Status GetNext(std::vector<Tensor>& element, bool& end_of_sequence) override {
     end_of_sequence = index_ >= elements_.size();
     if (!end_of_sequence) {
-      element = elements_[index_++];
+      element = elements_[index_];
+      index_ = (index_ + 1) % elements_.size();
     }
     return Status::OK();
   }
 
+  int64 Cardinality() const override { return kInfiniteCardinality; }
+
  private:
   std::vector<std::vector<Tensor>> elements_;
   int64 index_;
 };
 
 // Reads from the task runner, storing results in `*output`.
-Status RunConsumer(int64 consumer_index, int64 start_index,
-                   TaskRunner& task_runner,
-                   std::vector<std::vector<Tensor>>& output) {
+Status RunConsumer(int64 consumer_index, int64 start_index, int64 end_index,
+                   TaskRunner& task_runner, std::vector<int64>& output) {
   bool end_of_sequence = false;
-  int64 next_index = start_index;
-  while (!end_of_sequence) {
+  for (int64 next_index = start_index; next_index < end_index; ++next_index) {
     TaskRunner::Request request;
-    request.round_index = next_index++;
+    request.round_index = next_index;
     request.consumer_index = consumer_index;
     std::vector<Tensor> element;
     TF_RETURN_IF_ERROR(task_runner.GetNext(request, element, end_of_sequence));
     if (!end_of_sequence) {
-      output.push_back(element);
+      output.push_back(element[0].flat<int64>()(0));
     }
   }
   return Status::OK();
@@ -76,12 +81,6 @@ TEST(FirstComeFirstServedTaskRunner, GetNext) {
     ASSERT_EQ(element.size(), 1);
     test::ExpectEqual(element[0], expected_element[0]);
   }
-  for (int i = 0; i < 2; ++i) {
-    std::vector<Tensor> element;
-    bool end_of_sequence;
-    TF_ASSERT_OK(runner.GetNext(request, element, end_of_sequence));
-    ASSERT_TRUE(end_of_sequence);
-  }
 }
 
 class ConsumeParallelTest
@@ -98,8 +97,8 @@ TEST_P(ConsumeParallelTest, ConsumeParallel) {
     elements.push_back(element);
   }
   RoundRobinTaskRunner runner(absl::make_unique<TestTaskIterator>(elements),
-                              num_consumers);
-  std::vector<std::vector<std::vector<Tensor>>> per_consumer_results;
+                              num_consumers, kNoTimeoutUs);
+  std::vector<std::vector<int64>> per_consumer_results;
   std::vector<std::unique_ptr<Thread>> consumers;
   mutex mu;
   Status error;
@@ -108,8 +107,9 @@ TEST_P(ConsumeParallelTest, ConsumeParallel) {
     per_consumer_results.emplace_back();
     consumers.push_back(absl::WrapUnique(Env::Default()->StartThread(
         {}, absl::StrCat("consumer_", consumer), [&, consumer] {
-          std::vector<std::vector<Tensor>> results;
-          Status s = RunConsumer(consumer, /*start_index=*/0, runner, results);
+          std::vector<int64> results;
+          Status s = RunConsumer(consumer, /*start_index=*/0,
+                                 /*end_index=*/num_elements, runner, results);
           mutex_lock l(mu);
           if (!s.ok()) {
             error = s;
@@ -125,8 +125,7 @@ TEST_P(ConsumeParallelTest, ConsumeParallel) {
   for (int i = 0; i < num_elements; ++i) {
     int consumer = i % num_consumers;
     int round = i / num_consumers;
-    Tensor expected = elements[i][0];
-    test::ExpectEqual(per_consumer_results[consumer][round][0], expected);
+    EXPECT_EQ(per_consumer_results[consumer][round], i);
   }
 }
 
@@ -139,19 +138,20 @@ INSTANTIATE_TEST_SUITE_P(ConsumeParallelTests, ConsumeParallelTest,
                                            std::make_tuple(0, 20)));
 
 TEST(RoundRobinTaskRunner, ConsumeParallelPartialRound) {
-  int64 num_elements = 20;
   int64 num_consumers = 5;
   std::vector<int64> starting_rounds = {12, 11, 11, 12, 12};
-  int64 min_starting_round = 11;
+  int64 end_index = 15;
+  std::vector<std::vector<int64>> expected_consumer_results = {
+      {5, 10, 15}, {1, 6, 11, 16}, {2, 7, 12, 17}, {8, 13, 18}, {9, 14, 19}};
   std::vector<std::vector<Tensor>> elements;
-  for (int64 i = 0; i < num_elements; ++i) {
+  for (int64 i = 0; i < 30; ++i) {
     std::vector<Tensor> element;
     element.push_back(Tensor(i));
     elements.push_back(element);
   }
   RoundRobinTaskRunner runner(absl::make_unique<TestTaskIterator>(elements),
-                              num_consumers);
-  std::vector<std::vector<std::vector<Tensor>>> per_consumer_results;
+                              num_consumers, kNoTimeoutUs);
+  std::vector<std::vector<int64>> per_consumer_results;
   std::vector<std::unique_ptr<Thread>> consumers;
   mutex mu;
   Status error;
@@ -160,9 +160,9 @@ TEST(RoundRobinTaskRunner, ConsumeParallelPartialRound) {
     per_consumer_results.emplace_back();
     consumers.push_back(absl::WrapUnique(Env::Default()->StartThread(
         {}, absl::StrCat("consumer_", consumer), [&, consumer] {
-          std::vector<std::vector<Tensor>> results;
-          Status s =
-              RunConsumer(consumer, starting_rounds[consumer], runner, results);
+          std::vector<int64> results;
+          Status s = RunConsumer(consumer, starting_rounds[consumer], end_index,
+                                 runner, results);
           mutex_lock l(mu);
           if (!s.ok()) {
             error = s;
@@ -176,19 +176,8 @@ TEST(RoundRobinTaskRunner, ConsumeParallelPartialRound) {
   mutex_lock l(mu);
   TF_ASSERT_OK(error);
   for (int consumer = 0; consumer < num_consumers; ++consumer) {
-    auto& results = per_consumer_results[consumer];
-    int start = consumer;
-    int expected_elements = num_elements / num_consumers;
-    if (starting_rounds[consumer] != min_starting_round) {
-      start += num_consumers;
-      expected_elements--;
-    }
-    ASSERT_EQ(results.size(), expected_elements);
-    int index = 0;
-    for (int i = start; i < num_elements; i += num_consumers) {
-      Tensor expected = elements[i][0];
-      test::ExpectEqual(results[index++][0], expected);
-    }
+    EXPECT_EQ(per_consumer_results[consumer],
+              expected_consumer_results[consumer]);
   }
 }
 }  // namespace data
diff --git a/tensorflow/core/data/service/worker_impl.cc b/tensorflow/core/data/service/worker_impl.cc
index 2f81d0d5af0..5f1aa2930b2 100644
--- a/tensorflow/core/data/service/worker_impl.cc
+++ b/tensorflow/core/data/service/worker_impl.cc
@@ -180,6 +180,7 @@ Status DataServiceWorkerImpl::GetElement(const GetElementRequest* request,
   VLOG(3) << "Received GetElement request for task " << request->task_id();
   bool end_of_sequence = false;
   std::vector<tensorflow::Tensor> outputs;
+  Task* task;
   {
     mutex_lock l(mu_);
     if (!registered_) {
@@ -201,24 +202,24 @@ Status DataServiceWorkerImpl::GetElement(const GetElementRequest* request,
         return errors::Unavailable("Task ", request->task_id(), " not found");
       }
     }
-    auto& task = it->second;
+    task = it->second.get();
     TF_RETURN_IF_ERROR(EnsureTaskInitialized(*task));
-    TaskRunner::Request get_next_request;
-    if (request->optional_consumer_index_case() ==
-        GetElementRequest::kConsumerIndex) {
-      get_next_request.consumer_index = request->consumer_index();
-    }
-    if (request->optional_round_index_case() ==
-        GetElementRequest::kRoundIndex) {
-      get_next_request.round_index = request->round_index();
-    }
-    TF_RETURN_IF_ERROR(
-        task->task_runner->GetNext(get_next_request, outputs, end_of_sequence));
-    if (end_of_sequence) {
-      VLOG(3) << "Reached end_of_sequence for task " << request->task_id();
-      pending_completed_tasks_.insert(request->task_id());
-      task_completion_cv_.notify_one();
-    }
+  }
+  TaskRunner::Request get_next_request;
+  if (request->optional_consumer_index_case() ==
+      GetElementRequest::kConsumerIndex) {
+    get_next_request.consumer_index = request->consumer_index();
+  }
+  if (request->optional_round_index_case() == GetElementRequest::kRoundIndex) {
+    get_next_request.round_index = request->round_index();
+  }
+  TF_RETURN_IF_ERROR(
+      task->task_runner->GetNext(get_next_request, outputs, end_of_sequence));
+  if (end_of_sequence) {
+    mutex_lock l(mu_);
+    VLOG(3) << "Reached end_of_sequence for task " << request->task_id();
+    pending_completed_tasks_.insert(request->task_id());
+    task_completion_cv_.notify_one();
   }
 
   if (!end_of_sequence) {
@@ -249,7 +250,7 @@ Status DataServiceWorkerImpl::GetElement(const GetElementRequest* request,
           "it produced ",
           variant.TypeName());
     }
-    compressed->Swap(response->mutable_compressed_element());
+    *response->mutable_compressed_element() = *compressed;
   }
   response->set_end_of_sequence(end_of_sequence);
 
diff --git a/tensorflow/core/data/standalone.cc b/tensorflow/core/data/standalone.cc
index 033b9fb6d58..efafe5e8aae 100644
--- a/tensorflow/core/data/standalone.cc
+++ b/tensorflow/core/data/standalone.cc
@@ -139,6 +139,8 @@ Status Dataset::MakeSplitProvider(std::unique_ptr<SplitProvider>* result) {
   return dataset_->MakeSplitProvider(result);
 }
 
+const DatasetBase* Dataset::Get() const { return dataset_; }
+
 Dataset::Dataset(DatasetBase* dataset, DeviceMgr* device_mgr,
                  ProcessFunctionLibraryRuntime* pflr,
                  FunctionLibraryDefinition* flib_def, thread::ThreadPool* pool)
diff --git a/tensorflow/core/data/standalone.h b/tensorflow/core/data/standalone.h
index f72f20b08e4..fbb41b3c8ac 100644
--- a/tensorflow/core/data/standalone.h
+++ b/tensorflow/core/data/standalone.h
@@ -104,6 +104,8 @@ class Dataset {
 
   // Creates a split provider for this dataset.
   Status MakeSplitProvider(std::unique_ptr<SplitProvider>* result);
+  // Returns a pointer to the underlying dataset.
+  const DatasetBase* Get() const;
 
  private:
   Dataset(DatasetBase* dataset, DeviceMgr* device_mgr,
diff --git a/tensorflow/core/kernels/data/experimental/data_service_dataset_op.cc b/tensorflow/core/kernels/data/experimental/data_service_dataset_op.cc
index 93c2f57033e..762289d2cba 100644
--- a/tensorflow/core/kernels/data/experimental/data_service_dataset_op.cc
+++ b/tensorflow/core/kernels/data/experimental/data_service_dataset_op.cc
@@ -51,6 +51,8 @@ namespace data {
 /* static */ constexpr const char* const DataServiceDatasetOp::kAddress;
 /* static */ constexpr const char* const DataServiceDatasetOp::kProtocol;
 /* static */ constexpr const char* const DataServiceDatasetOp::kJobName;
+/* static */ constexpr const char* const DataServiceDatasetOp::kConsumerIndex;
+/* static */ constexpr const char* const DataServiceDatasetOp::kNumConsumers;
 /* static */ constexpr const char* const
     DataServiceDatasetOp::kMaxOutstandingRequests;
 /* static */ constexpr const char* const
@@ -63,6 +65,9 @@ namespace data {
 namespace {
 // Default interval between task list refreshes.
 const int64 kDefaultTaskRefreshIntervalMs = 1000;  // 1 second.
+
+constexpr char kDataServiceDatasetV1[] = "DataServiceDataset";
+constexpr char kDataServiceDatasetV2[] = "DataServiceDatasetV2";
 }  // namespace
 
 // Dataset for reading data from the tf.data service non-deterministically.
@@ -72,20 +77,24 @@ const int64 kDefaultTaskRefreshIntervalMs = 1000;  // 1 second.
 // to read from (in case workers are added or removed).
 class DataServiceDatasetOp::Dataset : public DatasetBase {
  public:
-  Dataset(OpKernelContext* ctx, int64 dataset_id,
+  Dataset(OpKernelContext* ctx, int op_version, int64 dataset_id,
           ProcessingMode processing_mode, const std::string& address,
           const std::string& protocol, const std::string& job_name,
-          int64 max_outstanding_requests, int64 task_refresh_interval_ms,
-          IterationCounter* iteration_counter, bool owns_resource,
-          ResourceHandle iteration_counter_handle,
+          absl::optional<int64> consumer_index,
+          absl::optional<int64> num_consumers, int64 max_outstanding_requests,
+          int64 task_refresh_interval_ms, IterationCounter* iteration_counter,
+          bool owns_resource, ResourceHandle iteration_counter_handle,
           const DataTypeVector& output_types,
           const std::vector<PartialTensorShape>& output_shapes)
       : DatasetBase(DatasetContext(ctx)),
+        op_version_(op_version),
         dataset_id_(dataset_id),
         processing_mode_(processing_mode),
         address_(address),
         protocol_(protocol),
         job_name_(job_name),
+        consumer_index_(consumer_index),
+        num_consumers_(num_consumers),
         max_outstanding_requests_(max_outstanding_requests),
         task_refresh_interval_ms_(task_refresh_interval_ms),
         iteration_counter_(iteration_counter),
@@ -135,39 +144,58 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
   Status AsGraphDefInternal(SerializationContext* ctx,
                             DatasetGraphDefBuilder* b,
                             Node** output) const override {
+    std::vector<Node*> inputs;
+
     Node* dataset_id;
     TF_RETURN_IF_ERROR(b->AddScalar(dataset_id_, &dataset_id));
+    inputs.push_back(dataset_id);
 
     Node* processing_mode;
     tstring processing_mode_str = ProcessingModeToString(processing_mode_);
     TF_RETURN_IF_ERROR(b->AddScalar(processing_mode_str, &processing_mode));
+    inputs.push_back(processing_mode);
 
     Node* address;
     TF_RETURN_IF_ERROR(b->AddScalar(address_, &address));
+    inputs.push_back(address);
 
     Node* protocol;
     TF_RETURN_IF_ERROR(b->AddScalar(protocol_, &protocol));
+    inputs.push_back(protocol);
 
     Node* job_name;
     TF_RETURN_IF_ERROR(b->AddScalar(job_name_, &job_name));
+    inputs.push_back(job_name);
+
+    if (op_version_ == 2) {
+      Node* consumer_index;
+      TF_RETURN_IF_ERROR(
+          b->AddScalar(consumer_index_.value_or(-1), &consumer_index));
+      inputs.push_back(consumer_index);
+
+      Node* num_consumers;
+      TF_RETURN_IF_ERROR(
+          b->AddScalar(num_consumers_.value_or(-1), &num_consumers));
+      inputs.push_back(num_consumers);
+    }
 
     Node* max_outstanding_requests;
     TF_RETURN_IF_ERROR(
         b->AddScalar(max_outstanding_requests_, &max_outstanding_requests));
+    inputs.push_back(max_outstanding_requests);
 
     Node* iteration_counter_handle = nullptr;
     Tensor handle(DT_RESOURCE, TensorShape({}));
     handle.scalar<ResourceHandle>()() = iteration_counter_handle_;
     TF_RETURN_IF_ERROR(b->AddTensor(handle, &iteration_counter_handle));
+    inputs.push_back(iteration_counter_handle);
 
     AttrValue task_refresh_interval_hint_ms;
     b->BuildAttrValue(task_refresh_interval_ms_,
                       &task_refresh_interval_hint_ms);
 
     TF_RETURN_IF_ERROR(
-        b->AddDataset(this,
-                      {dataset_id, processing_mode, address, protocol, job_name,
-                       max_outstanding_requests, iteration_counter_handle},
+        b->AddDataset(this, inputs,
                       {std::make_pair(kTaskRefreshIntervalHintMs,
                                       task_refresh_interval_hint_ms)},
                       output));
@@ -205,7 +233,6 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
 
     void CancelThreads() TF_LOCKS_EXCLUDED(mu_) {
       mutex_lock l(mu_);
-      VLOG(1) << "Cancelling threads in DataServiceDataset::Iterator";
       cancelled_ = true;
       worker_thread_cv_.notify_all();
       manager_thread_cv_.notify_all();
@@ -229,9 +256,9 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
       }
       TF_RETURN_IF_ERROR(grpc_util::Retry(
           [&]() {
-            return dispatcher_->GetOrCreateJob(dataset()->dataset_id_,
-                                               dataset()->processing_mode_, key,
-                                               job_client_id_);
+            return dispatcher_->GetOrCreateJob(
+                dataset()->dataset_id_, dataset()->processing_mode_, key,
+                dataset()->num_consumers_, job_client_id_);
           },
           /*description=*/
           strings::StrCat("get or create job with dispatcher at ",
@@ -254,27 +281,38 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
             });
       }
 
-      while (results_.empty() &&
+      while ((results_.empty() || !results_.front().ready) &&
              !(job_finished_ && num_running_worker_threads_ == 0) &&
              !cancelled_ && status_.ok()) {
+        VLOG(3) << "Blocking in GetNext. results_.size():" << results_.size()
+                << " results_.front().ready:"
+                << (!results_.empty() && results_.front().ready)
+                << " job_finished_:" << job_finished_
+                << " num_running_worker_threads_:"
+                << num_running_worker_threads_;
         get_next_cv_.wait(l);
       }
       if (cancelled_) {
+        VLOG(3) << "Returning from GetNext due to cancellation";
         return errors::Cancelled("Data service iterator was cancelled");
       }
       if (!status_.ok()) {
+        VLOG(3) << "Returning from GetNext with error " << status_;
         return status_;
       }
       if (results_.empty()) {
         *end_of_sequence = true;
+        VLOG(3) << "Returning from GetNext with end_of_sequence";
         return Status::OK();
       }
-      DCHECK(!results_.empty());
-      *end_of_sequence = false;
-      out_tensors->swap(results_.front());
+      *end_of_sequence = results_.front().end_of_sequence;
+      if (!*end_of_sequence) {
+        out_tensors->swap(results_.front().element);
+      }
       results_.pop();
       worker_thread_cv_.notify_one();
 
+      VLOG(3) << "Returning from GetNext with an element";
       return Status::OK();
     }
 
@@ -326,12 +364,22 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
       const std::string address;
       // Client for fetching task elements from the tf.data service worker.
       const std::unique_ptr<DataServiceWorkerClient> worker;
+      // Number of elements read by the task.
+      int64 elements_read = 0;
       // Indicates whether a worker thread is currently processing the task.
       bool in_use TF_GUARDED_BY(&Iterator::mu_) = false;
       // Indicates whether the worker has returned end_of_sequence for the task.
       bool end_of_sequence TF_GUARDED_BY(&Iterator::mu_) = false;
     };
 
+    struct Result {
+      // Whether the result has been computed yet. GetNext needs to block
+      // until the next result is ready.
+      bool ready TF_GUARDED_BY(&Iterator::mu_) = false;
+      std::vector<Tensor> element TF_GUARDED_BY(&Iterator::mu_);
+      bool end_of_sequence TF_GUARDED_BY(&Iterator::mu_) = false;
+    };
+
     // Periodically refresh the task list.
     // Maintain one thread fetching elements for each task.
     // TODO(aaudibert): Instead of polling, have dispatcher send updates when
@@ -347,7 +395,7 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
           // All units are microseconds.
           while (!cancelled_ && Env::Default()->NowMicros() < next_check) {
             int64 remaining_time = next_check - Env::Default()->NowMicros();
-            VLOG(3) << "Task thread manager waiting for " << remaining_time
+            VLOG(4) << "Task thread manager waiting for " << remaining_time
                     << "us";
             manager_thread_cv_.wait_for(
                 l, std::chrono::microseconds(remaining_time));
@@ -365,7 +413,7 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
     }
 
     void UpdateTasks() TF_LOCKS_EXCLUDED(mu_) {
-      VLOG(3) << "Updating tasks";
+      VLOG(4) << "Updating tasks";
       std::vector<TaskInfo> tasks;
       bool job_finished;
       Status s = dispatcher_->GetTasks(job_client_id_, tasks, job_finished);
@@ -400,8 +448,12 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
           tasks_.pop_back();
         }
       }
-      for (auto& new_task_entry : task_id_to_task) {
-        TaskInfo& task_info = new_task_entry.second;
+      for (auto& task : tasks) {
+        auto it = task_id_to_task.find(task.task_id());
+        if (it == task_id_to_task.end()) {
+          continue;
+        }
+        TaskInfo& task_info = it->second;
         std::unique_ptr<DataServiceWorkerClient> worker;
         Status s = CreateDataServiceWorkerClient(task_info.worker_address(),
                                                  dataset()->protocol_, worker);
@@ -446,6 +498,7 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
       VLOG(1) << "Starting worker thread";
       std::shared_ptr<Task> task_to_process;
       while (true) {
+        Result* result;
         {
           mutex_lock l(mu_);
           if (task_to_process) {
@@ -454,7 +507,7 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
             worker_thread_cv_.notify_one();
           }
           outstanding_requests_--;
-          while (!cancelled_ && !(SpaceInBuffer() && TaskAvailable()) &&
+          while (!cancelled_ && !(ElementSpaceAvailable() && TaskAvailable()) &&
                  !job_finished_) {
             if (VLOG_IS_ON(3)) {
               VLOG(3) << "Sleeping with results_.size=" << results_.size()
@@ -470,23 +523,40 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
           if (cancelled_ || job_finished_) {
             return;
           }
-          // Search for a task to update.
-          int num_tasks = tasks_.size();
-          for (int i = 0; i < num_tasks; ++i) {
-            int index = (next_task_index_ + i) % num_tasks;
-            std::shared_ptr<Task>& task = tasks_[index];
-            if (!task->in_use && !task->end_of_sequence) {
-              task->in_use = true;
-              task_to_process = task;
-              next_task_index_ = (index + 1) % num_tasks;
-              break;
+          if (StrictRoundRobin()) {
+            task_to_process = tasks_[next_task_index_];
+            // Reserve a spot in the results_ queue.
+            results_.emplace();
+            result = &results_.back();
+            next_task_index_ = (next_task_index_ + 1) % tasks_.size();
+            DCHECK(!task_to_process->in_use);
+          } else {
+            // Search for a task to update.
+            int num_tasks = tasks_.size();
+            for (int i = 0; i < num_tasks; ++i) {
+              int index = (next_task_index_ + i) % num_tasks;
+              std::shared_ptr<Task>& task = tasks_[index];
+              if (!task->in_use && !task->end_of_sequence) {
+                task_to_process = task;
+                next_task_index_ = (index + 1) % num_tasks;
+                break;
+              }
             }
           }
           DCHECK(task_to_process != nullptr);
+          task_to_process->in_use = true;
           VLOG(3) << "Processing task " << task_to_process->task_id;
         }
         int64 deadline_micros = kint64max;
-        Status s = GetElement(task_to_process.get(), deadline_micros);
+        Status s;
+        if (StrictRoundRobin()) {
+          s = GetElement(task_to_process.get(), deadline_micros,
+                         /*enqueue_result=*/false, *result);
+        } else {
+          Result r;
+          s = GetElement(task_to_process.get(), deadline_micros,
+                         /*enqueue_result=*/true, r);
+        }
         if (!s.ok()) {
           mutex_lock l(mu_);
           VLOG(1) << "Failed to get element from worker "
@@ -502,21 +572,28 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
       }
     }
 
-    // Gets an element from a task and adds the element to `results_`.
-    //
-    // If the task reaches end_of_sequence or is cancelled (e.g. due to a
-    // worker dying), GetElement returns Status::OK() without adding to
-    // `results_`.
-    Status GetElement(Task* task, int64 deadline_micros)
-        TF_LOCKS_EXCLUDED(mu_) {
+    // Gets an element from a task and stores the element in `result`. If
+    // `enqueue_result` is true, `GetElement` also enqueues (via std::move) any
+    // element-producing result in the `results_` queue.
+    Status GetElement(Task* task, int64 deadline_micros, bool enqueue_result,
+                      Result& result) TF_LOCKS_EXCLUDED(mu_) {
       VLOG(3) << "Getting an element for task id " << task->task_id;
       tensorflow::profiler::TraceMe activity(
           "GetDataServiceElement", tensorflow::profiler::TraceMeLevel::kInfo);
       CompressedElement compressed;
       bool end_of_sequence;
       for (int num_retries = 0;; ++num_retries) {
-        Status s = task->worker->GetElement(task->task_id, compressed,
-                                            end_of_sequence);
+        absl::optional<int64> consumer_index = dataset()->consumer_index_;
+        absl::optional<int64> round_index;
+        if (StrictRoundRobin()) {
+          round_index = task->elements_read;
+          VLOG(3) << "Requesting element from consumer index "
+                  << consumer_index.value() << ", round "
+                  << round_index.value();
+        }
+        Status s =
+            task->worker->GetElement(task->task_id, consumer_index, round_index,
+                                     compressed, end_of_sequence);
         if (s.ok()) {
           break;
         }
@@ -530,7 +607,8 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
           // If `UpdateTaskThreads` finds that the task has been cancelled, it
           // will set end_of_sequence to `true`.
           if (task->end_of_sequence || cancelled_) {
-            return Status::OK();
+            end_of_sequence = true;
+            break;
           }
         }
         const int64 now_micros = EnvTime::NowMicros();
@@ -559,26 +637,47 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
         element.push_back(tensor);
       }
       mutex_lock l(mu_);
+      result.ready = true;
+      result.end_of_sequence = end_of_sequence;
       if (end_of_sequence) {
         task->end_of_sequence = true;
         finished_tasks_++;
         return Status::OK();
       }
-      results_.push(std::move(element));
+      task->elements_read++;
+      result.element = std::move(element);
+      if (enqueue_result) {
+        results_.push(std::move(result));
+      }
       get_next_cv_.notify_all();
       VLOG(3) << "Got an element for task id " << task->task_id;
       return Status::OK();
     }
 
-    bool SpaceInBuffer() TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
+    // Reports whether we can request another element without violating
+    // max_outstanding_requests.
+    bool ElementSpaceAvailable() TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
+      // When doing round-robin reads, outstanding requests pre-allocate a
+      // result in `results_`, so we only need to check the size of `results_`.
+      if (StrictRoundRobin()) {
+        return results_.size() < max_outstanding_requests_;
+      }
+      // Otherwise, results aren't added to `results_` until the data has been
+      // successfully retrieved. We need to count requests already added to
+      // `results_` as well as in-progress requests.
       return results_.size() + outstanding_requests_ <
              max_outstanding_requests_;
     }
 
     bool TaskAvailable() TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {
+      if (StrictRoundRobin()) {
+        return !tasks_[next_task_index_]->in_use;
+      }
       return finished_tasks_ + outstanding_requests_ < tasks_.size();
     }
 
+    bool StrictRoundRobin() { return dataset()->num_consumers_.has_value(); }
+
     const int64 iterator_index_;
 
     mutable mutex mu_;
@@ -610,7 +709,12 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
     // A status to be returned from the next call to `GetNext`. This is set by
     // asynchronous threads when they encounter errors.
     Status status_ TF_GUARDED_BY(mu_) = Status::OK();
-    std::queue<std::vector<Tensor>> results_ TF_GUARDED_BY(mu_);
+    // A queue of results for `GetElement` requests to read from. When doing
+    // strict round robin reads, the queue will contain placeholder results with
+    // their `Result::ready` field false until their data has been retrieved
+    // from a worker. When not doing round-robin reads, results are only added
+    // to the queue after they are ready, to avoid head-of-line blocking.
+    std::queue<Result> results_ TF_GUARDED_BY(mu_);
 
     bool initialized_ = false;
     // Set once in Initialize().
@@ -622,11 +726,14 @@ class DataServiceDatasetOp::Dataset : public DatasetBase {
     std::unique_ptr<Thread> task_thread_manager_ TF_GUARDED_BY(mu_);
   };
 
+  const int op_version_;
   const int64 dataset_id_;
   const ProcessingMode processing_mode_;
   const tstring address_;
   const tstring protocol_;
   const tstring job_name_;
+  const absl::optional<int64> consumer_index_;
+  const absl::optional<int64> num_consumers_;
   const int64 max_outstanding_requests_;
   const int64 task_refresh_interval_ms_;
   IterationCounter* const iteration_counter_;  // Owned
@@ -646,6 +753,16 @@ DataServiceDatasetOp::DataServiceDatasetOp(OpKernelConstruction* ctx)
   }
   OP_REQUIRES_OK(ctx, ctx->GetAttr(kOutputTypes, &output_types_));
   OP_REQUIRES_OK(ctx, ctx->GetAttr(kOutputShapes, &output_shapes_));
+  auto& op_name = ctx->def().op();
+  if (op_name == kDataServiceDatasetV1) {
+    op_version_ = 1;
+  } else if (op_name == kDataServiceDatasetV2) {
+    op_version_ = 2;
+  } else {
+    ctx->CtxFailure(errors::FailedPrecondition(
+        "Unrecognized data service dataset op name: ", op_name));
+    return;
+  }
 }
 
 void DataServiceDatasetOp::MakeDataset(OpKernelContext* ctx,
@@ -673,6 +790,24 @@ void DataServiceDatasetOp::MakeDataset(OpKernelContext* ctx,
   tstring job_name;
   OP_REQUIRES_OK(ctx, ParseScalarArgument(ctx, kJobName, &job_name));
 
+  absl::optional<int64> consumer_index;
+  absl::optional<int64> num_consumers;
+  if (op_version_ >= 2) {
+    int64 consumer_index_int;
+    OP_REQUIRES_OK(
+        ctx, ParseScalarArgument(ctx, kConsumerIndex, &consumer_index_int));
+    if (consumer_index_int >= 0) {
+      consumer_index = consumer_index_int;
+    }
+
+    int64 num_consumers_int;
+    OP_REQUIRES_OK(ctx,
+                   ParseScalarArgument(ctx, kNumConsumers, &num_consumers_int));
+    if (num_consumers_int >= 0) {
+      num_consumers = num_consumers_int;
+    }
+  }
+
   int64 max_outstanding_requests;
   OP_REQUIRES_OK(ctx, ParseScalarArgument(ctx, kMaxOutstandingRequests,
                                           &max_outstanding_requests));
@@ -712,15 +847,17 @@ void DataServiceDatasetOp::MakeDataset(OpKernelContext* ctx,
       errors::InvalidArgument(kMaxOutstandingRequests, " must be positive or ",
                               model::kAutotune));
 
-  *output =
-      new Dataset(ctx, dataset_id, processing_mode, address, protocol, job_name,
-                  max_outstanding_requests, task_refresh_interval_hint_ms_,
-                  iteration_counter, owns_resource, iteration_counter_handle,
-                  output_types_, output_shapes_);
+  *output = new Dataset(
+      ctx, op_version_, dataset_id, processing_mode, address, protocol,
+      job_name, consumer_index, num_consumers, max_outstanding_requests,
+      task_refresh_interval_hint_ms_, iteration_counter, owns_resource,
+      iteration_counter_handle, output_types_, output_shapes_);
 }
 
 REGISTER_KERNEL_BUILDER(Name("DataServiceDataset").Device(DEVICE_CPU),
                         DataServiceDatasetOp);
+REGISTER_KERNEL_BUILDER(Name("DataServiceDatasetV2").Device(DEVICE_CPU),
+                        DataServiceDatasetOp);
 REGISTER_KERNEL_BUILDER(Name("DummyIterationCounter").Device(DEVICE_CPU),
                         DummyResourceOp<IterationCounter>);
 
diff --git a/tensorflow/core/kernels/data/experimental/data_service_dataset_op.h b/tensorflow/core/kernels/data/experimental/data_service_dataset_op.h
index ba8b2739341..efb013bccdc 100644
--- a/tensorflow/core/kernels/data/experimental/data_service_dataset_op.h
+++ b/tensorflow/core/kernels/data/experimental/data_service_dataset_op.h
@@ -52,6 +52,8 @@ class DataServiceDatasetOp : public DatasetOpKernel {
   static constexpr const char* const kAddress = "address";
   static constexpr const char* const kProtocol = "protocol";
   static constexpr const char* const kJobName = "job_name";
+  static constexpr const char* const kConsumerIndex = "consumer_index";
+  static constexpr const char* const kNumConsumers = "num_consumers";
   static constexpr const char* const kMaxOutstandingRequests =
       "max_outstanding_requests";
   static constexpr const char* const kTaskRefreshIntervalHintMs =
@@ -70,6 +72,7 @@ class DataServiceDatasetOp : public DatasetOpKernel {
  private:
   class Dataset;
 
+  int op_version_;
   int64 task_refresh_interval_hint_ms_;
   DataTypeVector output_types_;
   std::vector<PartialTensorShape> output_shapes_;
diff --git a/tensorflow/core/kernels/data/parallel_map_dataset_op.cc b/tensorflow/core/kernels/data/parallel_map_dataset_op.cc
index 170e00186db..f504be0fdfd 100644
--- a/tensorflow/core/kernels/data/parallel_map_dataset_op.cc
+++ b/tensorflow/core/kernels/data/parallel_map_dataset_op.cc
@@ -212,7 +212,9 @@ class ParallelMapDatasetOp::Dataset : public DatasetBase {
           autotune_(params.dataset->num_parallel_calls_ == model::kAutotune) {}
 
     ~Iterator() override {
+      cancellation_manager_->StartCancel();
       CancelThreads(/*wait=*/true);
+      input_impl_.reset();
       if (deregister_fn_) deregister_fn_();
     }
 
@@ -221,11 +223,15 @@ class ParallelMapDatasetOp::Dataset : public DatasetBase {
       if (num_parallel_calls_->value == model::kAutotune) {
         num_parallel_calls_->value = ctx->runner_threadpool_size();
       }
+      cancellation_manager_ =
+          absl::make_unique<CancellationManager>(ctx->cancellation_manager());
+      IteratorContext::Params params(ctx);
+      params.cancellation_manager = cancellation_manager_.get();
       TF_RETURN_IF_ERROR(RegisterCancellationCallback(
           ctx->cancellation_manager(),
           [this]() { CancelThreads(/*wait=*/false); }, &deregister_fn_));
-      TF_RETURN_IF_ERROR(
-          dataset()->input_->MakeIterator(ctx, this, prefix(), &input_impl_));
+      TF_RETURN_IF_ERROR(dataset()->input_->MakeIterator(
+          IteratorContext(params), this, prefix(), &input_impl_));
       return dataset()->captured_func_->Instantiate(
           ctx, &instantiated_captured_func_);
     }
@@ -640,12 +646,17 @@ class ParallelMapDatasetOp::Dataset : public DatasetBase {
     const bool autotune_;
     // Counts the number of outstanding calls.
     int64 num_calls_ TF_GUARDED_BY(*mu_) = 0;
+    // Controls cancellation of `input_impl_`.
+    // Must be ordered before `input_impl_` so that `input_impl_` is destroyed
+    // first.
+    std::unique_ptr<CancellationManager> cancellation_manager_;
     std::unique_ptr<InstantiatedCapturedFunction> instantiated_captured_func_;
+    // Must be ordered after `cancellation_manager_` so that `input_impl_` is
+    // destroyed first.
     std::unique_ptr<IteratorBase> input_impl_;
     // Buffer for storing the invocation results.
     std::deque<std::shared_ptr<InvocationResult>> invocation_results_
         TF_GUARDED_BY(*mu_);
-
     std::unique_ptr<Thread> runner_thread_ TF_GUARDED_BY(*mu_);
     std::unique_ptr<Thread> stats_thread_ TF_GUARDED_BY(*mu_);
     bool cancelled_ TF_GUARDED_BY(*mu_) = false;
diff --git a/tensorflow/core/ops/experimental_dataset_ops.cc b/tensorflow/core/ops/experimental_dataset_ops.cc
index 8c2ca1e259a..8a41f809b69 100644
--- a/tensorflow/core/ops/experimental_dataset_ops.cc
+++ b/tensorflow/core/ops/experimental_dataset_ops.cc
@@ -1187,6 +1187,25 @@ REGISTER_OP("DataServiceDataset")
     .SetIsStateful()
     .SetShapeFn(shape_inference::ScalarShape);
 
+// Adds `consumer_index` and `num_consumers` arguments to support round-robin
+// reads.
+REGISTER_OP("DataServiceDatasetV2")
+    .Input("dataset_id: int64")
+    .Input("processing_mode: string")
+    .Input("address: string")
+    .Input("protocol: string")
+    .Input("job_name: string")
+    .Input("consumer_index: int64")
+    .Input("num_consumers: int64")
+    .Input("max_outstanding_requests: int64")
+    .Input("iteration_counter: resource")
+    .Output("handle: variant")
+    .Attr("task_refresh_interval_hint_ms: int = -1")
+    .Attr("output_types: list(type) >= 1")
+    .Attr("output_shapes: list(shape) >= 1")
+    .SetIsStateful()
+    .SetShapeFn(shape_inference::ScalarShape);
+
 REGISTER_OP("RegisterDataset")
     .Input("dataset: variant")
     .Input("address: string")
diff --git a/tensorflow/python/data/experimental/kernel_tests/data_service_ops_test.py b/tensorflow/python/data/experimental/kernel_tests/data_service_ops_test.py
index 834dc486326..8a07b0e9be5 100644
--- a/tensorflow/python/data/experimental/kernel_tests/data_service_ops_test.py
+++ b/tensorflow/python/data/experimental/kernel_tests/data_service_ops_test.py
@@ -283,6 +283,133 @@ class DataServiceOpsTest(data_service_test_base.TestBase,
       results.append(elem.numpy())
     self.assertCountEqual(num_repetitions * list(range(num_elements)), results)
 
+  @combinations.generate(
+      combinations.times(
+          test_base.default_test_combinations(),
+          combinations.combine(num_workers=[1, 3], num_consumers=[1, 2, 5])))
+  def testRoundRobin(self, num_workers, num_consumers):
+    cluster = self.create_cluster(num_workers=num_workers)
+    num_elements = 100
+    ds = dataset_ops.Dataset.range(num_elements)
+    ds = ds.repeat()
+    consumers = []
+    for consumer_index in range(num_consumers):
+      consumers.append(
+          self.make_distributed_dataset(
+              ds,
+              cluster,
+              job_name="test",
+              consumer_index=consumer_index,
+              num_consumers=num_consumers))
+    # Use parallel interleave to read from consumers in parallel.
+    ds = dataset_ops.Dataset.from_tensor_slices(consumers)
+    ds = ds.interleave(
+        lambda x: x,
+        cycle_length=num_consumers,
+        num_parallel_calls=num_consumers)
+    ds = ds.take(2 * num_elements * num_workers)
+    results = self.getDatasetOutput(ds, requires_initialization=True)
+
+    expected = []
+    round_index = 0
+    while len(expected) < len(results):
+      for _ in range(num_workers):
+        for consumer in range(num_consumers):
+          expected.append(
+              (round_index * num_consumers + consumer) % num_elements)
+      round_index += 1
+    self.assertEqual(results, expected)
+
+  @combinations.generate(test_base.default_test_combinations())
+  def testRoundRobinBucketizing(self):
+    # Tests a common use case for round robin reads. At each step, all
+    # consumers should get batches with the same bucket size.
+    cluster = self.create_cluster(num_workers=4)
+    num_elements = 100
+    ds = dataset_ops.Dataset.range(num_elements, output_type=dtypes.int32)
+    ds = ds.shuffle(num_elements)
+    low_bucket_max = 30
+    mid_bucket_max = 60
+    bucket_boundaries = [low_bucket_max, mid_bucket_max]
+    batch_size = 10
+    num_consumers = 3
+    bucket_batch_sizes = [batch_size] * (len(bucket_boundaries) + 1)
+    ds = ds.apply(
+        grouping.bucket_by_sequence_length(
+            lambda x: x,
+            bucket_boundaries,
+            bucket_batch_sizes,
+            drop_remainder=True))
+    ds = ds.apply(
+        grouping.group_by_window(
+            lambda x: math_ops.cast(x[1], dtypes.int64),
+            lambda _, x: dataset_ops.Dataset.from_tensors(x),
+            window_size=num_consumers))
+    ds = ds.flat_map(lambda x: x)
+    ds = ds.repeat()
+
+    consumers = []
+    for consumer_index in range(num_consumers):
+      consumers.append(
+          self.make_distributed_dataset(
+              ds,
+              cluster,
+              job_name="test",
+              consumer_index=consumer_index,
+              num_consumers=num_consumers))
+    # Use parallel interleave to read from consumers in parallel.
+    ds = dataset_ops.Dataset.from_tensor_slices(consumers)
+    ds = ds.interleave(
+        lambda x: x.prefetch(num_elements),
+        cycle_length=num_consumers,
+        num_parallel_calls=num_consumers)
+
+    num_rounds = 10
+    get_next = self.getNext(ds, requires_initialization=True)
+    results = []
+    for _ in range(num_rounds):
+      results.append(self.evaluate(get_next()))
+
+    def get_bucket(elem):
+      bucket_ind = 0
+      while bucket_ind < len(
+          bucket_boundaries) and elem >= bucket_boundaries[bucket_ind]:
+        bucket_ind += 1
+      return bucket_ind
+
+    for i in range(0, len(results), num_consumers):
+      batches = results[num_consumers * i:num_consumers * i + num_consumers]
+      bucket_inds = [get_bucket(batch[0]) for batch in batches]
+      for bucket_ind in bucket_inds[1:]:
+        self.assertEqual(bucket_inds[0], bucket_ind)
+
+  @combinations.generate(test_base.v1_only_combinations())
+  def testRoundRobinFiniteV1(self):
+    cluster = self.create_cluster(num_workers=1)
+    num_elements = 100
+    ds = dataset_ops.Dataset.range(num_elements)
+    ds = self.make_distributed_dataset(
+        ds, cluster, job_name="test", consumer_index=0, num_consumers=1)
+
+    with self.assertRaisesRegex(
+        errors.FailedPreconditionError, "Encountered end of sequence on a "
+        "round-robin read iterator"):
+      self.getDatasetOutput(ds, requires_initialization=True)
+
+  @combinations.generate(test_base.v2_only_combinations())
+  def testRoundRobinFiniteV2(self):
+    cluster = self.create_cluster(num_workers=1)
+    num_elements = 100
+    ds = dataset_ops.Dataset.range(num_elements)
+    ds = self.make_distributed_dataset(
+        ds, cluster, job_name="test", consumer_index=0, num_consumers=1)
+
+    with self.assertRaisesRegex(
+        errors.FailedPreconditionError, "Round robin reads "
+        "require that the input dataset has infinite "
+        "cardinality, but the dataset has cardinality " + str(num_elements)):
+      self.getDatasetOutput(ds, requires_initialization=True)
+
   @combinations.generate(
       combinations.times(test_base.eager_only_combinations(),
                          combinations.combine(job_name=[None, "test"])))
diff --git a/tensorflow/python/data/experimental/kernel_tests/data_service_test_base.py b/tensorflow/python/data/experimental/kernel_tests/data_service_test_base.py
index 11d1fddd4a0..67b389029e6 100644
--- a/tensorflow/python/data/experimental/kernel_tests/data_service_test_base.py
+++ b/tensorflow/python/data/experimental/kernel_tests/data_service_test_base.py
@@ -205,6 +205,8 @@ class TestBase(test_base.DatasetTestBase):
                                cluster,
                                processing_mode="parallel_epochs",
                                job_name=None,
+                               consumer_index=None,
+                               num_consumers=None,
                                max_outstanding_requests=None):
     # pylint: disable=protected-access
     return dataset.apply(
@@ -212,6 +214,8 @@ class TestBase(test_base.DatasetTestBase):
             processing_mode,
             cluster.target,
             job_name=job_name,
+            consumer_index=consumer_index,
+            num_consumers=num_consumers,
             max_outstanding_requests=max_outstanding_requests,
             task_refresh_interval_hint_ms=20))
 
diff --git a/tensorflow/python/data/experimental/ops/data_service_ops.py b/tensorflow/python/data/experimental/ops/data_service_ops.py
index f6f967ebac1..47ace74fe21 100644
--- a/tensorflow/python/data/experimental/ops/data_service_ops.py
+++ b/tensorflow/python/data/experimental/ops/data_service_ops.py
@@ -60,6 +60,8 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):
                address,
                protocol,
                job_name=None,
+               consumer_index=None,
+               num_consumers=None,
                max_outstanding_requests=None,
                task_refresh_interval_hint_ms=None):
     """Constructs a _DataServiceDatasetV2.
@@ -77,6 +79,17 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):
       job_name: (Optional.) The name of the job. This argument makes it possible
         for multiple datasets to share the same job. The default behavior is
         that the dataset creates anonymous, exclusively owned jobs.
+      consumer_index: (Optional.) The index of the consumer in the range from
+        `0` to `num_consumers`. Must be specified alongside `num_consumers`.
+        When specified, consumers will read from the job in a strict round-robin
+        order, instead of the default first-come-first-served order.
+      num_consumers: (Optional.) The number of consumers which will consume from
+        the job. Must be specified alongside `consumer_index`. When specified,
+        consumers will read from the job in a strict round-robin order, instead
+        of the default first-come-first-served order. When `num_consumers` is
+        specified, the dataset must have infinite cardinality to prevent a
+        producer from running out of data early and causing consumers to go out
+        of sync.
       max_outstanding_requests: (Optional.) A limit on how many elements may be
         requested at the same time. You can use this option to control the
         amount of memory used, since `distribute` won't use more than
@@ -84,6 +97,13 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):
       task_refresh_interval_hint_ms: (Optional.) A hint for how often to query
         the dispatcher for task changes.
     """
+    if consumer_index is None != num_consumers is None:
+      raise ValueError(
+          "Must either set both consumer_index and num_consumers, or neither. ",
+          "consumer_index: ", consumer_index, ", num_consumers: ",
+          num_consumers)
+    if num_consumers is not None and job_name is None:
+      raise ValueError("job_name must be set when setting num_consumers")
 
     if job_name is None:
       job_name = ""
@@ -91,6 +111,10 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):
       max_outstanding_requests = dataset_ops.AUTOTUNE
     if task_refresh_interval_hint_ms is None:
       task_refresh_interval_hint_ms = dataset_ops.AUTOTUNE
+    if consumer_index is None:
+      consumer_index = -1
+    if num_consumers is None:
+      num_consumers = -1
 
     self._dataset_id = ops.convert_to_tensor(
         dataset_id, dtype=dtypes.int64, name="dataset_id")
@@ -102,6 +126,10 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):
         protocol, dtype=dtypes.string, name="protocol")
     self._job_name = ops.convert_to_tensor(
         job_name, dtype=dtypes.string, name="job_name")
+    self._consumer_index = ops.convert_to_tensor(
+        consumer_index, dtype=dtypes.int64, name="consumer_index")
+    self._num_consumers = ops.convert_to_tensor(
+        num_consumers, dtype=dtypes.int64, name="num_consumers")
     self._max_outstanding_requests = ops.convert_to_tensor(
         max_outstanding_requests,
         dtype=dtypes.int64,
@@ -110,17 +138,32 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):
     # represented by scalar DT_VARIANTs.
     self._element_spec = tensor_spec.TensorSpec(shape=(), dtype=dtypes.variant)
 
-    variant_tensor = gen_experimental_dataset_ops.data_service_dataset(
-        dataset_id=self._dataset_id,
-        processing_mode=self._processing_mode,
-        address=self._address,
-        protocol=self._protocol,
-        job_name=self._job_name,
-        max_outstanding_requests=self._max_outstanding_requests,
-        task_refresh_interval_hint_ms=task_refresh_interval_hint_ms,
-        iteration_counter=gen_experimental_dataset_ops.dummy_iteration_counter(
-        ),
-        **self._flat_structure)
+    if num_consumers >= 0:
+      variant_tensor = gen_experimental_dataset_ops.data_service_dataset_v2(
+          dataset_id=self._dataset_id,
+          processing_mode=self._processing_mode,
+          address=self._address,
+          protocol=self._protocol,
+          job_name=self._job_name,
+          consumer_index=self._consumer_index,
+          num_consumers=self._num_consumers,
+          max_outstanding_requests=self._max_outstanding_requests,
+          task_refresh_interval_hint_ms=task_refresh_interval_hint_ms,
+          iteration_counter=gen_experimental_dataset_ops
+          .dummy_iteration_counter(),
+          **self._flat_structure)
+    else:
+      variant_tensor = gen_experimental_dataset_ops.data_service_dataset(
+          dataset_id=self._dataset_id,
+          processing_mode=self._processing_mode,
+          address=self._address,
+          protocol=self._protocol,
+          job_name=self._job_name,
+          max_outstanding_requests=self._max_outstanding_requests,
+          task_refresh_interval_hint_ms=task_refresh_interval_hint_ms,
+          iteration_counter=gen_experimental_dataset_ops
+          .dummy_iteration_counter(),
+          **self._flat_structure)
     super(_DataServiceDatasetV2, self).__init__(variant_tensor)
 
   @property
@@ -133,7 +176,8 @@ class _DataServiceDatasetV1(dataset_ops.DatasetV1Adapter):
 
   @functools.wraps(_DataServiceDatasetV2.__init__)
   def __init__(self, dataset_id, processing_mode, address, protocol, job_name,
-               max_outstanding_requests, task_refresh_interval_hint_ms):
+               consumer_index, num_consumers, max_outstanding_requests,
+               task_refresh_interval_hint_ms):
 
     self._wrapped = _DataServiceDatasetV2(
         dataset_id=dataset_id,
@@ -141,6 +185,8 @@ class _DataServiceDatasetV1(dataset_ops.DatasetV1Adapter):
         address=address,
         protocol=protocol,
         job_name=job_name,
+        consumer_index=consumer_index,
+        num_consumers=num_consumers,
         max_outstanding_requests=max_outstanding_requests,
         task_refresh_interval_hint_ms=task_refresh_interval_hint_ms)
     super(_DataServiceDatasetV1, self).__init__(self._wrapped)
@@ -184,6 +230,8 @@ def _from_dataset_id(processing_mode,
                      dataset_id,
                      element_spec,
                      job_name=None,
+                     consumer_index=None,
+                     num_consumers=None,
                      max_outstanding_requests=None,
                      task_refresh_interval_hint_ms=None):
   """Creates a dataset which reads data from the tf.data service.
@@ -209,6 +257,17 @@ def _from_dataset_id(processing_mode,
     job_name: (Optional.) The name of the job. This argument makes it possible
       for multiple datasets to share the same job. The default behavior is that
       the dataset creates anonymous, exclusively owned jobs.
+    consumer_index: (Optional.) The index of the consumer in the range from
+      `0` to `num_consumers`. Must be specified alongside `num_consumers`.
+      When specified, consumers will read from the job in a strict round-robin
+      order, instead of the default first-come-first-served order.
+    num_consumers: (Optional.) The number of consumers which will consume from
+      the job. Must be specified alongside `consumer_index`. When specified,
+      consumers will read from the job in a strict round-robin order, instead
+      of the default first-come-first-served order. When `num_consumers` is
+      specified, the dataset must have infinite cardinality to prevent a
+      producer from running out of data early and causing consumers to go out of
+      sync.
     max_outstanding_requests: (Optional.) A limit on how many elements may be
       requested at the same time. You can use this option to control the amount
       of memory used, since `distribute` won't use more than `element_size` *
@@ -236,6 +295,8 @@ def _from_dataset_id(processing_mode,
       address=address,
       protocol=protocol,
       job_name=job_name,
+      consumer_index=consumer_index,
+      num_consumers=num_consumers,
       max_outstanding_requests=max_outstanding_requests,
       task_refresh_interval_hint_ms=task_refresh_interval_hint_ms)
   dataset = dataset.map(
@@ -253,6 +314,8 @@ def _from_dataset_id(processing_mode,
 def _distribute(processing_mode,
                 service,
                 job_name=None,
+                consumer_index=None,
+                num_consumers=None,
                 max_outstanding_requests=None,
                 task_refresh_interval_hint_ms=None):
   """A transformation that moves dataset processing to the tf.data service.
@@ -272,6 +335,17 @@ def _distribute(processing_mode,
     job_name: (Optional.) The name of the job. This argument makes it possible
       for multiple datasets to share the same job. The default behavior is that
       the dataset creates anonymous, exclusively owned jobs.
+    consumer_index: (Optional.) The index of the consumer in the range from
+      `0` to `num_consumers`. Must be specified alongside `num_consumers`.
+      When specified, consumers will read from the job in a strict round-robin
+      order, instead of the default first-come-first-served order.
+    num_consumers: (Optional.) The number of consumers which will consume from
+      the job. Must be specified alongside `consumer_index`. When specified,
+      consumers will read from the job in a strict round-robin order, instead
+      of the default first-come-first-served order. When `num_consumers` is
+      specified, the dataset must have infinite cardinality to prevent a
+      producer from running out of data early and causing consumers to go out of
+      sync.
     max_outstanding_requests: (Optional.) A limit on how many elements may be
       requested at the same time. You can use this option to control the amount
       of memory used, since `distribute` won't use more than `element_size` *
@@ -292,6 +366,8 @@ def _distribute(processing_mode,
         dataset_id,
         dataset.element_spec,
         job_name=job_name,
+        consumer_index=consumer_index,
+        num_consumers=num_consumers,
         max_outstanding_requests=max_outstanding_requests,
         task_refresh_interval_hint_ms=task_refresh_interval_hint_ms)
 
@@ -302,6 +378,8 @@ def _distribute(processing_mode,
 def distribute(processing_mode,
                service,
                job_name=None,
+               consumer_index=None,
+               num_consumers=None,
                max_outstanding_requests=None):
   """A transformation that moves dataset processing to the tf.data service.
 
@@ -440,6 +518,25 @@ def distribute(processing_mode,
   from `job_name="job"`, it will immediately receive end of input, without
   getting any data.
 
+  **Round Robin data consumption**
+
+  By default, when multiple consumers read from the same job, they receive data
+  on a first-come first-served basis. In some use cases, it works better to use
+  a strict round-robin order. For example, the tf.data service can be used to
+  coordinate example sizes across a cluster during sychronous training, so that
+  during each step all replicas train on similar-sized elements. To achieve
+  this, define a dataset which generates rounds of `num_consumers` consecutive
+  similar-sized batches, then enable round-robin reads by setting
+  `consumer_index` and `num_consumers`.
+
+  Consumers read data by cycling through all workers, reading one element from
+  each. First, each consumer will read an element from the first worker, then
+  each consumer will read an element from the second worker, and so on.
+
+  NOTE: To keep consumers in sync, round robin data consumption requires that
+  the dataset have infinite cardinality. You can get this by adding `.repeat()`
+  at the end of the dataset definition.
+
   **Keras and Distribution Strategies**
 
   The dataset produced by the `distribute` transformation can be passed to
@@ -468,6 +565,17 @@ def distribute(processing_mode,
     job_name: (Optional.) The name of the job. This argument makes it possible
       for multiple datasets to share the same job. The default behavior is that
       the dataset creates anonymous, exclusively owned jobs.
+    consumer_index: (Optional.) The index of the consumer in the range from
+      `0` to `num_consumers`. Must be specified alongside `num_consumers`.
+      When specified, consumers will read from the job in a strict round-robin
+      order, instead of the default first-come-first-served order.
+    num_consumers: (Optional.) The number of consumers which will consume from
+      the job. Must be specified alongside `consumer_index`. When specified,
+      consumers will read from the job in a strict round-robin order, instead
+      of the default first-come-first-served order. When `num_consumers` is
+      specified, the dataset must have infinite cardinality to prevent a
+      producer from running out of data early and causing consumers to go out of
+      sync.
     max_outstanding_requests: (Optional.) A limit on how many elements may be
       requested at the same time. You can use this option to control the amount
       of memory used, since `distribute` won't use more than `element_size` *
@@ -480,6 +588,8 @@ def distribute(processing_mode,
       processing_mode=processing_mode,
       service=service,
       job_name=job_name,
+      consumer_index=consumer_index,
+      num_consumers=num_consumers,
       max_outstanding_requests=max_outstanding_requests)
 
 
@@ -553,6 +663,8 @@ def from_dataset_id(processing_mode,
                     dataset_id,
                     element_spec=None,
                     job_name=None,
+                    consumer_index=None,
+                    num_consumers=None,
                     max_outstanding_requests=None):
   """Creates a dataset which reads data from the tf.data service.
 
@@ -612,6 +724,17 @@ def from_dataset_id(processing_mode,
     job_name: (Optional.) The name of the job. This argument makes it possible
       for multiple datasets to share the same job. The default behavior is that
       the dataset creates anonymous, exclusively owned jobs.
+    consumer_index: (Optional.) The index of the consumer in the range from
+      `0` to `num_consumers`. Must be specified alongside `num_consumers`.
+      When specified, consumers will read from the job in a strict round-robin
+      order, instead of the default first-come-first-served order.
+    num_consumers: (Optional.) The number of consumers which will consume from
+      the job. Must be specified alongside `consumer_index`. When specified,
+      consumers will read from the job in a strict round-robin order, instead
+      of the default first-come-first-served order. When `num_consumers` is
+      specified, the dataset must have infinite cardinality to prevent a
+      producer from running out of data early and causing consumers to go out of
+      sync.
     max_outstanding_requests: (Optional.) A limit on how many elements may be
       requested at the same time. You can use this option to control the amount
       of memory used, since `distribute` won't use more than `element_size` *
@@ -626,4 +749,6 @@ def from_dataset_id(processing_mode,
       dataset_id=dataset_id,
       element_spec=element_spec,
       job_name=job_name,
+      consumer_index=consumer_index,
+      num_consumers=num_consumers,
       max_outstanding_requests=max_outstanding_requests)
diff --git a/tensorflow/tools/api/golden/v1/tensorflow.data.experimental.service.pbtxt b/tensorflow/tools/api/golden/v1/tensorflow.data.experimental.service.pbtxt
index 1d91c01c2a5..35dc9254038 100644
--- a/tensorflow/tools/api/golden/v1/tensorflow.data.experimental.service.pbtxt
+++ b/tensorflow/tools/api/golden/v1/tensorflow.data.experimental.service.pbtxt
@@ -10,11 +10,11 @@ tf_module {
   }
   member_method {
     name: "distribute"
-    argspec: "args=[\'processing_mode\', \'service\', \'job_name\', \'max_outstanding_requests\'], varargs=None, keywords=None, defaults=[\'None\', \'None\'], "
+    argspec: "args=[\'processing_mode\', \'service\', \'job_name\', \'consumer_index\', \'num_consumers\', \'max_outstanding_requests\'], varargs=None, keywords=None, defaults=[\'None\', \'None\', \'None\', \'None\'], "
   }
   member_method {
     name: "from_dataset_id"
-    argspec: "args=[\'processing_mode\', \'service\', \'dataset_id\', \'element_spec\', \'job_name\', \'max_outstanding_requests\'], varargs=None, keywords=None, defaults=[\'None\', \'None\', \'None\'], "
+    argspec: "args=[\'processing_mode\', \'service\', \'dataset_id\', \'element_spec\', \'job_name\', \'consumer_index\', \'num_consumers\', \'max_outstanding_requests\'], varargs=None, keywords=None, defaults=[\'None\', \'None\', \'None\', \'None\', \'None\'], "
   }
   member_method {
     name: "register_dataset"
diff --git a/tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt b/tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt
index 7f6943d9e67..2f1ca248a9b 100644
--- a/tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt
+++ b/tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt
@@ -1008,6 +1008,10 @@ tf_module {
     name: "DataServiceDataset"
     argspec: "args=[\'dataset_id\', \'processing_mode\', \'address\', \'protocol\', \'job_name\', \'max_outstanding_requests\', \'iteration_counter\', \'output_types\', \'output_shapes\', \'task_refresh_interval_hint_ms\', \'name\'], varargs=None, keywords=None, defaults=[\'-1\', \'None\'], "
   }
+  member_method {
+    name: "DataServiceDatasetV2"
+    argspec: "args=[\'dataset_id\', \'processing_mode\', \'address\', \'protocol\', \'job_name\', \'consumer_index\', \'num_consumers\', \'max_outstanding_requests\', \'iteration_counter\', \'output_types\', \'output_shapes\', \'task_refresh_interval_hint_ms\', \'name\'], varargs=None, keywords=None, defaults=[\'-1\', \'None\'], "
+  }
   member_method {
     name: "DatasetCardinality"
     argspec: "args=[\'input_dataset\', \'name\'], varargs=None, keywords=None, defaults=[\'None\'], "
diff --git a/tensorflow/tools/api/golden/v2/tensorflow.data.experimental.service.pbtxt b/tensorflow/tools/api/golden/v2/tensorflow.data.experimental.service.pbtxt
index 0dd42fcdc24..d29349bb4fe 100644
--- a/tensorflow/tools/api/golden/v2/tensorflow.data.experimental.service.pbtxt
+++ b/tensorflow/tools/api/golden/v2/tensorflow.data.experimental.service.pbtxt
@@ -18,11 +18,11 @@ tf_module {
   }
   member_method {
     name: "distribute"
-    argspec: "args=[\'processing_mode\', \'service\', \'job_name\', \'max_outstanding_requests\'], varargs=None, keywords=None, defaults=[\'None\', \'None\'], "
+    argspec: "args=[\'processing_mode\', \'service\', \'job_name\', \'consumer_index\', \'num_consumers\', \'max_outstanding_requests\'], varargs=None, keywords=None, defaults=[\'None\', \'None\', \'None\', \'None\'], "
   }
   member_method {
     name: "from_dataset_id"
-    argspec: "args=[\'processing_mode\', \'service\', \'dataset_id\', \'element_spec\', \'job_name\', \'max_outstanding_requests\'], varargs=None, keywords=None, defaults=[\'None\', \'None\', \'None\'], "
+    argspec: "args=[\'processing_mode\', \'service\', \'dataset_id\', \'element_spec\', \'job_name\', \'consumer_index\', \'num_consumers\', \'max_outstanding_requests\'], varargs=None, keywords=None, defaults=[\'None\', \'None\', \'None\', \'None\', \'None\'], "
   }
   member_method {
     name: "register_dataset"
diff --git a/tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt b/tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt
index 7f6943d9e67..2f1ca248a9b 100644
--- a/tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt
+++ b/tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt
@@ -1008,6 +1008,10 @@ tf_module {
     name: "DataServiceDataset"
     argspec: "args=[\'dataset_id\', \'processing_mode\', \'address\', \'protocol\', \'job_name\', \'max_outstanding_requests\', \'iteration_counter\', \'output_types\', \'output_shapes\', \'task_refresh_interval_hint_ms\', \'name\'], varargs=None, keywords=None, defaults=[\'-1\', \'None\'], "
   }
+  member_method {
+    name: "DataServiceDatasetV2"
+    argspec: "args=[\'dataset_id\', \'processing_mode\', \'address\', \'protocol\', \'job_name\', \'consumer_index\', \'num_consumers\', \'max_outstanding_requests\', \'iteration_counter\', \'output_types\', \'output_shapes\', \'task_refresh_interval_hint_ms\', \'name\'], varargs=None, keywords=None, defaults=[\'-1\', \'None\'], "
+  }
   member_method {
     name: "DatasetCardinality"
     argspec: "args=[\'input_dataset\', \'name\'], varargs=None, keywords=None, defaults=[\'None\'], "

commit 894eaa01b5028432b3e7cf24ad778bfb1b53eea0
Author: Jing Pu <jingpu@google.com>
Date:   Wed Dec 2 17:42:44 2020 -0800

    Optimize `Mul::fold` efficiency with specialized implementations for f32 and bf16.
    
    Other changes includes:
    
    - Remove `ConstFoldBinaryOpScalarScalar` function since it is unused because all TFL ops operates on tensor types so there shouldn't be any scalar Attribute to be handled by the folder.
    
    - Remove `is_commutative` parameters in some functions that are not used eventually.
    
    PiperOrigin-RevId: 345349413
    Change-Id: I94c4095caddb41316aea0db4cd939b9f867be2e9

diff --git a/tensorflow/compiler/mlir/lite/BUILD b/tensorflow/compiler/mlir/lite/BUILD
index 1561474ac77..b7f6de76b78 100644
--- a/tensorflow/compiler/mlir/lite/BUILD
+++ b/tensorflow/compiler/mlir/lite/BUILD
@@ -238,6 +238,7 @@ cc_library(
         "//tensorflow/compiler/mlir/lite/quantization:quantization_lib",
         "//tensorflow/compiler/mlir/tensorflow:tensorflow_types",
         "//tensorflow/lite/schema:schema_fbs",
+        "//third_party/eigen3",
         "@llvm-project//llvm:Support",
         "@llvm-project//mlir:DerivedAttributeOpInterface",
         "@llvm-project//mlir:Dialect",
diff --git a/tensorflow/compiler/mlir/lite/ir/tfl_ops.cc b/tensorflow/compiler/mlir/lite/ir/tfl_ops.cc
index 76773f5335e..6f45636ea44 100644
--- a/tensorflow/compiler/mlir/lite/ir/tfl_ops.cc
+++ b/tensorflow/compiler/mlir/lite/ir/tfl_ops.cc
@@ -20,6 +20,7 @@ limitations under the License.
 #include <cstdint>
 #include <numeric>
 
+#include "third_party/eigen3/Eigen/Core"
 #include "llvm/ADT/APFloat.h"
 #include "llvm/ADT/APInt.h"
 #include "llvm/ADT/STLExtras.h"
@@ -299,27 +300,6 @@ inline bool IsBF16ShapedType(Type t) {
   return false;
 }
 
-// Performs const folding `calculate` with broadcast behavior on the two
-// attributes `operand1` and `operand2` and returns the result if possible.
-// The two operands are expected to both be scalar values.
-template <class AttrElementT,
-          class ElementValueT = typename AttrElementT::ValueType,
-          class CalculationT =
-              llvm::function_ref<ElementValueT(ElementValueT, ElementValueT)>>
-Attribute ConstFoldBinaryOpScalarScalar(Type result_type, Attribute operand1,
-                                        Attribute operand2,
-                                        const CalculationT &calculate) {
-  auto lhs = operand1.cast<AttrElementT>();
-  auto rhs = operand2.cast<AttrElementT>();
-
-  assert(lhs.getType() == result_type && rhs.getType() == result_type &&
-         "values of incompatible types should be caught by op verification");
-
-  // TODO: Need to handle overflow/underflow cases.
-  return AttrElementT::get(result_type,
-                           calculate(lhs.getValue(), rhs.getValue()));
-}
-
 // Returns new shape with rank 'new_dims' with padded ones on the
 // left if needed.
 inline std::vector<int64_t> GetPaddedShape(ArrayRef<int64_t> old_shape,
@@ -389,26 +369,20 @@ Attribute ConstFoldBinaryOpDenseDense(Type result_type, DenseElementsAttr lhs,
   }
 
   auto num_elements = type.getNumElements();
-  SmallVector<ElementValueT, 16> lhs_old_values;
-  SmallVector<ElementValueT, 16> rhs_old_values;
-  if (lhs_is_splat)
-    lhs_old_values.push_back(lhs.getSplatValue<ElementValueT>());
-  else
-    lhs_old_values = llvm::to_vector<16>(lhs.getValues<ElementValueT>());
-  if (rhs_is_splat)
-    rhs_old_values.push_back(rhs.getSplatValue<ElementValueT>());
-  else
-    rhs_old_values = llvm::to_vector<16>(rhs.getValues<ElementValueT>());
+
   SmallVector<ElementValueT, 16> new_values;
   new_values.reserve(num_elements);
   const auto result_shape = type.getShape();
   std::vector<int64_t> current_index(type.getRank(), 0);
   // Create the new shape with ones padded to the left.
-  std::vector<int64_t> lhs_new_shape =
+  const std::vector<int64_t> lhs_new_shape =
       GetPaddedShape(lhs.getType().getShape(), type.getRank());
-  std::vector<int64_t> rhs_new_shape =
+  const std::vector<int64_t> rhs_new_shape =
       GetPaddedShape(rhs.getType().getShape(), type.getRank());
 
+  auto lhs_old_values = lhs.getValues<ElementValueT>();
+  auto rhs_old_values = rhs.getValues<ElementValueT>();
+
   // Add each pair of the corresponding values in the dense elements
   // attributes.
   for (int64_t i = 0; i < num_elements; ++i) {
@@ -416,16 +390,16 @@ Attribute ConstFoldBinaryOpDenseDense(Type result_type, DenseElementsAttr lhs,
     // in the N-dimension tensor. GetElementIndex returns
     // the index in the flat representation of the original tensor
     // to use.
-    int64_t lhs_index =
+    const int64_t lhs_index =
         lhs_is_splat ? 0 : GetElementIndex(lhs_new_shape, current_index);
-    int64_t rhs_index =
+    const int64_t rhs_index =
         rhs_is_splat ? 0 : GetElementIndex(rhs_new_shape, current_index);
 
-    new_values.push_back(
-        calculate(lhs_old_values[lhs_index], rhs_old_values[rhs_index]));
+    new_values.push_back(calculate(*(lhs_old_values.begin() + lhs_index),
+                                   *(rhs_old_values.begin() + rhs_index)));
     IncrementIndex(result_shape, &current_index);
   }
-  return DenseElementsAttr::get(type, new_values);
+  return DenseElementsAttr::get(type, ArrayRef<ElementValueT>(new_values));
 }
 
 /// Performs const folding `calculate` with broadcast behavior on the two
@@ -437,16 +411,10 @@ template <class AttrElementT,
           class CalculationT =
               llvm::function_ref<ElementValueT(ElementValueT, ElementValueT)>>
 Attribute ConstFoldBinaryOp(Type result_type, Attribute operand1,
-                            Attribute operand2, const CalculationT &calculate,
-                            bool is_commutative) {
-  if (operand1.dyn_cast_or_null<AttrElementT>()) {
-    // Scalar op scalar case
-    if (operand2.dyn_cast_or_null<AttrElementT>())
-      return ConstFoldBinaryOpScalarScalar<AttrElementT>(result_type, operand1,
-                                                         operand2, calculate);
-  } else if (operand1.dyn_cast_or_null<DenseElementsAttr>() &&
-             operand2.dyn_cast_or_null<DenseElementsAttr>()) {
-    return ConstFoldBinaryOpDenseDense<AttrElementT>(
+                            Attribute operand2, const CalculationT &calculate) {
+  if (operand1.dyn_cast_or_null<DenseElementsAttr>() &&
+      operand2.dyn_cast_or_null<DenseElementsAttr>()) {
+    return ConstFoldBinaryOpDenseDense<AttrElementT, ElementValueT>(
         result_type, operand1.cast<DenseElementsAttr>(),
         operand2.cast<DenseElementsAttr>(), calculate);
   }
@@ -463,8 +431,7 @@ Attribute ConstFoldBinaryOp(Type result_type, Attribute operand1,
 Attribute ConstFoldBinaryOp(
     Type result_type, ArrayRef<Attribute> operands,
     llvm::function_ref<APFloat(APFloat, APFloat)> float_calculate,
-    llvm::function_ref<APInt(APInt, APInt)> int_calculate,
-    bool is_commutative) {
+    llvm::function_ref<APInt(APInt, APInt)> int_calculate) {
   // Note: All types are wrapped in tensor types in TFlite. E.g., f32 is
   // represented as tensor<f32>. So we are only handling tensor types here.
   auto type = result_type.dyn_cast<ShapedType>();
@@ -474,11 +441,11 @@ Attribute ConstFoldBinaryOp(
 
   if (elemType.isa<FloatType>())
     return ConstFoldBinaryOp<FloatAttr>(result_type, operands[0], operands[1],
-                                        float_calculate, is_commutative);
+                                        float_calculate);
 
   if (elemType.isSignlessInteger())
     return ConstFoldBinaryOp<IntegerAttr>(result_type, operands[0], operands[1],
-                                          int_calculate, is_commutative);
+                                          int_calculate);
 
   return {};
 }
@@ -557,7 +524,7 @@ OpFoldResult AddOp::fold(ArrayRef<Attribute> operands) {
   if (fused_activation_function() != "NONE") return {};
   return ConstFoldBinaryOp(
       getType(), operands, [](APFloat a, APFloat b) { return a + b; },
-      [](APInt a, APInt b) { return a + b; }, getOperation()->isCommutative());
+      [](APInt a, APInt b) { return a + b; });
 }
 
 //===----------------------------------------------------------------------===//
@@ -991,9 +958,30 @@ static LogicalResult Verify(ScatterNdOp op) {
 OpFoldResult MulOp::fold(ArrayRef<Attribute> operands) {
   // TODO(b/142478136): Handle fused ops.
   if (fused_activation_function() != "NONE") return {};
+
+  // This function is performance critical for op fusion patterns, e.g.
+  // FuseBinaryOpToPrecedingAffine and FuseMulOrDivWithConv2dOrDepthwiseConv2d.
+  // So a few specializations are provided to evaluate the math operation
+  // more efficiently.
+
+  // Specialization for f32 type.
+  if (getType().cast<ShapedType>().getElementType().isF32()) {
+    return ConstFoldBinaryOp<FloatAttr, float>(
+        getType(), operands[0], operands[1],
+        [](float a, float b) { return a * b; });
+  }
+
+  // Specialization for bf16 type.
+  if (getType().cast<ShapedType>().getElementType().isBF16()) {
+    return ConstFoldBinaryOp<FloatAttr, Eigen::bfloat16>(
+        getType(), operands[0], operands[1],
+        [](Eigen::bfloat16 a, Eigen::bfloat16 b) { return a * b; });
+  }
+
+  // Generic fallback with APFloat
   return ConstFoldBinaryOp(
       getType(), operands, [](APFloat a, APFloat b) { return a * b; },
-      [](APInt a, APInt b) { return a * b; }, getOperation()->isCommutative());
+      [](APInt a, APInt b) { return a * b; });
 }
 
 //===----------------------------------------------------------------------===//
@@ -1005,8 +993,7 @@ OpFoldResult DivOp::fold(ArrayRef<Attribute> operands) {
   if (fused_activation_function() != "NONE") return {};
   return ConstFoldBinaryOp(
       getType(), operands, [](APFloat a, APFloat b) { return a / b; },
-      [](APInt a, APInt b) { return a.sdiv(b); },
-      getOperation()->isCommutative());
+      [](APInt a, APInt b) { return a.sdiv(b); });
 }
 
 //===----------------------------------------------------------------------===//
@@ -1378,7 +1365,7 @@ OpFoldResult SubOp::fold(ArrayRef<Attribute> operands) {
   if (fused_activation_function() != "NONE") return {};
   return ConstFoldBinaryOp(
       getType(), operands, [](APFloat a, APFloat b) { return a - b; },
-      [](APInt a, APInt b) { return a - b; }, getOperation()->isCommutative());
+      [](APInt a, APInt b) { return a - b; });
 }
 
 //===----------------------------------------------------------------------===//
diff --git a/tensorflow/compiler/mlir/lite/tests/const-fold.mlir b/tensorflow/compiler/mlir/lite/tests/const-fold.mlir
index 88b084220e8..ff92f649d7c 100644
--- a/tensorflow/compiler/mlir/lite/tests/const-fold.mlir
+++ b/tensorflow/compiler/mlir/lite/tests/const-fold.mlir
@@ -109,6 +109,48 @@ func @mul_float() -> (tensor<f32>, tensor<4xf32>, tensor<4xf32>, tensor<4xf32>)
   return %5, %6, %7, %8 : tensor<f32>, tensor<4xf32>, tensor<4xf32>, tensor<4xf32>
 }
 
+// CHECK-LABEL: @mul_bf16
+func @mul_bf16() -> (tensor<bf16>, tensor<4xbf16>, tensor<4xbf16>, tensor<4xbf16>) {
+  %0 = constant dense<4.5> : tensor<bf16>
+  %1 = constant dense<1.5> : tensor<bf16>
+
+  %2 = constant dense< 3.5> : tensor<4xbf16>
+  %3 = constant dense<-0.5> : tensor<4xbf16>
+
+  // CHECK: %[[CST:.*]] = constant dense<6.750000e+00> : tensor<bf16>
+  // CHECK: %[[CST_0:.*]]  = constant dense<-2.250000e+00> : tensor<4xbf16>
+  // CHECK: %[[CST_1:.*]]  = constant dense<5.250000e+00> : tensor<4xbf16>
+  // CHECK: %[[CST_2:.*]]  = constant dense<-1.750000e+00> : tensor<4xbf16>
+
+  %5 = "tfl.mul"(%0, %1) {fused_activation_function = "NONE"} : (tensor<  bf16>, tensor<  bf16>) -> tensor<  bf16>
+  %6 = "tfl.mul"(%0, %3) {fused_activation_function = "NONE"} : (tensor<  bf16>, tensor<4xbf16>) -> tensor<4xbf16>
+  %7 = "tfl.mul"(%2, %1) {fused_activation_function = "NONE"} : (tensor<4xbf16>, tensor<  bf16>) -> tensor<4xbf16>
+  %8 = "tfl.mul"(%2, %3) {fused_activation_function = "NONE"} : (tensor<4xbf16>, tensor<4xbf16>) -> tensor<4xbf16>
+
+  return %5, %6, %7, %8 : tensor<bf16>, tensor<4xbf16>, tensor<4xbf16>, tensor<4xbf16>
+}
+
+// CHECK-LABEL: @mul_f16
+func @mul_f16() -> (tensor<f16>, tensor<4xf16>, tensor<4xf16>, tensor<4xf16>) {
+  %0 = constant dense<4.5> : tensor<f16>
+  %1 = constant dense<1.5> : tensor<f16>
+
+  %2 = constant dense< 3.5> : tensor<4xf16>
+  %3 = constant dense<-0.5> : tensor<4xf16>
+
+  // CHECK: %[[CST:.*]] = constant dense<6.750000e+00> : tensor<f16>
+  // CHECK: %[[CST_0:.*]]  = constant dense<-2.250000e+00> : tensor<4xf16>
+  // CHECK: %[[CST_1:.*]]  = constant dense<5.250000e+00> : tensor<4xf16>
+  // CHECK: %[[CST_2:.*]]  = constant dense<-1.750000e+00> : tensor<4xf16>
+
+  %5 = "tfl.mul"(%0, %1) {fused_activation_function = "NONE"} : (tensor<  f16>, tensor<  f16>) -> tensor<  f16>
+  %6 = "tfl.mul"(%0, %3) {fused_activation_function = "NONE"} : (tensor<  f16>, tensor<4xf16>) -> tensor<4xf16>
+  %7 = "tfl.mul"(%2, %1) {fused_activation_function = "NONE"} : (tensor<4xf16>, tensor<  f16>) -> tensor<4xf16>
+  %8 = "tfl.mul"(%2, %3) {fused_activation_function = "NONE"} : (tensor<4xf16>, tensor<4xf16>) -> tensor<4xf16>
+
+  return %5, %6, %7, %8 : tensor<f16>, tensor<4xf16>, tensor<4xf16>, tensor<4xf16>
+}
+
 // CHECK-LABEL: @elementwise_unary_ops
 func @elementwise_unary_ops() -> (tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>, tensor<f32>) {
   %0 = constant dense<-1.0> : tensor<f32>

commit befea92a3d74a010791eb845267f03c54315d660
Author: Jing Pu <jingpu@google.com>
Date:   Fri Nov 13 18:19:46 2020 -0800

    Call "Reserve" method in `ConvertXXXElementsAttr` functions which slightly improve the efficiency.
    
    PiperOrigin-RevId: 342374519
    Change-Id: I7b146e15145efbb43ff1d9ba4a0aed01c18b75cb

diff --git a/tensorflow/compiler/mlir/tensorflow/utils/convert_tensor.cc b/tensorflow/compiler/mlir/tensorflow/utils/convert_tensor.cc
index e2f2ae5ad5d..d1f4086359d 100644
--- a/tensorflow/compiler/mlir/tensorflow/utils/convert_tensor.cc
+++ b/tensorflow/compiler/mlir/tensorflow/utils/convert_tensor.cc
@@ -264,7 +264,8 @@ void ConvertElementsAttr(const mlir::DenseElementsAttr attr,
   if (attr.isSplat()) {
     output->Add(attr.getSplatValue<T>());
   } else {
-    for (auto value : attr.getValues<T>()) output->Add(value);
+    output->Reserve(attr.getNumElements());
+    for (auto value : attr.getValues<T>()) output->AddAlreadyReserved(value);
   }
 }
 
@@ -275,8 +276,9 @@ void ConvertHalfElementsAttr(const mlir::DenseElementsAttr attr,
   if (attr.isSplat()) {
     output->Add(attr.getSplatValue<Eigen::half>().x);
   } else {
+    output->Reserve(attr.getNumElements());
     for (const Eigen::half value : attr.getValues<Eigen::half>())
-      output->Add(value.x);
+      output->AddAlreadyReserved(value.x);
   }
 }
 
@@ -287,7 +289,9 @@ void ConvertIntElementsAttr(const mlir::DenseIntElementsAttr attr,
   if (attr.isSplat()) {
     output->Add((*attr.begin()).getSExtValue());
   } else {
-    for (const llvm::APInt val : attr) output->Add(val.getSExtValue());
+    output->Reserve(attr.getNumElements());
+    for (const llvm::APInt val : attr)
+      output->AddAlreadyReserved(val.getSExtValue());
   }
 }
 
@@ -296,8 +300,9 @@ void ConvertBfloat16ElementsAttr(const mlir::DenseElementsAttr attr,
   if (attr.isSplat()) {
     output->Add(attr.getSplatValue<bfloat16>().value);
   } else {
+    output->Reserve(attr.getNumElements());
     for (const bfloat16 value : attr.getValues<bfloat16>())
-      output->Add(value.value);
+      output->AddAlreadyReserved(value.value);
   }
 }
 

commit 9bf801ba1f26e4a7091695a7f5b512b0ed19e99b
Author: qqq.jq <895521320@qq.com>
Date:   Tue Nov 3 10:37:29 2020 +0800

    Remove the race check due to poor branching efficiency

diff --git a/tensorflow/core/kernels/segment_reduction_ops_gpu.cu.cc b/tensorflow/core/kernels/segment_reduction_ops_gpu.cu.cc
index 983b97b953f..b293321520b 100644
--- a/tensorflow/core/kernels/segment_reduction_ops_gpu.cu.cc
+++ b/tensorflow/core/kernels/segment_reduction_ops_gpu.cu.cc
@@ -66,9 +66,6 @@ __global__ void SortedSegmentReductionCustomKernel(
     T reduce_res = initial_value;
     Index first_segment_id = segment_ids[input_outer_dim_index_base];
     Index last_output_segment_id = output_outer_dim_size;
-    bool is_first_has_race = input_outer_dim_index_base > 0 \
-        && first_segment_id == segment_ids[input_outer_dim_index_base - 1] \
-        ? true : false;
 
     const Index actual_stripe_height =
         min(Index(OuterDimTileSize),
@@ -85,7 +82,7 @@ __global__ void SortedSegmentReductionCustomKernel(
             last_output_segment_id * inner_dim_size + segment_offset;
         // decide whether to write result to global memory using atomic
         // operations
-        if (last_output_segment_id == first_segment_id && is_first_has_race) {
+        if (last_output_segment_id == first_segment_id) {
           SegmentAtomicReductionF()(output + output_index, reduce_res);
         } else {
           SegmentReductionF()(output + output_index, reduce_res);
@@ -100,20 +97,9 @@ __global__ void SortedSegmentReductionCustomKernel(
     // For the last result in a strip, always write using atomic operations
     // due to possible race conditions with threads computing
     // the following strip.
-    Index last_input_outer_dim_index =
-        input_outer_dim_index_base + actual_stripe_height - 1;
-    bool is_last_has_race =
-        last_input_outer_dim_index < input_outer_dim_size - 1 \
-          && segment_ids[last_input_outer_dim_index] == \
-             segment_ids[last_input_outer_dim_index + 1] \
-          ? true : false;
     const Index output_index =
         last_output_segment_id * inner_dim_size + segment_offset;
-    if (is_last_has_race) {
-      SegmentAtomicReductionF()(output + output_index, reduce_res);
-    } else {
-      SegmentReductionF()(output + output_index, reduce_res);
-    }
+    SegmentAtomicReductionF()(output + output_index, reduce_res);
   }
 }
 

commit 153947b5c51bc2b936eebda609bff77c19599211
Author: Prakalp Srivastava <prakalps@google.com>
Date:   Wed Jul 1 10:49:32 2020 -0700

    Only insert TPUCopyWithLayout when resource generator is on CPU.
    
    The placement of iterator is not determined by the device attribute of tf.IteratorGetNext but by the device attribute of the alias resource generator op. The generator ops are either function arguments or tf.VarHandle ops. So, we modify the TPU dynamic layout pass to check device attribute of generator ops.
    
    This CL modifies the Resource Alias Analysis to return the set of aliases of a given value. For efficiency, it now keeps a map of both resource value --> unique resource ID and vice-versa.
    
    This improves the performance of few models running with MLIR TF/XLA bridge. For example, Resnet50 step time is reduced from 180 ms to 125 ms (on par with the old bridge).
    
    PiperOrigin-RevId: 319255334
    Change-Id: I4df9f26f480b580b8277caae981f06c3189e7bf4

diff --git a/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.cc b/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.cc
index 35f02ba8445..e4de66b59e2 100644
--- a/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.cc
+++ b/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.cc
@@ -118,7 +118,7 @@ ResourceAliasAnalysis::ResourceAliasAnalysis(Operation* op) {
 }
 
 void ResourceAliasAnalysis::AnalyzeFunction(FuncOp func_op) {
-  // This function populates resource_value_to_ids_.
+  // This function populates resource_value_to_ids_ and id_to_resource_values_.
 
   // If the "tf.resource_arg_unique_id" argument attributes are present for
   // resource-type arguments, respect them when choosing IDs; otherwise, they
@@ -142,9 +142,9 @@ void ResourceAliasAnalysis::AnalyzeFunction(FuncOp func_op) {
              "or all arguments.");
       auto emplace_res = attr_id_to_internal_id.try_emplace(id_attr.getInt(),
                                                             next_unique_id++);
-      resource_value_to_ids_[arg].insert(emplace_res.first->getSecond());
+      AddValueUniqueIDMapping(arg, emplace_res.first->getSecond());
     } else {
-      resource_value_to_ids_[arg].insert(next_unique_id++);
+      AddValueUniqueIDMapping(arg, next_unique_id++);
     }
   }
   llvm::StringMap<int64_t> var_handle_name_id_map;
@@ -164,7 +164,8 @@ void ResourceAliasAnalysis::AnalyzeFunction(FuncOp func_op) {
 
   func_op.walk([&](Operation* op) {
     if (auto var_handle = llvm::dyn_cast<TF::VarHandleOp>(op)) {
-      resource_value_to_ids_[var_handle.resource()].insert(
+      AddValueUniqueIDMapping(
+          var_handle.resource(),
           GetOrCreateIdForVarHandle(var_handle, &next_unique_id,
                                     &var_handle_name_id_map));
     } else if (llvm::isa<TF::IdentityNOp>(op) ||
@@ -180,7 +181,7 @@ void ResourceAliasAnalysis::AnalyzeFunction(FuncOp func_op) {
       // different resources.
       for (auto arg : replicate.GetBody().getArguments()) {
         if (mlir::getElementTypeOrSelf(arg.getType()).isa<TF::ResourceType>()) {
-          resource_value_to_ids_[arg].insert(next_unique_id++);
+          AddValueUniqueIDMapping(arg, next_unique_id++);
         }
       }
     } else if (auto while_op = llvm::dyn_cast<TF::WhileOp>(op)) {
@@ -198,7 +199,7 @@ void ResourceAliasAnalysis::AnalyzeFunction(FuncOp func_op) {
           forward_input_to_output(while_op.getOperand(passthrough_operand),
                                   result.value());
         } else {
-          resource_value_to_ids_[result.value()].insert(kUnknownResourceId);
+          AddValueUniqueIDMapping(result.value(), kUnknownResourceId);
         }
       }
     } else if (auto if_op = llvm::dyn_cast<TF::IfOp>(op)) {
@@ -223,7 +224,7 @@ void ResourceAliasAnalysis::AnalyzeFunction(FuncOp func_op) {
           forward_input_to_output(if_op.getOperand(passthrough_else_arg + 1),
                                   result.value());
         } else {
-          resource_value_to_ids_[result.value()].insert(kUnknownResourceId);
+          AddValueUniqueIDMapping(result.value(), kUnknownResourceId);
         }
       }
     } else {
@@ -231,7 +232,7 @@ void ResourceAliasAnalysis::AnalyzeFunction(FuncOp func_op) {
         if (!mlir::getElementTypeOrSelf(result.getType())
                  .isa<TF::ResourceType>())
           continue;
-        resource_value_to_ids_[result].insert(kUnknownResourceId);
+        AddValueUniqueIDMapping(result, kUnknownResourceId);
       }
     }
   });
@@ -254,6 +255,24 @@ const llvm::SmallSet<int64_t, 8>& ResourceAliasAnalysis::GetResourceUniqueIds(
   return it->getSecond();
 }
 
+const llvm::SmallSetVector<Value, 8>&
+ResourceAliasAnalysis::GetUniqueIdResources(const int64_t id) const {
+  auto it = id_to_resource_values_.find(id);
+  assert(it != id_to_resource_values_.end() && "Unseen id was queried");
+  return it->getSecond();
+}
+
+llvm::SmallSetVector<Value, 8> ResourceAliasAnalysis::GetResourceAliases(
+    const Value resource) const {
+  assert(!IsUnknownResource(resource) && "Unseen resource was queried");
+  llvm::SmallSetVector<Value, 8> aliases;
+  for (int64_t id : GetResourceUniqueIds(resource)) {
+    const llvm::SmallSetVector<Value, 8>& resources_aliasing_id =
+        GetUniqueIdResources(id);
+    aliases.insert(resources_aliasing_id.begin(), resources_aliasing_id.end());
+  }
+  return aliases;
+}
 namespace {
 
 // Returns a set that contains only kUnknownResourceId.
diff --git a/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h b/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h
index 5989494f9aa..a318c6667c6 100644
--- a/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h
+++ b/tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h
@@ -19,6 +19,7 @@ limitations under the License.
 #include <cstdint>
 #include <memory>
 
+#include "llvm/ADT/SetVector.h"
 #include "llvm/ADT/SmallSet.h"
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/StringMap.h"
@@ -49,15 +50,34 @@ class ResourceAliasAnalysis {
   const llvm::SmallSet<int64_t, 8>& GetResourceUniqueIds(
       const Value resource) const;
 
+  // Returns the set of values that are potentially aliases of `value`. Requires
+  // that IsUnknownResource(resource) == true.
+  llvm::SmallSetVector<Value, 8> GetResourceAliases(const Value resource) const;
+
  private:
   ResourceAliasAnalysis() = default;
 
-  // Runs the analysis on `func_op` and populates resource_value_to_ids_.
+  // Runs the analysis on `func_op` and populates two way resource values to
+  // unique ID mapping.
   void AnalyzeFunction(FuncOp func_op);
 
+  // Maps resource value to unique ID and vice-versa.
+  void AddValueUniqueIDMapping(Value value, int64_t id) {
+    resource_value_to_ids_[value].insert(id);
+    id_to_resource_values_[id].insert(value);
+  }
+
+  // Returns the set unique Values which map to `id`.
+  const llvm::SmallSetVector<Value, 8>& GetUniqueIdResources(int64_t id) const;
+
   // Maps each resource-type value to a set of unique IDs that it could alias.
   llvm::SmallDenseMap<Value, llvm::SmallSet<int64_t, 8>, 8>
       resource_value_to_ids_;
+
+  // Maps each unique ID to a set of resource-type values that could alias to
+  // it. This is inverse of `resource_value_to_ids_` map.
+  llvm::SmallDenseMap<int64_t, llvm::SmallSetVector<Value, 8>, 8>
+      id_to_resource_values_;
 };
 
 // An analysis that runs on a function and infers the control predecessors and
diff --git a/tensorflow/compiler/mlir/tensorflow/tests/tpu-dynamic-layout-pass.mlir b/tensorflow/compiler/mlir/tensorflow/tests/tpu-dynamic-layout-pass.mlir
index eb420505358..9467f890419 100644
--- a/tensorflow/compiler/mlir/tensorflow/tests/tpu-dynamic-layout-pass.mlir
+++ b/tensorflow/compiler/mlir/tensorflow/tests/tpu-dynamic-layout-pass.mlir
@@ -106,10 +106,107 @@ func @on_tpu_iter(%arg0: tensor<*x!tf.resource> {tf.device = "/device:TPU:0"}) -
 
 // -----
 
+// Tests that the pass does not transform when tf.IteratorGetNext is on CPU
+// but generator is on TPU.
+
+// CHECK-LABEL: func @arg_on_tpu_iter_on_cpu
+func @arg_on_tpu_iter_on_cpu(%arg0: tensor<*x!tf.resource> {tf.device = "/device:TPU:0"}) -> tensor<i32> {
+  %compile:2 = "tf_device.launch"() ( {
+    %1:2 = "tf._TPUCompileMlir"() {
+      NumDynamicShapes = 0 : i64,
+      // The metadata encodes 2 parameter and two return values.
+      metadata = "\0A\0E\08\01\18\01\22\08\08\01\1A\01\01\22\01\00\0A \08\01\12\10\12\02\08\03\12\02\08\03\12\02\08\01\12\02\08 \18\01\22\08\08\01\1A\01\01\22\01\00\12\0A\0A\08\08\01\1A\01\01\22\01\00\12\0A\0A\08\08\01\1A\01\01\22\01\00\18\02 \01",
+      mlir_module = "..."} : () -> (tensor<!tf.string>, tensor<!tf.string>)
+    tf_device.return %1#0, %1#1 : tensor<!tf.string>, tensor<!tf.string>
+  }) {device = "/device:CPU:0"} : () -> (tensor<!tf.string>, tensor<!tf.string>)
+  // CHECK-NOT: "tf.TPUGetLayoutOp"
+  // CHECK-NOT: "tf.TPUCopyWithLayout"
+  %2:2 = "tf.IteratorGetNext"(%arg0) {device = "/device:CPU:0"}
+    : (tensor<*x!tf.resource>) -> (tensor<3x3x1x32xf32>, tensor<3x3x1x32xf32>)
+  "tf_device.launch"() ( {
+    "tf.TPUCompileSucceededAssert"(%compile#0) : (tensor<!tf.string>) -> ()
+    tf_device.return
+  }) {device = "/device:CPU:0"} : () -> ()
+  %execute = "tf_device.launch"() ( {
+    %3 = "tf.TPUExecute"(%2#0, %2#1, %compile#1)
+      : (tensor<3x3x1x32xf32>, tensor<3x3x1x32xf32>, tensor<!tf.string>) -> tensor<i32>
+    tf_device.return %3 : tensor<i32>
+  }) {device = "/device:TPU:0"} : () -> tensor<i32>
+  return %execute : tensor<i32>
+}
+
+// -----
+
+// Tests that the pass does not transform when tf.IteratorGetNext is on CPU but
+// generator is on TPU. All intermediate nodes like tf.Identity between
+// generator and IteratorGetNext are on CPU too.
+
+// CHECK-LABEL: func @arg_on_tpu_intermediate_ops_on_cpu
+func @arg_on_tpu_intermediate_ops_on_cpu(%arg0: tensor<*x!tf.resource> {tf.device = "/device:TPU:0"}) -> tensor<i32> {
+  %compile:2 = "tf_device.launch"() ( {
+    %1:2 = "tf._TPUCompileMlir"() {
+      NumDynamicShapes = 0 : i64,
+      // The metadata encodes 2 parameter and two return values.
+      metadata = "\0A\0E\08\01\18\01\22\08\08\01\1A\01\01\22\01\00\0A \08\01\12\10\12\02\08\03\12\02\08\03\12\02\08\01\12\02\08 \18\01\22\08\08\01\1A\01\01\22\01\00\12\0A\0A\08\08\01\1A\01\01\22\01\00\12\0A\0A\08\08\01\1A\01\01\22\01\00\18\02 \01",
+      mlir_module = "..."} : () -> (tensor<!tf.string>, tensor<!tf.string>)
+    tf_device.return %1#0, %1#1 : tensor<!tf.string>, tensor<!tf.string>
+  }) {device = "/device:CPU:0"} : () -> (tensor<!tf.string>, tensor<!tf.string>)
+  %id1 = "tf.Identity"(%arg0) {device = "/device:CPU:0"} : (tensor<*x!tf.resource>) -> (tensor<*x!tf.resource>)
+  %id2 = "tf.Identity"(%id1) {device = "/device:CPU:0"} : (tensor<*x!tf.resource>) -> (tensor<*x!tf.resource>)
+  // CHECK-NOT: "tf.TPUGetLayoutOp"
+  // CHECK-NOT: "tf.TPUCopyWithLayout"
+  %2:2 = "tf.IteratorGetNext"(%id2) {device = "/device:CPU:0"}
+    : (tensor<*x!tf.resource>) -> (tensor<3x3x1x32xf32>, tensor<3x3x1x32xf32>)
+  "tf_device.launch"() ( {
+    "tf.TPUCompileSucceededAssert"(%compile#0) : (tensor<!tf.string>) -> ()
+    tf_device.return
+  }) {device = "/device:CPU:0"} : () -> ()
+  %execute = "tf_device.launch"() ( {
+    %3 = "tf.TPUExecute"(%2#0, %2#1, %compile#1)
+      : (tensor<3x3x1x32xf32>, tensor<3x3x1x32xf32>, tensor<!tf.string>) -> tensor<i32>
+    tf_device.return %3 : tensor<i32>
+  }) {device = "/device:TPU:0"} : () -> tensor<i32>
+  return %execute : tensor<i32>
+}
+
+// -----
+
+// Tests that the pass does not transform when tf.IteratorGetNext is on CPU but
+// generator is on TPU.
+
+// CHECK-LABEL: func @var_handle_on_tpu_iter_on_cpu
+func @var_handle_on_tpu_iter_on_cpu() -> tensor<i32> {
+  %compile:2 = "tf_device.launch"() ( {
+    %1:2 = "tf._TPUCompileMlir"() {
+      NumDynamicShapes = 0 : i64,
+      // The metadata encodes 2 parameter and two return values.
+      metadata = "\0A\0E\08\01\18\01\22\08\08\01\1A\01\01\22\01\00\0A \08\01\12\10\12\02\08\03\12\02\08\03\12\02\08\01\12\02\08 \18\01\22\08\08\01\1A\01\01\22\01\00\12\0A\0A\08\08\01\1A\01\01\22\01\00\12\0A\0A\08\08\01\1A\01\01\22\01\00\18\02 \01",
+      mlir_module = "..."} : () -> (tensor<!tf.string>, tensor<!tf.string>)
+    tf_device.return %1#0, %1#1 : tensor<!tf.string>, tensor<!tf.string>
+  }) {device = "/device:CPU:0"} : () -> (tensor<!tf.string>, tensor<!tf.string>)
+  %var = "tf.VarHandleOp"() {container = "c", shared_name = "v", device = "/device:TPU:0"} : () -> tensor<*x!tf.resource>
+  // CHECK-NOT: "tf.TPUGetLayoutOp"
+  // CHECK-NOT: "tf.TPUCopyWithLayout"
+  %2:2 = "tf.IteratorGetNext"(%var) {device = "/device:CPU:0"}
+    : (tensor<*x!tf.resource>) -> (tensor<3x3x1x32xf32>, tensor<3x3x1x32xf32>)
+  "tf_device.launch"() ( {
+    "tf.TPUCompileSucceededAssert"(%compile#0) : (tensor<!tf.string>) -> ()
+    tf_device.return
+  }) {device = "/device:CPU:0"} : () -> ()
+  %execute = "tf_device.launch"() ( {
+    %3 = "tf.TPUExecute"(%2#0, %2#1, %compile#1)
+      : (tensor<3x3x1x32xf32>, tensor<3x3x1x32xf32>, tensor<!tf.string>) -> tensor<i32>
+    tf_device.return %3 : tensor<i32>
+  }) {device = "/device:TPU:0"} : () -> tensor<i32>
+  return %execute : tensor<i32>
+}
+
+// -----
+
 // Tests that the pass does not change unsupported input ops.
 
 // CHECK-LABEL: func @unsupported_ops
-func @unsupported_ops(%arg0: tensor<3x3x1x32xf32>) -> tensor<i32> {
+func @unsupported_ops(%arg0: tensor<3x3x1x32xf32> {tf.device = "/device:CPU:0"}) -> tensor<i32> {
   %compile:2 = "tf_device.launch"() ( {
     %1:2 = "tf._TPUCompileMlir"() {
       NumDynamicShapes = 0 : i64,
@@ -183,7 +280,7 @@ func @replicated(%arg0: tensor<*x!tf.resource> {tf.device = "/device:CPU:0"}) ->
 // Tests that the pass does not change inputs inside replicate.
 
 // CHECK-LABEL: func @inside_replicated
-func @inside_replicated(%arg0: tensor<*x!tf.resource>, %arg1: tensor<*x!tf.resource>) -> tensor<i32> {
+func @inside_replicated(%arg0: tensor<*x!tf.resource> {tf.device = "/device:CPU:0"}, %arg1: tensor<*x!tf.resource> {tf.device = "/device:CPU:0"}) -> tensor<i32> {
   %compile:2 = "tf_device.launch"() ( {
     %1:2 = "tf._TPUCompileMlir"() {
       NumDynamicShapes = 0 : i64,
@@ -229,7 +326,7 @@ func @inside_replicated(%arg0: tensor<*x!tf.resource>, %arg1: tensor<*x!tf.resou
 // num_cores_per_replica: 2
 
 // CHECK-LABEL: func @parallel_execute
-func @parallel_execute(%arg0: tensor<*x!tf.resource>) {
+func @parallel_execute(%arg0: tensor<*x!tf.resource> {tf.device = "/device:CPU:0"}) {
   // CHECK: %[[COMPILE:.*]]:3 = "tf_device.launch"
   // CHECK-NEXT: "tf._TPUCompileMlir"()
   %compile:3 = "tf_device.launch"() ( {
@@ -293,8 +390,9 @@ func @parallel_execute(%arg0: tensor<*x!tf.resource>) {
 // num_cores_per_replica: 2
 
 // CHECK-LABEL: func @replicated_parallel_execute
-// CHECK-SAME: (%[[ARG0:[a-z0-9]+]]: tensor<*x!tf.resource>, %[[ARG1:[a-z0-9]+]]: tensor<*x!tf.resource>)
-func @replicated_parallel_execute(%arg0: tensor<*x!tf.resource>, %arg1: tensor<*x!tf.resource>) {
+// CHECK-SAME: %[[ARG0:[a-z0-9]+]]: tensor<*x!tf.resource>
+// CHECK-SAME: %[[ARG1:[a-z0-9]+]]: tensor<*x!tf.resource>
+func @replicated_parallel_execute(%arg0: tensor<*x!tf.resource> {tf.device = "/device:CPU:0"}, %arg1: tensor<*x!tf.resource> {tf.device = "/device:CPU:0"}) {
   // CHECK: %[[COMPILE:.*]]:3 = "tf_device.launch"
   // CHECK-NEXT: "tf._TPUCompileMlir"()
   %compile:3 = "tf_device.launch"() ( {
diff --git a/tensorflow/compiler/mlir/tensorflow/transforms/tpu_dynamic_layout_pass.cc b/tensorflow/compiler/mlir/tensorflow/transforms/tpu_dynamic_layout_pass.cc
index 3fbd8369b7e..e2f4fca1219 100644
--- a/tensorflow/compiler/mlir/tensorflow/transforms/tpu_dynamic_layout_pass.cc
+++ b/tensorflow/compiler/mlir/tensorflow/transforms/tpu_dynamic_layout_pass.cc
@@ -32,6 +32,7 @@ limitations under the License.
 #include "mlir/Pass/Pass.h"  // from @llvm-project
 #include "mlir/Pass/PassRegistry.h"  // from @llvm-project
 #include "mlir/Support/LLVM.h"  // from @llvm-project
+#include "tensorflow/compiler/mlir/tensorflow/analysis/side_effect_analysis.h"
 #include "tensorflow/compiler/mlir/tensorflow/ir/tf_device.h"
 #include "tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h"
 #include "tensorflow/compiler/mlir/tensorflow/ir/tf_types.h"
@@ -46,6 +47,8 @@ namespace TFTPU {
 namespace {
 
 constexpr char kDeviceAttr[] = "device";
+constexpr char kDeviceCPU[] = "CPU";
+constexpr char kFuncDeviceAttr[] = "tf.device";
 
 // A pass that allows TPU input layout to be determined after JIT compilation.
 // This is done by adding run-time ops that interpret compilation result and
@@ -79,17 +82,49 @@ struct TPUDynamicLayoutPass
 };
 
 // Checks if the input producer op is supported in this transform. Right now, we
-// only check if it is a host tf.IteratorGetNext.
-bool IsSupportedInputOp(Operation* op) {
-  if (!llvm::isa<TF::IteratorGetNextOp>(op)) return false;
-  auto device = op->getAttrOfType<StringAttr>(kDeviceAttr);
-  if (!device) return false;
-  tensorflow::DeviceNameUtils::ParsedName parsed_device;
-  if (!tensorflow::DeviceNameUtils::ParseFullName(device.getValue().str(),
-                                                  &parsed_device)) {
+// only check if it is a tf.IteratorGetNext where resource input is coming from
+// a VarHandle on CPU or a function argument assigned to CPU.
+bool IsSupportedInputOp(Operation* op,
+                        TF::ResourceAliasAnalysis* resource_alias_analysis) {
+  TF::IteratorGetNextOp iterator_op = llvm::dyn_cast<TF::IteratorGetNextOp>(op);
+  if (!iterator_op) return false;
+
+  Value resource_iterator = iterator_op.iterator();
+
+  if (resource_alias_analysis->IsUnknownResource(resource_iterator))
     return false;
-  }
-  return parsed_device.type == "CPU";
+  llvm::SmallSetVector<Value, 8> aliases =
+      resource_alias_analysis->GetResourceAliases(resource_iterator);
+
+  auto is_generator = [](Value val) {
+    if (val.isa<BlockArgument>()) return true;
+    Operation* definition = val.getDefiningOp();
+    return definition->getNumOperands() == 0 &&
+           definition->getNumResults() == 1;
+  };
+
+  // Check all generator aliases (ops or function argument) are on CPU.
+  FuncOp func = iterator_op.getParentOfType<FuncOp>();
+  return llvm::all_of(aliases, [&](Value alias) {
+    // Ignore non-generator aliases.
+    if (!is_generator(alias)) return true;
+
+    StringAttr device;
+    if (auto arg = alias.dyn_cast<BlockArgument>()) {
+      device = func.getArgAttrOfType<mlir::StringAttr>(arg.getArgNumber(),
+                                                       kFuncDeviceAttr);
+    } else {
+      device = alias.getDefiningOp()->getAttrOfType<StringAttr>(kDeviceAttr);
+    }
+
+    if (!device) return false;
+    tensorflow::DeviceNameUtils::ParsedName parsed_device;
+    if (!tensorflow::DeviceNameUtils::ParseFullName(device.getValue().str(),
+                                                    &parsed_device)) {
+      return false;
+    }
+    return parsed_device.has_type && parsed_device.type == kDeviceCPU;
+  });
 }
 
 OpBuilder CreateBuilderAfterOp(Operation* op) {
@@ -139,12 +174,11 @@ void HandleInput(Value input, const int64_t execute_arg_index,
 
 // Performs transformation for replicated inputs. Returns true if this is a
 // supported case (thus transform happened).
-bool HandleReplicatedInputs(const int64_t execute_arg_index,
-                            Value compilation_key,
-                            tf_device::LaunchOp execute_launch,
-                            tf_device::LaunchOp compile_launch,
-                            const int64_t replicate_arg_index,
-                            tf_device::ReplicateOp replicate) {
+bool HandleReplicatedInputs(
+    const int64_t execute_arg_index, Value compilation_key,
+    tf_device::LaunchOp execute_launch, tf_device::LaunchOp compile_launch,
+    const int64_t replicate_arg_index, tf_device::ReplicateOp replicate,
+    TF::ResourceAliasAnalysis* resource_alias_analysis) {
   // We need to know the devices to copy to.
   if (!replicate.devices()) return false;
   int64_t num_replicas = replicate.n().getZExtValue();
@@ -153,7 +187,8 @@ bool HandleReplicatedInputs(const int64_t execute_arg_index,
                     .take_front(num_replicas);
   for (auto entry : llvm::enumerate(inputs)) {
     auto input_op = entry.value().getDefiningOp();
-    if (!input_op || !IsSupportedInputOp(input_op)) return false;
+    if (!input_op || !IsSupportedInputOp(input_op, resource_alias_analysis))
+      return false;
   }
   OpBuilder builder = CreateBuilderAfterOp(compile_launch);
   auto get_layout = BuildGetLayout(execute_arg_index, compilation_key,
@@ -180,7 +215,8 @@ bool HandleReplicatedInputs(const int64_t execute_arg_index,
 // compile should not have other uses.
 void HandleCompileAndExecutes(
     tf_device::LaunchOp compile_launch,
-    llvm::MutableArrayRef<tf_device::LaunchOp> execute_launches) {
+    llvm::MutableArrayRef<tf_device::LaunchOp> execute_launches,
+    TF::ResourceAliasAnalysis* resource_alias_analysis) {
   auto compile =
       llvm::cast<TF::_TPUCompileMlirOp>(compile_launch.GetBody().front());
   tensorflow::tpu::TPUCompileMetadataProto metadata;
@@ -206,9 +242,10 @@ void HandleCompileAndExecutes(
         // For a block argument, consider transforms only when it is a
         // replicated input (defining ops will be outside the replicate node).
         if (maybe_replicate != block_arg.getParentRegion()->getParentOp() ||
-            !HandleReplicatedInputs(
-                execute_arg_index, execute.key(), execute_launch,
-                compile_launch, block_arg.getArgNumber(), maybe_replicate)) {
+            !HandleReplicatedInputs(execute_arg_index, execute.key(),
+                                    execute_launch, compile_launch,
+                                    block_arg.getArgNumber(), maybe_replicate,
+                                    resource_alias_analysis)) {
           continue;
         }
       } else {
@@ -221,7 +258,7 @@ void HandleCompileAndExecutes(
             maybe_replicate.body().isAncestor(input_op->getParentRegion())) {
           continue;
         }
-        if (!IsSupportedInputOp(input_op)) continue;
+        if (!IsSupportedInputOp(input_op, resource_alias_analysis)) continue;
         HandleInput(input, execute_arg_index, execute, execute_launch,
                     compile_launch);
       }
@@ -238,7 +275,8 @@ void HandleCompileAndExecutes(
 }
 
 void TPUDynamicLayoutPass::runOnFunction() {
-  getFunction().walk([](TF::_TPUCompileMlirOp compile) {
+  TF::ResourceAliasAnalysis resource_alias_analysis(getFunction());
+  getFunction().walk([&](TF::_TPUCompileMlirOp compile) {
     // Detect tf._TPUCompileMlir -> tf.TPUExecute(s).
     auto compile_launch =
         llvm::dyn_cast<tf_device::LaunchOp>(compile.getParentOp());
@@ -257,7 +295,8 @@ void TPUDynamicLayoutPass::runOnFunction() {
       execute_launches.push_back(execute_launch);
     }
 
-    HandleCompileAndExecutes(compile_launch, execute_launches);
+    HandleCompileAndExecutes(compile_launch, execute_launches,
+                             &resource_alias_analysis);
   });
 }
 

commit fc2d7fdacb35001e9b98ff8b844679985bbf61a4
Author: Derek Murray <mrry@google.com>
Date:   Fri Apr 3 16:57:16 2020 -0700

    [Executor] Reorganize code in `ExecutorState::NodeDone()` for efficiency.
    
    Executor microbenchmarks show a 3.22% to 4.16% improvement with this change, which avoids re-checking the status multiple times in the non-error case.
    
    PiperOrigin-RevId: 304719934
    Change-Id: I6a9e3d1db8b13f32eb558a57fcb272c07ba1079a

diff --git a/tensorflow/core/common_runtime/executor.cc b/tensorflow/core/common_runtime/executor.cc
index 84bf02eae02..f1177e8cba4 100644
--- a/tensorflow/core/common_runtime/executor.cc
+++ b/tensorflow/core/common_runtime/executor.cc
@@ -316,6 +316,8 @@ class ExecutorState {
   // nodes in 'ready' into 'inline_ready'.
   //
   // This method will clear `*ready` before returning.
+  //
+  // REQUIRES: `!ready->empty()`.
   void ScheduleReady(TaggedNodeSeq* ready, TaggedNodeReadyQueue* inline_ready);
 
   // Clean up when this executor is done.
@@ -1022,73 +1024,80 @@ template <class PropagatorStateType>
 bool ExecutorState<PropagatorStateType>::NodeDone(
     const Status& s, TaggedNodeSeq* ready, NodeExecStatsInterface* stats,
     TaggedNodeReadyQueue* inline_ready) {
-  nodestats::SetAllEnd(stats);
   if (stats) {
-    if (stats_collector_) {
-      stats->Done(immutable_state_.params().device->name());
+    nodestats::SetAllEnd(stats);
+    DCHECK_NE(stats_collector_, nullptr);
+    stats->Done(immutable_state_.params().device->name());
+  }
+
+  if (TF_PREDICT_TRUE(s.ok())) {
+    const size_t ready_size = ready->size();
+    if (ready_size == 0) {
+      return num_outstanding_ops_.fetch_sub(1) == 1;
     } else {
-      delete stats;
+      // NOTE: Avoid touching the atomic counter if only one node becomes ready.
+      if (ready_size > 1) {
+        num_outstanding_ops_.fetch_add(ready_size - 1,
+                                       std::memory_order_relaxed);
+      }
+
+      // Schedule the ready nodes in 'ready'.
+      ScheduleReady(ready, inline_ready);
+
+      return false;
     }
-  }
+  } else {
+    bool abort_run = false;
 
-  bool abort_run = false;
-  if (!s.ok()) {
     // Some error happened. This thread of computation is done.
-    mutex_lock l(mu_);
-    if (status_.ok()) {
-      abort_run = true;
-
-      // If execution has been cancelled, mark any new errors as being derived.
-      // This ensures any errors triggered by cancellation are marked as
-      // derived.
-      if (cancellation_manager_ && cancellation_manager_->IsCancelled()) {
-        status_ = StatusGroup::MakeDerived(s);
-      } else {
-        status_ = s;
+    {
+      mutex_lock l(mu_);
+      if (status_.ok()) {
+        // If this is the first node to fail in this run, we are responsible for
+        // aborting all other execution in the step.
+        abort_run = true;
+
+        // If execution has been cancelled, mark any new errors as being
+        // derived. This ensures any errors triggered by cancellation are marked
+        // as derived.
+        if (cancellation_manager_ && cancellation_manager_->IsCancelled()) {
+          status_ = StatusGroup::MakeDerived(s);
+        } else {
+          status_ = s;
+        }
       }
     }
-  }
-  if (abort_run) {
-    TRACEPRINTF("StartAbort: %s", s.ToString().c_str());
-    if (cancellation_manager_) {
-      // only log when the abort happens during the actual run time.
-      auto device_name = immutable_state_.params().device->name();
-      // Use VLOG instead of LOG(warning) because error status is expected when
-      // the executor is run under the grappler optimization phase or when
-      // iterating through a tf.data input pipeline.
-      VLOG(1) << "[" << device_name << "] Executor start aborting: " << s;
-    }
 
-    if (rendezvous_) {
-      rendezvous_->StartAbort(s);
-    }
-    if (collective_executor_) {
-      collective_executor_->StartAbort(s);
-    }
-    if (cancellation_manager_) {
-      cancellation_manager_->StartCancel();
-    }
-  }
+    if (abort_run) {
+      TRACEPRINTF("StartAbort: %s", s.ToString().c_str());
+      if (cancellation_manager_) {
+        // Only log when the abort happens during the actual run time.
+        // Use VLOG instead of LOG(warning) because error status is expected
+        // when the executor is run under the grappler optimization phase or
+        // when iterating through a tf.data input pipeline.
+        VLOG(1) << "[" << immutable_state_.params().device->name()
+                << "] Executor start aborting: " << s;
+      }
 
-  bool completed = false;
-  const size_t ready_size = ready->size();
-  if (ready_size == 0 || !s.ok()) {
-    completed = (num_outstanding_ops_.fetch_sub(1) == 1);
-  } else if (ready_size > 1) {
-    num_outstanding_ops_.fetch_add(ready_size - 1, std::memory_order_relaxed);
-  }
+      if (rendezvous_) {
+        rendezvous_->StartAbort(s);
+      }
+      if (collective_executor_) {
+        collective_executor_->StartAbort(s);
+      }
+      if (cancellation_manager_) {
+        cancellation_manager_->StartCancel();
+      }
+    }
 
-  // Schedule the ready nodes in 'ready'.
-  if (s.ok()) {
-    ScheduleReady(ready, inline_ready);
+    return num_outstanding_ops_.fetch_sub(1) == 1;
   }
-  return completed;
 }
 
 template <class PropagatorStateType>
 void ExecutorState<PropagatorStateType>::ScheduleReady(
     TaggedNodeSeq* ready, TaggedNodeReadyQueue* inline_ready) {
-  if (ready->empty()) return;
+  DCHECK(!ready->empty());
 
   int64 scheduled_nsec = 0;
   if (stats_collector_) {

commit f120f7d514d50428bc34b4435ea8253f5cece990
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Feb 20 14:59:42 2020 -0800

    Suppress 'conversion to a dense matrix' warning from LinearOperatorFullMatrix.solve().
    
    The current warning is inappropriate: since a LinearOperatorFullMatrix is inherently dense, no efficiency is lost when we treat it as dense.
    
    PiperOrigin-RevId: 296305093
    Change-Id: Id3b7e2a00f05d1e516374c4241cd84529844a056

diff --git a/tensorflow/python/ops/linalg/linear_operator.py b/tensorflow/python/ops/linalg/linear_operator.py
index 194889c1ad5..4a181d72f2a 100644
--- a/tensorflow/python/ops/linalg/linear_operator.py
+++ b/tensorflow/python/ops/linalg/linear_operator.py
@@ -751,14 +751,11 @@ class LinearOperator(module.Module):
     with self._name_scope(name):
       return self._log_abs_determinant()
 
-  def _solve(self, rhs, adjoint=False, adjoint_arg=False):
-    """Default implementation of _solve."""
-    if self.is_square is False:
+  def _dense_solve(self, rhs, adjoint=False, adjoint_arg=False):
+    """Solve by conversion to a dense matrix."""
+    if self.is_square is False:  # pylint: disable=g-bool-id-comparison
       raise NotImplementedError(
           "Solve is not yet implemented for non-square operators.")
-    logging.warn(
-        "Using (possibly slow) default implementation of solve."
-        "  Requires conversion to a dense matrix and O(N^3) operations.")
     rhs = linalg.adjoint(rhs) if adjoint_arg else rhs
     if self._can_use_cholesky():
       return linalg_ops.cholesky_solve(
@@ -766,6 +763,13 @@ class LinearOperator(module.Module):
     return linear_operator_util.matrix_solve_with_broadcast(
         self.to_dense(), rhs, adjoint=adjoint)
 
+  def _solve(self, rhs, adjoint=False, adjoint_arg=False):
+    """Default implementation of _solve."""
+    logging.warn(
+        "Using (possibly slow) default implementation of solve."
+        "  Requires conversion to a dense matrix and O(N^3) operations.")
+    return self._dense_solve(rhs, adjoint=adjoint, adjoint_arg=adjoint_arg)
+
   def solve(self, rhs, adjoint=False, adjoint_arg=False, name="solve"):
     """Solve (exact or approx) `R` (batch) systems of equations: `A X = rhs`.
 
diff --git a/tensorflow/python/ops/linalg/linear_operator_full_matrix.py b/tensorflow/python/ops/linalg/linear_operator_full_matrix.py
index 8fe68919250..8d92d1accaa 100644
--- a/tensorflow/python/ops/linalg/linear_operator_full_matrix.py
+++ b/tensorflow/python/ops/linalg/linear_operator_full_matrix.py
@@ -183,5 +183,8 @@ class LinearOperatorFullMatrix(linear_operator.LinearOperator):
     return math_ops.matmul(
         self._matrix, x, adjoint_a=adjoint, adjoint_b=adjoint_arg)
 
+  def _solve(self, rhs, adjoint=False, adjoint_arg=False):
+    return self._dense_solve(rhs, adjoint=adjoint, adjoint_arg=adjoint_arg)
+
   def _to_dense(self):
     return self._matrix

commit 942f618880c3616caf19882cbeb3445b2ea80f6a
Author: Shanqing Cai <cais@google.com>
Date:   Tue Dec 24 20:45:55 2019 -0800

    [tfdbg] Log device info for eager executions in DebugEvent file set
    
    - For space efficiency, devices are stored as IDs, instead of string names.
    - The DebuggedGraph proto and data object enables looking up device names from the said IDs.
    
    PiperOrigin-RevId: 287067889
    Change-Id: I027dff0367244ce427438377afd90df43bdf8e7a

diff --git a/tensorflow/core/protobuf/debug_event.proto b/tensorflow/core/protobuf/debug_event.proto
index ebbb93ee049..badd518fa69 100644
--- a/tensorflow/core/protobuf/debug_event.proto
+++ b/tensorflow/core/protobuf/debug_event.proto
@@ -100,6 +100,9 @@ message DebugEvent {
     // The ID of the graph (i.e., FuncGraph) executed here: applicable only
     // to the execution of a FuncGraph.
     string graph_id = 11;
+
+    // A device on which debugger-instrumented ops and/or tensors reside.
+    DebuggedDevice debugged_device = 12;
   }
 }
 
@@ -205,6 +208,18 @@ message DebuggedGraph {
   string outer_context_id = 6;
 }
 
+// A device on which ops and/or tensors are instrumented by the debugger.
+message DebuggedDevice {
+  // Name of the device.
+  string device_name = 1;
+
+  // A debugger-generated ID for the device. Guaranteed to be unique within
+  // the scope of the debugged TensorFlow program, including single-host and
+  // multi-host settings.
+  // TODO(cais): Test the uniqueness guarantee in multi-host settings.
+  int32 device_id = 2;
+}
+
 // Data relating to the eager execution of an op or a Graph.
 // For a op that generates N output tensors (N >= 0), only one
 // Execution proto will be used to describe the execution event.
@@ -237,6 +252,11 @@ message Execution {
   // Stack trace of the eager execution.
   CodeLocation code_location = 8;
 
+  // Debugged-generated IDs of the devices on which the output tensors reside.
+  // To look up details about the device (e.g., name), cross-reference this
+  // field with the DebuggedDevice messages.
+  repeated int32 output_tensor_device_ids = 9;
+
   // TODO(cais): When backporting to V1 Session.run() support, add more fields
   // such as fetches and feeds.
 }
diff --git a/tensorflow/core/util/BUILD b/tensorflow/core/util/BUILD
index b79d26631da..8568507cafd 100644
--- a/tensorflow/core/util/BUILD
+++ b/tensorflow/core/util/BUILD
@@ -490,6 +490,7 @@ tf_cc_tests(
         "//tensorflow/core/platform:regexp",
         "//third_party/eigen3",
         "@com_google_absl//absl/base",
+        "@com_google_absl//absl/container:flat_hash_set",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/strings",
     ],
diff --git a/tensorflow/core/util/debug_events_writer.cc b/tensorflow/core/util/debug_events_writer.cc
index 58994e7a9fd..595f92d07c0 100644
--- a/tensorflow/core/util/debug_events_writer.cc
+++ b/tensorflow/core/util/debug_events_writer.cc
@@ -322,6 +322,23 @@ void DebugEventsWriter::WriteSerializedExecutionDebugEvent(
   }
 }
 
+int DebugEventsWriter::RegisterDeviceAndGetId(const string& device_name) {
+  mutex_lock l(device_mu_);
+  int& device_id = device_name_to_id_[device_name];
+  if (device_id == 0) {
+    device_id = device_name_to_id_.size();
+    DebugEvent debug_event;
+    MaybeSetDebugEventTimestamp(&debug_event, env_);
+    DebuggedDevice* debugged_device = debug_event.mutable_debugged_device();
+    debugged_device->set_device_name(device_name);
+    debugged_device->set_device_id(device_id);
+    string serialized;
+    debug_event.SerializeToString(&serialized);
+    graphs_writer_->WriteSerializedDebugEvent(serialized);
+  }
+  return device_id;
+}
+
 Status DebugEventsWriter::FlushNonExecutionFiles() {
   TF_RETURN_IF_ERROR(Init());
   if (source_files_writer_ != nullptr) {
@@ -448,7 +465,9 @@ DebugEventsWriter::DebugEventsWriter(const string& dump_root,
       execution_buffer_(),
       execution_buffer_mu_(),
       graph_execution_trace_buffer_(),
-      graph_execution_trace_buffer_mu_() {}
+      graph_execution_trace_buffer_mu_(),
+      device_name_to_id_(),
+      device_mu_() {}
 
 Status DebugEventsWriter::InitNonMetadataFile(DebugEventFileType type) {
   std::unique_ptr<SingleDebugEventFileWriter>* writer = nullptr;
diff --git a/tensorflow/core/util/debug_events_writer.h b/tensorflow/core/util/debug_events_writer.h
index 951dcba1dfb..78c23e3b851 100644
--- a/tensorflow/core/util/debug_events_writer.h
+++ b/tensorflow/core/util/debug_events_writer.h
@@ -18,6 +18,7 @@ limitations under the License.
 
 #include <deque>
 
+#include "absl/container/flat_hash_map.h"
 #include "tensorflow/core/framework/tensor.h"
 #include "tensorflow/core/lib/core/status.h"
 #include "tensorflow/core/lib/io/record_writer.h"
@@ -177,6 +178,11 @@ class DebugEventsWriter {
   void WriteSerializedExecutionDebugEvent(const string& debug_event_str,
                                           DebugEventFileType type);
 
+  // Given name of the device, retrieve a unique integer ID. As a side effect,
+  // if this is the first time this object encounters the device name,
+  // writes a DebuggedDevice proto to the .graphs file in the file set.
+  int RegisterDeviceAndGetId(const string& device_name);
+
   // EventWriter automatically flushes and closes on destruction, but
   // this method is provided for users who want to write to disk sooner
   // and/or check for success.
@@ -233,6 +239,9 @@ class DebugEventsWriter {
       GUARDED_BY(graph_execution_trace_buffer_mu_);
   mutex graph_execution_trace_buffer_mu_;
 
+  absl::flat_hash_map<string, int> device_name_to_id_ GUARDED_BY(device_mu_);
+  mutex device_mu_;
+
   std::unique_ptr<SingleDebugEventFileWriter> metadata_writer_;
   std::unique_ptr<SingleDebugEventFileWriter> source_files_writer_;
   std::unique_ptr<SingleDebugEventFileWriter> stack_frames_writer_;
diff --git a/tensorflow/core/util/debug_events_writer_test.cc b/tensorflow/core/util/debug_events_writer_test.cc
index 6ce7a063b92..e442a417e99 100644
--- a/tensorflow/core/util/debug_events_writer_test.cc
+++ b/tensorflow/core/util/debug_events_writer_test.cc
@@ -17,6 +17,7 @@ limitations under the License.
 
 #include <vector>
 
+#include "absl/container/flat_hash_set.h"
 #include "tensorflow/core/lib/core/status_test_util.h"
 #include "tensorflow/core/lib/core/threadpool.h"
 #include "tensorflow/core/lib/io/path.h"
@@ -756,6 +757,50 @@ TEST_F(DebugEventsWriterTest, WriteGrahExecutionTraceWithCyclicBufferFlush) {
   EXPECT_EQ(actuals.size(), 0);
 }
 
+TEST_F(DebugEventsWriterTest, RegisterDeviceAndGetIdTrace) {
+  DebugEventsWriter* writer =
+      DebugEventsWriter::GetDebugEventsWriter(dump_root_);
+  TF_ASSERT_OK(writer->Init());
+
+  // Register and get some device IDs in a concurrent fashion.
+  thread::ThreadPool* thread_pool =
+      new thread::ThreadPool(Env::Default(), "test_pool", 8);
+  int device_ids[8];
+  for (int i = 0; i < 8; ++i) {
+    thread_pool->Schedule([i, &writer, &device_ids]() {
+      const string device_name = strings::Printf(
+          "/job:localhost/replica:0/task:0/device:GPU:%d", i % 4);
+      device_ids[i] = writer->RegisterDeviceAndGetId(device_name);
+    });
+  }
+  delete thread_pool;
+  TF_ASSERT_OK(writer->FlushNonExecutionFiles());
+  TF_ASSERT_OK(writer->Close());
+
+  // There should be only 4 unique device IDs, because there are only 4 unique
+  // device names.
+  EXPECT_EQ(device_ids[0], device_ids[4]);
+  EXPECT_EQ(device_ids[1], device_ids[5]);
+  EXPECT_EQ(device_ids[2], device_ids[6]);
+  EXPECT_EQ(device_ids[3], device_ids[7]);
+  // Assert that the four device IDs are all unique.
+  EXPECT_EQ(absl::flat_hash_set<int>(device_ids, device_ids + 8).size(), 4);
+
+  std::vector<DebugEvent> actuals;
+  ReadDebugEventProtos(writer, DebugEventFileType::GRAPHS, &actuals);
+  // Due to the `% 4`, there are only 4 unique device names, even though there
+  // are 8 threads each calling `RegisterDeviceAndGetId`.
+  EXPECT_EQ(actuals.size(), 4);
+  for (const DebugEvent& actual : actuals) {
+    const string& device_name = actual.debugged_device().device_name();
+    int device_index = -1;
+    CHECK(absl::SimpleAtoi(device_name.substr(strlen(
+                               "/job:localhost/replica:0/task:0/device:GPU:")),
+                           &device_index));
+    EXPECT_EQ(actual.debugged_device().device_id(), device_ids[device_index]);
+  }
+}
+
 TEST_F(DebugEventsWriterTest, DisableCyclicBufferBeahavior) {
   const size_t kCyclicBufferSize = 0;  // A value <= 0 disables cyclic behavior.
   DebugEventsWriter* writer =
diff --git a/tensorflow/python/client/debug_events_writer_wrapper.cc b/tensorflow/python/client/debug_events_writer_wrapper.cc
index 3c0cd311a7d..75abf70d749 100644
--- a/tensorflow/python/client/debug_events_writer_wrapper.cc
+++ b/tensorflow/python/client/debug_events_writer_wrapper.cc
@@ -29,7 +29,7 @@ PYBIND11_MODULE(_pywrap_debug_events_writer, m) {
   using namespace tensorflow::tfdbg;  // NOLINT(build/namespaces)
 
   m.def("Init",
-        [](const std::string dump_root, const int64 circular_buffer_size) {
+        [](const std::string& dump_root, const int64 circular_buffer_size) {
           DebugEventsWriter* writer = DebugEventsWriter::GetDebugEventsWriter(
               dump_root, circular_buffer_size);
           if (!writer->Init().ok()) {
@@ -39,7 +39,7 @@ PYBIND11_MODULE(_pywrap_debug_events_writer, m) {
           }
         });
   m.def("WriteSourceFile",
-        [](const std::string dump_root, const py::object obj) {
+        [](const std::string& dump_root, const py::object obj) {
           CheckProtoType(obj, "tensorflow.DebugEvent");
           DebugEventsWriter* writer =
               DebugEventsWriter::GetDebugEventsWriter(dump_root);
@@ -48,7 +48,7 @@ PYBIND11_MODULE(_pywrap_debug_events_writer, m) {
               tfdbg::DebugEventFileType::SOURCE_FILES);
         });
   m.def("WriteStackFrameWithId",
-        [](const std::string dump_root, const py::object obj) {
+        [](const std::string& dump_root, const py::object& obj) {
           CheckProtoType(obj, "tensorflow.DebugEvent");
           DebugEventsWriter* writer =
               DebugEventsWriter::GetDebugEventsWriter(dump_root);
@@ -57,7 +57,7 @@ PYBIND11_MODULE(_pywrap_debug_events_writer, m) {
               tfdbg::DebugEventFileType::STACK_FRAMES);
         });
   m.def("WriteGraphOpCreation",
-        [](const std::string dump_root, const py::object obj) {
+        [](const std::string& dump_root, const py::object& obj) {
           CheckProtoType(obj, "tensorflow.DebugEvent");
           DebugEventsWriter* writer =
               DebugEventsWriter::GetDebugEventsWriter(dump_root);
@@ -66,7 +66,7 @@ PYBIND11_MODULE(_pywrap_debug_events_writer, m) {
               tfdbg::DebugEventFileType::GRAPHS);
         });
   m.def("WriteDebuggedGraph",
-        [](const std::string dump_root, const py::object obj) {
+        [](const std::string& dump_root, const py::object& obj) {
           CheckProtoType(obj, "tensorflow.DebugEvent");
           DebugEventsWriter* writer =
               DebugEventsWriter::GetDebugEventsWriter(dump_root);
@@ -75,7 +75,7 @@ PYBIND11_MODULE(_pywrap_debug_events_writer, m) {
               tfdbg::DebugEventFileType::GRAPHS);
         });
   m.def("WriteExecution",
-        [](const std::string dump_root, const py::object obj) {
+        [](const std::string& dump_root, const py::object& obj) {
           CheckProtoType(obj, "tensorflow.DebugEvent");
           DebugEventsWriter* writer =
               DebugEventsWriter::GetDebugEventsWriter(dump_root);
@@ -84,7 +84,7 @@ PYBIND11_MODULE(_pywrap_debug_events_writer, m) {
               tfdbg::DebugEventFileType::EXECUTION);
         });
   m.def("WriteGraphExecutionTrace",
-        [](const std::string dump_root, const py::object obj) {
+        [](const std::string& dump_root, const py::object& obj) {
           CheckProtoType(obj, "tensorflow.DebugEvent");
           DebugEventsWriter* writer =
               DebugEventsWriter::GetDebugEventsWriter(dump_root);
@@ -92,17 +92,23 @@ PYBIND11_MODULE(_pywrap_debug_events_writer, m) {
               obj.attr("SerializeToString")().cast<std::string>(),
               tfdbg::DebugEventFileType::GRAPH_EXECUTION_TRACES);
         });
-  m.def("FlushNonExecutionFiles", [](const std::string dump_root) {
+  m.def("RegisterDeviceAndGetId",
+        [](const std::string& dump_root, const std::string& device_name) {
+          DebugEventsWriter* writer =
+              DebugEventsWriter::GetDebugEventsWriter(dump_root);
+          return writer->RegisterDeviceAndGetId(device_name);
+        });
+  m.def("FlushNonExecutionFiles", [](const std::string& dump_root) {
     DebugEventsWriter* writer =
         DebugEventsWriter::GetDebugEventsWriter(dump_root);
     writer->FlushNonExecutionFiles();
   });
-  m.def("FlushExecutionFiles", [](const std::string dump_root) {
+  m.def("FlushExecutionFiles", [](const std::string& dump_root) {
     DebugEventsWriter* writer =
         DebugEventsWriter::GetDebugEventsWriter(dump_root);
     writer->FlushExecutionFiles();
   });
-  m.def("Close", [](const std::string dump_root) {
+  m.def("Close", [](const std::string& dump_root) {
     DebugEventsWriter* writer =
         DebugEventsWriter::GetDebugEventsWriter(dump_root);
     writer->Close();
diff --git a/tensorflow/python/debug/lib/debug_events_reader.py b/tensorflow/python/debug/lib/debug_events_reader.py
index 1594b5f27f8..9033e48b9ed 100644
--- a/tensorflow/python/debug/lib/debug_events_reader.py
+++ b/tensorflow/python/debug/lib/debug_events_reader.py
@@ -233,19 +233,27 @@ class ExecutionDigest(BaseDigest):
       In the case of the execution of a tf.function (FuncGraph), this is the
       internally-generated name of the function (e.g.,
       "__inference_my_func_123").
+    output_tensor_device_ids: IDs of the devices on which the output tensors of
+      the execution reside. For no-output execution, this is `None`.
   """
 
   def __init__(self,
                wall_time,
                offset,
-               op_type):
+               op_type,
+               output_tensor_device_ids=None):
     super(ExecutionDigest, self).__init__(wall_time, offset)
     self._op_type = op_type
+    self._output_tensor_device_ids = output_tensor_device_ids
 
   @property
   def op_type(self):
     return self._op_type
 
+  @property
+  def output_tensor_device_ids(self):
+    return self._output_tensor_device_ids
+
   # TODO(cais): Implement to_json().
 
 
@@ -284,7 +292,8 @@ class Execution(ExecutionDigest):
     super(Execution, self).__init__(
         execution_digest.wall_time,
         execution_digest.offset,
-        execution_digest.op_type)
+        execution_digest.op_type,
+        output_tensor_device_ids=execution_digest.output_tensor_device_ids)
     self._stack_frame_ids = stack_frame_ids
     self._tensor_debug_mode = tensor_debug_mode
     self._graph_id = graph_id
@@ -394,6 +403,32 @@ class DebuggedGraph(object):
   # TODO(cais): Implement to_json().
 
 
+class DebuggedDevice(object):
+  """Debugger data regarding a device involved in the debugged program.
+
+  Properties:
+    device_name: Name of the device, as a str.
+    device_id: An integer ID for the device, unique for each device within
+      the scope of the debugged TensorFlow program.
+  """
+
+  def __init__(self,
+               device_name,
+               device_id):
+    self._device_name = device_name
+    self._device_id = device_id
+
+  @property
+  def device_name(self):
+    return self._device_name
+
+  @property
+  def device_id(self):
+    return self._device_id
+
+  # TODO(cais): Implement to_json().
+
+
 class GraphOpCreationDigest(BaseDigest):
   """Data object describing the creation of an op inside a graph.
 
@@ -614,6 +649,8 @@ class DebugDataReader(object):
     # case in which reading of the .stack_frames file gets ahead of the reading
     # of the .source_files file.
     self._unprocessed_stack_frames = dict()
+    # A dict mapping id to DebuggedDevice objects.
+    self._device_by_id = dict()
     # A dict mapping id to DebuggedGraph objects.
     self._graph_by_id = dict()
     self._graph_op_digests = []
@@ -695,6 +732,10 @@ class DebugDataReader(object):
         if graph_proto.outer_context_id:
           self._graph_by_id[
               graph_proto.outer_context_id].add_inner_graph_id(graph.graph_id)
+      elif debug_event.debugged_device.ByteSize():
+        device_proto = debug_event.debugged_device
+        self._device_by_id[device_proto.device_id] = DebuggedDevice(
+            device_proto.device_name, device_proto.device_id)
 
   def _load_graph_execution_traces(self):
     """Incrementally load the .graph_execution_traces file."""
@@ -730,7 +771,9 @@ class DebugDataReader(object):
       self._execution_digests.append(ExecutionDigest(
           debug_event.wall_time,
           offset,
-          debug_event.execution.op_type))
+          debug_event.execution.op_type,
+          output_tensor_device_ids=(
+              debug_event.execution.output_tensor_device_ids or None)))
 
   def update(self):
     """Perform incremental read of the file set."""
@@ -749,6 +792,14 @@ class DebugDataReader(object):
     """Get a DebuggedGraph object by its ID."""
     return self._graph_by_id[graph_id]
 
+  def device_name_by_id(self, device_id):
+    """Get the name of a device by the debugger-generated ID of the device."""
+    return self._device_by_id[device_id].device_name
+
+  def device_names(self):
+    """Get a set of all device names known to the debugger."""
+    return set(device.device_name for device in self._device_by_id.values())
+
   def graph_op_digests(self, op_type=None):
     """Get the list of the digests for graph-op creation so far.
 
diff --git a/tensorflow/python/debug/lib/debug_events_writer.py b/tensorflow/python/debug/lib/debug_events_writer.py
index 7f7ae38434c..3de0ab78b8a 100644
--- a/tensorflow/python/debug/lib/debug_events_writer.py
+++ b/tensorflow/python/debug/lib/debug_events_writer.py
@@ -128,6 +128,10 @@ class DebugEventsWriter(object):
     _pywrap_debug_events_writer.WriteGraphExecutionTrace(
         self._dump_root, debug_event)
 
+  def RegisterDeviceAndGetId(self, device_name):
+    return _pywrap_debug_events_writer.RegisterDeviceAndGetId(
+        self._dump_root, device_name)
+
   def FlushNonExecutionFiles(self):
     """Flush the non-execution debug event files."""
     _pywrap_debug_events_writer.FlushNonExecutionFiles(self._dump_root)
diff --git a/tensorflow/python/debug/lib/dumping_callback.py b/tensorflow/python/debug/lib/dumping_callback.py
index 98e7292a785..e51eedf45d7 100644
--- a/tensorflow/python/debug/lib/dumping_callback.py
+++ b/tensorflow/python/debug/lib/dumping_callback.py
@@ -386,6 +386,7 @@ class _DumpingCallback(object):
                           tensors,
                           op_type,
                           input_tensor_ids,
+                          output_tensor_device_ids,
                           graph_id=None):
     """Dump the value of eager tensors.
 
@@ -400,6 +401,9 @@ class _DumpingCallback(object):
         value transform.
       op_type: Type of the op that generates the tensors, as a string.
       input_tensor_ids: IDs of the input EagerTensors to the op.
+      output_tensor_device_ids: Debugged-generated IDs for the devices on which
+        the output tensors are allocated, as a `list` of `int`s. Must match
+        `tensors` in length.
       graph_id: ID of the executed graph, applicable only to eager execution of
         a FuncGraph.
 
@@ -409,6 +413,7 @@ class _DumpingCallback(object):
     tensor_debug_mode = self._tensor_debug_mode
     output_tensor_ids = [
         t._id for t in tensors]  # pylint:disable=protected-access
+    assert len(tensors) == len(output_tensor_device_ids)
     if tensor_debug_mode == debug_event_pb2.TensorDebugMode.NO_TENSOR:
       return debug_event_pb2.Execution(
           op_type=op_type,
@@ -416,6 +421,7 @@ class _DumpingCallback(object):
           num_outputs=len(tensors),
           input_tensor_ids=input_tensor_ids,
           output_tensor_ids=output_tensor_ids,
+          output_tensor_device_ids=output_tensor_device_ids,
           tensor_debug_mode=tensor_debug_mode,
           code_location=self._process_stack_frames())
     elif tensor_debug_mode in (debug_event_pb2.TensorDebugMode.CURT_HEALTH,
@@ -428,6 +434,7 @@ class _DumpingCallback(object):
           graph_id=graph_id,
           input_tensor_ids=input_tensor_ids,
           output_tensor_ids=output_tensor_ids,
+          output_tensor_device_ids=output_tensor_device_ids,
           tensor_debug_mode=tensor_debug_mode,
           code_location=self._process_stack_frames())
       for tensor in tensors:
@@ -505,8 +512,11 @@ class _DumpingCallback(object):
         return None
       context_id = self._func_graph_id_from_func_name(op_type)
       input_ids = [t._id for t in inputs]  # pylint:disable=protected-access
+      output_tensor_device_ids = [writer.RegisterDeviceAndGetId(output.device)
+                                  for output in outputs] if outputs else []
       writer.WriteExecution(self._dump_eager_tensors(
-          outputs, op_type, input_ids, graph_id=context_id))
+          outputs, op_type, input_ids, output_tensor_device_ids,
+          graph_id=context_id))
 
   def _func_graph_id_from_func_name(self, op_type):
     """Attempt to get the ID of a FuncGraph based on an op type name.
diff --git a/tensorflow/python/debug/lib/dumping_callback_test.py b/tensorflow/python/debug/lib/dumping_callback_test.py
index 061cb001639..115315a38ec 100644
--- a/tensorflow/python/debug/lib/dumping_callback_test.py
+++ b/tensorflow/python/debug/lib/dumping_callback_test.py
@@ -91,6 +91,13 @@ class TracingCallbackTest(
     self.assertTrue([
         frame for frame in stack_frames if frame[0] == _current_file_full_path])
 
+  def _expectedDefaultDeviceName(self):
+    gpu_name = test_util.gpu_device_name()
+    if gpu_name:
+      return "/job:localhost/replica:0/task:0" + gpu_name
+    else:
+      return "/job:localhost/replica:0/task:0/device:CPU:0"
+
   def testInvalidTensorDebugModeCausesError(self):
     with self.assertRaisesRegexp(
         ValueError,
@@ -147,6 +154,14 @@ class TracingCallbackTest(
         self.assertGreaterEqual(execution.wall_time, prev_wall_time)
         prev_wall_time = execution.wall_time
         executed_op_types.append(execution.op_type)
+        # Check the device name.
+        if execution.op_type in ("AddV2", "Mul", "RealDiv"):
+          self.assertLen(execution.output_tensor_device_ids, 1)
+          self.assertEqual(
+              reader.device_name_by_id(execution.output_tensor_device_ids[0]),
+              self._expectedDefaultDeviceName(),
+              "Unexpected device name from eager op %s" % execution.op_type)
+
         # No graph IDs should have been logged for eager op executions.
         self.assertFalse(execution.graph_id)
         self.assertTrue(execution.input_tensor_ids)
@@ -371,6 +386,12 @@ class TracingCallbackTest(
         self.assertLen(graph.inner_graph_ids, 1)
         inner_graph = reader.graph_by_id(graph.inner_graph_ids[0])
         self.assertEqual(inner_graph.name, "log_sum")
+        # Check device names.
+        self.assertLen(executions[0].output_tensor_device_ids, 1)
+        self.assertEqual(
+            reader.device_name_by_id(executions[0].output_tensor_device_ids[0]),
+            self._expectedDefaultDeviceName())
+        self.assertIn(self._expectedDefaultDeviceName(), reader.device_names())
 
       # Verify the recorded graph-building history.
       add_op_digests = reader.graph_op_digests(op_type="AddV2")

commit 9c52e7ce02532c22a79ae7139f37c663add4c90f
Author: Adrian Kuegel <akuegel@google.com>
Date:   Tue Oct 29 03:53:32 2019 -0700

    Make it clear that optional common attributes are not ignored in HloParser.
    
    Also fix the case when an attribute appears both as a non-proto attribute and
    as a proto attribute. Here we prefer parsing the proto attribute.
    Also use flat_hash_set and flat_hash_map for efficiency, and replace one usage
    of map with vector.
    
    PiperOrigin-RevId: 277249631
    Change-Id: I41396e5a98dca13be6a5a7681bcfd03f775fb576

diff --git a/tensorflow/compiler/xla/service/BUILD b/tensorflow/compiler/xla/service/BUILD
index 5a6f8cb6c92..584a4eb07ed 100755
--- a/tensorflow/compiler/xla/service/BUILD
+++ b/tensorflow/compiler/xla/service/BUILD
@@ -4091,6 +4091,8 @@ cc_library(
         "//tensorflow/core:lib",
         "//tensorflow/core:lib_internal",
         "@com_google_absl//absl/algorithm:container",
+        "@com_google_absl//absl/container:flat_hash_map",
+        "@com_google_absl//absl/container:flat_hash_set",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/strings",
         "@com_google_absl//absl/strings:str_format",
diff --git a/tensorflow/compiler/xla/service/hlo_parser.cc b/tensorflow/compiler/xla/service/hlo_parser.cc
index 4ddad34ae32..9f453feaba8 100644
--- a/tensorflow/compiler/xla/service/hlo_parser.cc
+++ b/tensorflow/compiler/xla/service/hlo_parser.cc
@@ -15,15 +15,14 @@ limitations under the License.
 
 #include "tensorflow/compiler/xla/service/hlo_parser.h"
 
-#include <map>
 #include <memory>
 #include <string>
 #include <type_traits>
-#include <unordered_map>
-#include <unordered_set>
 #include <vector>
 
 #include "absl/algorithm/container.h"
+#include "absl/container/flat_hash_map.h"
+#include "absl/container/flat_hash_set.h"
 #include "absl/memory/memory.h"
 #include "absl/strings/str_cat.h"
 #include "absl/strings/str_format.h"
@@ -103,7 +102,7 @@ class HloParserImpl : public HloParser {
 
  private:
   using InstrNameTable =
-      std::unordered_map<std::string, std::pair<HloInstruction*, LocTy>>;
+      absl::flat_hash_map<std::string, std::pair<HloInstruction*, LocTy>>;
 
   // Returns the map from the instruction name to the instruction itself and its
   // location in the current scope.
@@ -230,7 +229,7 @@ class HloParserImpl : public HloParser {
   //
   // Example usage:
   //
-  //  std::unordered_map<std::string, AttrConfig> attrs;
+  //  absl::flat_hash_map<std::string, AttrConfig> attrs;
   //  optional<int64> foo;
   //  attrs["foo"] = {/*required=*/false, AttrTy::kInt64, &foo};
   //  optional<Window> bar;
@@ -242,36 +241,36 @@ class HloParserImpl : public HloParser {
   //  if (foo) { // If attr foo is seen, do something with 'foo'. }
   //
   bool ParseAttributes(
-      const std::unordered_map<std::string, AttrConfig>& attrs);
+      const absl::flat_hash_map<std::string, AttrConfig>& attrs);
 
   // sub_attributes ::= '{' (','? attribute)* '}'
   //
   // Usage is the same as ParseAttributes. See immediately above.
   bool ParseSubAttributes(
-      const std::unordered_map<std::string, AttrConfig>& attrs);
+      const absl::flat_hash_map<std::string, AttrConfig>& attrs);
 
   // Parses one attribute. If it has already been seen, return error. Returns
   // true and adds to seen_attrs on success.
   //
   // Do not call this except in ParseAttributes or ParseSubAttributes.
   bool ParseAttributeHelper(
-      const std::unordered_map<std::string, AttrConfig>& attrs,
-      std::unordered_set<std::string>* seen_attrs);
+      const absl::flat_hash_map<std::string, AttrConfig>& attrs,
+      absl::flat_hash_set<std::string>* seen_attrs);
 
   // Copy attributes from `attrs` to `message`, unless the attribute name is in
-  // `ignored_attrs`.
+  // `non_proto_attrs`.
   bool CopyAttributeToProtoMessage(
-      std::vector<std::string> ignored_attrs,
-      const std::unordered_map<std::string, AttrConfig>& attrs,
+      absl::flat_hash_set<std::string> non_proto_attrs,
+      const absl::flat_hash_map<std::string, AttrConfig>& attrs,
       tensorflow::protobuf::Message* message);
 
   // Parses an attribute string into a protocol buffer `message`.
   // Since proto3 has no notion of mandatory fields, `required_attrs` gives the
   // set of mandatory attributes.
-  // `ignored_attrs` specifies attributes the parser ignores, without writing
-  // them into the proto, but without giving the error either.
+  // `non_proto_attrs` specifies attributes that are not written to the proto,
+  // but added to the HloInstruction.
   bool ParseAttributesAsProtoMessage(
-      const std::unordered_map<std::string, AttrConfig>& ignored_attrs,
+      const absl::flat_hash_map<std::string, AttrConfig>& non_proto_attrs,
       tensorflow::protobuf::Message* message);
 
   // Parses a name and finds the corresponding hlo computation.
@@ -386,7 +385,7 @@ class HloParserImpl : public HloParser {
   };
 
   // Map from the computation name to the computation itself and its location.
-  std::unordered_map<std::string, std::pair<HloComputation*, LocTy>>
+  absl::flat_hash_map<std::string, std::pair<HloComputation*, LocTy>>
       computation_pool_;
 
   std::vector<std::unique_ptr<HloComputation>> computations_;
@@ -516,7 +515,7 @@ bool HloParserImpl::ParseHloModule(HloModule* module) {
   }
 
   absl::optional<bool> is_scheduled;
-  std::unordered_map<std::string, AttrConfig> attrs;
+  absl::flat_hash_map<std::string, AttrConfig> attrs;
   attrs["is_scheduled"] = {/*required=*/false, AttrTy::kBool, &is_scheduled};
   if (!ParseAttributes(attrs)) {
     return false;
@@ -691,8 +690,9 @@ bool HloParserImpl::ParseInstructionRhs(HloComputation::Builder* builder,
     return false;
   }
 
-  // Add optional attributes.
-  std::unordered_map<std::string, AttrConfig> attrs;
+  // Add optional attributes. These are added to any HloInstruction type if
+  // present.
+  absl::flat_hash_map<std::string, AttrConfig> attrs;
   optional<OpSharding> sharding;
   optional<FrontendAttributes> frontend_attributes;
   attrs["sharding"] = {/*required=*/false, AttrTy::kSharding, &sharding};
@@ -1169,7 +1169,7 @@ bool HloParserImpl::ParseInstructionRhs(HloComputation::Builder* builder,
       TriangularSolveOptions options;
       if (!ParseOperands(&operands, /*expected_size=*/2) ||
           !ParseAttributesAsProtoMessage(
-              /*ignored_attrs=*/attrs, &options)) {
+              /*non_proto_attrs=*/attrs, &options)) {
         return false;
       }
       instruction =
@@ -1193,7 +1193,7 @@ bool HloParserImpl::ParseInstructionRhs(HloComputation::Builder* builder,
       CholeskyOptions options;
       if (!ParseOperands(&operands, /*expected_size=*/1) ||
           !ParseAttributesAsProtoMessage(
-              /*ignored_attrs=*/attrs, &options)) {
+              /*non_proto_attrs=*/attrs, &options)) {
         return false;
       }
       instruction = builder->AddInstruction(
@@ -2070,7 +2070,7 @@ bool HloParserImpl::ParseReplicaGroupsOnly(
 // domain ::= '{' 'kind=' domain_kind ',' 'entry=' entry_sharding ','
 //            'exit=' exit_sharding '}'
 bool HloParserImpl::ParseDomain(DomainData* domain) {
-  std::unordered_map<std::string, AttrConfig> attrs;
+  absl::flat_hash_map<std::string, AttrConfig> attrs;
   optional<std::string> kind;
   optional<OpSharding> entry_sharding;
   optional<OpSharding> exit_sharding;
@@ -2755,12 +2755,12 @@ bool HloParserImpl::ParseOperands(std::vector<HloInstruction*>* operands,
 
 // sub_attributes ::= '{' (','? attribute)* '}'
 bool HloParserImpl::ParseSubAttributes(
-    const std::unordered_map<std::string, AttrConfig>& attrs) {
+    const absl::flat_hash_map<std::string, AttrConfig>& attrs) {
   LocTy loc = lexer_.GetLoc();
   if (!ParseToken(TokKind::kLbrace, "expects '{' to start sub attributes")) {
     return false;
   }
-  std::unordered_set<std::string> seen_attrs;
+  absl::flat_hash_set<std::string> seen_attrs;
   if (lexer_.GetKind() == TokKind::kRbrace) {
     // empty
   } else {
@@ -2784,9 +2784,9 @@ bool HloParserImpl::ParseSubAttributes(
 
 // attributes ::= (',' attribute)*
 bool HloParserImpl::ParseAttributes(
-    const std::unordered_map<std::string, AttrConfig>& attrs) {
+    const absl::flat_hash_map<std::string, AttrConfig>& attrs) {
   LocTy loc = lexer_.GetLoc();
-  std::unordered_set<std::string> seen_attrs;
+  absl::flat_hash_set<std::string> seen_attrs;
   while (EatIfPresent(TokKind::kComma)) {
     if (!ParseAttributeHelper(attrs, &seen_attrs)) {
       return false;
@@ -2804,8 +2804,8 @@ bool HloParserImpl::ParseAttributes(
 }
 
 bool HloParserImpl::ParseAttributeHelper(
-    const std::unordered_map<std::string, AttrConfig>& attrs,
-    std::unordered_set<std::string>* seen_attrs) {
+    const absl::flat_hash_map<std::string, AttrConfig>& attrs,
+    absl::flat_hash_set<std::string>* seen_attrs) {
   LocTy loc = lexer_.GetLoc();
   std::string name;
   if (!ParseAttributeName(&name)) {
@@ -3074,15 +3074,15 @@ bool HloParserImpl::ParseAttributeHelper(
 }
 
 bool HloParserImpl::CopyAttributeToProtoMessage(
-    std::vector<std::string> ignored_attrs,
-    const std::unordered_map<std::string, AttrConfig>& attrs,
+    absl::flat_hash_set<std::string> non_proto_attrs,
+    const absl::flat_hash_map<std::string, AttrConfig>& attrs,
     tensorflow::protobuf::Message* message) {
   const tensorflow::protobuf::Descriptor* descriptor = message->GetDescriptor();
   const tensorflow::protobuf::Reflection* reflection = message->GetReflection();
 
   for (const auto& p : attrs) {
     const std::string& name = p.first;
-    if (absl::c_count(ignored_attrs, name)) {
+    if (non_proto_attrs.find(name) != non_proto_attrs.end()) {
       continue;
     }
     const tensorflow::protobuf::FieldDescriptor* fd =
@@ -3132,14 +3132,18 @@ bool HloParserImpl::CopyAttributeToProtoMessage(
 
 // attributes ::= (',' attribute)*
 bool HloParserImpl::ParseAttributesAsProtoMessage(
-    const std::unordered_map<std::string, AttrConfig>& ignored_attrs,
+    const absl::flat_hash_map<std::string, AttrConfig>& non_proto_attrs,
     tensorflow::protobuf::Message* message) {
   const tensorflow::protobuf::Descriptor* descriptor = message->GetDescriptor();
-  std::unordered_map<std::string, AttrConfig> attrs;
+  absl::flat_hash_map<std::string, AttrConfig> attrs;
 
   // Storage for attributes.
-  std::map<std::string, optional<bool>> bool_params;
-  std::map<std::string, optional<std::string>> string_params;
+  std::vector<optional<bool>> bool_params;
+  std::vector<optional<std::string>> string_params;
+  // Reserve enough capacity to make sure that the vector is not growing, so we
+  // can rely on the pointers to stay valid.
+  bool_params.reserve(descriptor->field_count());
+  string_params.reserve(descriptor->field_count());
 
   // Populate the storage of expected attributes from the protobuf description.
   for (int field_idx = 0; field_idx < descriptor->field_count(); field_idx++) {
@@ -3148,15 +3152,15 @@ bool HloParserImpl::ParseAttributesAsProtoMessage(
     const std::string& field_name = fd->name();
     switch (fd->type()) {
       case tensorflow::protobuf::FieldDescriptor::TYPE_BOOL: {
-        auto p = bool_params.emplace(field_name, absl::nullopt);
+        bool_params.emplace_back(absl::nullopt);
         attrs[field_name] = {/*is_required*/ false, AttrTy::kBool,
-                             &p.first->second};
+                             &bool_params.back()};
         break;
       }
       case tensorflow::protobuf::FieldDescriptor::TYPE_ENUM: {
-        auto p = string_params.emplace(field_name, absl::nullopt);
+        string_params.emplace_back(absl::nullopt);
         attrs[field_name] = {/*is_required*/ false, AttrTy::kEnum,
-                             &p.first->second};
+                             &string_params.back()};
         break;
       }
       default:
@@ -3165,21 +3169,24 @@ bool HloParserImpl::ParseAttributesAsProtoMessage(
     }
   }
 
-  std::vector<std::string> ignored_attrs_names;
-  ignored_attrs_names.reserve(ignored_attrs.size());
-  for (const auto& p : ignored_attrs) {
+  absl::flat_hash_set<std::string> non_proto_attrs_names;
+  non_proto_attrs_names.reserve(non_proto_attrs.size());
+  for (const auto& p : non_proto_attrs) {
     const std::string& attr_name = p.first;
+    // If an attribute is both specified within 'non_proto_attrs' and an
+    // attribute of the proto message, we prefer the attribute of the proto
+    // message.
     if (attrs.find(attr_name) == attrs.end()) {
-      ignored_attrs_names.push_back(attr_name);
+      non_proto_attrs_names.insert(attr_name);
+      attrs[attr_name] = p.second;
     }
-    attrs[attr_name] = p.second;
   }
 
   if (!ParseAttributes(attrs)) {
     return false;
   }
 
-  return CopyAttributeToProtoMessage(ignored_attrs_names, attrs, message);
+  return CopyAttributeToProtoMessage(non_proto_attrs_names, attrs, message);
 }
 
 bool HloParserImpl::ParseComputationName(HloComputation** value) {
@@ -3958,7 +3965,7 @@ bool HloParserImpl::ParsePaddingConfig(PaddingConfig* padding) {
 
 // '{' metadata_string '}'
 bool HloParserImpl::ParseMetadata(OpMetadata* metadata) {
-  std::unordered_map<std::string, AttrConfig> attrs;
+  absl::flat_hash_map<std::string, AttrConfig> attrs;
   optional<std::string> op_type;
   optional<std::string> op_name;
   optional<std::string> source_file;
diff --git a/tensorflow/compiler/xla/service/hlo_parser_test.cc b/tensorflow/compiler/xla/service/hlo_parser_test.cc
index 43b206bb3f6..0330ebb2195 100644
--- a/tensorflow/compiler/xla/service/hlo_parser_test.cc
+++ b/tensorflow/compiler/xla/service/hlo_parser_test.cc
@@ -1807,6 +1807,16 @@ ENTRY %blabla (a: f32[1,291,291]) -> f32[1,291,291] {
 )";
   auto result = ParseAndReturnVerifiedModule(original);
   EXPECT_EQ(Status::OK(), result.status());
+  EXPECT_EQ("Cholesky", result.ValueOrDie()
+                            ->entry_computation()
+                            ->root_instruction()
+                            ->metadata()
+                            .op_name());
+  EXPECT_EQ("Cholesky", result.ValueOrDie()
+                            ->entry_computation()
+                            ->root_instruction()
+                            ->metadata()
+                            .op_type());
 }
 
 TEST_F(HloParserTest, WrongShape) {

commit d969c96e5c77f99fd2c94aed277d729056c093e3
Author: Edward Loper <edloper@google.com>
Date:   Thu Oct 3 05:24:37 2019 -0700

    Extend tf.io.parse_sequence_example and tf.io.parse_single_sequence_example to handle ragged tensor features.
    
    tf.io.parse_single_sequence_example also now uses the fast-parsing code path that was written for parse_sequence_example, which should improve efficiency for large inputs.
    
    PiperOrigin-RevId: 272639850

diff --git a/tensorflow/core/api_def/base_api/api_def_ParseSequenceExampleV2.pbtxt b/tensorflow/core/api_def/base_api/api_def_ParseSequenceExampleV2.pbtxt
new file mode 100644
index 00000000000..e20bdf23e85
--- /dev/null
+++ b/tensorflow/core/api_def/base_api/api_def_ParseSequenceExampleV2.pbtxt
@@ -0,0 +1,147 @@
+op {
+  graph_op_name: "ParseSequenceExampleV2"
+  in_arg {
+    name: "serialized"
+    description: <<END
+A scalar or vector containing binary serialized SequenceExample protos.
+END
+  }
+  in_arg {
+    name: "debug_name"
+    description: <<END
+A scalar or vector containing the names of the serialized protos.
+May contain, for example, table key (descriptive) name for the
+corresponding serialized proto.  This is purely useful for debugging
+purposes, and the presence of values here has no effect on the output.
+May also be an empty vector if no name is available.
+END
+  }
+  in_arg {
+    name: "context_sparse_keys"
+    description: <<END
+The keys expected in the Examples' features associated with context_sparse
+values.
+END
+  }
+  in_arg {
+    name: "context_dense_keys"
+    description: <<END
+The keys expected in the SequenceExamples' context features associated with
+dense values.
+END
+  }
+  in_arg {
+    name: "context_ragged_keys"
+    description: <<END
+The keys expected in the Examples' features associated with context_ragged
+values.
+END
+  }
+  in_arg {
+    name: "feature_list_sparse_keys"
+    description: <<END
+The keys expected in the FeatureLists associated with sparse values.
+END
+  }
+  in_arg {
+    name: "feature_list_dense_keys"
+    description: <<END
+The keys expected in the SequenceExamples' feature_lists associated
+with lists of dense values.
+END
+  }
+  in_arg {
+    name: "feature_list_ragged_keys"
+    description: <<END
+The keys expected in the FeatureLists associated with ragged values.
+END
+  }
+  in_arg {
+    name: "feature_list_dense_missing_assumed_empty"
+    description: <<END
+A vector corresponding 1:1 with featue_list_dense_keys, indicating which
+features may be missing from the SequenceExamples.  If the associated
+FeatureList is missing, it is treated as empty.
+END
+  }
+  in_arg {
+    name: "context_dense_defaults"
+    description: <<END
+A list of Ncontext_dense Tensors (some may be empty).
+context_dense_defaults[j] provides default values
+when the SequenceExample's context map lacks context_dense_key[j].
+If an empty Tensor is provided for context_dense_defaults[j],
+then the Feature context_dense_keys[j] is required.
+The input type is inferred from context_dense_defaults[j], even when it's
+empty.  If context_dense_defaults[j] is not empty, its shape must match
+context_dense_shapes[j].
+END
+  }
+
+  attr {
+    name: "context_sparse_types"
+    description: <<END
+A list of Ncontext_sparse types; the data types of data in
+each context Feature given in context_sparse_keys.
+Currently the ParseSingleSequenceExample supports DT_FLOAT (FloatList),
+DT_INT64 (Int64List), and DT_STRING (BytesList).
+END
+  }
+  attr {
+    name: "context_dense_shapes"
+    description: <<END
+A list of Ncontext_dense shapes; the shapes of data in
+each context Feature given in context_dense_keys.
+The number of elements in the Feature corresponding to context_dense_key[j]
+must always equal context_dense_shapes[j].NumEntries().
+The shape of context_dense_values[j] will match context_dense_shapes[j].
+END
+  }
+  attr {
+    name: "feature_list_sparse_types"
+    description: <<END
+A list of Nfeature_list_sparse types; the data types
+of data in each FeatureList given in feature_list_sparse_keys.
+Currently the ParseSingleSequenceExample supports DT_FLOAT (FloatList),
+DT_INT64 (Int64List), and DT_STRING (BytesList).
+END
+  }
+  attr {
+    name: "feature_list_dense_shapes"
+    description: <<END
+A list of Nfeature_list_dense shapes; the shapes of
+data in each FeatureList given in feature_list_dense_keys.
+The shape of each Feature in the FeatureList corresponding to
+feature_list_dense_key[j] must always equal
+feature_list_dense_shapes[j].NumEntries().
+END
+  }
+  attr {
+    name: "context_ragged_value_types"
+    description: <<END
+RaggedTensor.value dtypes for the ragged context features.
+END
+  }
+  attr {
+    name: "context_ragged_split_types"
+    description: <<END
+RaggedTensor.row_split dtypes for the ragged context features.
+END
+  }
+  attr {
+    name: "feature_list_ragged_value_types"
+    description: <<END
+RaggedTensor.value dtypes for the ragged FeatureList features.
+END
+  }
+  attr {
+    name: "feature_list_ragged_split_types"
+    description: <<END
+RaggedTensor.row_split dtypes for the ragged FeatureList features.
+END
+  }
+  summary: <<END
+Transforms a vector of tf.io.SequenceExample protos (as strings) into
+typed tensors.
+END
+}
diff --git a/tensorflow/core/api_def/python_api/api_def_ParseSequenceExampleV2.pbtxt b/tensorflow/core/api_def/python_api/api_def_ParseSequenceExampleV2.pbtxt
new file mode 100644
index 00000000000..16d0da73301
--- /dev/null
+++ b/tensorflow/core/api_def/python_api/api_def_ParseSequenceExampleV2.pbtxt
@@ -0,0 +1,4 @@
+op {
+  graph_op_name: "ParseSequenceExampleV2"
+  visibility: HIDDEN
+}
diff --git a/tensorflow/core/kernels/example_parsing_ops.cc b/tensorflow/core/kernels/example_parsing_ops.cc
index 423b89d981a..0098d9583d5 100644
--- a/tensorflow/core/kernels/example_parsing_ops.cc
+++ b/tensorflow/core/kernels/example_parsing_ops.cc
@@ -38,6 +38,7 @@ namespace tensorflow {
 
 namespace {
 constexpr char kParseExampleV2[] = "ParseExampleV2";
+constexpr char kParseSequenceExampleV2[] = "ParseSequenceExampleV2";
 }  // namespace
 
 // Note: this kernel is used by both the ParseExample op and the ParseExampleV2
@@ -388,8 +389,10 @@ REGISTER_KERNEL_BUILDER(Name("ParseSingleExample").Device(DEVICE_CPU),
 
 class ParseSequenceExampleOp : public OpKernel {
  public:
-  explicit ParseSequenceExampleOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
-    OP_REQUIRES_OK(ctx, attrs_.Init(ctx));
+  explicit ParseSequenceExampleOp(OpKernelConstruction* ctx)
+      : OpKernel(ctx),
+        op_version_(ctx->def().op() == kParseSequenceExampleV2 ? 2 : 1) {
+    OP_REQUIRES_OK(ctx, attrs_.Init(ctx, op_version_));
     metrics::RecordParseDenseFeature(attrs_.context_dense_keys.size() +
                                      attrs_.feature_list_dense_keys.size());
     metrics::RecordParseSparseFeature(attrs_.context_sparse_keys.size() +
@@ -405,92 +408,222 @@ class ParseSequenceExampleOp : public OpKernel {
     OP_REQUIRES_OK(ctx, ctx->input("serialized", &serialized));
     OP_REQUIRES_OK(ctx, ctx->input_list("context_dense_defaults",
                                         &context_dense_defaults));
-
-    bool has_debug_name = (debug_name->NumElements() > 0);
-    if (has_debug_name) {
-      OP_REQUIRES(ctx, TensorShapeUtils::IsVector(debug_name->shape()),
-                  errors::InvalidArgument(
-                      "Expected debug_name to be a vector, got shape: ",
-                      debug_name->shape().DebugString()));
+    const Tensor* context_dense_keys = nullptr;
+    const Tensor* context_sparse_keys = nullptr;
+    const Tensor* context_ragged_keys = nullptr;
+    const Tensor* feature_list_dense_keys = nullptr;
+    const Tensor* feature_list_sparse_keys = nullptr;
+    const Tensor* feature_list_ragged_keys = nullptr;
+    const Tensor* feature_list_dense_missing_assumed_empty = nullptr;
+    if (op_version_ == 2) {
+      OP_REQUIRES_OK(ctx,
+                     ctx->input("feature_list_dense_missing_assumed_empty",
+                                &feature_list_dense_missing_assumed_empty));
+      OP_REQUIRES_OK(ctx,
+                     ctx->input("context_dense_keys", &context_dense_keys));
+      OP_REQUIRES_OK(ctx,
+                     ctx->input("context_sparse_keys", &context_sparse_keys));
+      OP_REQUIRES_OK(ctx,
+                     ctx->input("context_ragged_keys", &context_ragged_keys));
+      OP_REQUIRES_OK(
+          ctx, ctx->input("feature_list_dense_keys", &feature_list_dense_keys));
+      OP_REQUIRES_OK(ctx, ctx->input("feature_list_sparse_keys",
+                                     &feature_list_sparse_keys));
+      OP_REQUIRES_OK(ctx, ctx->input("feature_list_ragged_keys",
+                                     &feature_list_ragged_keys));
+      std::call_once(flag_, [&]() {
+        metrics::RecordParseDenseFeature(
+            context_dense_keys->NumElements() +
+            feature_list_dense_keys->NumElements());
+        metrics::RecordParseSparseFeature(
+            context_sparse_keys->NumElements() +
+            feature_list_sparse_keys->NumElements());
+        metrics::RecordParseRaggedFeature(
+            context_ragged_keys->NumElements() +
+            feature_list_ragged_keys->NumElements());
+      });
     }
 
-    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(serialized->shape()),
-                errors::InvalidArgument(
-                    "Expected serialized to be a vector, got shape: ",
-                    serialized->shape().DebugString()));
+    // Validate input tensor shapes.
+    OP_REQUIRES_OK(ctx, CheckInputShapes(
+                            serialized, debug_name, context_dense_defaults,
+                            context_dense_keys, context_sparse_keys,
+                            context_ragged_keys, feature_list_dense_keys,
+                            feature_list_sparse_keys, feature_list_ragged_keys,
+                            feature_list_dense_missing_assumed_empty));
+
+    example::FastParseExampleConfig context_config =
+        MakeContextConfig(context_dense_keys, context_sparse_keys,
+                          context_ragged_keys, context_dense_defaults);
+    example::FastParseExampleConfig feature_list_config = MakeFeatureListConfig(
+        feature_list_dense_keys, feature_list_sparse_keys,
+        feature_list_ragged_keys, feature_list_dense_missing_assumed_empty);
+
+    bool is_batch = TensorShapeUtils::IsVector(serialized->shape());
+    auto serialized_t = serialized->flat<tstring>();
+    auto debug_name_t = debug_name->flat<tstring>();
+    gtl::ArraySlice<tstring> slice(serialized_t.data(), serialized_t.size());
+    gtl::ArraySlice<tstring> names_slice(debug_name_t.data(),
+                                         debug_name_t.size());
 
-    OP_REQUIRES(ctx, context_dense_defaults.size() == attrs_.num_context_dense,
-                errors::InvalidArgument("Expected len(context_dense_defaults) "
-                                        "== len(context_dense_keys) but got: ",
-                                        context_dense_defaults.size(), " vs. ",
-                                        attrs_.num_context_dense));
+    example::Result context_result, feature_list_result;
+    std::vector<Tensor> dense_feature_lengths;
+    OP_REQUIRES_OK(
+        ctx, FastParseSequenceExample(
+                 context_config, feature_list_config, slice, names_slice,
+                 ctx->device()->tensorflow_cpu_worker_threads()->workers,
+                 &context_result, &feature_list_result, &dense_feature_lengths,
+                 is_batch));
+
+    OP_REQUIRES_OK(ctx, WriteOutput(context_result, feature_list_result,
+                                    dense_feature_lengths, ctx));
+  }
 
-    std::vector<bool> required(attrs_.num_context_dense);
+ protected:
+  Status CheckInputShapes(
+      const Tensor* serialized, const Tensor* names,
+      const OpInputList& context_dense_defaults,
+
+      const Tensor* context_dense_keys, const Tensor* context_sparse_keys,
+      const Tensor* context_ragged_keys, const Tensor* feature_list_dense_keys,
+      const Tensor* feature_list_sparse_keys,
+      const Tensor* feature_list_ragged_keys,
+      const Tensor* feature_list_dense_missing_assumed_empty) const {
+    if (TensorShapeUtils::IsMatrixOrHigher(serialized->shape())) {
+      return errors::InvalidArgument(
+          "Expected serialized to be a scalar or vector, got shape: ",
+          serialized->shape().DebugString());
+    }
+    if (op_version_ > 1) {
+      if (context_dense_keys->NumElements() != attrs_.num_context_dense) {
+        return errors::InvalidArgument(
+            "Expected len(context_dense_keys) to match len(Tcontext_dense)");
+      }
+      if (context_sparse_keys->NumElements() != attrs_.num_context_sparse) {
+        return errors::InvalidArgument(
+            "Expected len(context_sparse_keys) to match Ncontext_sparse");
+      }
+      if (context_ragged_keys->NumElements() != attrs_.num_context_ragged) {
+        return errors::InvalidArgument(
+            "Expected len(context_ragged_keys) to match "
+            "len(context_ragged_value_types)");
+      }
+      if (feature_list_dense_keys->NumElements() !=
+          attrs_.num_feature_list_dense) {
+        return errors::InvalidArgument(
+            "Expected len(feature_list_dense_keys) to match "
+            "Nfeature_list_dense");
+      }
+      if (feature_list_dense_missing_assumed_empty->NumElements() !=
+          attrs_.num_feature_list_dense) {
+        return errors::InvalidArgument(
+            "Expected len(feature_list_dense_missing_assumed_empty to match "
+            "Nfeature_list_dense");
+      }
+      if (feature_list_sparse_keys->NumElements() !=
+          attrs_.num_feature_list_sparse) {
+        return errors::InvalidArgument(
+            "Expected len(feature_list_sparse_keys) to match "
+            "Nfeature_list_sparse");
+      }
+      if (feature_list_ragged_keys->NumElements() !=
+          attrs_.num_feature_list_ragged) {
+        return errors::InvalidArgument(
+            "Expected len(feature_list_ragged_keys) to match "
+            "len(feature_list_ragged_value_types)");
+      }
+    }
+    if (context_dense_defaults.size() != attrs_.num_context_dense) {
+      return errors::InvalidArgument(
+          "Expected len(context_dense_defaults) "
+          "== len(context_dense_keys) but got: ",
+          context_dense_defaults.size(), " vs. ", attrs_.num_context_dense);
+    }
     for (int d = 0; d < attrs_.num_context_dense; ++d) {
       const Tensor& def_value = context_dense_defaults[d];
-      required[d] = (def_value.NumElements() == 0);  // No default provided.
-
       if (def_value.NumElements() > 0) {
-        OP_REQUIRES(ctx, def_value.shape() == attrs_.context_dense_shapes[d],
-                    errors::InvalidArgument(
-                        "default_value[", d,
-                        "].shape() == ", def_value.shape().DebugString(),
-                        " != context_dense_shapes[", d,
-                        "] == ", attrs_.context_dense_shapes[d].DebugString()));
-        OP_REQUIRES(
-            ctx, def_value.dtype() == attrs_.context_dense_types[d],
-            errors::InvalidArgument(
-                "context_dense_defaults[", d, "].dtype() == ",
-                DataTypeString(def_value.dtype()), " != context_dense_types[",
-                d, "] == ", DataTypeString(attrs_.context_dense_types[d])));
+        if (def_value.shape() != attrs_.context_dense_shapes[d]) {
+          return errors::InvalidArgument(
+              "default_value[", d,
+              "].shape() == ", def_value.shape().DebugString(),
+              " != context_dense_shapes[", d,
+              "] == ", attrs_.context_dense_shapes[d].DebugString());
+        }
+        if (def_value.dtype() != attrs_.context_dense_types[d]) {
+          return errors::InvalidArgument(
+              "context_dense_defaults[", d,
+              "].dtype() == ", DataTypeString(def_value.dtype()),
+              " != context_dense_types[", d,
+              "] == ", DataTypeString(attrs_.context_dense_types[d]));
+        }
       }
     }
+    return Status::OK();
+  }
 
-    example::Result context_result, feature_list_result;
-    std::vector<Tensor> dense_feature_lengths;
-
-    example::FastParseExampleConfig context_config;
+  example::FastParseExampleConfig MakeContextConfig(
+      const Tensor* dense_keys, const Tensor* sparse_keys,
+      const Tensor* ragged_keys,
+      const OpInputList& context_dense_defaults) const {
+    example::FastParseExampleConfig config;
     for (int d = 0; d < attrs_.num_context_dense; ++d) {
-      context_config.dense.push_back(
-          {attrs_.context_dense_keys[d], attrs_.context_dense_types[d],
-           attrs_.context_dense_shapes[d], context_dense_defaults[d],
-           false /* attrs_.context_variable_length[d] */,
-           0 /*attrs_.context_elements_per_stride[d] */});
+      const string& key = dense_keys ? dense_keys->flat<tstring>()(d)
+                                     : attrs_.context_dense_keys[d];
+      config.dense.push_back({key, attrs_.context_dense_types[d],
+                              attrs_.context_dense_shapes[d],
+                              context_dense_defaults[d],
+                              false /* attrs_.context_variable_length[d] */,
+                              0 /*attrs_.context_elements_per_stride[d] */});
     }
     for (int d = 0; d < attrs_.num_context_sparse; ++d) {
-      context_config.sparse.push_back(
-          {attrs_.context_sparse_keys[d], attrs_.context_sparse_types[d]});
+      const string& key = sparse_keys ? sparse_keys->flat<tstring>()(d)
+                                      : attrs_.context_sparse_keys[d];
+      config.sparse.push_back({key, attrs_.context_sparse_types[d]});
+    }
+    for (int d = 0; d < attrs_.num_context_ragged; ++d) {
+      config.ragged.push_back({ragged_keys->flat<tstring>()(d),
+                               attrs_.context_ragged_value_types[d],
+                               attrs_.context_ragged_split_types[d]});
     }
-    example::FastParseExampleConfig feature_list_config;
+    return config;
+  }
+
+  example::FastParseExampleConfig MakeFeatureListConfig(
+      const Tensor* dense_keys, const Tensor* sparse_keys,
+      const Tensor* ragged_keys,
+      const Tensor* feature_list_dense_missing_assumed_empty) const {
+    example::FastParseExampleConfig config;
     for (int d = 0; d < attrs_.num_feature_list_dense; ++d) {
+      const string& key = dense_keys ? dense_keys->flat<tstring>()(d)
+                                     : attrs_.feature_list_dense_keys[d];
+      bool missing_assumed_empty =
+          feature_list_dense_missing_assumed_empty
+              ? feature_list_dense_missing_assumed_empty->flat<bool>()(d)
+              : attrs_.feature_list_dense_missing_assumed_empty.count(key) > 0;
       DataType dtype = attrs_.feature_list_dense_types[d];
       Tensor default_value = Tensor(dtype, TensorShape({}));
-      feature_list_config.dense.push_back(
-          {attrs_.feature_list_dense_keys[d], dtype,
-           attrs_.feature_list_dense_shapes[d], default_value,
-           (attrs_.feature_list_dense_missing_assumed_empty.count(
-                attrs_.feature_list_dense_keys[d]) > 0),
-           0 /*attrs_.context_elements_per_stride[d] */});
+      config.dense.push_back(
+          {key, dtype, attrs_.feature_list_dense_shapes[d], default_value,
+           missing_assumed_empty,
+           0 /*attrs_.feature_list_elements_per_stride[d] */});
     }
     for (int d = 0; d < attrs_.num_feature_list_sparse; ++d) {
-      feature_list_config.sparse.push_back(
-          {attrs_.feature_list_sparse_keys[d],
-           attrs_.feature_list_sparse_types[d]});
+      const string& key = sparse_keys ? sparse_keys->flat<tstring>()(d)
+                                      : attrs_.feature_list_sparse_keys[d];
+      config.sparse.push_back({key, attrs_.feature_list_sparse_types[d]});
     }
+    for (int d = 0; d < attrs_.num_feature_list_ragged; ++d) {
+      config.ragged.push_back({ragged_keys->flat<tstring>()(d),
+                               attrs_.feature_list_ragged_value_types[d],
+                               attrs_.feature_list_ragged_split_types[d]});
+    }
+    return config;
+  }
 
-    auto serialized_t = serialized->flat<tstring>();
-    auto debug_name_t = debug_name->flat<tstring>();
-    gtl::ArraySlice<tstring> slice(serialized_t.data(), serialized_t.size());
-    gtl::ArraySlice<tstring> names_slice(debug_name_t.data(),
-                                         debug_name_t.size());
-
-    OP_REQUIRES_OK(
-        ctx,
-        FastParseSequenceExample(
-            context_config, feature_list_config, slice, names_slice,
-            ctx->device()->tensorflow_cpu_worker_threads()->workers,
-            &context_result, &feature_list_result, &dense_feature_lengths));
-
+  Status WriteOutput(const example::Result& context_result,
+                     const example::Result& feature_list_result,
+                     const std::vector<Tensor>& dense_feature_lengths,
+                     OpKernelContext* ctx) const {
     OpOutputList context_sparse_indices;
     OpOutputList context_sparse_values;
     OpOutputList context_sparse_shapes;
@@ -501,31 +634,29 @@ class ParseSequenceExampleOp : public OpKernel {
     OpOutputList feature_list_dense_values;
     OpOutputList feature_list_dense_lengths;
 
-    OP_REQUIRES_OK(ctx, ctx->output_list("context_sparse_indices",
-                                         &context_sparse_indices));
-    OP_REQUIRES_OK(
-        ctx, ctx->output_list("context_sparse_values", &context_sparse_values));
-    OP_REQUIRES_OK(
-        ctx, ctx->output_list("context_sparse_shapes", &context_sparse_shapes));
-    OP_REQUIRES_OK(
-        ctx, ctx->output_list("context_dense_values", &context_dense_values));
-    OP_REQUIRES_OK(ctx, ctx->output_list("context_sparse_indices",
-                                         &context_sparse_indices));
-    OP_REQUIRES_OK(ctx, ctx->output_list("feature_list_sparse_indices",
-                                         &feature_list_sparse_indices));
-    OP_REQUIRES_OK(ctx, ctx->output_list("feature_list_sparse_values",
-                                         &feature_list_sparse_values));
-    OP_REQUIRES_OK(ctx, ctx->output_list("feature_list_sparse_shapes",
-                                         &feature_list_sparse_shapes));
-    OP_REQUIRES_OK(ctx, ctx->output_list("feature_list_dense_values",
-                                         &feature_list_dense_values));
-    OP_REQUIRES_OK(ctx, ctx->output_list("feature_list_dense_lengths",
-                                         &feature_list_dense_lengths));
+    TF_RETURN_IF_ERROR(
+        ctx->output_list("context_sparse_indices", &context_sparse_indices));
+    TF_RETURN_IF_ERROR(
+        ctx->output_list("context_sparse_values", &context_sparse_values));
+    TF_RETURN_IF_ERROR(
+        ctx->output_list("context_sparse_shapes", &context_sparse_shapes));
+    TF_RETURN_IF_ERROR(
+        ctx->output_list("context_dense_values", &context_dense_values));
+    TF_RETURN_IF_ERROR(
+        ctx->output_list("context_sparse_indices", &context_sparse_indices));
+    TF_RETURN_IF_ERROR(ctx->output_list("feature_list_sparse_indices",
+                                        &feature_list_sparse_indices));
+    TF_RETURN_IF_ERROR(ctx->output_list("feature_list_sparse_values",
+                                        &feature_list_sparse_values));
+    TF_RETURN_IF_ERROR(ctx->output_list("feature_list_sparse_shapes",
+                                        &feature_list_sparse_shapes));
+    TF_RETURN_IF_ERROR(ctx->output_list("feature_list_dense_values",
+                                        &feature_list_dense_values));
+    TF_RETURN_IF_ERROR(ctx->output_list("feature_list_dense_lengths",
+                                        &feature_list_dense_lengths));
     for (int d = 0; d < attrs_.num_context_dense; ++d) {
       context_dense_values.set(d, context_result.dense_values[d]);
     }
-    TensorShape lengths_shape;
-    lengths_shape.AddDim(serialized_t.size());
     for (int d = 0; d < attrs_.num_feature_list_dense; ++d) {
       feature_list_dense_values.set(d, feature_list_result.dense_values[d]);
       feature_list_dense_lengths.set(d, dense_feature_lengths[d]);
@@ -540,14 +671,46 @@ class ParseSequenceExampleOp : public OpKernel {
       feature_list_sparse_values.set(d, feature_list_result.sparse_values[d]);
       feature_list_sparse_shapes.set(d, feature_list_result.sparse_shapes[d]);
     }
+    if (op_version_ == 2) {
+      OpOutputList context_ragged_values;
+      OpOutputList context_ragged_splits;
+      OpOutputList feature_list_ragged_values;
+      OpOutputList feature_list_ragged_inner_splits;
+      OpOutputList feature_list_ragged_outer_splits;
+      TF_RETURN_IF_ERROR(
+          ctx->output_list("context_ragged_values", &context_ragged_values));
+      TF_RETURN_IF_ERROR(ctx->output_list("context_ragged_row_splits",
+                                          &context_ragged_splits));
+      TF_RETURN_IF_ERROR(ctx->output_list("feature_list_ragged_values",
+                                          &feature_list_ragged_values));
+      TF_RETURN_IF_ERROR(ctx->output_list("feature_list_ragged_inner_splits",
+                                          &feature_list_ragged_inner_splits));
+      TF_RETURN_IF_ERROR(ctx->output_list("feature_list_ragged_outer_splits",
+                                          &feature_list_ragged_outer_splits));
+      for (int d = 0; d < attrs_.num_context_ragged; ++d) {
+        context_ragged_values.set(d, context_result.ragged_values[d]);
+        context_ragged_splits.set(d, context_result.ragged_splits[d]);
+      }
+      for (int d = 0; d < attrs_.num_feature_list_ragged; ++d) {
+        feature_list_ragged_values.set(d, feature_list_result.ragged_values[d]);
+        feature_list_ragged_outer_splits.set(
+            d, feature_list_result.ragged_outer_splits[d]);
+        feature_list_ragged_inner_splits.set(
+            d, feature_list_result.ragged_splits[d]);
+      }
+    }
+    return Status::OK();
   }
 
- protected:
   ParseSequenceExampleAttrs attrs_;
+  int op_version_;
+  std::once_flag flag_;
 };
 
 REGISTER_KERNEL_BUILDER(Name("ParseSequenceExample").Device(DEVICE_CPU),
                         ParseSequenceExampleOp);
+REGISTER_KERNEL_BUILDER(Name("ParseSequenceExampleV2").Device(DEVICE_CPU),
+                        ParseSequenceExampleOp);
 
 class ParseSingleSequenceExampleOp : public OpKernel {
  public:
diff --git a/tensorflow/core/ops/parsing_ops.cc b/tensorflow/core/ops/parsing_ops.cc
index dfd6848e242..b4e742ee507 100644
--- a/tensorflow/core/ops/parsing_ops.cc
+++ b/tensorflow/core/ops/parsing_ops.cc
@@ -71,16 +71,16 @@ Status AddRaggedOutputShapes(int num_ragged, bool ragged_rank_2,
   for (int i = 0; i < num_ragged; ++i) {
     c->set_output((*output_idx)++, c->Vector(c->UnknownDim()));
   }
-  // Inner row_splits
+  // Outer row_splits.
+  for (int i = 0; i < num_ragged; ++i) {
+    c->set_output((*output_idx)++, c->Vector(num_splits));
+  }
+  // Inner row_splits  (for ParseSequenceExample feature_list features)
   if (ragged_rank_2) {
     for (int i = 0; i < num_ragged; ++i) {
       c->set_output((*output_idx)++, c->Vector(c->UnknownDim()));
     }
   }
-  // Outer row_splits.
-  for (int i = 0; i < num_ragged; ++i) {
-    c->set_output((*output_idx)++, c->Vector(num_splits));
-  }
   return Status::OK();
 }
 
@@ -307,6 +307,95 @@ REGISTER_OP("ParseSequenceExample")
       return Status::OK();
     });
 
+// Differences between ParseSequenceExample and ParseSequenceExampleV2:
+//   * Supports ragged features.
+//   * `serialized` may be a vector or a scalar.  (With v1, `serialized` could
+//      only be a vector).
+//   * Each set of keys is passed with a vector instead of an attr list.
+//   * feature_list_dense_missing_assumed_empty is passed with as a boolean
+//     vector (aligned 1:1 w/ feature_list_dense_kyes) rather than an attrib
+//     containing a list of strings.
+//   * No Ncontext_dense attribute (not needed).
+REGISTER_OP("ParseSequenceExampleV2")
+    .Input("serialized: string")
+    .Input("debug_name: string")
+    // Inputs: context features
+    .Input("context_sparse_keys: string")
+    .Input("context_dense_keys:  string")
+    .Input("context_ragged_keys: string")
+    // Inputs: feature lists
+    .Input("feature_list_sparse_keys: string")
+    .Input("feature_list_dense_keys: string")
+    .Input("feature_list_ragged_keys: string")
+    .Input("feature_list_dense_missing_assumed_empty: bool")
+    .Input("context_dense_defaults: Tcontext_dense")
+    // Outputs: context features
+    .Output("context_sparse_indices: Ncontext_sparse * int64")
+    .Output("context_sparse_values: context_sparse_types")
+    .Output("context_sparse_shapes: Ncontext_sparse * int64")
+    .Output("context_dense_values: Tcontext_dense")
+    .Output("context_ragged_values: context_ragged_value_types")
+    .Output("context_ragged_row_splits: context_ragged_split_types")
+    // Outputs: feature lists
+    .Output("feature_list_sparse_indices: Nfeature_list_sparse * int64")
+    .Output("feature_list_sparse_values: feature_list_sparse_types")
+    .Output("feature_list_sparse_shapes: Nfeature_list_sparse * int64")
+    .Output("feature_list_dense_values: feature_list_dense_types")
+    .Output("feature_list_dense_lengths: Nfeature_list_dense * int64")
+    .Output("feature_list_ragged_values: feature_list_ragged_value_types")
+    .Output("feature_list_ragged_outer_splits: feature_list_ragged_split_types")
+    .Output("feature_list_ragged_inner_splits: feature_list_ragged_split_types")
+    // Attribs: context features
+    .Attr("Ncontext_sparse: int >= 0 = 0")
+    .Attr("Tcontext_dense: list({float,int64,string}) >= 0 = []")  // inferred
+    .Attr("context_sparse_types: list({float,int64,string}) >= 0 = []")
+    .Attr("context_ragged_value_types: list({float,int64,string}) >= 0 = []")
+    .Attr("context_ragged_split_types: list({int32,int64}) >= 0 = []")
+    .Attr("context_dense_shapes: list(shape) >= 0 = []")
+    // Attribs: feature lists
+    .Attr("Nfeature_list_sparse: int >= 0 = 0")
+    .Attr("Nfeature_list_dense: int >= 0 = 0")
+    .Attr("feature_list_dense_types: list({float,int64,string}) >= 0 = []")
+    .Attr("feature_list_sparse_types: list({float,int64,string}) >= 0 = []")
+    .Attr(
+        "feature_list_ragged_value_types: list({float,int64,string}) >= 0 = []")
+    .Attr("feature_list_ragged_split_types: list({int32,int64}) >= 0 = []")
+    .Attr("feature_list_dense_shapes: list(shape) >= 0 = []")
+    .SetShapeFn([](InferenceContext* c) {
+      ParseSequenceExampleAttrs attrs;
+      TF_RETURN_IF_ERROR(attrs.Init(c, /*op_version=*/2));
+      ShapeHandle input;
+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &input));
+      ShapeHandle names;
+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &names));
+      ShapeHandle feature_list_dense_prefix;
+      TF_RETURN_IF_ERROR(c->Concatenate(input, c->UnknownShapeOfRank(1),
+                                        &feature_list_dense_prefix));
+      DimensionHandle num_examples = c->UnknownDim();
+      if (c->RankKnown(input) && c->Rank(input) == 1) {
+        num_examples = c->Dim(input, 0);
+      }
+
+      int output_idx = 0;
+      // Context outputs.
+      AddSparseOutputShapes(attrs.num_context_sparse, input, 1, c, &output_idx);
+      TF_RETURN_IF_ERROR(AddDenseOutputShapes(attrs.context_dense_shapes, input,
+                                              c, &output_idx));
+      TF_RETURN_IF_ERROR(AddRaggedOutputShapes(attrs.num_context_ragged, false,
+                                               num_examples, c, &output_idx));
+      // FeatureList outputs.
+      AddSparseOutputShapes(attrs.num_feature_list_sparse, input, 2, c,
+                            &output_idx);
+      TF_RETURN_IF_ERROR(AddDenseOutputShapes(attrs.feature_list_dense_shapes,
+                                              feature_list_dense_prefix, c,
+                                              &output_idx));
+      AddDenseLengthsShapes(attrs.num_feature_list_dense, input, c,
+                            &output_idx);
+      TF_RETURN_IF_ERROR(AddRaggedOutputShapes(
+          attrs.num_feature_list_ragged, true, num_examples, c, &output_idx));
+      return Status::OK();
+    });
+
 REGISTER_OP("ParseSingleSequenceExample")
     .Input("serialized: string")
     .Input("feature_list_dense_missing_assumed_empty: string")
diff --git a/tensorflow/core/ops/parsing_ops_test.cc b/tensorflow/core/ops/parsing_ops_test.cc
index 00576de4ddf..d791278b3d9 100644
--- a/tensorflow/core/ops/parsing_ops_test.cc
+++ b/tensorflow/core/ops/parsing_ops_test.cc
@@ -226,7 +226,7 @@ TEST(ParsingOpsTest, ParseSequenceExample_ShapeFn) {
   // Confirm an error from ParseSequenceExampleAttrs.Init().
   set_outputs(1, 1, 1, 1, true /* add_extra_shape */);
   INFER_ERROR(
-      "num_context_dense (1) must match the size of context_dense_keys (1), "
+      "num_context_dense (1) must match the size of "
       "context_dense_types (1) and context_dense_shapes (2)",
       op, "[?];[?];?");
 }
@@ -386,4 +386,211 @@ TEST(ParsingOpsTest, ParseExampleV2_ShapeFn) {
               "?;?;?;?;?;?;?;?");
 }
 
+TEST(ParsingOpsTest, ParseSequenceExampleV2_ShapeFn) {
+  ShapeInferenceTestOp op("ParseSequenceExampleV2");
+  auto set_outputs = [&op](int num_context_sparse, int num_context_dense,
+                           int num_context_ragged, int num_feature_list_sparse,
+                           int num_feature_list_dense,
+                           int num_feature_list_ragged,
+                           bool add_extra_shape = false) {
+    using NodeOutList = std::vector<NodeDefBuilder::NodeOut>;
+    using DataTypeList = std::vector<DataType>;
+    string string_in("test");
+    NodeDefBuilder::NodeOut node_in{"a", 0, DT_STRING};
+    TF_ASSERT_OK(
+        NodeDefBuilder("test", "ParseSequenceExampleV2")
+            .Input("serialized", 0, DT_STRING)
+            .Input("debug_name", 0, DT_STRING)
+            .Input("context_sparse_keys", 0, DT_STRING)
+            .Input("context_dense_keys", 0, DT_STRING)
+            .Input("context_ragged_keys", 0, DT_STRING)
+            .Input("feature_list_sparse_keys", 0, DT_STRING)
+            .Input("feature_list_dense_keys", 0, DT_STRING)
+            .Input("feature_list_ragged_keys", 0, DT_STRING)
+            .Input("feature_list_dense_missing_assumed_empty", 0, DT_BOOL)
+            .Input(NodeOutList(num_context_dense, node_in))
+            .Attr("Ncontext_sparse", num_context_sparse)
+            .Attr("Nfeature_list_sparse", num_feature_list_sparse)
+            .Attr("Nfeature_list_dense", num_feature_list_dense)
+            .Attr("context_sparse_types",
+                  DataTypeList(num_context_sparse, DT_FLOAT))
+            .Attr("context_dense_types",
+                  DataTypeList(num_context_dense, DT_FLOAT))
+            .Attr("context_dense_shapes",
+                  MakeDenseShapes(num_context_dense, add_extra_shape, 0))
+            .Attr("feature_list_sparse_types",
+                  DataTypeList(num_feature_list_sparse, DT_FLOAT))
+            .Attr("feature_list_dense_types",
+                  DataTypeList(num_feature_list_dense, DT_FLOAT))
+            .Attr("feature_list_dense_shapes",
+                  MakeDenseShapes(num_feature_list_dense, add_extra_shape, 0))
+            .Attr("context_ragged_value_types",
+                  DataTypeList(num_context_ragged, DT_FLOAT))
+            .Attr("context_ragged_split_types",
+                  DataTypeList(num_context_ragged, DT_INT32))
+            .Attr("feature_list_ragged_value_types",
+                  DataTypeList(num_feature_list_ragged, DT_FLOAT))
+            .Attr("feature_list_ragged_split_types",
+                  DataTypeList(num_feature_list_ragged, DT_INT32))
+            .Finalize(&op.node_def));
+  };
+
+  // Verify inputs 'serialized' and 'debug_name'.
+  set_outputs(0, 0, 0, 0, 0, 0);  // no features
+  INFER_OK(op, "?;[?];?;?;?;?;?;?;?", "");
+  INFER_OK(op, "[?];[?];?;?;?;?;?;?;?", "");
+  INFER_OK(op, "[8];[8];?;?;?;?;?;?;?", "");
+  INFER_OK(op, "[];[];?;?;?;?;?;?;?", "");
+  INFER_ERROR("must be at most rank 1", op, "[1,2];?;?;?;?;?;?;?;?");
+  INFER_ERROR("must be at most rank 1", op, "?;[2,3];?;?;?;?;?;?;?");
+
+  // context inputs with no feature_list inputs.
+  set_outputs(2 /* num_context_sparse */, 3 /* num_context_dense */,
+              4 /* num_ragged */, 0, 0, 0);
+  INFER_OK(op, "[?];[?];?;?;?;?;?;?;?;?;?;?",    // Vector input, unknown size
+           ("[?,2];[?,2];"                       //  context sparse indices
+            "[?];[?];"                           //  context sparse values
+            "[2];[2];"                           //  context sparse dense_shapes
+            "[d0_0,1];[d0_0,1,2];[d0_0,1,2,3];"  //  context dense outputs
+            "[?];[?];[?];[?];"                   //  context ragged values
+            "[?];[?];[?];[?]"));                 //  context ragged row_splits
+  INFER_OK(op, "[5];[?];?;?;?;?;?;?;?;?;?;?",    // Vector input, known size
+           ("[?,2];[?,2];"                       //  context sparse indices
+            "[?];[?];"                           //  context sparse values
+            "[2];[2];"                           //  context sparse dense_shapes
+            "[d0_0,1];[d0_0,1,2];[d0_0,1,2,3];"  //  context dense outputs
+            "[?];[?];[?];[?];"                   //  context ragged values
+            "[6];[6];[6];[6]"));                 //  context ragged row_splits
+  INFER_OK(op, "[];[?];?;?;?;?;?;?;?;?;?;?",     // Scalar input
+           ("[?,1];[?,1];"                       //  context sparse indices
+            "[?];[?];"                           //  context sparse values
+            "[1];[1];"                           //  context sparse dense_shapes
+            "[1];[1,2];[1,2,3];"                 //  context dense outputs
+            "[?];[?];[?];[?];"                   //  context ragged values
+            "[?];[?];[?];[?]"));                 //  context ragged row_splits
+  INFER_OK(op, "?;[?];?;?;?;?;?;?;?;?;?;?",      // Unknown rank
+           ("[?,?];[?,?];"                       //  context sparse indices
+            "[?];[?];"                           //  context sparse values
+            "[?];[?];"                           //  context sparse dense_shapes
+            "?;?;?;"                             //  context dense outputs
+            "[?];[?];[?];[?];"                   //  context ragged values
+            "[?];[?];[?];[?]"));                 //  context ragged row_splits
+
+  // feature_list inputs with no context inputs.
+  set_outputs(0, 0, 0, 2 /* num_context_sparse */, 3 /* num_context_dense */,
+              4 /* num_ragged */);
+  INFER_OK(op, "[?];[?];?;?;?;?;?;?;?",  // Vector input, unknown size
+           ("[?,3];[?,3];"               //  f_list sparse indices
+            "[?];[?];"                   //  f_list sparse values
+            "[3];[3];"                   //  f_list sparse dense_shapes
+            "[d0_0,?,1];[d0_0,?,1,2];"   //  f_list dense outputs
+            "[d0_0,?,1,2,3];"            //     (continued)
+            "in0;in0;in0;"               //  f_list dense lengths
+            "[?];[?];[?];[?];"           //  f_list ragged values
+            "[?];[?];[?];[?];"           //  f_list ragged outer_splits
+            "[?];[?];[?];[?]"));         //  f_list ragged inner_splits
+  INFER_OK(op, "[5];[?];?;?;?;?;?;?;?",  // Vector input, known size
+           ("[?,3];[?,3];"               //  f_list sparse indices
+            "[?];[?];"                   //  f_list sparse values
+            "[3];[3];"                   //  f_list sparse dense_shapes
+            "[d0_0,?,1];[d0_0,?,1,2];"   //  f_list dense outputs
+            "[d0_0,?,1,2,3];"            //    (continued)
+            "in0;in0;in0;"               //  f_list dense lengths
+            "[?];[?];[?];[?];"           //  f_list ragged values
+            "[6];[6];[6];[6];"           //  f_list ragged outer_splits
+            "[?];[?];[?];[?]"));         //  f_list ragged inner_splits
+  INFER_OK(op, "[];[?];?;?;?;?;?;?;?",   // Scalar input
+           ("[?,2];[?,2];"               //  f_list sparse indices
+            "[?];[?];"                   //  f_list sparse values
+            "[2];[2];"                   //  f_list sparse dense_shapes
+            "[?,1];[?,1,2];[?,1,2,3];"   //  f_list dense outputs
+            "in0;in0;in0;"               //  f_list dense lengths
+            "[?];[?];[?];[?];"           //  f_list ragged values
+            "[?];[?];[?];[?];"           //  f_list ragged outer_splits
+            "[?];[?];[?];[?]"));         //  f_list ragged inner_splits
+  INFER_OK(op, "?;[?];?;?;?;?;?;?;?",    // Unknown rank
+           ("[?,?];[?,?];"               //  f_list sparse indices
+            "[?];[?];"                   //  f_list sparse values
+            "[?];[?];"                   //  f_list sparse dense_shapes
+            "?;?;?;"                     //  f_list dense outputs
+            "in0;in0;in0;"               //  f_list dense lengths
+            "[?];[?];[?];[?];"           //  f_list ragged values
+            "[?];[?];[?];[?];"           //  f_list ragged outer_splits
+            "[?];[?];[?];[?]"));         //  f_list ragged inner_splits
+
+  // Combine previous two test cases.
+  set_outputs(2 /* num_context_sparse */, 3 /* num_context_dense */,
+              4 /* num_ragged */, 2 /* num_context_sparse */,
+              3 /* num_context_dense */, 4 /* num_ragged */);
+  INFER_OK(op, "[?];[?];?;?;?;?;?;?;?;?;?;?",    // Vector input, unknown size
+           ("[?,2];[?,2];"                       //  context sparse indices
+            "[?];[?];"                           //  context sparse values
+            "[2];[2];"                           //  context sparse dense_shapes
+            "[d0_0,1];[d0_0,1,2];[d0_0,1,2,3];"  //  context dense outputs
+            "[?];[?];[?];[?];"                   //  context ragged values
+            "[?];[?];[?];[?];"                   //  context ragged row_splits
+            "[?,3];[?,3];"                       //  f_list sparse indices
+            "[?];[?];"                           //  f_list sparse values
+            "[3];[3];"                           //  f_list sparse dense_shapes
+            "[d0_0,?,1];[d0_0,?,1,2];"           //  f_list dense outputs
+            "[d0_0,?,1,2,3];"                    //     (continued)
+            "in0;in0;in0;"                       //  f_list dense lengths
+            "[?];[?];[?];[?];"                   //  f_list ragged values
+            "[?];[?];[?];[?];"                   //  f_list ragged outer_splits
+            "[?];[?];[?];[?]"));                 //  f_list ragged inner_splits
+  INFER_OK(op, "[5];[?];?;?;?;?;?;?;?;?;?;?",    // Vector input, known size
+           ("[?,2];[?,2];"                       //  context sparse indices
+            "[?];[?];"                           //  context sparse values
+            "[2];[2];"                           //  context sparse dense_shapes
+            "[d0_0,1];[d0_0,1,2];[d0_0,1,2,3];"  //  context dense outputs
+            "[?];[?];[?];[?];"                   //  context ragged values
+            "[6];[6];[6];[6];"                   //  context ragged row_splits
+            "[?,3];[?,3];"                       //  f_list sparse indices
+            "[?];[?];"                           //  f_list sparse values
+            "[3];[3];"                           //  f_list sparse dense_shapes
+            "[d0_0,?,1];[d0_0,?,1,2];"           //  f_list dense outputs
+            "[d0_0,?,1,2,3];"                    //    (continued)
+            "in0;in0;in0;"                       //  f_list dense lengths
+            "[?];[?];[?];[?];"                   //  f_list ragged values
+            "[6];[6];[6];[6];"                   //  f_list ragged outer_splits
+            "[?];[?];[?];[?]"));                 //  f_list ragged inner_splits
+  INFER_OK(op, "[];[?];?;?;?;?;?;?;?;?;?;?",     // Scalar input
+           ("[?,1];[?,1];"                       //  context sparse indices
+            "[?];[?];"                           //  context sparse values
+            "[1];[1];"                           //  context sparse dense_shapes
+            "[1];[1,2];[1,2,3];"                 //  context dense outputs
+            "[?];[?];[?];[?];"                   //  context ragged values
+            "[?];[?];[?];[?];"                   //  context ragged row_splits
+            "[?,2];[?,2];"                       //  f_list sparse indices
+            "[?];[?];"                           //  f_list sparse values
+            "[2];[2];"                           //  f_list sparse dense_shapes
+            "[?,1];[?,1,2];[?,1,2,3];"           //  f_list dense outputs
+            "in0;in0;in0;"                       //  f_list dense lengths
+            "[?];[?];[?];[?];"                   //  f_list ragged values
+            "[?];[?];[?];[?];"                   //  f_list ragged outer_splits
+            "[?];[?];[?];[?]"));                 //  f_list ragged inner_splits
+  INFER_OK(op, "?;[?];?;?;?;?;?;?;?;?;?;?",      // Unknown rank
+           ("[?,?];[?,?];"                       //  context sparse indices
+            "[?];[?];"                           //  context sparse values
+            "[?];[?];"                           //  context sparse dense_shapes
+            "?;?;?;"                             //  context dense outputs
+            "[?];[?];[?];[?];"                   //  context ragged values
+            "[?];[?];[?];[?];"                   //  context ragged row_splits
+            "[?,?];[?,?];"                       //  f_list sparse indices
+            "[?];[?];"                           //  f_list sparse values
+            "[?];[?];"                           //  f_list sparse dense_shapes
+            "?;?;?;"                             //  f_list dense outputs
+            "in0;in0;in0;"                       //  f_list dense lengths
+            "[?];[?];[?];[?];"                   //  f_list ragged values
+            "[?];[?];[?];[?];"                   //  f_list ragged outer_splits
+            "[?];[?];[?];[?]"));                 //  f_list ragged inner_splits
+
+  // Confirm an error from ParseSequenceExampleAttrs.Init().
+  set_outputs(1, 1, 1, 1, 1, 1, true /* add_extra_shape */);
+  INFER_ERROR(
+      "num_context_dense (1) must match the size of "
+      "context_dense_types (1) and context_dense_shapes (2)",
+      op, "[?];[?];?;?;?;?;?;?;?;?");
+}
+
 }  // end namespace tensorflow
diff --git a/tensorflow/core/util/example_proto_fast_parsing.cc b/tensorflow/core/util/example_proto_fast_parsing.cc
index 5656291a4f9..c627e23700b 100644
--- a/tensorflow/core/util/example_proto_fast_parsing.cc
+++ b/tensorflow/core/util/example_proto_fast_parsing.cc
@@ -460,7 +460,10 @@ void ParallelFor(const std::function<void(size_t)>& f, size_t n,
   }
 }
 
-enum class Type { Sparse, Dense, Ragged };
+// Enumeration for distinguishing feature types.
+// Note: FastParseSequenceExample constructs a map that includes Type values,
+// and relies on the fact that they are default-initialized to Dense.
+enum class Type { Dense, Sparse, Ragged };
 
 // Note: We use SparseBuffer for sparse, ragged, and dense_varlen features.
 struct SparseBuffer {
@@ -915,6 +918,14 @@ Status CheckConfigDataType(DataType dtype) {
   }
 }
 
+// Use this in the "default" clause of switch statements when dispatching
+// on a dtype variable that was checked by CheckConfigDataType():
+inline void ReportUnexpectedDataType(DataType dtype) {
+  DCHECK(false)
+      << "Encountered unexpected DataType " << DataTypeString(dtype)
+      << "in variable that should have been checked by CheckConfigDataType().";
+}
+
 Status CheckConfigDataTypes(Config config) {
   // Check config so we can safely CHECK(false) in switches on config.*.dtype
   for (auto& c : config.sparse) {
@@ -1071,9 +1082,7 @@ void CopySparseBufferToTensor(DataType dtype, size_t offset, SparseBuffer* src,
       break;
     }
     default:
-      // We checked that dtype was one of these three values with
-      // CheckConfigDataTypes().
-      DCHECK(false) << "Should not happen.";
+      ReportUnexpectedDataType(dtype);
   }
 }
 
@@ -1364,7 +1373,7 @@ Status FastParseExample(const Config& config,
         break;
       }
       default:
-        LOG(FATAL) << "Should not happen.";
+        ReportUnexpectedDataType(config.dense[d].dtype);
     }
   };
 
@@ -1571,7 +1580,7 @@ Status FastParseSingleExample(const Config& config,
           break;
         }
         default:
-          LOG(FATAL) << "Should not happen.";
+          ReportUnexpectedDataType(example_dtype);
       }
 
     } else {  // if variable length
@@ -1644,7 +1653,8 @@ Status FastParseSingleExample(const Config& config,
           break;
         }
         default:
-          LOG(FATAL) << "Should not happen. " << DataTypeString(example_dtype);
+          num_elements = 0;
+          ReportUnexpectedDataType(example_dtype);
       }
 
       if (num_elements % num_elements_divisor != 0) {
@@ -1712,7 +1722,7 @@ Status FastParseSingleExample(const Config& config,
           break;
         }
         default:
-          LOG(FATAL) << "Should not happen.";
+          ReportUnexpectedDataType(example_dtype);
       }
     }
   }
@@ -1762,6 +1772,35 @@ Status FastParseSingleExample(const Config& config,
   return Status::OK();
 }
 
+// Private helper functions for FastParseSequenceExample.
+namespace {
+
+// A struct used by FastParseSequenceExample to hold the serialized proto
+// substrings for a single feature, plus some auxiliary information derived
+// from those protos (such as the total value length).
+struct FeatureProtos {
+  // Proto substrings from each serialized SequenceExamle that correspond
+  // with this feature.  `protos_present` records whether the proto had a
+  // value defined (even if that value is empty).
+  std::vector<StringPiece> protos;
+  std::vector<bool> protos_present;
+
+  // Information derived from protos:
+  size_t length;    // total length for ragged/sparse, max row length for dense.
+  size_t num_rows;  // only populated for ragged sequence features.
+
+  // Information from the config:
+  Type type;  // Whether this feature is sparse, ragged, or dense.
+  DataType dtype;
+};
+
+// Map from feature name to FeatureProtos for that feature.
+using FeatureProtosMap = absl::flat_hash_map<StringPiece, FeatureProtos>;
+
+string ExampleName(const gtl::ArraySlice<tstring> example_names, int n) {
+  return example_names.empty() ? "<unknown>" : example_names[n];
+}
+
 // Return the number of bytes elements parsed, or -1 on error. If out is null,
 // this method simply counts the number of elements without any copying.
 inline int ParseBytesFeature(protobuf::io::CodedInputStream* stream,
@@ -1914,6 +1953,52 @@ inline int ParseInt64Feature(protobuf::io::CodedInputStream* stream,
   return num_elements;
 }
 
+// Parses the next feature on `stream` into `out` starting at `out_offset`.
+// Updates `out_offset`, and returns the number of values added.
+// Returns -1 if the next feature on `stream` doesn't match `dtype`.
+inline int ParseFeature(DataType dtype, protobuf::io::CodedInputStream* stream,
+                        Tensor* out, size_t* out_offset) {
+  int delta;
+  switch (dtype) {
+    case DT_STRING:
+      delta =
+          ParseBytesFeature(stream, out->flat<tstring>().data() + *out_offset);
+      break;
+    case DT_FLOAT:
+      delta =
+          ParseFloatFeature(stream, out->flat<float>().data() + *out_offset);
+      break;
+    case DT_INT64:
+      delta =
+          ParseInt64Feature(stream, out->flat<int64>().data() + *out_offset);
+      break;
+    default:
+      ReportUnexpectedDataType(dtype);
+      delta = 0;
+  }
+  if (delta > 0) {
+    *out_offset += delta;
+  }
+  return delta;
+}
+
+// Returns the length of the next feature on `stream`.
+// Returns -1 if the next feature on `stream` doesn't match `dtype`.
+inline int GetFeatureLength(DataType dtype,
+                            protobuf::io::CodedInputStream* stream) {
+  switch (dtype) {
+    case DT_STRING:
+      return ParseBytesFeature(stream, nullptr);
+    case DT_FLOAT:
+      return ParseFloatFeature(stream, nullptr);
+    case DT_INT64:
+      return ParseInt64Feature(stream, nullptr);
+    default:
+      ReportUnexpectedDataType(dtype);
+      return -1;
+  }
+}
+
 inline DataType ParseDataType(protobuf::io::CodedInputStream* stream) {
   uint8 peek_tag = PeekTag(stream);
   switch (peek_tag) {
@@ -1953,88 +2038,13 @@ inline bool SkipEmptyFeature(protobuf::io::CodedInputStream* stream,
   return stream->ReadVarint32(&length) && length == 0;
 }
 
-// TODO(sundberg): Use the threadpool to parallelize example parsing.
-// TODO(b/111553342): Support extracting feature statistics from the examples.
-Status FastParseSequenceExample(
-    const FastParseExampleConfig& context_config,
-    const FastParseExampleConfig& feature_list_config,
-    gtl::ArraySlice<tstring> serialized, gtl::ArraySlice<tstring> example_names,
-    thread::ThreadPool* thread_pool, Result* context_result,
-    Result* feature_list_result, std::vector<Tensor>* dense_feature_lengths) {
-  int num_examples = serialized.size();
-  DCHECK(context_result != nullptr);
-  DCHECK(feature_list_result != nullptr);
-  DCHECK(dense_feature_lengths != nullptr);
-  size_t num_context_features =
-      context_config.sparse.size() + context_config.dense.size();
-  absl::flat_hash_map<StringPiece, bool> context_is_sparse;
-  context_is_sparse.reserve(num_context_features);
-  absl::flat_hash_map<StringPiece, std::pair<DataType, size_t>>
-      context_feature_type_and_lengths;
-  context_feature_type_and_lengths.reserve(num_context_features);
-  if (!example_names.empty() && example_names.size() != num_examples) {
-    return errors::InvalidArgument(
-        "example_names must be empty or have the correct number of elements");
-  }
-  for (auto& c : context_config.sparse) {
-    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
-    context_feature_type_and_lengths[c.feature_name] =
-        std::make_pair(c.dtype, 0);
-    context_is_sparse[c.feature_name] = true;
-  }
-  for (auto& c : context_config.dense) {
-    if (context_is_sparse[c.feature_name]) {
-      return errors::InvalidArgument("Context feature " + c.feature_name +
-                                     " cannot be both dense and sparse");
-    }
-    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
-    context_feature_type_and_lengths[c.feature_name] =
-        std::make_pair(c.dtype, c.default_value.NumElements());
-    if (c.default_value.NumElements() > 0) {
-      if (!c.shape.IsCompatibleWith(c.default_value.shape())) {
-        return errors::InvalidArgument("Default value for context feature ",
-                                       c.feature_name,
-                                       " has an incorrect shape: saw ",
-                                       c.default_value.shape().DebugString(),
-                                       " but expected ", c.shape.DebugString());
-      }
-    }
-  }
-  size_t num_sequence_features =
-      feature_list_config.sparse.size() + feature_list_config.dense.size();
-  absl::flat_hash_map<StringPiece, bool> sequence_is_sparse;
-  sequence_is_sparse.reserve(num_sequence_features);
-  absl::flat_hash_map<StringPiece, std::pair<DataType, size_t>>
-      sequence_feature_type_and_lengths;
-  sequence_feature_type_and_lengths.reserve(num_sequence_features);
-  for (auto& c : feature_list_config.sparse) {
-    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
-    sequence_feature_type_and_lengths[c.feature_name] =
-        std::make_pair(c.dtype, 0);
-    sequence_is_sparse[c.feature_name] = true;
-  }
-  for (auto& c : feature_list_config.dense) {
-    if (sequence_is_sparse[c.feature_name]) {
-      return errors::InvalidArgument("Sequence feature " + c.feature_name +
-                                     " cannot be both dense and sparse");
-    }
-    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
-    sequence_feature_type_and_lengths[c.feature_name] =
-        std::make_pair(c.dtype, 0);
-  }
-
-  std::vector<absl::flat_hash_map<StringPiece, StringPiece>>
-      all_context_features(num_examples);
-  std::vector<absl::flat_hash_map<StringPiece, StringPiece>>
-      all_sequence_features(num_examples);
-  const tstring kUnknown = "<unknown>";
-  for (int d = 0; d < num_examples; d++) {
-    const tstring& example = serialized[d];
-    const tstring& example_name =
-        example_names.empty() ? kUnknown : example_names[d];
-    auto* context_features = &all_context_features[d];
-    auto* sequence_features = &all_sequence_features[d];
-
+// Reads an example proto, and extracts a StringPiece pointer to each feature.
+Status ExtractFeaturesFromSequenceExamples(
+    const gtl::ArraySlice<tstring> examples,
+    const gtl::ArraySlice<tstring> example_names,
+    FeatureProtosMap* context_features, FeatureProtosMap* sequence_features) {
+  for (int d = 0; d < examples.size(); d++) {
+    const tstring& example = examples[d];
     protobuf::io::CodedInputStream stream(
         reinterpret_cast<const uint8*>(example.data()), example.size());
     // Not clear what this does. Why not stream.EnableAliasing()?
@@ -2042,26 +2052,24 @@ Status FastParseSequenceExample(
 
     // Extract pointers to all features within this serialized example.
     while (!stream.ExpectAtEnd()) {
-      absl::flat_hash_map<StringPiece, StringPiece>* features = nullptr;
-      const absl::flat_hash_map<StringPiece, std::pair<DataType, size_t>>*
-          config = nullptr;
+      FeatureProtosMap* features = nullptr;
       if (stream.ExpectTag(kDelimitedTag(1))) {
         // Context
         features = context_features;
-        config = &context_feature_type_and_lengths;
       } else if (stream.ExpectTag(kDelimitedTag(2))) {
         // Sequence
         features = sequence_features;
-        config = &sequence_feature_type_and_lengths;
       } else if (!SkipExtraneousTag(&stream)) {
         return errors::InvalidArgument(
-            "Invalid protocol message input, example id: ", example_name);
+            "Invalid protocol message input, example id: ",
+            ExampleName(example_names, d));
       }
       if (features != nullptr) {
         uint32 length;
         if (!stream.ReadVarint32(&length)) {
           return errors::InvalidArgument(
-              "Invalid protocol message input, example id: ", example_name);
+              "Invalid protocol message input, example id: ",
+              ExampleName(example_names, d));
         }
         auto limit = stream.PushLimit(length);
         while (!stream.ExpectAtEnd()) {
@@ -2070,7 +2078,8 @@ Status FastParseSequenceExample(
           if (!stream.ExpectTag(kDelimitedTag(1)) ||
               !stream.ReadVarint32(&length)) {
             return errors::InvalidArgument(
-                "Invalid protocol message input, example id: ", example_name);
+                "Invalid protocol message input, example id: ",
+                ExampleName(example_names, d));
           }
           auto limit = stream.PushLimit(length);
           if (!stream.ExpectTag(kDelimitedTag(1)) ||
@@ -2078,177 +2087,193 @@ Status FastParseSequenceExample(
               !stream.ExpectTag(kDelimitedTag(2)) ||
               !ParseString(&stream, &value) || !stream.ExpectAtEnd()) {
             return errors::InvalidArgument(
-                "Invalid protocol message input, example id: ", example_name);
+                "Invalid protocol message input, example id: ",
+                ExampleName(example_names, d));
           }
           stream.PopLimit(limit);
           // Only save if this feature was requested.
-          if (config->count(key) > 0) {
-            (*features)[key] = value;
+          auto feature_iter = features->find(key);
+          if (feature_iter != features->end()) {
+            auto& feature = feature_iter->second;
+            feature.protos[d] = value;
+            feature.protos_present[d] = true;
           }
         }
         stream.PopLimit(limit);
       }
     }
+  }
+  return Status::OK();
+}
 
-    for (const auto& c : *context_features) {
-      size_t num_elements = 0;
-      if (!c.second.empty()) {
-        protobuf::io::CodedInputStream stream(
-            reinterpret_cast<const uint8*>(c.second.data()), c.second.size());
-        EnableAliasing(&stream);
-        DataType dtype = context_feature_type_and_lengths[c.first].first;
-        int64 num;
-        switch (dtype) {
-          case DT_STRING:
-            num = ParseBytesFeature(&stream, nullptr);
-            break;
-          case DT_FLOAT:
-            num = ParseFloatFeature(&stream, nullptr);
-            break;
-          case DT_INT64:
-            num = ParseInt64Feature(&stream, nullptr);
-            break;
-          default:
-            num = -1;
-            break;
-        }
-        if (num == -1) {
-          return errors::InvalidArgument("Error in context feature ", c.first,
-                                         " in example ", example_name);
-        }
-        num_elements += num;
+// Populates context_fatures[k].length based on context_features[k].protos
+// (for all k).
+Status GetContextFeatureLengths(const gtl::ArraySlice<tstring> example_names,
+                                FeatureProtosMap* context_features) {
+  for (auto& c : *context_features) {
+    FeatureProtos& feature = c.second;
+    for (int d = 0; d < feature.protos.size(); ++d) {
+      const auto& proto = feature.protos[d];
+      if (proto.empty()) continue;
+      protobuf::io::CodedInputStream stream(
+          reinterpret_cast<const uint8*>(proto.data()), proto.size());
+      EnableAliasing(&stream);
+      int num_elements = GetFeatureLength(feature.dtype, &stream);
+      if (num_elements < 0) {
+        return errors::InvalidArgument(
+            "Name: ", ExampleName(example_names, d),
+            ", Context feature: ", c.first,
+            ".  Data types don't match. Expected type: ",
+            DataTypeString(feature.dtype));
       }
-      if (context_is_sparse[c.first]) {
-        context_feature_type_and_lengths[c.first].second += num_elements;
-      } else {
-        size_t current_max = context_feature_type_and_lengths[c.first].second;
-        context_feature_type_and_lengths[c.first].second =
-            std::max(current_max, num_elements);
+      switch (feature.type) {
+        case Type::Sparse:  // intentional fall-through
+        case Type::Ragged:
+          feature.length += num_elements;
+          break;
+        case Type::Dense:
+          feature.length =
+              std::max(feature.length, static_cast<size_t>(num_elements));
+          break;
       }
     }
-    for (const auto& c : *sequence_features) {
+  }
+  return Status::OK();
+}
+
+// Populates sequence_fatures[k].length and sequence_features[k].num_rows based
+// on sequence_features[k].protos (for all k).
+Status GetSequenceFeatureLengths(const gtl::ArraySlice<tstring> example_names,
+                                 FeatureProtosMap* sequence_features) {
+  for (auto& c : *sequence_features) {
+    FeatureProtos& feature = c.second;
+    for (int d = 0; d < feature.protos.size(); ++d) {
+      const auto& proto = feature.protos[d];
+      if (proto.empty()) continue;
+
+      size_t num_rows = 0;
       size_t num_elements = 0;
-      if (!c.second.empty()) {
-        protobuf::io::CodedInputStream stream(
-            reinterpret_cast<const uint8*>(c.second.data()), c.second.size());
-        EnableAliasing(&stream);
-        DataType dtype = sequence_feature_type_and_lengths[c.first].first;
-        while (!stream.ExpectAtEnd()) {
-          uint32 feature_length;
-          if (!stream.ExpectTag(kDelimitedTag(1)) ||
-              !stream.ReadVarint32(&feature_length)) {
-            return errors::InvalidArgument("Error in sequence feature ",
-                                           c.first, " in example ",
-                                           example_name);
+      protobuf::io::CodedInputStream stream(
+          reinterpret_cast<const uint8*>(proto.data()), proto.size());
+      EnableAliasing(&stream);
+      while (!stream.ExpectAtEnd()) {
+        uint32 feature_bytes;
+        if (!stream.ExpectTag(kDelimitedTag(1)) ||
+            !stream.ReadVarint32(&feature_bytes)) {
+          return errors::InvalidArgument("Error in sequence feature ", c.first,
+                                         " in example ",
+                                         ExampleName(example_names, d));
+        }
+        if (feature_bytes > 2) {
+          auto limit = stream.PushLimit(feature_bytes);
+          int delta = GetFeatureLength(feature.dtype, &stream);
+          if (delta < 0) {
+            return errors::InvalidArgument(
+                "Name: ", ExampleName(example_names, d),
+                ", Feature list: ", c.first, ", Index: ", num_rows,
+                ".  Data types don't match. Expected type: ",
+                DataTypeString(feature.dtype));
           }
-          if (feature_length > 2) {
-            auto limit = stream.PushLimit(feature_length);
-            int64 num;
-            switch (dtype) {
-              case DT_STRING:
-                num = ParseBytesFeature(&stream, nullptr);
-                break;
-              case DT_FLOAT:
-                num = ParseFloatFeature(&stream, nullptr);
-                break;
-              case DT_INT64:
-                num = ParseInt64Feature(&stream, nullptr);
-                break;
-              default:
-                num = -1;
-                break;
-            }
-            if (num == -1) {
-              return errors::InvalidArgument("Error in sequence feature ",
-                                             c.first, " in example ",
-                                             example_name);
-            }
-            num_elements += num;
-            stream.PopLimit(limit);
-          } else if (feature_length == 2) {
-            if (!SkipEmptyFeature(&stream, dtype)) {
-              return errors::InvalidArgument("Error in sequence feature ",
-                                             c.first, " in example ",
-                                             example_name);
-            }
-          } else if (feature_length != 0) {
-            return errors::InvalidArgument("Error in sequence feature ",
-                                           c.first, " in example ",
-                                           example_name);
+          num_elements += delta;
+          stream.PopLimit(limit);
+        } else if (feature_bytes == 2) {
+          if (!SkipEmptyFeature(&stream, feature.dtype)) {
+            return errors::InvalidArgument(
+                "Name: ", ExampleName(example_names, d),
+                ", Feature list: ", c.first, ", Index: ", num_rows,
+                ".  Data types don't match. Expected type: ",
+                DataTypeString(feature.dtype));
           }
+        } else if (feature_bytes != 0) {
+          return errors::InvalidArgument("Error in sequence feature ", c.first,
+                                         " in example ",
+                                         ExampleName(example_names, d));
         }
+        ++num_rows;
       }
-      if (sequence_is_sparse[c.first]) {
-        sequence_feature_type_and_lengths[c.first].second += num_elements;
-      } else {
-        size_t current_max = sequence_feature_type_and_lengths[c.first].second;
-        sequence_feature_type_and_lengths[c.first].second =
-            std::max(current_max, num_elements);
+      switch (feature.type) {
+        case Type::Sparse:
+          feature.length += num_elements;
+          break;
+        case Type::Ragged:
+          feature.length += num_elements;
+          feature.num_rows += num_rows;
+          break;
+        case Type::Dense:
+          feature.length = std::max(feature.length, num_elements);
+          break;
       }
     }
   }
+  return Status::OK();
+}
 
-  // Allocate memory.
-  context_result->sparse_values.resize(context_config.sparse.size());
-  context_result->sparse_indices.resize(context_config.sparse.size());
-  context_result->sparse_shapes.resize(context_config.sparse.size());
-  context_result->dense_values.resize(context_config.dense.size());
-  feature_list_result->sparse_values.resize(feature_list_config.sparse.size());
-  feature_list_result->sparse_indices.resize(feature_list_config.sparse.size());
-  feature_list_result->sparse_shapes.resize(feature_list_config.sparse.size());
-  feature_list_result->dense_values.resize(feature_list_config.dense.size());
-  dense_feature_lengths->resize(feature_list_config.dense.size());
-
-  // NOTE(mrry): Cache the CPU allocator here and use it in Tensor construction,
-  // to avoid lock contention in `tensorflow::cpu_allocator()`.
-  Allocator* allocator = tensorflow::cpu_allocator();
+// Copies src into dst[dst_offset:dst_offset+src.size], and then increments
+// dst_offset by src.size.
+void CopyTensorIntoTensor(DataType dtype, const Tensor& src, Tensor* dst,
+                          size_t* dst_offset) {
+  size_t src_size = src.NumElements();
+  switch (dtype) {
+    case DT_INT64: {
+      auto src_t = src.flat<int64>().data();
+      std::copy(src_t, src_t + src_size,
+                dst->flat<int64>().data() + *dst_offset);
+      break;
+    }
+    case DT_FLOAT: {
+      auto src_t = src.flat<float>().data();
+      std::copy(src_t, src_t + src_size,
+                dst->flat<float>().data() + *dst_offset);
+      break;
+    }
+    case DT_STRING: {
+      auto src_t = src.flat<tstring>().data();
+      std::copy(src_t, src_t + src_size,
+                dst->flat<tstring>().data() + *dst_offset);
+      break;
+    }
+    default:
+      ReportUnexpectedDataType(dtype);
+  }
+  *dst_offset += src_size;
+}
 
-  int t = 0;
-  for (const auto& c : context_config.dense) {
+// Parses dense features in `context_features`, and writes their parsed
+// values to `context_results`.
+Status ParseContextDenseFeatures(const FeatureProtosMap& context_features,
+                                 const FastParseExampleConfig& context_config,
+                                 gtl::ArraySlice<tstring> example_names,
+                                 bool is_batch, int num_examples,
+                                 Allocator* allocator, Result* context_result) {
+  for (int t = 0; t < context_config.dense.size(); ++t) {
+    const auto& c = context_config.dense[t];
+    const FeatureProtos& feature =
+        context_features.find(c.feature_name)->second;
     TensorShape dense_shape, example_shape;
     DataType dtype = c.dtype;
-    const size_t expected_max_elements =
-        context_feature_type_and_lengths[c.feature_name].second;
+    const size_t expected_max_elements = feature.length;
     if (!c.shape.AsTensorShape(&example_shape) ||
         expected_max_elements != example_shape.num_elements()) {
       return errors::InvalidArgument(
           "Inconsistent number of elements for feature ", c.feature_name, ": ",
           expected_max_elements, " vs ", dense_shape.num_elements());
     }
-    dense_shape.AddDim(num_examples);
+    if (is_batch) {
+      dense_shape.AddDim(num_examples);
+    }
     for (const int dim : c.shape.dim_sizes()) {
       dense_shape.AddDim(dim);
     }
     context_result->dense_values[t] = Tensor(allocator, dtype, dense_shape);
 
-    // TODO(sundberg): Refactor to reduce code duplication, and add bounds
-    // checking for the outputs.
-    tstring* out_bytes = nullptr;
-    float* out_float = nullptr;
-    int64* out_int64 = nullptr;
-    switch (dtype) {
-      case DT_STRING:
-        out_bytes = context_result->dense_values[t].flat<tstring>().data();
-        break;
-      case DT_FLOAT:
-        out_float = context_result->dense_values[t].flat<float>().data();
-        break;
-      case DT_INT64:
-        out_int64 = context_result->dense_values[t].flat<int64>().data();
-        break;
-      default:
-        return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                       " in feature ", c.feature_name);
-    }
-    t++;
+    Tensor& out = context_result->dense_values[t];
+    size_t out_offset = 0;
 
     // Fill in the values.
     for (int e = 0; e < num_examples; e++) {
       size_t num_elements = 0;
-      const auto feature_iter = all_context_features[e].find(c.feature_name);
-      const tstring& example_name =
-          example_names.empty() ? kUnknown : example_names[e];
-      if (feature_iter == all_context_features[e].end()) {
+      const auto& feature_proto = feature.protos[e];
+      if (!feature.protos_present[e]) {
         // Copy the default value, if present. If not, return an error.
         if (c.default_value.NumElements() == 0) {
           return errors::InvalidArgument(
@@ -2256,171 +2281,204 @@ Status FastParseSequenceExample(
               " (data type: ", DataTypeString(c.dtype), ")",
               " is required but could not be found.");
         }
-        const tstring* in_bytes = nullptr;
-        const float* in_float = nullptr;
-        const int64* in_int64 = nullptr;
-        size_t num = 0;
-        switch (dtype) {
-          case DT_STRING:
-            in_bytes = c.default_value.flat<tstring>().data();
-            num = c.default_value.NumElements();
-            for (int p = 0; p < num; p++) {
-              *out_bytes++ = *in_bytes++;
-            }
-            break;
-          case DT_FLOAT:
-            in_float = c.default_value.flat<float>().data();
-            num = c.default_value.NumElements();
-            for (int p = 0; p < num; p++) {
-              *out_float++ = *in_float++;
-            }
-            break;
-          case DT_INT64:
-            in_int64 = c.default_value.flat<int64>().data();
-            num = c.default_value.NumElements();
-            for (int p = 0; p < num; p++) {
-              *out_int64++ = *in_int64++;
-            }
-            break;
-          default:
-            return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                           " in example ", example_name);
-        }
-        num_elements += num;
-      } else if (!feature_iter->second.empty()) {
-        const auto& feature = feature_iter->second;
+        CopyTensorIntoTensor(dtype, c.default_value, &out, &out_offset);
+        num_elements += c.default_value.NumElements();
+      } else if (!feature_proto.empty()) {
         protobuf::io::CodedInputStream stream(
-            reinterpret_cast<const uint8*>(feature.data()), feature.size());
+            reinterpret_cast<const uint8*>(feature_proto.data()),
+            feature_proto.size());
         EnableAliasing(&stream);
-        size_t num_added;
-        switch (dtype) {
-          case DT_STRING:
-            num_added = ParseBytesFeature(&stream, out_bytes);
-            out_bytes += num_added;
-            break;
-          case DT_FLOAT:
-            num_added = ParseFloatFeature(&stream, out_float);
-            out_float += num_added;
-            break;
-          case DT_INT64:
-            num_added = ParseInt64Feature(&stream, out_int64);
-            out_int64 += num_added;
-            break;
-          default:
-            return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                           " in example ", example_name);
-        }
-        num_elements += num_added;
+        num_elements += ParseFeature(dtype, &stream, &out, &out_offset);
       }
       if (num_elements != expected_max_elements) {
         return errors::InvalidArgument(
-            "Unexpected number of elements in example ", example_name);
+            "Unexpected number of elements in example ",
+            ExampleName(example_names, e));
       }
     }
   }
-  t = 0;
-  for (const auto& c : context_config.sparse) {
+  return Status::OK();
+}
+
+// Parses sparse features in `context_features`, and writes their parsed
+// values to `context_results`.
+Status ParseContextSparseFeatures(const FeatureProtosMap& context_features,
+                                  const FastParseExampleConfig& context_config,
+                                  gtl::ArraySlice<tstring> example_names,
+                                  bool is_batch, int num_examples,
+                                  Allocator* allocator,
+                                  Result* context_result) {
+  for (int t = 0; t < context_config.sparse.size(); ++t) {
+    const auto& c = context_config.sparse[t];
+    const FeatureProtos& feature =
+        context_features.find(c.feature_name)->second;
     TensorShape indices_shape, values_shape;
     DataType dtype = c.dtype;
-    size_t expected_num_elements =
-        context_feature_type_and_lengths[c.feature_name].second;
+    size_t expected_num_elements = feature.length;
     indices_shape.AddDim(expected_num_elements);
-    indices_shape.AddDim(2);
+    indices_shape.AddDim(is_batch ? 2 : 1);
     values_shape.AddDim(expected_num_elements);
     context_result->sparse_indices[t] =
         Tensor(allocator, DT_INT64, indices_shape);
     context_result->sparse_values[t] = Tensor(allocator, dtype, values_shape);
     context_result->sparse_shapes[t] =
-        Tensor(allocator, DT_INT64, TensorShape({2}));
-    // TODO(sundberg): Refactor to reduce code duplication, and add bounds
-    // checking for the outputs.
-    tstring* out_bytes = nullptr;
-    float* out_float = nullptr;
-    int64* out_int64 = nullptr;
-    switch (dtype) {
-      case DT_STRING:
-        out_bytes = context_result->sparse_values[t].flat<tstring>().data();
-        break;
-      case DT_FLOAT:
-        out_float = context_result->sparse_values[t].flat<float>().data();
-        break;
-      case DT_INT64:
-        out_int64 = context_result->sparse_values[t].flat<int64>().data();
-        break;
-      default:
-        return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                       " in feature ", c.feature_name);
-    }
+        Tensor(allocator, DT_INT64, TensorShape({is_batch ? 2 : 1}));
+    Tensor& out_values = context_result->sparse_values[t];
+    size_t out_values_offset = 0;
     int64* out_indices = context_result->sparse_indices[t].flat<int64>().data();
     auto out_shape = context_result->sparse_shapes[t].vec<int64>();
-    t++;
 
     // Fill in the values.
     size_t num_elements = 0;
     size_t max_num_cols = 0;
     for (int e = 0; e < num_examples; e++) {
-      const auto& feature = all_context_features[e][c.feature_name];
-      const tstring& example_name =
-          example_names.empty() ? kUnknown : example_names[e];
-      if (!feature.empty()) {
+      const auto& feature_proto = feature.protos[e];
+      if (feature_proto.empty()) continue;
+      protobuf::io::CodedInputStream stream(
+          reinterpret_cast<const uint8*>(feature_proto.data()),
+          feature_proto.size());
+      EnableAliasing(&stream);
+      size_t num_added =
+          ParseFeature(dtype, &stream, &out_values, &out_values_offset);
+      num_elements += num_added;
+      max_num_cols = std::max(max_num_cols, num_added);
+      for (int i = 0; i < num_added; i++) {
+        if (is_batch) *out_indices++ = e;
+        *out_indices++ = i;
+      }
+    }
+    if (num_elements != expected_num_elements) {
+      return errors::InvalidArgument(
+          "Unexpected total number of elements in feature ", c.feature_name);
+    }
+    if (is_batch) {
+      out_shape(0) = num_examples;
+      out_shape(1) = max_num_cols;
+    } else {
+      out_shape(0) = max_num_cols;
+    }
+  }
+  return Status::OK();
+}
+
+// Parses ragged features in `context_features`, and writes their parsed
+// values to `context_results`.
+Status ParseContextRaggedFeatures(const FeatureProtosMap& context_features,
+                                  const FastParseExampleConfig& context_config,
+                                  gtl::ArraySlice<tstring> example_names,
+                                  bool is_batch, int num_examples,
+                                  Allocator* allocator,
+                                  Result* context_result) {
+  for (int t = 0; t < context_config.ragged.size(); ++t) {
+    const auto& c = context_config.ragged[t];
+    const FeatureProtos& feature =
+        context_features.find(c.feature_name)->second;
+    TensorShape values_shape, splits_shape;
+    DataType dtype = c.dtype;
+    DataType splits_dtype = c.splits_dtype;
+    size_t expected_num_elements = feature.length;
+    values_shape.AddDim(expected_num_elements);
+    if (is_batch) {
+      splits_shape.AddDim(num_examples + 1);
+    }
+    context_result->ragged_values[t] = Tensor(allocator, dtype, values_shape);
+    context_result->ragged_splits[t] =
+        Tensor(allocator, splits_dtype, splits_shape);
+    Tensor& out_values = context_result->ragged_values[t];
+    size_t out_values_offset = 0;
+    int32* int32_splits =
+        is_batch && splits_dtype == DT_INT32
+            ? context_result->ragged_splits[t].vec<int32>().data()
+            : nullptr;
+    int64* int64_splits =
+        is_batch && splits_dtype == DT_INT64
+            ? context_result->ragged_splits[t].vec<int64>().data()
+            : nullptr;
+    if (int32_splits) {
+      *int32_splits++ = 0;
+    } else if (int64_splits) {
+      *int64_splits++ = 0;
+    }
+
+    // Fill in the values.
+    size_t split = 0;  // = total number of elements we've seen so far
+    for (int e = 0; e < num_examples; e++) {
+      const auto& feature_proto = feature.protos[e];
+      if (!feature_proto.empty()) {
         protobuf::io::CodedInputStream stream(
-            reinterpret_cast<const uint8*>(feature.data()), feature.size());
+            reinterpret_cast<const uint8*>(feature_proto.data()),
+            feature_proto.size());
         EnableAliasing(&stream);
-        size_t num_added;
-        switch (dtype) {
-          case DT_STRING:
-            num_added = ParseBytesFeature(&stream, out_bytes);
-            out_bytes += num_added;
-            break;
-          case DT_FLOAT:
-            num_added = ParseFloatFeature(&stream, out_float);
-            out_float += num_added;
-            break;
-          case DT_INT64:
-            num_added = ParseInt64Feature(&stream, out_int64);
-            out_int64 += num_added;
-            break;
-          default:
-            return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                           " in example ", example_name);
-        }
-        num_elements += num_added;
-        max_num_cols = std::max(max_num_cols, num_added);
-        for (int i = 0; i < num_added; i++) {
-          *out_indices++ = e;
-          *out_indices++ = i;
-        }
+        size_t num_added =
+            ParseFeature(dtype, &stream, &out_values, &out_values_offset);
+        split += num_added;
+      }
+      if (int32_splits) {
+        *int32_splits++ = split;
+      } else if (int64_splits) {
+        *int64_splits++ = split;
       }
     }
-    if (num_elements != expected_num_elements) {
+    if (split != expected_num_elements) {
       return errors::InvalidArgument(
           "Unexpected total number of elements in feature ", c.feature_name);
     }
-    out_shape(0) = num_examples;
-    out_shape(1) = max_num_cols;
+    if (int32_splits || int64_splits) {
+      int actual_splits =
+          int32_splits
+              ? int32_splits -
+                    context_result->ragged_splits[t].vec<int32>().data()
+              : int64_splits -
+                    context_result->ragged_splits[t].vec<int64>().data();
+      if (actual_splits != num_examples + 1) {
+        return errors::InvalidArgument(
+            "Unexpected number of examples for feature ", c.feature_name);
+      }
+    }
   }
-  t = 0;
-  TensorShape dense_length_shape({num_examples});
-  for (const auto& c : feature_list_config.dense) {
+  return Status::OK();
+}
+
+// Parses dense features in `sequence_features`, and writes their parsed
+// values to `sequence_result`.
+Status ParseSequenceDenseFeatures(const FeatureProtosMap& sequence_features,
+                                  const FastParseExampleConfig& sequence_config,
+                                  gtl::ArraySlice<tstring> example_names,
+                                  bool is_batch, int num_examples,
+                                  Allocator* allocator, Result* sequence_result,
+                                  std::vector<Tensor>* dense_feature_lengths) {
+  TensorShape dense_length_shape;
+  if (is_batch) {
+    dense_length_shape.AddDim(num_examples);
+  }
+  for (int t = 0; t < sequence_config.dense.size(); ++t) {
+    const auto& c = sequence_config.dense[t];
+    const FeatureProtos& feature =
+        sequence_features.find(c.feature_name)->second;
     TensorShape dense_shape, row_shape;
     DataType dtype = c.dtype;
-    const size_t expected_max_elements =
-        sequence_feature_type_and_lengths[c.feature_name].second;
+    const size_t expected_max_elements = feature.length;
     if (!c.shape.AsTensorShape(&row_shape) ||
         expected_max_elements !=
             (expected_max_elements / row_shape.num_elements()) *
                 row_shape.num_elements()) {
-      return errors::InvalidArgument("Unexpected shape error in feature ",
-                                     c.feature_name);
+      PartialTensorShape total_shape = row_shape;
+      total_shape.InsertDim(0, -1);
+      return errors::InvalidArgument(
+          "Feature list '", c.feature_name,
+          "' has an unexpected number of values.  Total values size: ",
+          expected_max_elements,
+          " is not consistent with output shape: ", total_shape.DebugString());
     }
     int64 expected_max_rows = expected_max_elements / row_shape.num_elements();
-    dense_shape.AddDim(num_examples);
+    if (is_batch) {
+      dense_shape.AddDim(num_examples);
+    }
     dense_shape.AddDim(expected_max_rows);
-    for (const int dim : feature_list_config.dense[t].shape.dim_sizes()) {
+    for (const int dim : sequence_config.dense[t].shape.dim_sizes()) {
       dense_shape.AddDim(dim);
     }
-    feature_list_result->dense_values[t] =
-        Tensor(allocator, dtype, dense_shape);
+    sequence_result->dense_values[t] = Tensor(allocator, dtype, dense_shape);
     (*dense_feature_lengths)[t] =
         Tensor(allocator, DT_INT64, dense_length_shape);
     int64* out_lengths = (*dense_feature_lengths)[t].flat<int64>().data();
@@ -2430,37 +2488,38 @@ Status FastParseSequenceExample(
     int64* out_int64 = nullptr;
     switch (dtype) {
       case DT_STRING:
-        out_bytes = feature_list_result->dense_values[t].flat<tstring>().data();
+        out_bytes = sequence_result->dense_values[t].flat<tstring>().data();
         break;
       case DT_FLOAT:
-        out_float = feature_list_result->dense_values[t].flat<float>().data();
+        out_float = sequence_result->dense_values[t].flat<float>().data();
         break;
       case DT_INT64:
-        out_int64 = feature_list_result->dense_values[t].flat<int64>().data();
+        out_int64 = sequence_result->dense_values[t].flat<int64>().data();
         break;
       default:
-        return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                       " in feature ", c.feature_name);
+        ReportUnexpectedDataType(dtype);
     }
-    t++;
 
     // Fill in the values.
     for (int e = 0; e < num_examples; e++) {
       size_t num_elements = 0, num_rows = 0;
-      const auto feature_iter = all_sequence_features[e].find(c.feature_name);
-      const tstring& example_name =
-          example_names.empty() ? kUnknown : example_names[e];
-      if (feature_iter == all_sequence_features[e].end()) {
+      const auto& feature_proto = feature.protos[e];
+      if (!feature.protos_present[e]) {
         // Return an error if this feature was not allowed to be missing.
         // Otherwise, we'll pad as needed below.
         if (!c.variable_length) {
-          return errors::InvalidArgument("Missing feature ", c.feature_name,
-                                         " in example ", example_name);
+          return errors::InvalidArgument(
+              "Name: ", ExampleName(example_names, e), ", Feature list '",
+              c.feature_name,
+              "' is required but could not be found.  "
+              "Did you mean to include it in "
+              "feature_list_dense_missing_assumed_empty or "
+              "feature_list_dense_defaults?");
         }
-      } else if (!feature_iter->second.empty()) {
-        const auto& feature = feature_iter->second;
+      } else if (!feature_proto.empty()) {
         protobuf::io::CodedInputStream stream(
-            reinterpret_cast<const uint8*>(feature.data()), feature.size());
+            reinterpret_cast<const uint8*>(feature_proto.data()),
+            feature_proto.size());
         EnableAliasing(&stream);
         while (!stream.ExpectAtEnd()) {
           uint32 feature_length;
@@ -2468,34 +2527,45 @@ Status FastParseSequenceExample(
               !stream.ReadVarint32(&feature_length)) {
             return errors::InvalidArgument("Error in sequence feature ",
                                            c.feature_name, " in example ",
-                                           example_name);
+                                           ExampleName(example_names, e));
           }
           auto limit = stream.PushLimit(feature_length);
-          size_t num_added;
-          switch (dtype) {
-            case DT_STRING:
-              num_added = ParseBytesFeature(&stream, out_bytes);
-              out_bytes += num_added;
-              break;
-            case DT_FLOAT:
-              num_added = ParseFloatFeature(&stream, out_float);
-              out_float += num_added;
-              break;
-            case DT_INT64:
-              num_added = ParseInt64Feature(&stream, out_int64);
-              out_int64 += num_added;
-              break;
-            default:
-              return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                             " in example ", example_name);
+          int num_added = 0;
+          if (feature_length > 2) {
+            switch (dtype) {
+              case DT_STRING:
+                num_added = ParseBytesFeature(&stream, out_bytes);
+                out_bytes += num_added;
+                break;
+              case DT_FLOAT:
+                num_added = ParseFloatFeature(&stream, out_float);
+                out_float += num_added;
+                break;
+              case DT_INT64:
+                num_added = ParseInt64Feature(&stream, out_int64);
+                out_int64 += num_added;
+                break;
+              default:
+                ReportUnexpectedDataType(dtype);
+                num_added = 0;
+            }
+            if (num_added < 0) {
+              // This should be unreachable -- we already scanned the feature in
+              // GetSequenceFeatureLengths, and it hasn't changed since then.
+              return errors::InvalidArgument("Error in sequence feature ",
+                                             c.feature_name, " in example ",
+                                             ExampleName(example_names, e));
+            }
           }
-          num_elements += num_added;
-          num_rows++;
           if (num_added != row_shape.num_elements()) {
             return errors::InvalidArgument(
-                "Unexpected number of elements in feature ", c.feature_name,
-                ", example ", example_name);
+                "Name: ", ExampleName(example_names, e),
+                ", Key: ", c.feature_name, ", Index: ", num_rows,
+                ".  Number of values != expected.  values size: ", num_added,
+                " but output shape: ", row_shape.DebugString());
           }
+          num_elements += num_added;
+          num_rows++;
           stream.PopLimit(limit);
         }
       }
@@ -2515,123 +2585,444 @@ Status FastParseSequenceExample(
           out_int64 += num_to_pad;
           break;
         default:
-          return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                         " in example ", example_name);
+          ReportUnexpectedDataType(dtype);
       }
     }
   }
-  t = 0;
-  for (const auto& c : feature_list_config.sparse) {
+  return Status::OK();
+}
+
+// Parses sparse features in `sequence_features`, and writes their parsed
+// values to `sequence_result`.
+Status ParseSequenceSparseFeatures(
+    const FeatureProtosMap& sequence_features,
+    const FastParseExampleConfig& sequence_config,
+    gtl::ArraySlice<tstring> example_names, bool is_batch, int num_examples,
+    Allocator* allocator, Result* sequence_result) {
+  for (int t = 0; t < sequence_config.sparse.size(); ++t) {
+    const auto& c = sequence_config.sparse[t];
+    const FeatureProtos& feature =
+        sequence_features.find(c.feature_name)->second;
     TensorShape indices_shape, values_shape;
     DataType dtype = c.dtype;
-    size_t expected_num_elements =
-        sequence_feature_type_and_lengths[c.feature_name].second;
+    size_t expected_num_elements = feature.length;
     indices_shape.AddDim(expected_num_elements);
-    indices_shape.AddDim(3);
+    indices_shape.AddDim(is_batch ? 3 : 2);
     values_shape.AddDim(expected_num_elements);
-    feature_list_result->sparse_indices[t] =
+    sequence_result->sparse_indices[t] =
         Tensor(allocator, DT_INT64, indices_shape);
-    feature_list_result->sparse_values[t] =
-        Tensor(allocator, dtype, values_shape);
-    feature_list_result->sparse_shapes[t] =
-        Tensor(allocator, DT_INT64, TensorShape({3}));
+    sequence_result->sparse_values[t] = Tensor(allocator, dtype, values_shape);
+    sequence_result->sparse_shapes[t] =
+        Tensor(allocator, DT_INT64, TensorShape({is_batch ? 3 : 2}));
 
     tstring* out_bytes = nullptr;
     float* out_float = nullptr;
     int64* out_int64 = nullptr;
     switch (dtype) {
       case DT_STRING:
-        out_bytes =
-            feature_list_result->sparse_values[t].flat<tstring>().data();
+        out_bytes = sequence_result->sparse_values[t].flat<tstring>().data();
         break;
       case DT_FLOAT:
-        out_float = feature_list_result->sparse_values[t].flat<float>().data();
+        out_float = sequence_result->sparse_values[t].flat<float>().data();
         break;
       case DT_INT64:
-        out_int64 = feature_list_result->sparse_values[t].flat<int64>().data();
+        out_int64 = sequence_result->sparse_values[t].flat<int64>().data();
         break;
       default:
-        return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                       " in feature ", c.feature_name);
+        ReportUnexpectedDataType(dtype);
     }
     int64* out_indices =
-        feature_list_result->sparse_indices[t].flat<int64>().data();
-    auto out_shape = feature_list_result->sparse_shapes[t].vec<int64>();
-    t++;
+        sequence_result->sparse_indices[t].flat<int64>().data();
+    auto out_shape = sequence_result->sparse_shapes[t].vec<int64>();
 
     // Fill in the values.
     size_t num_elements = 0;
     size_t max_num_rows = 0;
     size_t max_num_cols = 0;
     for (int e = 0; e < num_examples; e++) {
-      const auto& feature = all_sequence_features[e][c.feature_name];
-      const tstring& example_name =
-          example_names.empty() ? kUnknown : example_names[e];
-      if (!feature.empty()) {
+      const auto& feature_proto = feature.protos[e];
+      if (feature_proto.empty()) continue;
+      protobuf::io::CodedInputStream stream(
+          reinterpret_cast<const uint8*>(feature_proto.data()),
+          feature_proto.size());
+      EnableAliasing(&stream);
+      size_t num_rows = 0;
+      while (!stream.ExpectAtEnd()) {
+        uint32 feature_length;
+        if (!stream.ExpectTag(kDelimitedTag(1)) ||
+            !stream.ReadVarint32(&feature_length)) {
+          // This should be unreachable -- we already scanned the feature in
+          // GetSequenceFeatureLengths, and it hasn't changed since then.
+          return errors::InvalidArgument("Error in sequence feature ",
+                                         c.feature_name, " in example ",
+                                         ExampleName(example_names, e));
+        }
+        if (feature_length > 2) {
+          auto limit = stream.PushLimit(feature_length);
+          size_t num_added;
+          switch (dtype) {
+            case DT_STRING:
+              num_added = ParseBytesFeature(&stream, out_bytes);
+              out_bytes += num_added;
+              break;
+            case DT_FLOAT:
+              num_added = ParseFloatFeature(&stream, out_float);
+              out_float += num_added;
+              break;
+            case DT_INT64:
+              num_added = ParseInt64Feature(&stream, out_int64);
+              out_int64 += num_added;
+              break;
+            default:
+              ReportUnexpectedDataType(dtype);
+              num_added = 0;
+          }
+          num_elements += num_added;
+          max_num_cols = std::max(max_num_cols, num_added);
+          for (int i = 0; i < num_added; i++) {
+            if (is_batch) *out_indices++ = e;
+            *out_indices++ = num_rows;
+            *out_indices++ = i;
+          }
+          stream.PopLimit(limit);
+        } else if (feature_length == 2) {
+          if (!SkipEmptyFeature(&stream, dtype)) {
+            // This should be unreachable -- we already scanned the feature in
+            // GetSequenceFeatureLengths, and it hasn't changed since then.
+            return errors::InvalidArgument("Error in sequence feature ",
+                                           c.feature_name, " in example ",
+                                           ExampleName(example_names, e));
+          }
+        } else if (feature_length != 0) {
+          // This should be unreachable -- we already scanned the feature in
+          // GetSequenceFeatureLengths, and it hasn't changed since then.
+          return errors::InvalidArgument("Error in sequence feature ",
+                                         c.feature_name, " in example ",
+                                         ExampleName(example_names, e));
+        }
+        num_rows++;
+      }
+      max_num_rows = std::max(max_num_rows, num_rows);
+    }
+    if (num_elements != expected_num_elements) {
+      return errors::InvalidArgument(
+          "Unexpected number of elements in feature ", c.feature_name);
+    }
+    if (is_batch) {
+      out_shape(0) = num_examples;
+      out_shape(1) = max_num_rows;
+      out_shape(2) = max_num_cols;
+    } else {
+      out_shape(0) = max_num_rows;
+      out_shape(1) = max_num_cols;
+    }
+  }
+  return Status::OK();
+}
+
+// Parses ragged features in `sequence_features`, and writes their parsed
+// values to `sequence_result`.
+Status ParseSequenceRaggedFeatures(
+    const FeatureProtosMap& sequence_features,
+    const FastParseExampleConfig& sequence_config,
+    gtl::ArraySlice<tstring> example_names, bool is_batch, int num_examples,
+    Allocator* allocator, Result* sequence_result) {
+  for (int t = 0; t < sequence_config.ragged.size(); ++t) {
+    const auto& c = sequence_config.ragged[t];
+    const FeatureProtos& feature =
+        sequence_features.find(c.feature_name)->second;
+    TensorShape values_shape, inner_splits_shape, outer_splits_shape;
+    DataType dtype = c.dtype;
+    DataType splits_dtype = c.splits_dtype;
+    size_t expected_num_elements = feature.length;
+    size_t expected_num_rows = feature.num_rows;
+    values_shape.AddDim(expected_num_elements);
+    inner_splits_shape.AddDim(expected_num_rows + 1);
+    if (is_batch) {
+      outer_splits_shape.AddDim(num_examples + 1);
+    }
+    sequence_result->ragged_values[t] = Tensor(allocator, dtype, values_shape);
+    sequence_result->ragged_splits[t] =
+        Tensor(allocator, splits_dtype, inner_splits_shape);
+    sequence_result->ragged_outer_splits[t] =
+        Tensor(allocator, splits_dtype, outer_splits_shape);
+    Tensor& out_values = sequence_result->ragged_values[t];
+    size_t out_values_offset = 0;
+    int32* int32_inner_splits =
+        splits_dtype == DT_INT32
+            ? sequence_result->ragged_splits[t].vec<int32>().data()
+            : nullptr;
+    int64* int64_inner_splits =
+        splits_dtype == DT_INT64
+            ? sequence_result->ragged_splits[t].vec<int64>().data()
+            : nullptr;
+    int32* int32_outer_splits =
+        is_batch && splits_dtype == DT_INT32
+            ? sequence_result->ragged_outer_splits[t].vec<int32>().data()
+            : nullptr;
+    int64* int64_outer_splits =
+        is_batch && splits_dtype == DT_INT64
+            ? sequence_result->ragged_outer_splits[t].vec<int64>().data()
+            : nullptr;
+    if (int32_inner_splits) {
+      *int32_inner_splits++ = 0;
+    } else if (int64_inner_splits) {
+      *int64_inner_splits++ = 0;
+    }
+    if (int32_outer_splits) {
+      *int32_outer_splits++ = 0;
+    } else if (int64_outer_splits) {
+      *int64_outer_splits++ = 0;
+    }
+
+    // Fill in the values.
+    size_t inner_split = 0;  // total number of elements we've seen so far
+    size_t outer_split = 0;  // total number of rows we've seen so far
+    for (int e = 0; e < num_examples; e++) {
+      const auto& feature_proto = feature.protos[e];
+      if (!feature_proto.empty()) {
         protobuf::io::CodedInputStream stream(
-            reinterpret_cast<const uint8*>(feature.data()), feature.size());
+            reinterpret_cast<const uint8*>(feature_proto.data()),
+            feature_proto.size());
         EnableAliasing(&stream);
-        size_t num_rows = 0;
         while (!stream.ExpectAtEnd()) {
           uint32 feature_length;
           if (!stream.ExpectTag(kDelimitedTag(1)) ||
               !stream.ReadVarint32(&feature_length)) {
+            // This should be unreachable -- we already scanned the feature in
+            // GetSequenceFeatureLengths, and it hasn't changed since then.
             return errors::InvalidArgument("Error in sequence feature ",
                                            c.feature_name, " in example ",
-                                           example_name);
+                                           ExampleName(example_names, e));
           }
           if (feature_length > 2) {
             auto limit = stream.PushLimit(feature_length);
-            size_t num_added;
-            switch (dtype) {
-              case DT_STRING:
-                num_added = ParseBytesFeature(&stream, out_bytes);
-                out_bytes += num_added;
-                break;
-              case DT_FLOAT:
-                num_added = ParseFloatFeature(&stream, out_float);
-                out_float += num_added;
-                break;
-              case DT_INT64:
-                num_added = ParseInt64Feature(&stream, out_int64);
-                out_int64 += num_added;
-                break;
-              default:
-                return errors::InvalidArgument("Unexpected dtype ", dtype,
-                                               " in example ", example_name);
-            }
-            num_elements += num_added;
-            max_num_cols = std::max(max_num_cols, num_added);
-            for (int i = 0; i < num_added; i++) {
-              *out_indices++ = e;
-              *out_indices++ = num_rows;
-              *out_indices++ = i;
-            }
+            size_t num_added =
+                ParseFeature(dtype, &stream, &out_values, &out_values_offset);
+            inner_split += num_added;
             stream.PopLimit(limit);
           } else if (feature_length == 2) {
             if (!SkipEmptyFeature(&stream, dtype)) {
+              // This should be unreachable -- we already scanned the feature in
+              // GetSequenceFeatureLengths, and it hasn't changed since then.
               return errors::InvalidArgument("Error in sequence feature ",
                                              c.feature_name, " in example ",
-                                             example_name);
+                                             ExampleName(example_names, e));
             }
           } else if (feature_length != 0) {
+            // This should be unreachable -- we already scanned the feature in
+            // GetSequenceFeatureLengths, and it hasn't changed since then.
             return errors::InvalidArgument("Error in sequence feature ",
                                            c.feature_name, " in example ",
-                                           example_name);
+                                           ExampleName(example_names, e));
           }
-          num_rows++;
+          if (int32_inner_splits) {
+            *int32_inner_splits++ = inner_split;
+          } else if (int64_inner_splits) {
+            *int64_inner_splits++ = inner_split;
+          }
+          outer_split++;
         }
-        max_num_rows = std::max(max_num_rows, num_rows);
+      }
+      if (int32_outer_splits) {
+        *int32_outer_splits++ = outer_split;
+      } else if (int64_outer_splits) {
+        *int64_outer_splits++ = outer_split;
       }
     }
-    if (num_elements != expected_num_elements) {
+    if (outer_split != expected_num_rows) {
+      return errors::InvalidArgument("Unexpected number of rows for feature ",
+                                     c.feature_name);
+    }
+    if (inner_split != expected_num_elements) {
       return errors::InvalidArgument(
-          "Unexpected number of elements in feature ", c.feature_name);
+          "Unexpected number of elements for feature ", c.feature_name);
+    }
+
+    if (int32_inner_splits || int64_inner_splits) {
+      const auto& inner_splits = sequence_result->ragged_splits[t];
+      int num_inner_splits =
+          int32_inner_splits
+              ? int32_inner_splits - inner_splits.vec<int32>().data()
+              : int64_inner_splits - inner_splits.vec<int64>().data();
+      if (num_inner_splits != expected_num_rows + 1) {
+        return errors::InvalidArgument("Unexpected number of rows for feature ",
+                                       c.feature_name);
+      }
+    }
+    if (int32_outer_splits || int64_outer_splits) {
+      const auto& outer_splits = sequence_result->ragged_outer_splits[t];
+      int num_outer_splits =
+          int32_outer_splits
+              ? int32_outer_splits - outer_splits.vec<int32>().data()
+              : int64_outer_splits - outer_splits.vec<int64>().data();
+      if (num_outer_splits != num_examples + 1) {
+        return errors::InvalidArgument(
+            "Unexpected number of examples for feature ", c.feature_name);
+      }
+    }
+  }
+  return Status::OK();
+}
+
+}  // namespace
+
+// TODO(sundberg): Use the threadpool to parallelize example parsing.
+// TODO(b/111553342): Support extracting feature statistics from the examples.
+Status FastParseSequenceExample(const FastParseExampleConfig& context_config,
+                                const FastParseExampleConfig& sequence_config,
+                                gtl::ArraySlice<tstring> serialized,
+                                gtl::ArraySlice<tstring> example_names,
+                                thread::ThreadPool* thread_pool,
+                                Result* context_result, Result* sequence_result,
+                                std::vector<Tensor>* dense_feature_lengths,
+                                bool is_batch) {
+  int num_examples = serialized.size();
+  DCHECK(context_result != nullptr);
+  DCHECK(sequence_result != nullptr);
+  DCHECK(dense_feature_lengths != nullptr);
+  size_t num_context_features = context_config.sparse.size() +
+                                context_config.dense.size() +
+                                context_config.ragged.size();
+  FeatureProtosMap context_features;
+  context_features.reserve(num_context_features);
+
+  if (!example_names.empty() && example_names.size() != num_examples) {
+    return errors::InvalidArgument(
+        "example_names must be empty or have the correct number of elements");
+  }
+  for (auto& c : context_config.sparse) {
+    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
+    FeatureProtos& feature = context_features[c.feature_name];
+    feature.dtype = c.dtype;
+    feature.length = 0;
+    feature.type = Type::Sparse;
+    feature.protos.resize(num_examples);
+    feature.protos_present.resize(num_examples);
+  }
+  for (auto& c : context_config.ragged) {
+    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
+    FeatureProtos& feature = context_features[c.feature_name];
+    if (feature.type == Type::Sparse) {
+      return errors::InvalidArgument("Context feature " + c.feature_name +
+                                     " cannot be both ragged and sparse");
     }
-    out_shape(0) = num_examples;
-    out_shape(1) = max_num_rows;
-    out_shape(2) = max_num_cols;
+    feature.dtype = c.dtype;
+    feature.length = 0;
+    feature.type = Type::Ragged;
+    feature.protos.resize(num_examples);
+    feature.protos_present.resize(num_examples);
   }
+  for (auto& c : context_config.dense) {
+    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
+    FeatureProtos& feature = context_features[c.feature_name];
+    if (feature.type != Type::Dense) {
+      return errors::InvalidArgument("Context feature " + c.feature_name +
+                                     " cannot be both dense and sparse");
+    }
+    if (c.default_value.NumElements() > 0) {
+      if (!c.shape.IsCompatibleWith(c.default_value.shape())) {
+        return errors::InvalidArgument("Default value for context feature ",
+                                       c.feature_name,
+                                       " has an incorrect shape: saw ",
+                                       c.default_value.shape().DebugString(),
+                                       " but expected ", c.shape.DebugString());
+      }
+    }
+    feature.dtype = c.dtype;
+    feature.length = c.default_value.NumElements();
+    feature.protos.resize(num_examples);
+    feature.protos_present.resize(num_examples);
+  }
+  size_t num_sequence_features = sequence_config.sparse.size() +
+                                 sequence_config.dense.size() +
+                                 sequence_config.ragged.size();
+  FeatureProtosMap sequence_features;
+  sequence_features.reserve(num_sequence_features);
+  for (auto& c : sequence_config.sparse) {
+    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
+    FeatureProtos& feature = sequence_features[c.feature_name];
+    feature.dtype = c.dtype;
+    feature.length = 0;
+    feature.type = Type::Sparse;
+    feature.protos.resize(num_examples);
+    feature.protos_present.resize(num_examples);
+  }
+  for (auto& c : sequence_config.ragged) {
+    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
+    FeatureProtos& feature = sequence_features[c.feature_name];
+    if (feature.type == Type::Sparse) {
+      return errors::InvalidArgument("Sequence feature " + c.feature_name +
+                                     " cannot be both ragged and sparse");
+    }
+    feature.dtype = c.dtype;
+    feature.length = 0;
+    feature.type = Type::Ragged;
+    feature.protos.resize(num_examples);
+    feature.protos_present.resize(num_examples);
+  }
+  for (auto& c : sequence_config.dense) {
+    TF_RETURN_IF_ERROR(CheckConfigDataType(c.dtype));
+    FeatureProtos& feature = sequence_features[c.feature_name];
+    if (feature.type != Type::Dense) {
+      return errors::InvalidArgument("Sequence feature " + c.feature_name +
+                                     " cannot be both dense and sparse");
+    }
+    feature.dtype = c.dtype;
+    feature.length = 0;
+    feature.protos.resize(num_examples);
+    feature.protos_present.resize(num_examples);
+  }
+
+  // Find the serialized proto substrings for each feature.
+  TF_RETURN_IF_ERROR(ExtractFeaturesFromSequenceExamples(
+      serialized, example_names, &context_features, &sequence_features));
+
+  // Scan through the protos to determine how much memory we need to allocate.
+  TF_RETURN_IF_ERROR(
+      GetContextFeatureLengths(example_names, &context_features));
+  TF_RETURN_IF_ERROR(
+      GetSequenceFeatureLengths(example_names, &sequence_features));
+
+  // Allocate memory.
+  context_result->sparse_values.resize(context_config.sparse.size());
+  context_result->sparse_indices.resize(context_config.sparse.size());
+  context_result->sparse_shapes.resize(context_config.sparse.size());
+  context_result->dense_values.resize(context_config.dense.size());
+  context_result->ragged_values.resize(context_config.ragged.size());
+  context_result->ragged_splits.resize(context_config.ragged.size());
+  context_result->ragged_outer_splits.resize(context_config.ragged.size());
+  sequence_result->sparse_values.resize(sequence_config.sparse.size());
+  sequence_result->sparse_indices.resize(sequence_config.sparse.size());
+  sequence_result->sparse_shapes.resize(sequence_config.sparse.size());
+  sequence_result->dense_values.resize(sequence_config.dense.size());
+  sequence_result->ragged_values.resize(sequence_config.ragged.size());
+  sequence_result->ragged_splits.resize(sequence_config.ragged.size());
+  sequence_result->ragged_outer_splits.resize(sequence_config.ragged.size());
+  dense_feature_lengths->resize(sequence_config.dense.size());
+
+  // NOTE(mrry): Cache the CPU allocator here and use it in Tensor construction,
+  // to avoid lock contention in `tensorflow::cpu_allocator()`.
+  Allocator* allocator = tensorflow::cpu_allocator();
+
+  TF_RETURN_IF_ERROR(ParseContextDenseFeatures(
+      context_features, context_config, example_names, is_batch, num_examples,
+      allocator, context_result));
+  TF_RETURN_IF_ERROR(ParseContextSparseFeatures(
+      context_features, context_config, example_names, is_batch, num_examples,
+      allocator, context_result));
+  TF_RETURN_IF_ERROR(ParseContextRaggedFeatures(
+      context_features, context_config, example_names, is_batch, num_examples,
+      allocator, context_result));
+  TF_RETURN_IF_ERROR(ParseSequenceDenseFeatures(
+      sequence_features, sequence_config, example_names, is_batch, num_examples,
+      allocator, sequence_result, dense_feature_lengths));
+  TF_RETURN_IF_ERROR(ParseSequenceSparseFeatures(
+      sequence_features, sequence_config, example_names, is_batch, num_examples,
+      allocator, sequence_result));
+  TF_RETURN_IF_ERROR(ParseSequenceRaggedFeatures(
+      sequence_features, sequence_config, example_names, is_batch, num_examples,
+      allocator, sequence_result));
 
   return Status::OK();
 }
diff --git a/tensorflow/core/util/example_proto_fast_parsing.h b/tensorflow/core/util/example_proto_fast_parsing.h
index a2f4f7b5e19..9594747bbb5 100644
--- a/tensorflow/core/util/example_proto_fast_parsing.h
+++ b/tensorflow/core/util/example_proto_fast_parsing.h
@@ -97,6 +97,7 @@ struct Result {
   std::vector<Tensor> dense_values;
   std::vector<Tensor> ragged_values;
   std::vector<Tensor> ragged_splits;
+  std::vector<Tensor> ragged_outer_splits;  // For SequenceExamples
 
   // This vector will be populated with one element per example if
   // `FastParseExampleConfig::collect_feature_stats` is set to `true`.
@@ -122,13 +123,14 @@ Status FastParseSingleExample(const FastParseSingleExampleConfig& config,
 // result according to given config.
 // Given example names have to either be empty or the same size as serialized.
 // example_names are used only for error messages.
+// (If batch=true, then this parses a single SequenceExample.)
 Status FastParseSequenceExample(
     const example::FastParseExampleConfig& context_config,
     const example::FastParseExampleConfig& feature_list_config,
     gtl::ArraySlice<tstring> serialized, gtl::ArraySlice<tstring> example_names,
     thread::ThreadPool* thread_pool, example::Result* context_result,
     example::Result* feature_list_result,
-    std::vector<Tensor>* dense_feature_lengths);
+    std::vector<Tensor>* dense_feature_lengths, bool is_batch = true);
 
 // This function parses serialized Example and populates given example.
 // It uses the same specialized parser as FastParseExample which is efficient.
diff --git a/tensorflow/core/util/example_proto_helper.cc b/tensorflow/core/util/example_proto_helper.cc
index af006f28d0f..117991a2f64 100644
--- a/tensorflow/core/util/example_proto_helper.cc
+++ b/tensorflow/core/util/example_proto_helper.cc
@@ -472,43 +472,89 @@ Status ParseSingleExampleAttrs::FinishInit() {
   return Status::OK();
 }
 
-Status ParseSequenceExampleAttrs::FinishInit() {
-  if (num_context_sparse != context_sparse_keys.size() ||
-      num_context_sparse != context_sparse_types.size()) {
+Status ParseSequenceExampleAttrs::FinishInit(int op_version) {
+  switch (op_version) {
+    case 1:
+      num_context_ragged = 0;
+      num_feature_list_ragged = 0;
+      if (num_context_sparse != context_sparse_keys.size()) {
+        return errors::InvalidArgument(
+            "num_context_sparse (", num_context_sparse,
+            ") must match the size of context_sparse_keys (",
+            context_sparse_keys.size(), ")");
+      }
+      if (num_context_dense != context_dense_keys.size()) {
+        return errors::InvalidArgument(
+            "num_context_dense (", num_context_dense,
+            ") must match the size of context_dense_keys (",
+            context_dense_keys.size(), ")");
+      }
+      if (num_feature_list_sparse != feature_list_sparse_keys.size()) {
+        return errors::InvalidArgument(
+            "num_feature_list_sparse (", num_feature_list_sparse,
+            ") must match the size of feature_list_sparse_keys (",
+            feature_list_sparse_keys.size(), ")");
+      }
+      if (num_feature_list_dense != feature_list_dense_keys.size()) {
+        return errors::InvalidArgument(
+            "num_feature_list_dense (", num_feature_list_dense,
+            ") must match the size of feature_list_dense_keys (",
+            feature_list_dense_keys.size(), ")");
+      }
+      break;
+    case 2:
+      num_context_dense = context_dense_types.size();
+      num_context_ragged = context_ragged_value_types.size();
+      num_feature_list_ragged = feature_list_ragged_value_types.size();
+      break;
+    default:
+      return errors::InvalidArgument("Unexpected op_version", op_version);
+  }
+  if (num_context_sparse != context_sparse_types.size()) {
     return errors::InvalidArgument(
         "num_context_sparse (", num_context_sparse,
-        ") must match the size of context_sparse_keys (",
-        context_sparse_keys.size(), ") and context_sparse_types (",
+        ") must match the size of context_sparse_types (",
         context_sparse_types.size(), ")");
   }
-  if (num_context_dense != context_dense_keys.size() ||
-      num_context_dense != context_dense_types.size() ||
+  if (num_context_dense != context_dense_types.size() ||
       num_context_dense != context_dense_shapes.size()) {
     return errors::InvalidArgument(
         "num_context_dense (", num_context_dense,
-        ") must match the size of context_dense_keys (",
-        context_dense_keys.size(), "), context_dense_types (",
+        ") must match the size of context_dense_types (",
         context_dense_types.size(), ") and context_dense_shapes (",
         context_dense_shapes.size(), ")");
   }
-  if (num_feature_list_sparse != feature_list_sparse_keys.size() ||
-      num_feature_list_sparse != feature_list_sparse_types.size()) {
+  if ((num_context_ragged != context_ragged_value_types.size()) ||
+      (num_context_ragged != context_ragged_split_types.size())) {
+    return errors::InvalidArgument(
+        "num_context_ragged (", num_context_ragged,
+        ") must match the size of context_ragged_value_types (",
+        context_ragged_value_types.size(), ") and context_ragged_split_types (",
+        context_ragged_split_types.size(), ")");
+  }
+  if (num_feature_list_sparse != feature_list_sparse_types.size()) {
     return errors::InvalidArgument(
         "num_feature_list_sparse (", num_feature_list_sparse,
-        ") must match the size of feature_list_sparse_keys (",
-        feature_list_sparse_keys.size(), ") and feature_list_sparse_types (",
+        ") must match the size of feature_list_sparse_types (",
         feature_list_sparse_types.size(), ")");
   }
-  if (num_feature_list_dense != feature_list_dense_keys.size() ||
-      num_feature_list_dense != feature_list_dense_types.size() ||
+  if (num_feature_list_dense != feature_list_dense_types.size() ||
       num_feature_list_dense != feature_list_dense_shapes.size()) {
     return errors::InvalidArgument(
         "num_feature_list_dense (", num_feature_list_dense,
-        ") must match the size of feature_list_dense_keys (",
-        feature_list_dense_keys.size(), "), feature_list_dense_types (",
+        ") must match the size of feature_list_dense_types (",
         feature_list_dense_types.size(), ") and feature_list_dense_shapes (",
         feature_list_dense_shapes.size(), ")");
   }
+  if ((num_feature_list_ragged != feature_list_ragged_value_types.size()) ||
+      (num_feature_list_ragged != feature_list_ragged_split_types.size())) {
+    return errors::InvalidArgument(
+        "num_feature_list_ragged (", num_feature_list_ragged,
+        ") must match the size of feature_list_ragged_value_types (",
+        feature_list_ragged_value_types.size(),
+        ") and feature_list_ragged_split_types (",
+        feature_list_ragged_split_types.size(), ")");
+  }
   for (const DataType& type : context_dense_types) {
     TF_RETURN_IF_ERROR(CheckValidType(type));
   }
@@ -521,6 +567,24 @@ Status ParseSequenceExampleAttrs::FinishInit() {
   for (const DataType& type : feature_list_sparse_types) {
     TF_RETURN_IF_ERROR(CheckValidType(type));
   }
+  for (const DataType& type : context_ragged_value_types) {
+    TF_RETURN_IF_ERROR(CheckValidType(type));
+  }
+  for (const DataType& type : context_ragged_split_types) {
+    if (!(type == DT_INT64 || type == DT_INT32)) {
+      return errors::InvalidArgument("Invalid context_ragged_split_type: ",
+                                     DataTypeString(type));
+    }
+  }
+  for (const DataType& type : feature_list_ragged_value_types) {
+    TF_RETURN_IF_ERROR(CheckValidType(type));
+  }
+  for (const DataType& type : feature_list_ragged_split_types) {
+    if (!(type == DT_INT64 || type == DT_INT32)) {
+      return errors::InvalidArgument("Invalid feature_list_ragged_split_type: ",
+                                     DataTypeString(type));
+    }
+  }
 
   return Status::OK();
 }
diff --git a/tensorflow/core/util/example_proto_helper.h b/tensorflow/core/util/example_proto_helper.h
index 6b0e2781590..81cedc2c834 100644
--- a/tensorflow/core/util/example_proto_helper.h
+++ b/tensorflow/core/util/example_proto_helper.h
@@ -243,24 +243,41 @@ struct ParseSingleExampleAttrs {
 struct ParseSequenceExampleAttrs {
  public:
   template <typename ContextType>
-  Status Init(ContextType* ctx) {
-    std::vector<string> feature_list_dense_missing_assumed_empty_tmp;
-    TF_RETURN_IF_ERROR(
-        ctx->GetAttr("feature_list_dense_missing_assumed_empty",
-                     &feature_list_dense_missing_assumed_empty_tmp));
-    for (const string& feature : feature_list_dense_missing_assumed_empty_tmp) {
-      feature_list_dense_missing_assumed_empty.insert(feature);
+  Status Init(ContextType* ctx, int op_version = 1) {
+    switch (op_version) {
+      case 1: {
+        std::vector<string> missing_empty_vector;
+        TF_RETURN_IF_ERROR(ctx->GetAttr(
+            "feature_list_dense_missing_assumed_empty", &missing_empty_vector));
+        for (const string& feature : missing_empty_vector) {
+          feature_list_dense_missing_assumed_empty.insert(feature);
+        }
+      }
+        TF_RETURN_IF_ERROR(
+            ctx->GetAttr("context_sparse_keys", &context_sparse_keys));
+        TF_RETURN_IF_ERROR(
+            ctx->GetAttr("context_dense_keys", &context_dense_keys));
+        TF_RETURN_IF_ERROR(ctx->GetAttr("feature_list_sparse_keys",
+                                        &feature_list_sparse_keys));
+        TF_RETURN_IF_ERROR(
+            ctx->GetAttr("feature_list_dense_keys", &feature_list_dense_keys));
+        TF_RETURN_IF_ERROR(ctx->GetAttr("Ncontext_dense", &num_context_dense));
+        break;
+      case 2:
+        TF_RETURN_IF_ERROR(ctx->GetAttr("context_ragged_value_types",
+                                        &context_ragged_value_types));
+        TF_RETURN_IF_ERROR(ctx->GetAttr("context_ragged_split_types",
+                                        &context_ragged_split_types));
+        TF_RETURN_IF_ERROR(ctx->GetAttr("feature_list_ragged_value_types",
+                                        &feature_list_ragged_value_types));
+        TF_RETURN_IF_ERROR(ctx->GetAttr("feature_list_ragged_split_types",
+                                        &feature_list_ragged_split_types));
+        break;
+      default:
+        return errors::InvalidArgument("Unexpected op_version", op_version);
     }
-    TF_RETURN_IF_ERROR(
-        ctx->GetAttr("context_sparse_keys", &context_sparse_keys));
-    TF_RETURN_IF_ERROR(ctx->GetAttr("context_dense_keys", &context_dense_keys));
-    TF_RETURN_IF_ERROR(
-        ctx->GetAttr("feature_list_sparse_keys", &feature_list_sparse_keys));
-    TF_RETURN_IF_ERROR(
-        ctx->GetAttr("feature_list_dense_keys", &feature_list_dense_keys));
     TF_RETURN_IF_ERROR(
         ctx->GetAttr("context_sparse_types", &context_sparse_types));
-    TF_RETURN_IF_ERROR(ctx->GetAttr("Ncontext_dense", &num_context_dense));
     TF_RETURN_IF_ERROR(
         ctx->GetAttr("Nfeature_list_dense", &num_feature_list_dense));
     TF_RETURN_IF_ERROR(ctx->GetAttr("Ncontext_sparse", &num_context_sparse));
@@ -275,14 +292,16 @@ struct ParseSequenceExampleAttrs {
         ctx->GetAttr("context_dense_shapes", &context_dense_shapes));
     TF_RETURN_IF_ERROR(
         ctx->GetAttr("feature_list_dense_shapes", &feature_list_dense_shapes));
-    return FinishInit();
+    return FinishInit(op_version);
   }
 
   std::unordered_set<string> feature_list_dense_missing_assumed_empty;
   int64 num_context_sparse;
   int64 num_context_dense;
+  int64 num_context_ragged;
   int64 num_feature_list_sparse;
   int64 num_feature_list_dense;
+  int64 num_feature_list_ragged;
   std::vector<string> context_sparse_keys;
   std::vector<string> context_dense_keys;
   std::vector<string> feature_list_sparse_keys;
@@ -293,9 +312,13 @@ struct ParseSequenceExampleAttrs {
   std::vector<DataType> feature_list_sparse_types;
   std::vector<DataType> feature_list_dense_types;
   std::vector<TensorShape> feature_list_dense_shapes;
+  std::vector<DataType> context_ragged_value_types;
+  std::vector<DataType> context_ragged_split_types;
+  std::vector<DataType> feature_list_ragged_value_types;
+  std::vector<DataType> feature_list_ragged_split_types;
 
  private:
-  Status FinishInit();  // for context-independent parts of Init.
+  Status FinishInit(int op_version);  // for context-independent parts of Init.
 };
 
 // Parses the attributes passed to ParseSingleSequenceExample.
diff --git a/tensorflow/python/kernel_tests/BUILD b/tensorflow/python/kernel_tests/BUILD
index a815a5892d7..aefe1a7e03b 100644
--- a/tensorflow/python/kernel_tests/BUILD
+++ b/tensorflow/python/kernel_tests/BUILD
@@ -766,7 +766,7 @@ cuda_py_test(
 
 tf_py_test(
     name = "parsing_ops_test",
-    size = "small",
+    size = "medium",
     srcs = ["parsing_ops_test.py"],
     additional_deps = [
         "//third_party/py/numpy",
diff --git a/tensorflow/python/kernel_tests/parsing_ops_test.py b/tensorflow/python/kernel_tests/parsing_ops_test.py
index 26a703c8060..843b6fa6430 100644
--- a/tensorflow/python/kernel_tests/parsing_ops_test.py
+++ b/tensorflow/python/kernel_tests/parsing_ops_test.py
@@ -1521,9 +1521,6 @@ class ParseSequenceExampleTest(test.TestCase):
     kwargs["serialized"] = [kwargs.pop("serialized")]
     kwargs["example_names"] = [kwargs.pop("example_name")
                               ] if "example_name" in kwargs else None
-    # Disable error string matching; it's not consistent for batch mode.
-    if expected_err:
-      expected_err = (expected_err[0], "")
 
     # Add a batch dimension to expected output
     if expected_context_values:
@@ -1531,7 +1528,7 @@ class ParseSequenceExampleTest(test.TestCase):
       for k in expected_context_values:
         v = expected_context_values[k]
         if isinstance(kwargs["context_features"][k],
-                      parsing_ops.FixedLenFeature):
+                      (parsing_ops.FixedLenFeature, parsing_ops.RaggedFeature)):
           new_values[k] = np.expand_dims(v, axis=0)
         else:
           # Sparse tensor.
@@ -1548,6 +1545,9 @@ class ParseSequenceExampleTest(test.TestCase):
                       parsing_ops.FixedLenSequenceFeature):
           expected_length_values[k] = [np.shape(v)[0]]
           new_values[k] = np.expand_dims(v, axis=0)
+        elif isinstance(kwargs["sequence_features"][k],
+                        parsing_ops.RaggedFeature):
+          new_values[k] = np.expand_dims(v, axis=0)
         else:
           # Sparse tensor.
           new_values[k] = (np.insert(v[0], 0, 0, axis=1), v[1],
@@ -1562,6 +1562,7 @@ class ParseSequenceExampleTest(test.TestCase):
         expected_err=expected_err,
         batch=True)
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleWithSparseAndDenseContext(self):
     original = sequence_example(
         context=features({
@@ -1605,6 +1606,7 @@ class ParseSequenceExampleTest(test.TestCase):
         },
         expected_context_values=expected_context_output)
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleWithMultipleSizeFeatureLists(self):
     original = sequence_example(
         feature_lists=feature_lists({
@@ -1668,6 +1670,7 @@ class ParseSequenceExampleTest(test.TestCase):
         },
         expected_feat_list_values=expected_feature_list_output)
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleWithoutDebugName(self):
     original = sequence_example(
         feature_lists=feature_lists({
@@ -1725,6 +1728,7 @@ class ParseSequenceExampleTest(test.TestCase):
         },
         expected_feat_list_values=expected_feature_list_output)
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleWithSparseAndDenseFeatureLists(self):
     original = sequence_example(
         feature_lists=feature_lists({
@@ -1783,6 +1787,7 @@ class ParseSequenceExampleTest(test.TestCase):
         },
         expected_feat_list_values=expected_feature_list_output)
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleWithEmptyFeatureInFeatureLists(self):
     original = sequence_example(
         feature_lists=feature_lists({
@@ -1815,6 +1820,7 @@ class ParseSequenceExampleTest(test.TestCase):
         },
         expected_feat_list_values=expected_feature_list_output)
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleListWithInconsistentDataFails(self):
     original = sequence_example(
         feature_lists=feature_lists({
@@ -1835,6 +1841,7 @@ class ParseSequenceExampleTest(test.TestCase):
         expected_err=(errors_impl.OpError, "Feature list: a, Index: 1."
                       "  Data types don't match. Expected type: int64"))
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleListWithWrongDataTypeFails(self):
     original = sequence_example(
         feature_lists=feature_lists({
@@ -1855,6 +1862,7 @@ class ParseSequenceExampleTest(test.TestCase):
                       "Feature list: a, Index: 0.  Data types don't match."
                       " Expected type: int64"))
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleListWithWrongSparseDataTypeFails(self):
     original = sequence_example(
         feature_lists=feature_lists({
@@ -1878,9 +1886,9 @@ class ParseSequenceExampleTest(test.TestCase):
         },
         expected_err=(errors_impl.OpError,
                       "Name: in1, Feature list: a, Index: 2."
-                      "  Data types don't match. Expected type: int64"
-                      "  Feature is: float_list"))
+                      "  Data types don't match. Expected type: int64"))
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleListWithWrongShapeFails(self):
     original = sequence_example(
         feature_lists=feature_lists({
@@ -1899,10 +1907,44 @@ class ParseSequenceExampleTest(test.TestCase):
                 "a": parsing_ops.FixedLenSequenceFeature((2,), dtypes.int64)
             }
         },
-        expected_err=(errors_impl.OpError, r"Name: in1, Key: a, Index: 1."
-                      r"  Number of int64 values != expected."
-                      r"  values size: 3 but output shape: \[2\]"))
+        expected_err=(
+            errors_impl.OpError,
+            # message from ParseSingleExample.
+            r"Name: in1, Key: a, Index: 1."
+            r"  Number of int64 values != expected."
+            r"  values size: 3 but output shape: \[2\]"
+            # or message from FastParseSequenceExample
+            r"|Feature list 'a' has an unexpected number of values.  "
+            r"Total values size: 5 is not consistent with output "
+            r"shape: \[\?,2\]"))
+
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
+  def testSequenceExampleListWithWrongShapeFails2(self):
+    # This exercises a different code path for FastParseSequenceExample than
+    # testSequenceExampleListWithWrongShapeFails (in that test, we can tell that
+    # the shape is bad based on the total number of values; in this test, we
+    # can't tell the shape is bad until we look at individual rows.)
+    original = sequence_example(
+        feature_lists=feature_lists({
+            "a": feature_list([int64_feature([2]),
+                               int64_feature([2, 3, 4])]),
+        }))
+
+    serialized = original.SerializeToString()
+
+    self._testBoth(
+        {
+            "example_name": "in1",
+            "serialized": ops.convert_to_tensor(serialized),
+            "sequence_features": {
+                "a": parsing_ops.FixedLenSequenceFeature((2,), dtypes.int64)
+            }
+        },
+        expected_err=(errors_impl.OpError, r"Name: in1, Key: a, Index: 0."
+                      r"  Number of (int64 )?values != expected."
+                      r"  values size: 1 but output shape: \[2\]"))
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleWithMissingFeatureListFails(self):
     original = sequence_example(feature_lists=feature_lists({}))
 
@@ -1923,6 +1965,7 @@ class ParseSequenceExampleTest(test.TestCase):
             " feature_list_dense_missing_assumed_empty or"
             " feature_list_dense_defaults?"))
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
   def testSequenceExampleBatch(self):
     first = sequence_example(
         feature_lists=feature_lists({
@@ -1935,14 +1978,21 @@ class ParseSequenceExampleTest(test.TestCase):
                 ])
         }))
     second = sequence_example(
+        context=features({"c": float_feature([7])}),
         feature_lists=feature_lists({
             "a": feature_list([
                 int64_feature([21, 2, 11]),
-            ])
+            ]),
+            "b": feature_list([
+                int64_feature([5]),
+            ]),
         }))
 
     serialized = [first.SerializeToString(), second.SerializeToString()]
 
+    expected_context_output = {
+        "c": np.array([-1, 7], dtype=np.float32),
+    }
     expected_feature_list_output = {
         "a":
             np.array(
@@ -1961,6 +2011,8 @@ class ParseSequenceExampleTest(test.TestCase):
                     ]
                 ],
                 dtype=np.int64),
+        "b":
+            np.array([[0], [5]], dtype=np.int64),
         "d":
             np.empty(shape=(2, 0, 5), dtype=np.float32),  # allowed_missing
     }
@@ -1969,21 +2021,289 @@ class ParseSequenceExampleTest(test.TestCase):
         {
             "example_names": ops.convert_to_tensor(["in1", "in2"]),
             "serialized": ops.convert_to_tensor(serialized),
+            "context_features": {
+                "c":
+                    parsing_ops.FixedLenFeature(
+                        (), dtypes.float32, default_value=-1),
+            },
             "sequence_features": {
                 "a":
                     parsing_ops.FixedLenSequenceFeature((1, 3), dtypes.int64),
+                "b":
+                    parsing_ops.FixedLenSequenceFeature(
+                        (), dtypes.int64, allow_missing=True),
                 "d":
                     parsing_ops.FixedLenSequenceFeature(
                         (5,), dtypes.float32, allow_missing=True),
             }
         },
+        expected_context_values=expected_context_output,
         expected_feat_list_values=expected_feature_list_output,
         expected_length_values={
             "a": [4, 1],
+            "b": [0, 1],
             "d": [0, 0]
         },
         batch=True)
 
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
+  def testSerializedContainingRaggedFeatureWithNoPartitions(self):
+    original = [
+        sequence_example(
+            context=features({"a": float_feature([3, 4])}),
+            feature_lists=feature_lists({
+                "b": feature_list([float_feature([5]),
+                                   float_feature([3])]),
+                "c": feature_list([int64_feature([6, 7, 8, 9])])
+            })),
+        sequence_example(
+            context=features({"a": float_feature([9])}),
+            feature_lists=feature_lists({
+                "b": feature_list([]),
+                "c": feature_list([int64_feature([]),
+                                   int64_feature([1, 2, 3])])
+            })),
+        sequence_example(
+            feature_lists=feature_lists({
+                "b":
+                    feature_list([
+                        float_feature([1]),
+                        float_feature([1, 2]),
+                        float_feature([1, 2, 3])
+                    ])
+            })),
+        sequence_example(
+            context=features({"a": feature()}),
+            feature_lists=feature_lists({
+                "b": feature_list([feature()]),
+                "c": feature_list([int64_feature([3, 3, 3])])
+            }))
+    ]
+    serialized = [m.SerializeToString() for m in original]
+
+    context_features = {"a": parsing_ops.RaggedFeature(dtype=dtypes.float32)}
+    sequence_features = {
+        "b":
+            parsing_ops.RaggedFeature(dtype=dtypes.float32),
+        "c":
+            parsing_ops.RaggedFeature(
+                dtype=dtypes.int64, row_splits_dtype=dtypes.int64)
+    }
+
+    expected_a = ragged_factory_ops.constant([[3, 4], [9], [], []],
+                                             dtype=dtypes.float32,
+                                             row_splits_dtype=dtypes.int32)
+    expected_b = ragged_factory_ops.constant(
+        [[[5], [3]], [], [[1], [1, 2], [1, 2, 3]], [[]]],
+        dtype=dtypes.float32,
+        row_splits_dtype=dtypes.int32)
+    expected_c = ragged_factory_ops.constant(
+        [[[6, 7, 8, 9]], [[], [1, 2, 3]], [], [[3, 3, 3]]],
+        dtype=dtypes.int64,
+        row_splits_dtype=dtypes.int64)
+
+    expected_context_output = dict(a=expected_a)
+    expected_feature_list_output = dict(b=expected_b, c=expected_c)
+
+    self._test(
+        {
+            "serialized": ops.convert_to_tensor(serialized),
+            "context_features": context_features,
+            "sequence_features": sequence_features,
+        },
+        expected_context_output,
+        expected_feature_list_output,
+        batch=True)
+
+    self._test(
+        {
+            "serialized": ops.convert_to_tensor(serialized)[0],
+            "context_features": context_features,
+            "sequence_features": sequence_features,
+        },
+        expected_context_values={"a": [3, 4]},
+        expected_feat_list_values={
+            "b": [[5], [3]],
+            "c": [[6, 7, 8, 9]]
+        },
+        batch=False)
+
+    # Test with a larger batch of examples.
+    batch_serialized = serialized * 64
+    batch_context_expected_out = {
+        "a": ragged_concat_ops.concat([expected_a] * 64, axis=0)
+    }
+    batch_feature_list_expected_out = {
+        "b": ragged_concat_ops.concat([expected_b] * 64, axis=0),
+        "c": ragged_concat_ops.concat([expected_c] * 64, axis=0)
+    }
+    self._test(
+        {
+            "serialized": ops.convert_to_tensor(batch_serialized),
+            "context_features": context_features,
+            "sequence_features": sequence_features,
+        },
+        batch_context_expected_out,
+        batch_feature_list_expected_out,
+        batch=True)
+
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
+  def testSerializedContainingNestedRaggedFeature(self):
+    """Test RaggedFeatures with nested partitions."""
+    original = [
+        # rt shape: [(batch), 2, None, None]
+        sequence_example(
+            context=features({
+                # a[0] = [[[[1]], [[2, 3], [4]]], [[], [[5, 6, 7]]]]
+                "a_values": float_feature([1, 2, 3, 4, 5, 6, 7]),
+                "a_lengths_axis2": int64_feature([1, 2, 0, 1]),
+                "a_lengths_axis3": int64_feature([1, 2, 1, 3]),
+                "a_splits_axis3": int64_feature([0, 1, 3, 4, 7])
+            }),
+            feature_lists=feature_lists({
+                # b[0] = [[[1], [2, 3, 4]], [[2, 4], [6]]]
+                "b_values":
+                    feature_list(
+                        [float_feature([1, 2, 3, 4]),
+                         float_feature([2, 4, 6])]),
+                "b_splits":
+                    feature_list(
+                        [int64_feature([0, 1, 4]),
+                         int64_feature([0, 2, 3])]),
+            })),
+        sequence_example(
+            # a[1] = []
+            # b[1] = []
+        ),
+        sequence_example(
+            context=features({
+                # a[2] = [[[[1, 2, 3], [4]], [[5], [6], [7, 8]]]]
+                "a_values": float_feature([1, 2, 3, 4, 5, 6, 7, 8]),
+                "a_lengths_axis2": int64_feature([2, 3]),
+                "a_lengths_axis3": int64_feature([3, 1, 1, 1, 2]),
+                "a_splits_axis3": int64_feature([0, 3, 4, 5, 6, 8])
+            }),
+            feature_lists=feature_lists({
+                # b[2] = [[[9], [8, 7, 6], [5]], [[4, 3, 2, 1]], [[0]]]
+                "b_values":
+                    feature_list([
+                        float_feature([9, 8, 7, 6, 5]),
+                        float_feature([4, 3, 2, 1]),
+                        float_feature([0])
+                    ]),
+                "b_splits":
+                    feature_list([
+                        int64_feature([0, 1, 4, 5]),
+                        int64_feature([0, 4]),
+                        int64_feature([0, 1])
+                    ])
+            }))
+    ]
+    serialized = [m.SerializeToString() for m in original]
+
+    context_features = {
+        "a":
+            parsing_ops.RaggedFeature(
+                value_key="a_values",
+                partitions=[
+                    parsing_ops.RaggedFeature.UniformRowLength(2),
+                    parsing_ops.RaggedFeature.RowLengths("a_lengths_axis2"),
+                    parsing_ops.RaggedFeature.RowSplits("a_splits_axis3"),
+                ],
+                dtype=dtypes.float32,
+                row_splits_dtype=dtypes.int64,
+            )
+    }
+    sequence_features = {
+        "b":
+            parsing_ops.RaggedFeature(
+                value_key="b_values",
+                dtype=dtypes.float32,
+                partitions=[parsing_ops.RaggedFeature.RowSplits("b_splits")]),
+        "c":
+            parsing_ops.RaggedFeature(
+                value_key="b_values",
+                dtype=dtypes.float32,
+                partitions=[parsing_ops.RaggedFeature.UniformRowLength(1)]),
+    }
+
+    expected_context = {
+        "a":
+            ragged_factory_ops.constant(
+                [[[[[1]], [[2, 3], [4]]], [[], [[5, 6, 7]]]], [],
+                 [[[[1, 2, 3], [4]], [[5], [6], [7, 8]]]]],
+                dtype=dtypes.float32,
+                row_splits_dtype=dtypes.int64)
+    }
+    expected_feature_list = {
+        "b":
+            ragged_factory_ops.constant(
+                [[[[1], [2, 3, 4]], [[2, 4], [6]]], [],
+                 [[[9], [8, 7, 6], [5]], [[4, 3, 2, 1]], [[0]]]],
+                dtype=dtypes.float32,
+                row_splits_dtype=dtypes.int32),
+        "c":
+            ragged_factory_ops.constant(
+                [[[[1], [2], [3], [4]], [[2], [4], [6]]], [],
+                 [[[9], [8], [7], [6], [5]], [[4], [3], [2], [1]], [[0]]]],
+                ragged_rank=2,
+                dtype=dtypes.float32,
+                row_splits_dtype=dtypes.int32),
+    }
+
+    self._test(
+        dict(
+            serialized=ops.convert_to_tensor(serialized),
+            context_features=context_features,
+            sequence_features=sequence_features),
+        expected_context,
+        expected_feature_list,
+        batch=True)
+
+    self._test(
+        dict(
+            serialized=ops.convert_to_tensor(serialized)[0],
+            context_features=context_features,
+            sequence_features=sequence_features),
+        {"a": expected_context["a"][0]}, {
+            "b": expected_feature_list["b"][0],
+            "c": expected_feature_list["c"][0]
+        },
+        batch=False)
+
+  @test_util.with_forward_compatibility_horizons(None, [2019, 10, 31])
+  def testSerializedContainingMisalignedNestedRaggedFeature(self):
+    """FeatureList with 2 value tensors but only one splits tensor."""
+    original = sequence_example(
+        feature_lists=feature_lists({
+            "b_values":
+                feature_list(
+                    [float_feature([1, 2, 3, 4]),
+                     float_feature([2, 4, 6])]),
+            "b_splits":
+                feature_list([int64_feature([0, 1, 4])]),
+        }))
+    sequence_features = {
+        "b":
+            parsing_ops.RaggedFeature(
+                value_key="b_values",
+                dtype=dtypes.float32,
+                partitions=[parsing_ops.RaggedFeature.RowSplits("b_splits")],
+                validate=True)
+    }
+    self._testBoth(
+        dict(
+            serialized=ops.convert_to_tensor(original.SerializeToString()),
+            sequence_features=sequence_features),
+        expected_err=(
+            (errors_impl.OpError, ValueError),
+            # Message for batch=true:
+            "Feature b: values and partitions are not aligned"
+            # Message for batch=false in graph mode:
+            "|.* do not form a valid RaggedTensor"
+            # Message for batch=false in eager mode:
+            "|Dimensions 2 and 1 are not compatible"))
+
 
 @test_util.run_all_in_graph_and_eager_modes
 class DecodeRawTest(test.TestCase):
diff --git a/tensorflow/python/ops/parsing_config.py b/tensorflow/python/ops/parsing_config.py
index 38a61f71d08..bed17c7859e 100644
--- a/tensorflow/python/ops/parsing_config.py
+++ b/tensorflow/python/ops/parsing_config.py
@@ -26,6 +26,7 @@ from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
 from tensorflow.python.ops import array_ops
+from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import sparse_ops
 from tensorflow.python.platform import tf_logging
@@ -721,12 +722,25 @@ def _construct_tensors_for_composite_features(features, tensor_dict):
       value_key = key if feature.value_key is None else feature.value_key
       rt = tensor_dict[value_key]
       if isinstance(rt, ragged_tensor.RaggedTensor):
-        # We processed a vector of serialized tf.Examples.
+        # We processed a batch of tf.Example or tf.SequenceExample, or single
+        # tf.SequenceExample.
+        if rt.ragged_rank > 1:
+          # We're processing a batch of SequenceExample, and we effectively have
+          # two batch dimensions.  Cllapse those batch dimensions here, and
+          # restore them below (using outer_splits).
+          outer_splits = rt.row_splits
+          rt = rt.values
+        else:
+          outer_splits = None
         for partition in reversed(feature.partitions):
           rt = _add_batched_ragged_partition(rt, partition, tensor_dict,
-                                             feature.validate)
+                                             key, feature.validate,
+                                             outer_splits)
+        if outer_splits is not None:
+          rt = ragged_tensor.RaggedTensor.from_row_splits(
+              rt, outer_splits, validate=feature.validate)
       else:
-        # We processed a single serialized tf.Example.
+        # We processed a single tf.Example.
         for partition in reversed(feature.partitions):
           rt = _add_ragged_partition(rt, partition, tensor_dict,
                                      feature.row_splits_dtype, feature.validate)
@@ -789,7 +803,8 @@ def _add_ragged_partition(values, partition, tensor_dict, row_splits_dtype,
     raise ValueError("Unhandled partition type %r" % partition)
 
 
-def _add_batched_ragged_partition(rt, partition, tensor_dict, validate):
+def _add_batched_ragged_partition(rt, partition, tensor_dict, feature_key,
+                                  validate, outer_splits=None):
   """Adds a batched ragged partition tensor to a batched ragged tensor.
 
   Args:
@@ -799,7 +814,11 @@ def _add_batched_ragged_partition(rt, partition, tensor_dict, validate):
       RaggedFeature.UniformRowLength, in which case there is no partition
       tensor).  The specified tensor must have shape [batch_size, ...].
     tensor_dict: The dictionary mapping keys to tensors.
+    feature_key: The name of the feature being parsed (for error messages).
     validate: Whether to validate that the values form a valid RaggedTensor.
+    outer_splits: If not None, then we have two batch dimensions, and this
+      is the row-splits for the collapsed batch dimension.  Every partition
+      tensor must have an outer row_splits that matches this value.
 
   Returns:
     A new RaggedTensor where each batch item `rt[i]` has been partitioned
@@ -823,40 +842,59 @@ def _add_batched_ragged_partition(rt, partition, tensor_dict, validate):
   if partition_t.values.dtype != rt.row_splits.dtype:
     partition_t = math_ops.cast(partition_t, rt.row_splits.dtype)
 
-  if isinstance(partition, (RaggedFeature.RowSplits, RaggedFeature.RowLimits)):
-    if isinstance(partition, RaggedFeature.RowSplits):
-      partition_t = partition_t[:, 1:]
-    adjusted_limits = partition_t.values + array_ops.repeat(
-        rt.row_starts(), partition_t.row_lengths())
-    return partition_t.with_values(
-        ragged_tensor.RaggedTensor.from_row_limits(
-            rt.values, adjusted_limits, validate=validate))
-  elif isinstance(partition, RaggedFeature.RowStarts):
-    adjusted_starts = partition_t.values + array_ops.repeat(
-        rt.row_starts(), partition_t.row_lengths())
-    return partition_t.with_values(
-        ragged_tensor.RaggedTensor.from_row_starts(
-            rt.values, adjusted_starts, validate=validate))
-  elif isinstance(partition, RaggedFeature.RowLengths):
-    return partition_t.with_values(
-        ragged_tensor.RaggedTensor.from_row_lengths(
-            rt.values, partition_t.values, validate=validate))
-  elif isinstance(partition, RaggedFeature.ValueRowIds):
-    nrows = math_ops.maximum(  # number of rows in each batch item
-        ragged_math_ops.reduce_max(partition_t + 1, axis=1), 0)
-    adjusted_rowids = partition_t.values + array_ops.repeat(
-        math_ops.cumsum(nrows, exclusive=True), partition_t.row_lengths())
-    return ragged_tensor.RaggedTensor.from_row_lengths(
-        ragged_tensor.RaggedTensor.from_value_rowids(
-            rt.values, adjusted_rowids, validate=validate),
-        nrows,
-        validate=validate)
-
-  raise ValueError("Unhandled partition type %r" % partition)
-
-
-def _build_ragged_tensors(serialized_shape, ragged_values, ragged_row_splits):
+  checks = []
+  if outer_splits is not None:
+    if validate:
+      checks.append(check_ops.assert_equal(
+          outer_splits, partition_t.row_splits,
+          message="Feature %s: values and partitions are not aligned"
+          % feature_key))
+    partition_t = partition_t.values
+
+  with ops.control_dependencies(checks):
+    if isinstance(partition, (RaggedFeature.RowSplits,
+                              RaggedFeature.RowLimits)):
+      if isinstance(partition, RaggedFeature.RowSplits):
+        partition_t = partition_t[:, 1:]
+      adjusted_limits = partition_t.values + array_ops.repeat(
+          rt.row_starts(), partition_t.row_lengths())
+      return partition_t.with_values(
+          ragged_tensor.RaggedTensor.from_row_limits(
+              rt.values, adjusted_limits, validate=validate))
+    elif isinstance(partition, RaggedFeature.RowStarts):
+      adjusted_starts = partition_t.values + array_ops.repeat(
+          rt.row_starts(), partition_t.row_lengths())
+      return partition_t.with_values(
+          ragged_tensor.RaggedTensor.from_row_starts(
+              rt.values, adjusted_starts, validate=validate))
+    elif isinstance(partition, RaggedFeature.RowLengths):
+      return partition_t.with_values(
+          ragged_tensor.RaggedTensor.from_row_lengths(
+              rt.values, partition_t.values, validate=validate))
+    elif isinstance(partition, RaggedFeature.ValueRowIds):
+      nrows = math_ops.maximum(  # number of rows in each batch item
+          ragged_math_ops.reduce_max(partition_t + 1, axis=1), 0)
+      adjusted_rowids = partition_t.values + array_ops.repeat(
+          math_ops.cumsum(nrows, exclusive=True), partition_t.row_lengths())
+      return ragged_tensor.RaggedTensor.from_row_lengths(
+          ragged_tensor.RaggedTensor.from_value_rowids(
+              rt.values, adjusted_rowids, validate=validate),
+          nrows,
+          validate=validate)
+
+    raise ValueError("Unhandled partition type %r" % partition)
+
+
+def _build_ragged_tensors(serialized_shape,
+                          ragged_values,
+                          ragged_row_splits,
+                          ragged_inner_splits=None):
   """Builds RaggedTensors from the outputs of a parse op."""
+  if ragged_inner_splits is not None:
+    ragged_values = [
+        ragged_tensor.RaggedTensor.from_row_splits(val, split, validate=False)
+        for (val, split) in zip(ragged_values, ragged_inner_splits)
+    ]
   if serialized_shape.ndims == 0:
     return ragged_values
   else:
diff --git a/tensorflow/python/ops/parsing_ops.py b/tensorflow/python/ops/parsing_ops.py
index bc8480bfb43..82a5ea3fd0e 100644
--- a/tensorflow/python/ops/parsing_ops.py
+++ b/tensorflow/python/ops/parsing_ops.py
@@ -470,7 +470,6 @@ def parse_single_example_v2_unoptimized(
                         [serialized, example_names]):
       serialized = ops.convert_to_tensor(serialized, name="serialized")
       serialized = _assert_scalar(serialized, "serialized")
-      serialized.set_shape([])
       return parse_example_v2(serialized, features, example_names, name)
   if example_names is None:
     return parse_single_example_v2(serialized, features, name)
@@ -557,16 +556,19 @@ def parse_sequence_example(serialized,
   of `sequence_features` values may vary between `SequenceExample` protos,
   and even between `feature_list` keys within the same `SequenceExample`.
 
-  `context_features` contains `VarLenFeature` and `FixedLenFeature` objects.
-  Each `VarLenFeature` is mapped to a `SparseTensor`, and each `FixedLenFeature`
-  is mapped to a `Tensor`, of the specified type, shape, and default value.
-
-  `sequence_features` contains `VarLenFeature` and `FixedLenSequenceFeature`
-  objects. Each `VarLenFeature` is mapped to a `SparseTensor`, and each
-  `FixedLenSequenceFeature` is mapped to a `Tensor`, each of the specified type.
-  The shape will be `(B,T,) + df.dense_shape` for `FixedLenSequenceFeature`
-  `df`, where `B` is the batch size, and `T` is the length of the associated
-  `FeatureList` in the `SequenceExample`. For instance,
+  `context_features` contains `VarLenFeature`, `RaggedFeature`, and
+  `FixedLenFeature`  objects. Each `VarLenFeature` is mapped to a
+  `SparseTensor`; each `RaggedFeature` is  mapped to a `RaggedTensor`; and each
+  `FixedLenFeature` is mapped to a `Tensor`, of the specified type, shape, and
+  default value.
+
+  `sequence_features` contains `VarLenFeature`, `RaggedFeature`, and
+  `FixedLenSequenceFeature` objects. Each `VarLenFeature` is mapped to a
+  `SparseTensor`; each `RaggedFeature` is mapped to a `RaggedTensor; and
+  each `FixedLenSequenceFeature` is mapped to a `Tensor`, each of the specified
+  type. The shape will be `(B,T,) + df.dense_shape` for
+  `FixedLenSequenceFeature` `df`, where `B` is the batch size, and `T` is the
+  length of the associated `FeatureList` in the `SequenceExample`. For instance,
   `FixedLenSequenceFeature([])` yields a scalar 2-D `Tensor` of static shape
   `[None, None]` and dynamic shape `[B, T]`, while
   `FixedLenSequenceFeature([k])` (for `int k >= 1`) yields a 3-D matrix `Tensor`
@@ -600,21 +602,21 @@ def parse_sequence_example(serialized,
     serialized: A vector (1-D Tensor) of type string containing binary
       serialized `SequenceExample` protos.
     context_features: A `dict` mapping feature keys to `FixedLenFeature` or
-      `VarLenFeature` values. These features are associated with a
-      `SequenceExample` as a whole.
+      `VarLenFeature` or `RaggedFeature` values. These features are associated
+      with a `SequenceExample` as a whole.
     sequence_features: A `dict` mapping feature keys to
-      `FixedLenSequenceFeature` or `VarLenFeature` values. These features are
-      associated with data within the `FeatureList` section of the
-      `SequenceExample` proto.
+      `FixedLenSequenceFeature` or `VarLenFeature` or `RaggedFeature` values.
+      These features are associated with data within the `FeatureList` section
+      of the `SequenceExample` proto.
     example_names: A vector (1-D Tensor) of strings (optional), the name of the
       serialized protos.
     name: A name for this operation (optional).
 
   Returns:
-    A tuple of three `dict`s, each mapping keys to `Tensor`s and
-    `SparseTensor`s. The first dict contains the context key/values,
-    the second dict contains the feature_list key/values, and the final dict
-    contains the lengths of any dense feature_list features.
+    A tuple of three `dict`s, each mapping keys to `Tensor`s,
+    `SparseTensor`s, and `RaggedTensor`. The first dict contains the context
+    key/values, the second dict contains the feature_list key/values, and the
+    final dict contains the lengths of any dense feature_list features.
 
   Raises:
     ValueError: if any feature is invalid.
@@ -622,12 +624,26 @@ def parse_sequence_example(serialized,
   if not (context_features or sequence_features):
     raise ValueError("Missing features.")
   context_params = _ParseOpParams.from_features(
-      context_features, [VarLenFeature, FixedLenFeature])
+      context_features, [VarLenFeature, FixedLenFeature, RaggedFeature])
   feature_list_params = _ParseOpParams.from_features(
-      sequence_features, [VarLenFeature, FixedLenSequenceFeature])
+      sequence_features,
+      [VarLenFeature, FixedLenSequenceFeature, RaggedFeature])
+
+  with ops.name_scope(name, "ParseSequenceExample",
+                      [serialized, example_names]):
+    outputs = _parse_sequence_example_raw(serialized, example_names,
+                                          context_params, feature_list_params,
+                                          name)
+    context_output, feature_list_output, feature_list_lengths = outputs
 
-  return _parse_sequence_example_raw(serialized, example_names, context_params,
-                                     feature_list_params, name)
+    if context_params.ragged_keys:
+      context_output = _construct_tensors_for_composite_features(
+          context_features, context_output)
+    if feature_list_params.ragged_keys:
+      feature_list_output = _construct_tensors_for_composite_features(
+          sequence_features, feature_list_output)
+
+    return context_output, feature_list_output, feature_list_lengths
 
 
 def _parse_sequence_example_raw(serialized,
@@ -649,9 +665,9 @@ def _parse_sequence_example_raw(serialized,
     name: A name for this operation (optional).
 
   Returns:
-    A tuple of three `dict`s, each mapping keys to `Tensor`s and
-    `SparseTensor`s. The first dict contains the context key/values,
-    the second dict contains the feature_list key/values, and the final dict
+    A tuple of three `dict`s, each mapping keys to `Tensor`s, `SparseTensor`s,
+    and `RaggedTensor`s. The first dict contains the context key/values, the
+    second dict contains the feature_list key/values, and the final dict
     contains the lengths of any dense feature_list features.
 
   Raises:
@@ -670,33 +686,84 @@ def _parse_sequence_example_raw(serialized,
                          k)
       feature_list_dense_missing_assumed_empty.append(k)
 
-    # pylint: disable=protected-access
-    outputs = gen_parsing_ops.parse_sequence_example(
-        serialized=serialized,
-        debug_name=debug_name,
-        Ncontext_sparse=len(context.sparse_keys),
-        Ncontext_dense=len(context.dense_keys),
-        Nfeature_list_sparse=len(feature_list.sparse_keys),
-        Nfeature_list_dense=len(feature_list.dense_keys),
-        context_dense_defaults=context.dense_defaults_vec,
-        context_sparse_keys=context.sparse_keys,
-        context_sparse_types=context.sparse_types,
-        context_dense_keys=context.dense_keys,
-        context_dense_shapes=context.dense_shapes_as_proto,
-        feature_list_sparse_keys=feature_list.sparse_keys,
-        feature_list_sparse_types=feature_list.sparse_types,
-        feature_list_dense_keys=feature_list.dense_keys,
-        feature_list_dense_types=feature_list.dense_types,
-        feature_list_dense_shapes=feature_list.dense_shapes,
-        feature_list_dense_missing_assumed_empty=(
-            feature_list_dense_missing_assumed_empty),
-        name=name)
-    # pylint: enable=protected-access
+    has_ragged = context.ragged_keys or feature_list.ragged_keys
+    if compat.forward_compatible(2019, 10, 26) or has_ragged:
+      serialized = ops.convert_to_tensor(serialized, name="serialized")
+      if has_ragged and serialized.shape.ndims is None:
+        raise ValueError("serialized must have statically-known rank to "
+                         "parse ragged features.")
+      feature_list_dense_missing_assumed_empty_vector = [
+          key in feature_list_dense_missing_assumed_empty
+          for key in feature_list.dense_keys
+      ]
+      outputs = gen_parsing_ops.parse_sequence_example_v2(
+          # Inputs
+          serialized=serialized,
+          debug_name=debug_name,
+          context_sparse_keys=context.sparse_keys,
+          context_dense_keys=context.dense_keys,
+          context_ragged_keys=context.ragged_keys,
+          feature_list_sparse_keys=feature_list.sparse_keys,
+          feature_list_dense_keys=feature_list.dense_keys,
+          feature_list_ragged_keys=feature_list.ragged_keys,
+          feature_list_dense_missing_assumed_empty=(
+              feature_list_dense_missing_assumed_empty_vector),
+          context_dense_defaults=context.dense_defaults_vec,
+          # Attrs
+          Ncontext_sparse=len(context.sparse_keys),
+          Nfeature_list_sparse=len(feature_list.sparse_keys),
+          Nfeature_list_dense=len(feature_list.dense_keys),
+          context_sparse_types=context.sparse_types,
+          context_ragged_value_types=context.ragged_value_types,
+          context_ragged_split_types=context.ragged_split_types,
+          feature_list_dense_types=feature_list.dense_types,
+          feature_list_sparse_types=feature_list.sparse_types,
+          feature_list_ragged_value_types=feature_list.ragged_value_types,
+          feature_list_ragged_split_types=feature_list.ragged_split_types,
+          context_dense_shapes=context.dense_shapes_as_proto,
+          feature_list_dense_shapes=feature_list.dense_shapes,
+          name=name)
+      (context_sparse_indices, context_sparse_values, context_sparse_shapes,
+       context_dense_values, context_ragged_values, context_ragged_row_splits,
+       feature_list_sparse_indices, feature_list_sparse_values,
+       feature_list_sparse_shapes, feature_list_dense_values,
+       feature_list_dense_lengths, feature_list_ragged_values,
+       feature_list_ragged_outer_splits,
+       feature_list_ragged_inner_splits) = outputs
+      # pylint: disable=protected-access
+      context_ragged_tensors = parsing_config._build_ragged_tensors(
+          serialized.shape, context_ragged_values, context_ragged_row_splits)
+      feature_list_ragged_tensors = parsing_config._build_ragged_tensors(
+          serialized.shape, feature_list_ragged_values,
+          feature_list_ragged_outer_splits, feature_list_ragged_inner_splits)
+    else:
+      outputs = gen_parsing_ops.parse_sequence_example(
+          serialized=serialized,
+          debug_name=debug_name,
+          Ncontext_sparse=len(context.sparse_keys),
+          Ncontext_dense=len(context.dense_keys),
+          Nfeature_list_sparse=len(feature_list.sparse_keys),
+          Nfeature_list_dense=len(feature_list.dense_keys),
+          context_dense_defaults=context.dense_defaults_vec,
+          context_sparse_keys=context.sparse_keys,
+          context_sparse_types=context.sparse_types,
+          context_dense_keys=context.dense_keys,
+          context_dense_shapes=context.dense_shapes_as_proto,
+          feature_list_sparse_keys=feature_list.sparse_keys,
+          feature_list_sparse_types=feature_list.sparse_types,
+          feature_list_dense_keys=feature_list.dense_keys,
+          feature_list_dense_types=feature_list.dense_types,
+          feature_list_dense_shapes=feature_list.dense_shapes,
+          feature_list_dense_missing_assumed_empty=(
+              feature_list_dense_missing_assumed_empty),
+          name=name)
 
-    (context_sparse_indices, context_sparse_values, context_sparse_shapes,
-     context_dense_values, feature_list_sparse_indices,
-     feature_list_sparse_values, feature_list_sparse_shapes,
-     feature_list_dense_values, feature_list_dense_lengths) = outputs
+      (context_sparse_indices, context_sparse_values, context_sparse_shapes,
+       context_dense_values, feature_list_sparse_indices,
+       feature_list_sparse_values, feature_list_sparse_shapes,
+       feature_list_dense_values, feature_list_dense_lengths) = outputs
+      context_ragged_tensors = []
+      feature_list_ragged_tensors = []
 
     context_sparse_tensors = [
         sparse_tensor.SparseTensor(ix, val, shape)
@@ -713,19 +780,21 @@ def _parse_sequence_example_raw(serialized,
     ]
 
     context_output = dict(
-        zip(context.sparse_keys + context.dense_keys,
-            context_sparse_tensors + context_dense_values))
+        zip(
+            context.sparse_keys + context.dense_keys + context.ragged_keys,
+            context_sparse_tensors + context_dense_values +
+            context_ragged_tensors))
     feature_list_output = dict(
-        zip(feature_list.sparse_keys + feature_list.dense_keys,
-            feature_list_sparse_tensors + feature_list_dense_values))
+        zip(
+            feature_list.sparse_keys + feature_list.dense_keys +
+            feature_list.ragged_keys, feature_list_sparse_tensors +
+            feature_list_dense_values + feature_list_ragged_tensors))
     feature_list_lengths = dict(
         zip(feature_list.dense_keys, feature_list_dense_lengths))
 
     return (context_output, feature_list_output, feature_list_lengths)
 
 
-# TODO(sundberg): rewrite this method to call the batch version, which is more
-# efficient especially for large inputs.
 @tf_export("io.parse_single_sequence_example",
            v1=["io.parse_single_sequence_example",
                "parse_single_sequence_example"])
@@ -755,17 +824,19 @@ def parse_single_sequence_example(
   of `sequence_features` values may vary between `SequenceExample` protos,
   and even between `feature_list` keys within the same `SequenceExample`.
 
-  `context_features` contains `VarLenFeature` and `FixedLenFeature` objects.
-  Each `VarLenFeature` is mapped to a `SparseTensor`, and each `FixedLenFeature`
+  `context_features` contains `VarLenFeature`, `RaggedFeature`, and
+  `FixedLenFeature` objects. Each `VarLenFeature` is mapped to a `SparseTensor`;
+  each `RaggedFeature` is mapped to a `RaggedTensor`; and each `FixedLenFeature`
   is mapped to a `Tensor`, of the specified type, shape, and default value.
 
-  `sequence_features` contains `VarLenFeature` and `FixedLenSequenceFeature`
-  objects. Each `VarLenFeature` is mapped to a `SparseTensor`, and each
+  `sequence_features` contains `VarLenFeature`, `RaggedFeature`, and
+  `FixedLenSequenceFeature` objects. Each `VarLenFeature` is mapped to a
+  `SparseTensor`; each `RaggedFeature` is mapped to a `RaggedTensor`; and each
   `FixedLenSequenceFeature` is mapped to a `Tensor`, each of the specified type.
-  The shape will be `(T,) + df.dense_shape` for `FixedLenSequenceFeature` `df`, where
-  `T` is the length of the associated `FeatureList` in the `SequenceExample`.
-  For instance, `FixedLenSequenceFeature([])` yields a scalar 1-D `Tensor` of
-  static shape `[None]` and dynamic shape `[T]`, while
+  The shape will be `(T,) + df.dense_shape` for `FixedLenSequenceFeature` `df`,
+  where `T` is the length of the associated `FeatureList` in the
+  `SequenceExample`. For instance, `FixedLenSequenceFeature([])` yields a scalar
+  1-D `Tensor` of static shape `[None]` and dynamic shape `[T]`, while
   `FixedLenSequenceFeature([k])` (for `int k >= 1`) yields a 2-D matrix `Tensor`
   of static shape `[None, k]` and dynamic shape `[T, k]`.
 
@@ -790,20 +861,22 @@ def parse_single_sequence_example(
     serialized: A scalar (0-D Tensor) of type string, a single binary
       serialized `SequenceExample` proto.
     context_features: A `dict` mapping feature keys to `FixedLenFeature` or
-      `VarLenFeature` values. These features are associated with a
-      `SequenceExample` as a whole.
+      `VarLenFeature` or `RaggedFeature` values. These features are associated
+      with a `SequenceExample` as a whole.
     sequence_features: A `dict` mapping feature keys to
-      `FixedLenSequenceFeature` or `VarLenFeature` values. These features are
-      associated with data within the `FeatureList` section of the
-      `SequenceExample` proto.
+      `FixedLenSequenceFeature` or `VarLenFeature` or `RaggedFeature` values.
+      These features are associated with data within the `FeatureList` section
+      of the `SequenceExample` proto.
     example_name: A scalar (0-D Tensor) of strings (optional), the name of
       the serialized proto.
     name: A name for this operation (optional).
 
   Returns:
-    A tuple of two `dict`s, each mapping keys to `Tensor`s and `SparseTensor`s.
-    The first dict contains the context key/values.
-    The second dict contains the feature_list key/values.
+    A tuple of two `dict`s, each mapping keys to `Tensor`s and `SparseTensor`s
+    and `RaggedTensor`s.
+
+    * The first dict contains the context key/values.
+    * The second dict contains the feature_list key/values.
 
   Raises:
     ValueError: if any feature is invalid.
@@ -812,13 +885,26 @@ def parse_single_sequence_example(
   if not (context_features or sequence_features):
     raise ValueError("Missing features.")
   context_params = _ParseOpParams.from_features(
-      context_features, [VarLenFeature, FixedLenFeature])
+      context_features, [VarLenFeature, FixedLenFeature, RaggedFeature])
   feature_list_params = _ParseOpParams.from_features(
-      sequence_features, [VarLenFeature, FixedLenSequenceFeature])
+      sequence_features,
+      [VarLenFeature, FixedLenSequenceFeature, RaggedFeature])
 
-  return _parse_single_sequence_example_raw(serialized, context_params,
-                                            feature_list_params, example_name,
-                                            name)
+  with ops.name_scope(name, "ParseSingleSequenceExample",
+                      [serialized, example_name]):
+    context_output, feature_list_output = (
+        _parse_single_sequence_example_raw(serialized, context_params,
+                                           feature_list_params, example_name,
+                                           name))
+
+    if context_params.ragged_keys:
+      context_output = _construct_tensors_for_composite_features(
+          context_features, context_output)
+    if feature_list_params.ragged_keys:
+      feature_list_output = _construct_tensors_for_composite_features(
+          sequence_features, feature_list_output)
+
+    return context_output, feature_list_output
 
 
 def _parse_single_sequence_example_raw(serialized,
@@ -847,6 +933,14 @@ def _parse_single_sequence_example_raw(serialized,
   Raises:
     TypeError: if feature_list.dense_defaults is not either None or a dict.
   """
+  has_ragged = context.ragged_keys or feature_list.ragged_keys
+  if compat.forward_compatible(2019, 10, 26) or has_ragged:
+    with ops.name_scope(name, "ParseSingleExample", [serialized, debug_name]):
+      serialized = ops.convert_to_tensor(serialized, name="serialized")
+      serialized = _assert_scalar(serialized, "serialized")
+    return _parse_sequence_example_raw(serialized, debug_name, context,
+                                       feature_list, name)[:2]
+
   if context.num_features + feature_list.num_features == 0:
     raise ValueError("Must provide at least one feature key")
   with ops.name_scope(name, "ParseSingleSequenceExample", [serialized]):
@@ -1214,9 +1308,11 @@ def _assert_scalar(value, name):
         math_ops.equal(array_ops.rank(value), 0),
         ["Input %s must be a scalar" % name],
         name="%sIsScalar" % name.capitalize())
-    return control_flow_ops.with_dependencies([check],
-                                              value,
-                                              name="%sDependencies" % name)
+    result = control_flow_ops.with_dependencies([check],
+                                                value,
+                                                name="%sDependencies" % name)
+    result.set_shape([])
+    return result
   elif value_rank == 0:
     return value
   else:
diff --git a/tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt b/tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt
index c8f41d8b533..f202f3ef4ae 100644
--- a/tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt
+++ b/tensorflow/tools/api/golden/v1/tensorflow.raw_ops.pbtxt
@@ -2556,6 +2556,10 @@ tf_module {
     name: "ParseSequenceExample"
     argspec: "args=[\'serialized\', \'debug_name\', \'context_dense_defaults\', \'feature_list_dense_missing_assumed_empty\', \'context_sparse_keys\', \'context_dense_keys\', \'feature_list_sparse_keys\', \'feature_list_dense_keys\', \'Ncontext_sparse\', \'Ncontext_dense\', \'Nfeature_list_sparse\', \'Nfeature_list_dense\', \'context_sparse_types\', \'feature_list_dense_types\', \'context_dense_shapes\', \'feature_list_sparse_types\', \'feature_list_dense_shapes\', \'name\'], varargs=None, keywords=None, defaults=[\'0\', \'0\', \'0\', \'0\', \'[]\', \'[]\', \'[]\', \'[]\', \'[]\', \'None\'], "
   }
+  member_method {
+    name: "ParseSequenceExampleV2"
+    argspec: "args=[\'serialized\', \'debug_name\', \'context_sparse_keys\', \'context_dense_keys\', \'context_ragged_keys\', \'feature_list_sparse_keys\', \'feature_list_dense_keys\', \'feature_list_ragged_keys\', \'feature_list_dense_missing_assumed_empty\', \'context_dense_defaults\', \'Ncontext_sparse\', \'context_sparse_types\', \'context_ragged_value_types\', \'context_ragged_split_types\', \'context_dense_shapes\', \'Nfeature_list_sparse\', \'Nfeature_list_dense\', \'feature_list_dense_types\', \'feature_list_sparse_types\', \'feature_list_ragged_value_types\', \'feature_list_ragged_split_types\', \'feature_list_dense_shapes\', \'name\'], varargs=None, keywords=None, defaults=[\'0\', \'[]\', \'[]\', \'[]\', \'[]\', \'0\', \'0\', \'[]\', \'[]\', \'[]\', \'[]\', \'[]\', \'None\'], "
+  }
   member_method {
     name: "ParseSingleExample"
     argspec: "args=[\'serialized\', \'dense_defaults\', \'num_sparse\', \'sparse_keys\', \'dense_keys\', \'sparse_types\', \'dense_shapes\', \'name\'], varargs=None, keywords=None, defaults=[\'None\'], "
diff --git a/tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt b/tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt
index c8f41d8b533..f202f3ef4ae 100644
--- a/tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt
+++ b/tensorflow/tools/api/golden/v2/tensorflow.raw_ops.pbtxt
@@ -2556,6 +2556,10 @@ tf_module {
     name: "ParseSequenceExample"
     argspec: "args=[\'serialized\', \'debug_name\', \'context_dense_defaults\', \'feature_list_dense_missing_assumed_empty\', \'context_sparse_keys\', \'context_dense_keys\', \'feature_list_sparse_keys\', \'feature_list_dense_keys\', \'Ncontext_sparse\', \'Ncontext_dense\', \'Nfeature_list_sparse\', \'Nfeature_list_dense\', \'context_sparse_types\', \'feature_list_dense_types\', \'context_dense_shapes\', \'feature_list_sparse_types\', \'feature_list_dense_shapes\', \'name\'], varargs=None, keywords=None, defaults=[\'0\', \'0\', \'0\', \'0\', \'[]\', \'[]\', \'[]\', \'[]\', \'[]\', \'None\'], "
   }
+  member_method {
+    name: "ParseSequenceExampleV2"
+    argspec: "args=[\'serialized\', \'debug_name\', \'context_sparse_keys\', \'context_dense_keys\', \'context_ragged_keys\', \'feature_list_sparse_keys\', \'feature_list_dense_keys\', \'feature_list_ragged_keys\', \'feature_list_dense_missing_assumed_empty\', \'context_dense_defaults\', \'Ncontext_sparse\', \'context_sparse_types\', \'context_ragged_value_types\', \'context_ragged_split_types\', \'context_dense_shapes\', \'Nfeature_list_sparse\', \'Nfeature_list_dense\', \'feature_list_dense_types\', \'feature_list_sparse_types\', \'feature_list_ragged_value_types\', \'feature_list_ragged_split_types\', \'feature_list_dense_shapes\', \'name\'], varargs=None, keywords=None, defaults=[\'0\', \'[]\', \'[]\', \'[]\', \'[]\', \'0\', \'0\', \'[]\', \'[]\', \'[]\', \'[]\', \'[]\', \'None\'], "
+  }
   member_method {
     name: "ParseSingleExample"
     argspec: "args=[\'serialized\', \'dense_defaults\', \'num_sparse\', \'sparse_keys\', \'dense_keys\', \'sparse_types\', \'dense_shapes\', \'name\'], varargs=None, keywords=None, defaults=[\'None\'], "

commit b73b36714d316b3dc79db6b6b6dae206d8dcf05f
Author: Sanjoy Das <sanjoy@google.com>
Date:   Tue Apr 16 13:50:07 2019 -0700

    Refactor to make MarkForCompilationPass more hackable:
    
    1. Put more information in the Cluster class.  E.g. things like
       effective_cluster_size and has_functional_control_flow logically belong to
       Cluster instances.
    
    2. Use pointers to Cluster instead of Cluster values in the UnionFind data
       structure.  Besides being a minor efficiency fix, this fixes a gotcha that we
       used to have:
    
         UnionFind<Cluster>* cluster_a = ...;
         UnionFind<Cluster>* cluster_b = ...;
         Cluster* cluster_instance_a = &cluster_a->Get();
         // ...
         cluster_a->Merge(cluster_b);
    
         // We now have to remember to do this if we want to continue using
         // cluster_instance_a.
         cluster_instance_a = &cluster_a->Get();
    
       In general UnionFind<T> is easier to use if we can use T as a value type.
    
    3. Remove isolated_nodes_.  Putting nodes in isolated_nodes_ is equivalent to
    not putting them in compilation_candidates_.
    
    4. Map nodes that are not candidates to nullptr Cluster* instances to make bugs
    in this area more obvious.
    
    I did not investigate the change to the test case in depth because the new
    clustering produced after this CL is also correct (so the change is probably
    caused by a difference in iteration order or something like that).
    
    PiperOrigin-RevId: 243872193

diff --git a/tensorflow/compiler/jit/mark_for_compilation_pass.cc b/tensorflow/compiler/jit/mark_for_compilation_pass.cc
index 50f16a344ee..61f5a5c732d 100644
--- a/tensorflow/compiler/jit/mark_for_compilation_pass.cc
+++ b/tensorflow/compiler/jit/mark_for_compilation_pass.cc
@@ -54,6 +54,7 @@ limitations under the License.
 namespace tensorflow {
 
 namespace {
+using DeadnessPredicate = DeadnessAnalysis::DeadnessPredicate;
 using xla::StatusOr;
 
 bool HasResourceOutput(const Node& node) {
@@ -417,28 +418,85 @@ class MarkForCompilationPassImpl {
   Status Run();
 
  private:
-  struct Cluster {
-    // Identifies the node that represents this cluster in the cycle detection
-    // graph.
-    int representative = -1;
+  // Represents a "cluster" or a connected subgraph of a TensorFlow graph.
+  class Cluster {
+   public:
+    // Constructs a trivial cluster representing a single TF node.
+    Cluster(int tf_graph_node_id, int effective_cluster_size,
+            bool has_functional_control_flow,
+            absl::flat_hash_set<string> devices, string resource_op_device,
+            absl::optional<DeadnessPredicate> deadness_predicate,
+            bool is_xla_compile_attr_true, absl::optional<string> xla_scope)
+        : cycles_graph_node_id_(tf_graph_node_id),
+          effective_cluster_size_(effective_cluster_size),
+          has_functional_control_flow_(has_functional_control_flow),
+          devices_(std::move(devices)),
+          resource_op_device_(std::move(resource_op_device)),
+          deadness_predicate_(deadness_predicate),
+          is_xla_compile_attr_true_(is_xla_compile_attr_true),
+          xla_scope_(std::move(xla_scope)) {}
+
+    // Merges `other` into this cluster, and clears `other`.  This method is
+    // closely tied with the implementation of `MarkForCompilationPassImpl`.
+    void Merge(Cluster* other);
+
+    // If this is a trivial cluster containing only one node then return the ID
+    // of that node.  May not be called otherwise.
+    int GetIdOfOnlyNode() const {
+      DCHECK_EQ(cluster_size(), 1);
+      return cycles_graph_node_id();
+    }
 
-    // The set of devices the nodes in this cluster are placed on.
-    absl::flat_hash_set<string> devices;
+    // The number of TF nodes in this cluster.
+    int cluster_size() const { return cluster_size_; }
 
-    // If there are resource operation in the cluster then this is the device
-    // that resource operations are placed on.  All resource operations in a
-    // cluster must be placed on the same device.
-    string resource_op_device;
+    // The ID of the cluster as represented in `cycles_graph_`.
+    int cycles_graph_node_id() const { return cycles_graph_node_id_; }
+
+    // The size of the cluster excluding constant and identity nodes.
+    int effective_cluster_size() const { return effective_cluster_size_; }
+
+    // True if the cluster has functional control flow like `If` and `While`.
+    bool has_functional_control_flow() const {
+      return has_functional_control_flow_;
+    }
 
-    // If set then it is a predicate that is true iff the cluster is alive
-    // (clusters are alive or dead as a single unit).  If unset we've decided to
-    // (unsafely) ignore deadness analysis because the user asked us to.  If
-    // this is unset on a single Cluster instance then it is unset on all
-    // Cluster instances.
-    absl::optional<DeadnessAnalysis::DeadnessPredicate> deadness_predicate;
+    // The set of devices nodes in the cluster are placed on.
+    const absl::flat_hash_set<string>& devices() const { return devices_; }
 
-    // True if any node in the cluster has an _XlaCompile attribute set to true.
-    bool has_xla_compile_attr;
+    // If the cluster has a resource operation then the device the resource
+    // operation is placed on.  A cluster may have resource ops placed only on a
+    // single device.
+    const string& resource_op_device() const { return resource_op_device_; }
+
+    // If not nullopt the a predicate that is true iff the cluster is alive.
+    // Otherwise the user has (unsafely) disabled deadness analysis.  If this is
+    // unset on a single Cluster instance then it is unset on all Cluster
+    // instances.
+    const absl::optional<DeadnessPredicate>& deadness_predicate() const {
+      return deadness_predicate_;
+    }
+
+    // If true then the cluster has a XlaCompile=true attribute on one of its
+    // nodes.
+    bool is_xla_compile_attr_true() const { return is_xla_compile_attr_true_; }
+
+    // If not nullopt then the all nodes in the cluster either do not have the
+    // XlaScope attribute set or have it set to the value returned.
+    const absl::optional<string>& xla_scope() const { return xla_scope_; }
+
+   private:
+    int cluster_size_ = 1;
+    int cycles_graph_node_id_;
+    int effective_cluster_size_;
+    bool has_functional_control_flow_;
+    absl::flat_hash_set<string> devices_;
+    string resource_op_device_;
+    absl::optional<DeadnessPredicate> deadness_predicate_;
+    bool is_xla_compile_attr_true_;
+    absl::optional<string> xla_scope_;
+
+    TF_DISALLOW_COPY_AND_ASSIGN(Cluster);
   };
 
   // ---------------------------------------------------------------------------
@@ -466,16 +524,14 @@ class MarkForCompilationPassImpl {
 
   // Tries to contract the edge from cluster `from` to cluster `to`.  Returns
   // true if successful.
-  StatusOr<bool> TryToContractEdge(const Cluster& from, int to);
+  StatusOr<bool> TryToContractEdge(Cluster* from, Cluster* to);
 
   // Tries to contract each edge from `cluster_from`.  Returns true as soon as a
   // single edge contraction is successful.  Returns true if no edges were
   // contracted.
   StatusOr<bool> TryToContractEdgeFrom(Cluster* cluster_from);
 
-  // Nodes that XLA can compile are put in `candidates_`.  Nodes put in
-  // `isolated_nodes_` must either be unclustered or be put in trivial
-  // single-node clusters.
+  // Nodes that XLA can compile are put in `compilation_candidates_`.
   Status FindCompilationCandidates();
 
   bool CompilationDisallowedByXlaCompileAttr(Node* node,
@@ -488,9 +544,8 @@ class MarkForCompilationPassImpl {
 
   StatusOr<bool> ShouldCompileCluster(const Cluster& cluster);
 
-  bool HasMismatchingXlaScope(Node* node_from, Node* node_to);
-
-  StatusOr<bool> ClusteringWillIntroduceInterDeviceDependency(int to);
+  StatusOr<bool> ClusteringWillIntroduceInterDeviceDependency(
+      const Cluster& to);
 
   // Returns true if the devices in `cluster_a` and `cluster_b` are compatible
   // and therefore not a hindrance for combining the two clusters into a larger
@@ -501,27 +556,112 @@ class MarkForCompilationPassImpl {
   void DumpPostClusteringGraphs();
   void VLogClusteringSummary();
 
+  Cluster* MakeNewCluster(int cycles_graph_node_id, int effective_cluster_size,
+                          bool has_functional_control_flow,
+                          absl::flat_hash_set<string> devices,
+                          string resource_op_device,
+                          absl::optional<DeadnessPredicate> deadness_predicate,
+                          bool is_xla_compile_attr_true,
+                          absl::optional<string> xla_scope) {
+    cluster_storage_.push_back(absl::make_unique<Cluster>(
+        cycles_graph_node_id, effective_cluster_size,
+        has_functional_control_flow, std::move(devices),
+        std::move(resource_op_device), deadness_predicate,
+        is_xla_compile_attr_true, xla_scope));
+    return cluster_storage_.back().get();
+  }
+
+  absl::optional<string> GetXlaScope(Node* n);
+
+  // Returns the cluster for node `n`.  If two nodes, N1 and N2, are placed in
+  // the same cluster by the clustering algorithm then this function will return
+  // the same Cluster instance for N1 and N2.
+  //
+  // Returns nullptr if `n` is not a compilation candidate.
+  Cluster* GetClusterForNode(Node* n) {
+    return cluster_for_node_[n->id()].Get();
+  }
+
+  // Returns the cluster for a node in `cycles_graph_`.  This uses the same
+  // underlying map because of how we set things up, but we can do an additional
+  // CHECK in this accessor.
+  //
+  // Returns nullptr if `node_id` is not a compilation candidate.
+  Cluster* GetClusterForCyclesGraphNode(int node_id) {
+    Cluster* cluster = cluster_for_node_[node_id].Get();
+    if (cluster) {
+      DCHECK_EQ(cluster->cycles_graph_node_id(), node_id);
+    }
+    return cluster;
+  }
+
+  // Merge the clusters `cluster_from` and `cluster_to`.  After this step the
+  // larger combined cluster is represented by `cluster_from`'s ID in
+  // `cycles_graph_`.
+  bool MergeClusters(Cluster* cluster_from, Cluster* cluster_to) {
+    int from = cluster_from->cycles_graph_node_id();
+    int to = cluster_to->cycles_graph_node_id();
+
+    if (!graph_cycles_.ContractEdge(from, to)) {
+      return false;
+    }
+
+    // Merge the clusters.
+    cluster_from->Merge(cluster_to);
+
+    // Merge the UnionFind<Cluster*>.
+    cluster_for_node_[from].Merge(&cluster_for_node_[to]);
+
+    return true;
+  }
+
   DebugOptions debug_options_;
   Graph* graph_;
   FunctionLibraryDefinition* flib_def_;
   Env* env_;
   OptimizerOptions::GlobalJitLevel global_jit_level_;
-  absl::flat_hash_map<int, bool> should_compile_cluster_cache_;
+  absl::flat_hash_map<const Cluster*, bool> should_compile_cluster_cache_;
   DeviceInfoCache device_info_cache_;
 
   bool initialized_ = false;
   bool edges_contracted_ = false;
   bool clusters_created_ = false;
 
-  std::vector<UnionFind<Cluster>> clusters_;
-  std::deque<UnionFind<Cluster>*> worklist_;
+  std::vector<std::unique_ptr<Cluster>> cluster_storage_;
+  std::vector<UnionFind<Cluster*>> cluster_for_node_;
+  std::deque<Cluster*> worklist_;
   GraphCycles graph_cycles_;
   OrderedNodeSet compilation_candidates_;
-  absl::flat_hash_set<Node*> isolated_nodes_;
   std::unique_ptr<DeadnessAnalysis> deadness_analysis_;
   int64 iteration_count_ = 0;
 };
 
+void MarkForCompilationPassImpl::Cluster::Merge(Cluster* other) {
+  // We keep our own cycles_graph_node_id_ to mirror what GraphCycles does.
+
+  // Clearing out data structures in `other` is just a memory saving
+  // optimization and not needed for correctness.
+
+  cluster_size_ += other->cluster_size_;
+  effective_cluster_size_ += other->effective_cluster_size_;
+  has_functional_control_flow_ |= other->has_functional_control_flow_;
+
+  for (string other_device : other->devices_) {
+    devices_.insert(other_device);
+  }
+  other->devices_.clear();
+
+  if (resource_op_device_.empty()) {
+    resource_op_device_ = std::move(other->resource_op_device_);
+  }
+
+  is_xla_compile_attr_true_ |= other->is_xla_compile_attr_true_;
+
+  if (!xla_scope_.has_value()) {
+    xla_scope_ = std::move(other->xla_scope_);
+  }
+}
+
 Status IgnoreResourceOpForSafetyAnalysis(DeviceInfoCache* device_info_cache,
                                          const Node& n, bool* ignore) {
   // If a resource operation is assigned to XLA_CPU or XLA_GPU explicitly then
@@ -602,11 +742,11 @@ Status MarkForCompilationPassImpl::RunEdgeContractionLoop() {
   // TODO(hpucha): Handle the case where kXlaClusterAttr is already set (for
   // example, from the Grappler fusion pass).
   while (!worklist_.empty()) {
-    UnionFind<Cluster>* cluster_from = worklist_.front();
+    Cluster* cluster_from = worklist_.front();
     worklist_.pop_front();
 
     TF_ASSIGN_OR_RETURN(bool contracted_one_edge,
-                        TryToContractEdgeFrom(&cluster_from->Get()));
+                        TryToContractEdgeFrom(cluster_from));
 
     if (contracted_one_edge) {
       worklist_.push_back(cluster_from);
@@ -627,28 +767,6 @@ Status MarkForCompilationPassImpl::CreateClusters() {
 
   static std::atomic<int64> cluster_sequence_num;
 
-  // Count the number of non-trivial elements in each cluster.
-  std::vector<int> effective_cluster_sizes(graph_->num_node_ids());
-
-  // has_functional_control_flow remembers if a cluster contains a functional
-  // control flow node.
-  std::vector<bool> has_functional_control_flow(graph_->num_node_ids());
-
-  for (const Node* n : compilation_candidates_) {
-    int cluster = clusters_[n->id()].Get().representative;
-    // We want clusters to be big enough that the benefit from XLA's
-    // optimizations offsets XLA related overhead (for instance we add some
-    // Switch/Merge nodes into the graph to implement lazy compilation).  To
-    // this end, we don't count Identity and Constant nodes because they do not
-    // enable interesting optimizations by themselves.
-    if (!n->IsIdentity() && !n->IsConstant()) {
-      effective_cluster_sizes[cluster]++;
-    }
-    if (n->type_string() == "While" || n->type_string() == "If") {
-      has_functional_control_flow[cluster] = true;
-    }
-  }
-
   // Names for each cluster.
   std::unordered_map<int, string> cluster_names;
 
@@ -663,37 +781,27 @@ Status MarkForCompilationPassImpl::CreateClusters() {
   //   only if compilation is enabled, otherwise there will be no such
   //   candidates).
   for (Node* n : compilation_candidates_) {
-    const Cluster& cluster = clusters_[n->id()].Get();
+    Cluster* cluster = GetClusterForNode(n);
     TF_ASSIGN_OR_RETURN(bool should_compile_cluster,
-                        ShouldCompileCluster(cluster));
+                        ShouldCompileCluster(*cluster));
     if (!should_compile_cluster) {
       continue;
     }
 
-    int cluster_repr = cluster.representative;
-
-    // Compile if the user marked this node _XlaCompile=true
-    bool compile_attr = false;
-    bool marked_for_compilation = false;
-    if (GetNodeAttr(n->attrs(), kXlaCompileAttr, &compile_attr).ok()) {
-      marked_for_compilation = compile_attr;
-    } else if (flib_def_->GetAttr(*n, kXlaCompileAttr, &compile_attr).ok()) {
-      marked_for_compilation = compile_attr;
-    }
-
     // We assume that functional If and While nodes have at least
     // min_cluster_size non-trivial nodes in them.  It would be more principled
     // to (recursively) verify this fact, but that's probably not worth the
     // trouble.
 
-    if (effective_cluster_sizes[cluster_repr] >=
-            debug_options_.min_cluster_size ||
-        has_functional_control_flow[cluster_repr] || marked_for_compilation) {
-      string& name = cluster_names[cluster_repr];
+    if (cluster->effective_cluster_size() >= debug_options_.min_cluster_size ||
+        cluster->has_functional_control_flow() ||
+        cluster->is_xla_compile_attr_true()) {
+      string& name = cluster_names[cluster->cycles_graph_node_id()];
 
       if (name.empty()) {
         name = absl::StrCat("cluster_", cluster_sequence_num++);
       }
+
       n->AddAttr(kXlaClusterAttr, name);
       n->AddAttr(kXlaAlreadyClustered, true);
       VLOG(3) << "Assigning node " << n->name() << " to cluster " << name;
@@ -717,9 +825,7 @@ Status MarkForCompilationPassImpl::DumpDebugInfo() {
 
 StatusOr<bool>
 MarkForCompilationPassImpl::ClusteringWillIntroduceInterDeviceDependency(
-    int to) {
-  const Cluster& cluster_to = clusters_[to].Get();
-
+    const Cluster& cluster_to) {
   // If any of the consumer's producers are on a different device, do not
   // cluster these nodes. This prevents other work on this device from being
   // delayed by work on other devices. We consider predecessors of the entire
@@ -730,16 +836,16 @@ MarkForCompilationPassImpl::ClusteringWillIntroduceInterDeviceDependency(
   //
   // TODO(b/117085735): We probably want to handle the reciprocal of this case
   // where a cluster is producing data for multiple devices.
-  for (const auto& in_id : graph_cycles_.Predecessors(to)) {
+  for (const auto& in_id :
+       graph_cycles_.Predecessors(cluster_to.cycles_graph_node_id())) {
     if (in_id >= graph_->num_node_ids()) {
       continue;
     }
 
-    Node* in = graph_->FindNodeId(in_id);
-    const Cluster& cluster_in = clusters_[in_id].Get();
-    if (IsCompilationCandidate(in)) {
+    const Cluster* cluster_in = GetClusterForCyclesGraphNode(in_id);
+    if (cluster_in) {
       TF_ASSIGN_OR_RETURN(bool devices_compatible,
-                          AreDevicesCompatible(cluster_to, cluster_in));
+                          AreDevicesCompatible(cluster_to, *cluster_in));
       if (!devices_compatible) {
         return true;
       }
@@ -749,8 +855,7 @@ MarkForCompilationPassImpl::ClusteringWillIntroduceInterDeviceDependency(
   return false;
 }
 
-bool MarkForCompilationPassImpl::HasMismatchingXlaScope(Node* node_from,
-                                                        Node* node_to) {
+absl::optional<string> MarkForCompilationPassImpl::GetXlaScope(Node* node) {
   // Look for an _XlaScope on both nodes.  If both nodes have a scope and the
   // scopes do not match, do not cluster along this edge. This restriction is
   // overridden if the global_jit_level_ is ON. If even one of the nodes lacks
@@ -759,49 +864,76 @@ bool MarkForCompilationPassImpl::HasMismatchingXlaScope(Node* node_from,
   // nodes marked with _XlaCompile=true to also have a _XlaScope property set
   // (and raise an error otherwise); but for now we don't do this.
   if (global_jit_level_ != OptimizerOptions::OFF) {
-    return false;
+    return absl::nullopt;
   }
 
-  string from_scope, to_scope;
-  return GetNodeAttr(node_from->attrs(), kXlaScopeAttr, &from_scope).ok() &&
-         GetNodeAttr(node_to->attrs(), kXlaScopeAttr, &to_scope).ok() &&
-         from_scope != to_scope;
+  string scope;
+  if (GetNodeAttr(node->attrs(), kXlaScopeAttr, &scope).ok()) {
+    return scope;
+  }
+
+  return absl::nullopt;
 }
 
 Status MarkForCompilationPassImpl::BuildInitialClusterSet() {
-  clusters_.resize(graph_->num_node_ids());
+  cluster_for_node_.resize(graph_->num_node_ids());
+  for (Node* node : graph_->nodes()) {
+    if (!IsCompilationCandidate(node)) {
+      cluster_for_node_[node->id()].Get() = nullptr;
+      continue;
+    }
 
-  for (Node* node : compilation_candidates_) {
-    Cluster* cluster = &clusters_[node->id()].Get();
-    cluster->representative = node->id();
+    // We want clusters to be big enough that the benefit from XLA's
+    // optimizations offsets XLA related overhead (for instance we add some
+    // Switch/Merge nodes into the graph to implement lazy compilation).  To
+    // this end, we don't count Identity and Constant nodes because they do not
+    // enable interesting optimizations by themselves.
+    int effective_cluster_size =
+        (node->IsIdentity() || node->IsConstant()) ? 0 : 1;
 
+    bool has_functional_control_flow =
+        node->type_string() == "While" || node->type_string() == "If";
+
+    absl::optional<DeadnessPredicate> deadness_predicate;
     if (deadness_analysis_) {
       TF_ASSIGN_OR_RETURN(
-          cluster->deadness_predicate,
+          deadness_predicate,
           deadness_analysis_->GetPredicateFor(node, Graph::kControlSlot));
     }
 
     const string& device = !node->assigned_device_name().empty()
                                ? node->assigned_device_name()
                                : node->requested_device();
+
+    string resource_op_device;
     if (HasResourceInput(*node) || HasResourceOutput(*node)) {
-      cluster->resource_op_device = device;
+      resource_op_device = device;
     }
 
-    cluster->has_xla_compile_attr = false;
+    bool is_xla_compile_attr_true = false;
 
     bool xla_compile_attr;
     if (GetNodeAttr(node->attrs(), kXlaCompileAttr, &xla_compile_attr).ok()) {
-      cluster->has_xla_compile_attr |= xla_compile_attr;
+      is_xla_compile_attr_true |= xla_compile_attr;
     }
 
     if (flib_def_->GetAttr(*node, kXlaCompileAttr, &xla_compile_attr).ok()) {
-      cluster->has_xla_compile_attr |= xla_compile_attr;
+      is_xla_compile_attr_true |= xla_compile_attr;
     }
 
-    cluster->devices.insert(device);
+    absl::flat_hash_set<string> devices;
+    devices.insert(device);
+
+    Cluster* new_cluster = MakeNewCluster(
+        /*cycles_graph_node_id=*/node->id(),
+        /*effective_cluster_size=*/effective_cluster_size,
+        /*has_functional_control_flow=*/has_functional_control_flow,
+        std::move(devices), std::move(resource_op_device), deadness_predicate,
+        /*is_xla_compile_attr_true=*/is_xla_compile_attr_true,
+        GetXlaScope(node));
 
-    worklist_.push_back(&clusters_[node->id()]);
+    worklist_.push_back(new_cluster);
+    cluster_for_node_[node->id()].Get() = new_cluster;
   }
 
   return Status::OK();
@@ -923,7 +1055,7 @@ Status MarkForCompilationPassImpl::FindCompilationCandidates() {
         if (!is_tensor_array_or_stack_op) {
           VLOG(2) << "Isolating " << node->name()
                   << ": must-be-constant stateful op";
-          isolated_nodes_.insert(node);
+          continue;
         }
       }
     }
@@ -979,85 +1111,52 @@ bool IsShapeConsumerOp(const Node& node) {
          node.type_string() == "Size";
 }
 
-StatusOr<bool> MarkForCompilationPassImpl::TryToContractEdge(
-    const Cluster& cluster_from, int to) {
-  Node* node_to = graph_->FindNodeId(to);
-  if (!IsCompilationCandidate(node_to)) {
-    return false;
-  }
-
-  const Cluster& cluster_to = clusters_[to].Get();
-  DCHECK(cluster_from.deadness_predicate.has_value() ==
-         cluster_to.deadness_predicate.has_value());
-  if (cluster_from.deadness_predicate != cluster_to.deadness_predicate) {
+StatusOr<bool> MarkForCompilationPassImpl::TryToContractEdge(Cluster* from,
+                                                             Cluster* to) {
+  DCHECK(from->deadness_predicate().has_value() ==
+         to->deadness_predicate().has_value());
+  if (from->deadness_predicate() != to->deadness_predicate()) {
     return false;
   }
 
   TF_ASSIGN_OR_RETURN(bool devices_compatible,
-                      AreDevicesCompatible(cluster_from, cluster_to));
+                      AreDevicesCompatible(*from, *to));
   if (!devices_compatible) {
     return false;
   }
 
-  if (isolated_nodes_.contains(node_to)) {
-    return false;
-  }
-
-  int from = cluster_from.representative;
-  Node* node_from = graph_->FindNodeId(from);
-
-  if (HasMismatchingXlaScope(node_from, node_to)) {
+  if (from->xla_scope().has_value() && to->xla_scope().has_value() &&
+      *from->xla_scope() != *to->xla_scope()) {
     return false;
   }
 
   // Ops that consume shapes cannot be the root of a cluster. This is an
   // optimization.
-  if (clusters_[from].Size() == 1 && IsShapeConsumerOp(*node_from)) {
+  if (from->cluster_size() == 1 &&
+      IsShapeConsumerOp(*graph_->FindNodeId(from->GetIdOfOnlyNode()))) {
     return false;
   }
 
   // Don't exceed the maximum cluster size.
-  if (clusters_[from].Size() + clusters_[to].Size() >
+  if (from->cluster_size() + to->cluster_size() >
       debug_options_.max_cluster_size) {
     return false;
   }
 
   TF_ASSIGN_OR_RETURN(bool will_introduce_cross_device_dependency,
-                      ClusteringWillIntroduceInterDeviceDependency(to));
+                      ClusteringWillIntroduceInterDeviceDependency(*to));
 
   if (will_introduce_cross_device_dependency) {
     return false;
   }
 
-  // If contracting the edge would create a cycle, bail out.  However, just
-  // because we can't merge the clusters now does not mean we won't be able
-  // to merge them in the future.  e.g., if we have edges 1->2, 2->3 and
-  // 1->3, we cannot contract edge 1->3. But if we first contract 1->2 then
-  // we can later contract 1->3.
-  return graph_cycles_.ContractEdge(from, to);
+  return MergeClusters(from, to);
 }
 
 StatusOr<bool> MarkForCompilationPassImpl::TryToContractEdgeFrom(
     Cluster* cluster_from) {
-  int from = cluster_from->representative;
-
-  Node* node_from = graph_->FindNodeId(from);
-  if (node_from->IsControlFlow()) {
-    // Control flow nodes aren't compilation candidates and should never
-    // appear.
-    return errors::Internal("Found control flow node in clustering worklist: ",
-                            node_from->type_string());
-  }
-
-  if (!IsCompilationCandidate(node_from)) {
-    return false;
-  }
-
-  if (isolated_nodes_.count(node_from)) {
-    return false;
-  }
-
-  for (int to : graph_cycles_.Successors(from)) {
+  for (int to :
+       graph_cycles_.Successors(cluster_from->cycles_graph_node_id())) {
     iteration_count_++;
     if (to >= graph_->num_node_ids()) {
       // Node is a fictitious node that is present only in the cycle detection
@@ -1065,27 +1164,17 @@ StatusOr<bool> MarkForCompilationPassImpl::TryToContractEdgeFrom(
       continue;
     }
 
-    TF_ASSIGN_OR_RETURN(bool contracted_edge,
-                        TryToContractEdge(*cluster_from, to));
-
-    if (!contracted_edge) {
+    Cluster* cluster_to = GetClusterForCyclesGraphNode(to);
+    if (!cluster_to) {
       continue;
     }
 
-    const Cluster& cluster_to = clusters_[to].Get();
+    TF_ASSIGN_OR_RETURN(bool contracted_edge,
+                        TryToContractEdge(cluster_from, cluster_to));
 
-    // Merge the clusters. ContractEdge uses 'from' as the number of the
-    // merged node, so make sure 'from' is the chosen representative.
-    cluster_from->devices.insert(cluster_to.devices.begin(),
-                                 cluster_to.devices.end());
-    if (!cluster_to.resource_op_device.empty()) {
-      cluster_from->resource_op_device = cluster_to.resource_op_device;
+    if (contracted_edge) {
+      return true;
     }
-
-    cluster_from->has_xla_compile_attr |= cluster_to.has_xla_compile_attr;
-    clusters_[from].Merge(&clusters_[to]);
-
-    return true;
   }
 
   return false;
@@ -1269,8 +1358,8 @@ void MarkForCompilationPassImpl::VLogClusteringSummary() {
 StatusOr<bool> MarkForCompilationPassImpl::AreDevicesCompatible(
     const Cluster& cluster_a, const Cluster& cluster_b) {
   std::vector<string> devices;
-  absl::c_remove_copy(cluster_a.devices, std::back_inserter(devices), "");
-  absl::c_remove_copy(cluster_b.devices, std::back_inserter(devices), "");
+  absl::c_remove_copy(cluster_a.devices(), std::back_inserter(devices), "");
+  absl::c_remove_copy(cluster_b.devices(), std::back_inserter(devices), "");
   absl::c_sort(devices);
 
   if (devices.empty()) {
@@ -1300,8 +1389,8 @@ StatusOr<bool> MarkForCompilationPassImpl::AreDevicesCompatible(
     return resource_op_device.empty() || resource_op_device == chosen_device;
   };
 
-  if (!resource_op_device_ok(cluster_a.resource_op_device) ||
-      !resource_op_device_ok(cluster_b.resource_op_device)) {
+  if (!resource_op_device_ok(cluster_a.resource_op_device()) ||
+      !resource_op_device_ok(cluster_b.resource_op_device())) {
     return false;
   }
 
@@ -1319,7 +1408,8 @@ StatusOr<bool> MarkForCompilationPassImpl::AreDevicesCompatible(
       << "chosen device = " << chosen_device << "; devices (" << devices.size()
       << ") = " << absl::StrJoin(devices, ", ");
 
-  return cluster_a.has_xla_compile_attr || cluster_b.has_xla_compile_attr ||
+  return cluster_a.is_xla_compile_attr_true() ||
+         cluster_b.is_xla_compile_attr_true() ||
          registration->autoclustering_policy ==
              XlaOpRegistry::AutoclusteringPolicy::kAlways ||
          (registration->autoclustering_policy ==
@@ -1331,7 +1421,7 @@ StatusOr<bool> MarkForCompilationPassImpl::AreDevicesCompatible(
 StatusOr<bool> MarkForCompilationPassImpl::ShouldCompileClusterImpl(
     const Cluster& cluster) {
   std::vector<string> devices;
-  absl::c_remove_copy(cluster.devices, std::back_inserter(devices), "");
+  absl::c_remove_copy(cluster.devices(), std::back_inserter(devices), "");
   absl::c_sort(devices);
 
   string chosen_device;
@@ -1348,7 +1438,7 @@ StatusOr<bool> MarkForCompilationPassImpl::ShouldCompileClusterImpl(
       << devices.size() << ") = " << absl::StrJoin(devices, ", ");
 
   bool should_compile =
-      cluster.has_xla_compile_attr ||
+      cluster.is_xla_compile_attr_true() ||
       registration->autoclustering_policy ==
           XlaOpRegistry::AutoclusteringPolicy::kAlways ||
       (registration->autoclustering_policy ==
@@ -1386,14 +1476,13 @@ StatusOr<bool> MarkForCompilationPassImpl::ShouldCompileClusterImpl(
 
 StatusOr<bool> MarkForCompilationPassImpl::ShouldCompileCluster(
     const Cluster& cluster) {
-  auto it = should_compile_cluster_cache_.find(cluster.representative);
+  auto it = should_compile_cluster_cache_.find(&cluster);
   if (it != should_compile_cluster_cache_.end()) {
     return it->second;
   }
 
   TF_ASSIGN_OR_RETURN(bool should_compile, ShouldCompileClusterImpl(cluster));
-  should_compile_cluster_cache_.insert(
-      {cluster.representative, should_compile});
+  should_compile_cluster_cache_.insert({&cluster, should_compile});
   return should_compile;
 }
 
diff --git a/tensorflow/compiler/jit/mark_for_compilation_pass_test.cc b/tensorflow/compiler/jit/mark_for_compilation_pass_test.cc
index 295492ff79d..fb6fccee9c1 100644
--- a/tensorflow/compiler/jit/mark_for_compilation_pass_test.cc
+++ b/tensorflow/compiler/jit/mark_for_compilation_pass_test.cc
@@ -774,7 +774,7 @@ TEST(XlaCompilationTest, ChainOfOps) {
   ASSERT_EQ(cluster_sets.size(), 1);
 
   std::vector<string> expected_clustered_nodes_a = {
-      "AssignmentW1", "ConstN1", "ReadR0", "ValueToAssignW1"};
+      "AssignmentW1", "ConstN0", "ReadR0", "ValueToAssignW1"};
   ASSERT_EQ(cluster_sets[cluster_names[0]], expected_clustered_nodes_a);
 }
 

commit d1cdcae41be613830f5b681a4e041648cf2f02c6
Author: Uday Bondhugula <bondhugula@google.com>
Date:   Wed Jan 30 10:43:27 2019 -0800

    3000x speed improvement on compose-affine-maps by dropping NestedMatcher for
    a trivial inst walker :-) (reduces pass time from several minutes non-terminating to 120ms) - (fixes b/123541184)
    
    - use a simple 7-line inst walker to collect affine_apply op's instead of the nested
      matcher; -compose-affine-maps pass runs in 120ms now instead of 5 minutes + (non-
      terminating / out of memory) - on a realistic test case that is 20,000 lines 12-d
      loop nest
    
    - this CL is also pushing for simple existing/standard patterns unless there
      is a real efficiency issue (OTOH, fixing nested matcher to address this issue requires
      cl/231400521)
    
    - the improvement is from swapping out the nested walker as opposed to from a bug
      or anything else that this CL changes
    
    - update stale comment
    
    PiperOrigin-RevId: 231623619

diff --git a/lib/Transforms/ComposeAffineMaps.cpp b/lib/Transforms/ComposeAffineMaps.cpp
index 2457d868ae5..d7327d997c2 100644
--- a/lib/Transforms/ComposeAffineMaps.cpp
+++ b/lib/Transforms/ComposeAffineMaps.cpp
@@ -22,7 +22,6 @@
 //===----------------------------------------------------------------------===//
 
 #include "mlir/Analysis/AffineAnalysis.h"
-#include "mlir/Analysis/NestedMatcher.h"
 #include "mlir/IR/AffineMap.h"
 #include "mlir/IR/Attributes.h"
 #include "mlir/IR/Builders.h"
@@ -39,17 +38,19 @@ using namespace mlir;
 
 namespace {
 
-// ComposeAffineMaps walks inst blocks in a Function, and for each
-// AffineApplyOp, forward substitutes its results into any users which are
-// also AffineApplyOps. After forward subtituting its results, AffineApplyOps
-// with no remaining uses are collected and erased after the walk.
+// ComposeAffineMaps walks all affine apply op's in a function, and for each
+// such op, composes into it the results of any other AffineApplyOps - so
+// that all operands of the composed AffineApplyOp are guaranteed to be either
+// loop IVs or terminal symbols, (i.e., Values that are themselves not the
+// result of any AffineApplyOp). After this composition, AffineApplyOps with no
+// remaining uses are erased.
 // TODO(andydavis) Remove this when Chris adds instruction combiner pass.
-struct ComposeAffineMaps : public FunctionPass {
+struct ComposeAffineMaps : public FunctionPass, InstWalker<ComposeAffineMaps> {
   explicit ComposeAffineMaps() : FunctionPass(&ComposeAffineMaps::passID) {}
   PassResult runOnFunction(Function *f) override;
+  void visitOperationInst(OperationInst *opInst);
 
-  // Thread-safe RAII contexts local to pass, BumpPtrAllocator freed on exit.
-  NestedPatternContext MLContext;
+  SmallVector<OpPointer<AffineApplyOp>, 8> affineApplyOps;
 
   static char passID;
 };
@@ -67,30 +68,33 @@ static bool affineApplyOp(const Instruction &inst) {
   return opInst.isa<AffineApplyOp>();
 }
 
-PassResult ComposeAffineMaps::runOnFunction(Function *f) {
-  using matcher::Op;
+void ComposeAffineMaps::visitOperationInst(OperationInst *opInst) {
+  if (auto afOp = opInst->dyn_cast<AffineApplyOp>()) {
+    affineApplyOps.push_back(afOp);
+  }
+}
 
-  auto pattern = Op(affineApplyOp);
-  auto apps = pattern.match(f);
-  for (auto m : apps) {
-    auto app = cast<OperationInst>(m.first)->cast<AffineApplyOp>();
-    SmallVector<Value *, 8> operands(app->getOperands());
-    FuncBuilder b(m.first);
-    auto newApp = makeComposedAffineApply(&b, app->getLoc(),
-                                          app->getAffineMap(), operands);
-    app->replaceAllUsesWith(newApp);
+PassResult ComposeAffineMaps::runOnFunction(Function *f) {
+  // If needed for future efficiency, reserve space based on a pre-walk.
+  affineApplyOps.clear();
+  walk(f);
+  for (auto afOp : affineApplyOps) {
+    SmallVector<Value *, 8> operands(afOp->getOperands());
+    FuncBuilder b(afOp->getInstruction());
+    auto newAfOp = makeComposedAffineApply(&b, afOp->getLoc(),
+                                           afOp->getAffineMap(), operands);
+    afOp->replaceAllUsesWith(newAfOp);
   }
-  {
-    auto pattern = Op(affineApplyOp);
-    auto apps = pattern.match(f);
-    std::reverse(apps.begin(), apps.end());
-    for (auto m : apps) {
-      auto app = cast<OperationInst>(m.first)->cast<AffineApplyOp>();
-      if (app->use_empty()) {
-        m.first->erase();
-      }
+
+  // Erase dead affine apply ops.
+  affineApplyOps.clear();
+  walk(f);
+  for (auto it = affineApplyOps.rbegin(); it != affineApplyOps.rend(); ++it) {
+    if ((*it)->use_empty()) {
+      (*it)->erase();
     }
   }
+
   return success();
 }
 

commit c87da114fbdfd59a1b965b53f2044020a6621154
Author: Uday Bondhugula <bondhugula@google.com>
Date:   Sat Dec 29 15:51:30 2018 -0800

    Fix b/122139732; update FlatAffineConstraints::isEmpty() to eliminate IDs in a
    better order.
    
    - update isEmpty() to eliminate IDs in a better order. Speed improvement for
      complex cases (for eg. high-d reshape's involving mod's/div's).
    - minor efficiency update to projectOut (was earlier making an extra albeit
      benign call to gaussianEliminateIds) (NFC).
    - move getBestIdToEliminate further up in the file (NFC).
    - add the failing test case.
    - add debug info to checkMemRefAccessDependence.
    
    PiperOrigin-RevId: 227244634

diff --git a/lib/Analysis/AffineAnalysis.cpp b/lib/Analysis/AffineAnalysis.cpp
index 186b076af30..bfdfb1a79b4 100644
--- a/lib/Analysis/AffineAnalysis.cpp
+++ b/lib/Analysis/AffineAnalysis.cpp
@@ -30,7 +30,9 @@
 #include "mlir/Support/Functional.h"
 #include "mlir/Support/MathExtras.h"
 #include "llvm/ADT/DenseMap.h"
-#include "llvm/Support/raw_ostream.h"
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "affine-analysis"
 
 using namespace mlir;
 
@@ -1193,6 +1195,11 @@ bool mlir::checkMemrefAccessDependence(
     const MemRefAccess &srcAccess, const MemRefAccess &dstAccess,
     unsigned loopDepth, FlatAffineConstraints *dependenceConstraints,
     llvm::SmallVector<DependenceComponent, 2> *dependenceComponents) {
+  LLVM_DEBUG(llvm::dbgs() << "Checking for dependence at depth: "
+                          << Twine(loopDepth) << " between:\n";);
+  LLVM_DEBUG(srcAccess.opInst->dump(););
+  LLVM_DEBUG(dstAccess.opInst->dump(););
+
   // Return 'false' if these accesses do not acces the same memref.
   if (srcAccess.memref != dstAccess.memref)
     return false;
@@ -1269,10 +1276,14 @@ bool mlir::checkMemrefAccessDependence(
   if (dependenceConstraints->isEmpty()) {
     return false;
   }
+
   // Compute dependence direction vector and return true.
   if (dependenceComponents != nullptr) {
     computeDirectionVector(srcDomain, dstDomain, loopDepth,
                            dependenceConstraints, dependenceComponents);
   }
+
+  LLVM_DEBUG(llvm::dbgs() << "Dependence polyhedron:\n");
+  LLVM_DEBUG(dependenceConstraints->dump());
   return true;
 }
diff --git a/lib/Analysis/AffineStructures.cpp b/lib/Analysis/AffineStructures.cpp
index d4b8a05dbf8..11d0f170550 100644
--- a/lib/Analysis/AffineStructures.cpp
+++ b/lib/Analysis/AffineStructures.cpp
@@ -961,6 +961,40 @@ void FlatAffineConstraints::removeIdRange(unsigned idStart, unsigned idLimit) {
   // No resize necessary. numReservedCols remains the same.
 }
 
+/// Returns the position of the identifier that has the minimum <number of lower
+/// bounds> times <number of upper bounds> from the specified range of
+/// identifiers [start, end). It is often best to eliminate in the increasing
+/// order of these counts when doing Fourier-Motzkin elimination since FM adds
+/// that many new constraints.
+static unsigned getBestIdToEliminate(const FlatAffineConstraints &cst,
+                                     unsigned start, unsigned end) {
+  assert(start < cst.getNumIds() && end < cst.getNumIds() + 1);
+
+  auto getProductOfNumLowerUpperBounds = [&](unsigned pos) {
+    unsigned numLb = 0;
+    unsigned numUb = 0;
+    for (unsigned r = 0, e = cst.getNumInequalities(); r < e; r++) {
+      if (cst.atIneq(r, pos) > 0) {
+        ++numLb;
+      } else if (cst.atIneq(r, pos) < 0) {
+        ++numUb;
+      }
+    }
+    return numLb * numUb;
+  };
+
+  unsigned minLoc = start;
+  unsigned min = getProductOfNumLowerUpperBounds(start);
+  for (unsigned c = start + 1; c < end; c++) {
+    unsigned numLbUbProduct = getProductOfNumLowerUpperBounds(c);
+    if (numLbUbProduct < min) {
+      min = numLbUbProduct;
+      minLoc = c;
+    }
+  }
+  return minLoc;
+}
+
 // Checks for emptiness of the set by eliminating identifiers successively and
 // using the GCD test (on all equality constraints) and checking for trivially
 // invalid constraints. Returns 'true' if the constraint system is found to be
@@ -969,23 +1003,29 @@ bool FlatAffineConstraints::isEmpty() const {
   if (isEmptyByGCDTest() || hasInvalidConstraint())
     return true;
 
-  auto tmpCst = clone();
-  for (unsigned i = 0, e = tmpCst->getNumIds(); i < e; i++) {
+  // First, eliminate as many identifiers as possible using Gaussian
+  // elimination.
+  FlatAffineConstraints tmpCst(*this);
+  unsigned currentPos = 0;
+  while (currentPos < tmpCst.getNumIds()) {
+    tmpCst.gaussianEliminateIds(currentPos, tmpCst.getNumIds());
+    ++currentPos;
     // We check emptiness through trivial checks after eliminating each ID to
     // detect emptiness early. Since the checks isEmptyByGCDTest() and
     // hasInvalidConstraint() are linear time and single sweep on the constraint
     // buffer, this appears reasonable - but can optimize in the future.
-    if (tmpCst->gaussianEliminateId(0)) {
-      if (tmpCst->hasInvalidConstraint() || tmpCst->isEmptyByGCDTest())
-        return true;
-    } else {
-      tmpCst->FourierMotzkinEliminate(0);
-      // If the variable couldn't be eliminated by Gaussian, FM wouldn't have
-      // modified the equalities in any way. So no need to again run GCD test.
-      // Check for trivial invalid constraints.
-      if (tmpCst->hasInvalidConstraint())
-        return true;
-    }
+    if (tmpCst.hasInvalidConstraint() || tmpCst.isEmptyByGCDTest())
+      return true;
+  }
+
+  // Eliminate the remaining using FM.
+  for (unsigned i = 0, e = tmpCst.getNumIds(); i < e; i++) {
+    tmpCst.FourierMotzkinEliminate(
+        getBestIdToEliminate(tmpCst, 0, tmpCst.getNumIds()));
+    // FM wouldn't have modified the equalities in any way. So no need to again
+    // run GCD test. Check for trivial invalid constraints.
+    if (tmpCst.hasInvalidConstraint())
+      return true;
   }
   return false;
 }
@@ -1049,7 +1089,7 @@ void FlatAffineConstraints::GCDTightenInequalities() {
 unsigned FlatAffineConstraints::gaussianEliminateIds(unsigned posStart,
                                                      unsigned posLimit) {
   // Return if identifier positions to eliminate are out of range.
-  assert(posStart >= 0 && posLimit <= numIds);
+  assert(posLimit <= numIds);
   assert(hasConsistentState());
 
   if (posStart >= posLimit)
@@ -1869,45 +1909,11 @@ void FlatAffineConstraints::FourierMotzkinEliminate(
   LLVM_DEBUG(dump());
 }
 
-/// Returns the position of the identifier that has the minimum <number of lower
-/// bounds> times <number of upper bounds> from the specified range of
-/// identifiers [start, end). It is often best to eliminate in the increasing
-/// order of these counts when doing Fourier-Motzkin elimination since FM adds
-/// that many new constraints.
-static unsigned getBestIdToEliminate(const FlatAffineConstraints &cst,
-                                     unsigned start, unsigned end) {
-  assert(start < cst.getNumIds() && end < cst.getNumIds() + 1);
-
-  auto getProductOfNumLowerUpperBounds = [&](unsigned pos) {
-    unsigned numLb = 0;
-    unsigned numUb = 0;
-    for (unsigned r = 0, e = cst.getNumInequalities(); r < e; r++) {
-      if (cst.atIneq(r, pos) > 0) {
-        ++numLb;
-      } else if (cst.atIneq(r, pos) < 0) {
-        ++numUb;
-      }
-    }
-    return numLb * numUb;
-  };
-
-  unsigned minLoc = start;
-  unsigned min = getProductOfNumLowerUpperBounds(start);
-  for (unsigned c = start + 1; c < end; c++) {
-    unsigned numLbUbProduct = getProductOfNumLowerUpperBounds(c);
-    if (numLbUbProduct < min) {
-      min = numLbUbProduct;
-      minLoc = c;
-    }
-  }
-  return minLoc;
-}
-
 void FlatAffineConstraints::projectOut(unsigned pos, unsigned num) {
   if (num == 0)
     return;
 
-  // 'pos' can be at most getNumCols() - 2.
+  // 'pos' can be at most getNumCols() - 2 if num > 0.
   assert(pos <= getNumCols() - 2 && "invalid position");
   assert(pos + num < getNumCols() && "invalid range");
 
@@ -1915,17 +1921,14 @@ void FlatAffineConstraints::projectOut(unsigned pos, unsigned num) {
   unsigned currentPos = pos;
   unsigned numToEliminate = num;
   unsigned numGaussianEliminated = 0;
-  do {
+
+  while (currentPos < getNumIds()) {
     unsigned curNumEliminated =
         gaussianEliminateIds(currentPos, currentPos + numToEliminate);
-    if (curNumEliminated == 0) {
-      ++currentPos;
-      --numToEliminate;
-    } else {
-      numToEliminate -= curNumEliminated;
-    }
+    ++currentPos;
+    numToEliminate -= curNumEliminated + 1;
     numGaussianEliminated += curNumEliminated;
-  } while (numToEliminate != 0);
+  }
 
   // Eliminate the remaining using Fourier-Motzkin.
   for (unsigned i = 0; i < num - numGaussianEliminated; i++) {
diff --git a/test/Transforms/memref-dependence-check.mlir b/test/Transforms/memref-dependence-check.mlir
index a0ce93ee972..48663882d5a 100644
--- a/test/Transforms/memref-dependence-check.mlir
+++ b/test/Transforms/memref-dependence-check.mlir
@@ -698,4 +698,70 @@ mlfunc @mod_div_3d() {
   return
 }
 
-// TODO(bondhugula): add more test cases exercising mod/div affine_apply's.
+// -----
+// This test case arises in the context of a 6-d to 2-d reshape.
+// CHECK-LABEL: mlfunc @delinearize_mod_floordiv
+mlfunc @delinearize_mod_floordiv() {
+  %c0 = constant 0 : index
+  %val = constant 0 : i32
+  %in = alloc() : memref<2x2x3x3x16x1xi32>
+  %out = alloc() : memref<64x9xi32>
+
+  for %i0 = 0 to 2 {
+    for %i1 = 0 to 2 {
+      for %i2 = 0 to 3 {
+        for %i3 = 0 to 3 {
+          for %i4 = 0 to 16 {
+            for %i5 = 0 to 1 {
+              store %val, %in[%i0, %i1, %i2, %i3, %i4, %i5] : memref<2x2x3x3x16x1xi32>
+// expected-note@-1 {{dependence from 0 to 0 at depth 1 = false}}
+// expected-note@-2 {{dependence from 0 to 0 at depth 2 = false}}
+// expected-note@-3 {{dependence from 0 to 0 at depth 3 = false}}
+// expected-note@-4 {{dependence from 0 to 0 at depth 4 = false}}
+// expected-note@-5 {{dependence from 0 to 0 at depth 5 = false}}
+// expected-note@-6 {{dependence from 0 to 0 at depth 6 = false}}
+// expected-note@-7 {{dependence from 0 to 0 at depth 7 = false}}
+// expected-note@-8 {{dependence from 0 to 1 at depth 1 = true}}
+// expected-note@-9 {{dependence from 0 to 2 at depth 1 = false}}
+            }
+          }
+        }
+      }
+    }
+  }
+
+  for %ii = 0 to 64 {
+    for %jj = 0 to 9 {
+      %a0 = affine_apply (d0, d1) -> (d0 * (9 * 1024) + d1 * 128) (%ii, %jj)
+      %a1 = affine_apply (d0) ->
+        (d0 floordiv (2 * 3 * 3 * 128 * 128),
+        (d0 mod 294912) floordiv (3 * 3 * 128 * 128),
+        (((d0 mod 294912) mod 147456) floordiv 1152) floordiv 8,
+        (((d0 mod 294912) mod 147456) mod 1152) floordiv 384,
+        ((((d0 mod 294912) mod 147456) mod 1152) mod 384) floordiv 128,
+        (((((d0 mod 294912) mod 147456) mod 1152) mod 384) mod 128)
+          floordiv 128) (%a0)
+      %v0 = load %in[%a1#0, %a1#1, %a1#3, %a1#4, %a1#2, %a1#5] : memref<2x2x3x3x16x1xi32>
+// expected-note@-1 {{dependence from 1 to 0 at depth 1 = false}}
+// expected-note@-2 {{dependence from 1 to 1 at depth 1 = false}}
+// expected-note@-3 {{dependence from 1 to 1 at depth 2 = false}}
+// expected-note@-4 {{dependence from 1 to 1 at depth 3 = false}}
+// expected-note@-5 {{dependence from 1 to 2 at depth 1 = false}}
+// expected-note@-6 {{dependence from 1 to 2 at depth 2 = false}}
+// expected-note@-7 {{dependence from 1 to 2 at depth 3 = false}}
+// TODO(andydavis): the dep tester shouldn't be printing out these messages
+// below; they are redundant.
+      store %v0, %out[%ii, %jj] : memref<64x9xi32>
+// expected-note@-1 {{dependence from 2 to 0 at depth 1 = false}}
+// expected-note@-2 {{dependence from 2 to 1 at depth 1 = false}}
+// expected-note@-3 {{dependence from 2 to 1 at depth 2 = false}}
+// expected-note@-4 {{dependence from 2 to 1 at depth 3 = false}}
+// expected-note@-5 {{dependence from 2 to 2 at depth 1 = false}}
+// expected-note@-6 {{dependence from 2 to 2 at depth 2 = false}}
+// expected-note@-7 {{dependence from 2 to 2 at depth 3 = false}}
+    }
+  }
+  return
+}
+
+// TODO(bondhugula): add more test cases involving mod's/div's.

commit 2dcb0a07c88ce34196d8fe55ab8f415e537e1484
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Feb 6 10:56:19 2019 -0800

    New Timestamped BFCAllocator and GPUKernelTracker.
    
    The first part of this change extends BFCAllocator with an optional
    timing counter for recording the time at which each Chunk is freed.
    This has no effect for conventional memory management (as
    applied to CPU RAM), but can achieve a new behavior when applied
    to GPU RAM management.  The default TensorFlow memory allocation
    convention for GPU RAM is to Unref the tensors Ref'd by a GPU Op as
    soon as the Op has queued its kernel (and before that kernel is known
    to have completed execution).  This is safe if the memory is
    subsequently allocated to another GPU Op (the usual case) because that
    second Op will be sequential on the single GPU compute stream and
    hence won't touch the memory until the prior kernel has completed.
    But this practice is unsafe if the memory is used for I/O or for an Op
    queued on a different compute stream unless some further
    synchronization is inserted.
    
    Currently, I/O between a GPU and another device is made safe by
    inserting stream dependencies.  Multi-compute-stream computation is
    made safe by delaying the Unref of Ref'd tensors until the kernel is
    known to have completed, via callback through the GPU-specific
    EventMgr.  RDMA networking using GPUDirect is another difficult case
    where stream synchronization is not possible and it is necessary to
    wait until kernels are known to have completed before allowing
    reallocation of the used memory.
    
    Simply delaying the deallocation of memory until kernels are known to
    have completed is unsatisfactory because it substantially raises the
    high-water memory requirements of a program, drastically affecting the
    model architectures that are feasible on a particular GPU model.  The
    new freed-at count on BFCAllocator::Chunk is part of a strategy
    for maintaining the high-water size efficiency of our current
    single-compute-stream GPU memory allocation strategy while reducing
    synchronization stalls in I/O uses of GPU RAM.  In the future it
    may also be applied to multi-compute-stream execution.
    
    The key idea is that when a request to allocate GPU memory is made we
    can also pass along a 'freed-by' count and the allocator is free
    to return any Chunk whose freed_count is <= that threshold.
    This way we can continue to early-allocate GPU RAM without
    restrictions to GPU kernels to be executed on a single compute stream,
    while simultaneously satisfying the correctness constraints
    needed for off-stream use.
    
    GPUKernelTracker is the other component needed to make this new
    strategy work.  It keeps track of the stream queuing and real
    completion times of GPU kernels thus making it possible to pick the
    largest safe freed-by count when making a request for GPU memory
    that must be unemcumbered by other uses immediately.  A secondary
    capability of the GPUKernelTracker is that it enables capping the
    number of GPU kernels queued on a stream.  Without this cap some TF
    models can experience moments when hundreds of kernels are queued on
    the single compute stream.  Those queued but-not-executing kernels can
    tie up memory that could be used for other purposes before its really
    needed, and can delay I/O operations which are queued later and need
    to wait for the compute stream to clear, for safety.
    
    The new timestamped memory allocation strategy and pending-kernel
    capping are considered experimental features and default off for
    now, until more experience is gained.
    
    PiperOrigin-RevId: 232705088

diff --git a/tensorflow/core/BUILD b/tensorflow/core/BUILD
index cf212fc5d7c..4cf37a67585 100644
--- a/tensorflow/core/BUILD
+++ b/tensorflow/core/BUILD
@@ -2926,6 +2926,7 @@ tf_cuda_library(
 
 CORE_CPU_LIB_HEADERS = CORE_CPU_BASE_HDRS + [
     "common_runtime/allocator_retry.h",
+    "common_runtime/shared_counter.h",
     "common_runtime/base_collective_executor.h",
     "common_runtime/bfc_allocator.h",
     "common_runtime/hierarchical_tree_broadcaster.h",
diff --git a/tensorflow/core/common_runtime/bfc_allocator.cc b/tensorflow/core/common_runtime/bfc_allocator.cc
index 3843ea9e60c..c7e535cc808 100644
--- a/tensorflow/core/common_runtime/bfc_allocator.cc
+++ b/tensorflow/core/common_runtime/bfc_allocator.cc
@@ -18,6 +18,7 @@ limitations under the License.
 #include "tensorflow/core/common_runtime/bfc_allocator.h"
 
 #include "tensorflow/core/common_runtime/allocator_retry.h"
+#include "tensorflow/core/framework/device_base.h"
 #include "tensorflow/core/lib/core/bits.h"
 #include "tensorflow/core/lib/gtl/stl_util.h"
 #include "tensorflow/core/lib/strings/numbers.h"
@@ -152,6 +153,7 @@ bool BFCAllocator::Extend(size_t alignment, size_t rounded_bytes) {
   c->allocation_id = -1;
   c->prev = kInvalidChunkHandle;
   c->next = kInvalidChunkHandle;
+  c->freed_count = 0;
 
   region_manager_.set_handle(c->ptr, h);
 
@@ -180,29 +182,46 @@ void BFCAllocator::DeallocateChunk(ChunkHandle h) {
   free_chunks_list_ = h;
 }
 
-void* BFCAllocator::AllocateRaw(size_t unused_alignment, size_t num_bytes) {
+void* BFCAllocator::AllocateRawInternalWithRetry(
+    size_t unused_alignment, size_t num_bytes,
+    const AllocationAttributes& allocation_attr) {
   // Fast path: Try once to allocate without getting the retry_helper_ involved
-  void* r = AllocateRawInternal(unused_alignment, num_bytes, false);
+  uint64 freed_by_count = 0;
+  if (allocation_attr.freed_by_func != nullptr) {
+    freed_by_count = allocation_attr.freed_by_func();
+  }
+  void* r =
+      AllocateRawInternal(unused_alignment, num_bytes, false, freed_by_count);
   if (r != nullptr) {
     return r;
   } else {
     static const int64 kMaxMillisToWait = 10000;  // 10 seconds
-    return retry_helper_.AllocateRaw(
-        [this](size_t a, size_t nb, bool v) {
-          return AllocateRawInternal(a, nb, v);
+    r = retry_helper_.AllocateRaw(
+        [this, &allocation_attr](size_t a, size_t nb, bool v) {
+          uint64 freed_by_count = 0;
+          if (allocation_attr.freed_by_func != nullptr) {
+            freed_by_count = allocation_attr.freed_by_func();
+          }
+          return AllocateRawInternal(a, nb, v, freed_by_count);
         },
         kMaxMillisToWait, unused_alignment, num_bytes);
+    return r;
   }
 }
 
 void* BFCAllocator::AllocateRaw(size_t unused_alignment, size_t num_bytes,
                                 const AllocationAttributes& allocation_attr) {
+  VLOG(1) << "AllocateRaw " << Name() << "  " << num_bytes;
   if (allocation_attr.no_retry_on_failure) {
     // Return immediately upon the first failure if this is for allocating an
     // optional scratch space.
     bool dump_log_on_failure = VLOG_IS_ON(2);
-    void* result =
-        AllocateRawInternal(unused_alignment, num_bytes, dump_log_on_failure);
+    uint64 freed_by_count = 0;
+    if (allocation_attr.freed_by_func != nullptr) {
+      freed_by_count = allocation_attr.freed_by_func();
+    }
+    void* result = AllocateRawInternal(unused_alignment, num_bytes,
+                                       dump_log_on_failure, freed_by_count);
     if (result == nullptr) {
       static std::atomic<int32> log_counter{0};
       int32 counter_value = log_counter.load(std::memory_order_relaxed);
@@ -218,7 +237,8 @@ void* BFCAllocator::AllocateRaw(size_t unused_alignment, size_t num_bytes,
     }
     return result;
   } else {
-    return AllocateRaw(unused_alignment, num_bytes);
+    return AllocateRawInternalWithRetry(unused_alignment, num_bytes,
+                                        allocation_attr);
   }
 }
 
@@ -233,7 +253,8 @@ size_t BFCAllocator::RoundedBytes(size_t bytes) {
 
 void* BFCAllocator::AllocateRawInternal(size_t unused_alignment,
                                         size_t num_bytes,
-                                        bool dump_log_on_failure) {
+                                        bool dump_log_on_failure,
+                                        uint64 freed_before) {
   if (num_bytes == 0) {
     LOG(ERROR) << "tried to allocate 0 bytes";
     return nullptr;
@@ -247,14 +268,14 @@ void* BFCAllocator::AllocateRawInternal(size_t unused_alignment,
   BinNum bin_num = BinNumForSize(rounded_bytes);
 
   mutex_lock l(lock_);
-  void* ptr = FindChunkPtr(bin_num, rounded_bytes, num_bytes);
+  void* ptr = FindChunkPtr(bin_num, rounded_bytes, num_bytes, freed_before);
   if (ptr != nullptr) {
     return ptr;
   }
 
   // Try to extend
   if (Extend(unused_alignment, rounded_bytes)) {
-    ptr = FindChunkPtr(bin_num, rounded_bytes, num_bytes);
+    ptr = FindChunkPtr(bin_num, rounded_bytes, num_bytes, freed_before);
     if (ptr != nullptr) {
       return ptr;
     }
@@ -274,7 +295,7 @@ void* BFCAllocator::AllocateRawInternal(size_t unused_alignment,
 }
 
 void* BFCAllocator::FindChunkPtr(BinNum bin_num, size_t rounded_bytes,
-                                 size_t num_bytes) {
+                                 size_t num_bytes, uint64 freed_before) {
   // First identify the first bin that could satisfy rounded_bytes.
   for (; bin_num < kNumBins; bin_num++) {
     // Start searching from the first bin for the smallest chunk that fits
@@ -285,6 +306,9 @@ void* BFCAllocator::FindChunkPtr(BinNum bin_num, size_t rounded_bytes,
       const BFCAllocator::ChunkHandle h = (*citer);
       BFCAllocator::Chunk* chunk = ChunkFromHandle(h);
       DCHECK(!chunk->in_use());
+      if (freed_before > 0 && freed_before < chunk->freed_count) {
+        continue;
+      }
       if (chunk->size >= rounded_bytes) {
         // We found an existing chunk that fits us that wasn't in use, so remove
         // it from the free bin structure prior to using.
@@ -347,6 +371,9 @@ void BFCAllocator::SplitChunk(BFCAllocator::ChunkHandle h, size_t num_bytes) {
   // The new chunk is not in use.
   new_chunk->allocation_id = -1;
 
+  // It inherits the freed time.
+  new_chunk->freed_count = c->freed_count;
+
   // Maintain the pointers.
   // c <-> c_neighbor becomes
   // c <-> new_chunk <-> c_neighbor
@@ -415,6 +442,9 @@ void BFCAllocator::Merge(BFCAllocator::ChunkHandle h1,
   // Set the new size
   c1->size += c2->size;
 
+  // Pick latest free time.
+  c1->freed_count = std::max(c1->freed_count, c2->freed_count);
+
   DeleteChunk(h2);
 }
 
@@ -460,6 +490,11 @@ void BFCAllocator::FreeAndMaybeCoalesce(BFCAllocator::ChunkHandle h) {
   // Mark the chunk as no longer in use.
   c->allocation_id = -1;
 
+  // Optionally record the free time.
+  if (timing_counter_) {
+    c->freed_count = timing_counter_->next();
+  }
+
   // Updates the stats.
   stats_.bytes_in_use -= c->size;
 
@@ -630,7 +665,10 @@ void BFCAllocator::DumpMemoryLog(size_t num_bytes) {
         in_use_by_size[c->size]++;
       }
       LOG(INFO) << (c->in_use() ? "Chunk" : "Free ") << " at " << c->ptr
-                << " of size " << c->size;
+                << " of size " << c->size
+                << (timing_counter_
+                        ? strings::StrCat(" freed_count ", c->freed_count)
+                        : "");
       h = c->next;
     }
   }
diff --git a/tensorflow/core/common_runtime/bfc_allocator.h b/tensorflow/core/common_runtime/bfc_allocator.h
index 2d74bf2b286..261bacbaac7 100644
--- a/tensorflow/core/common_runtime/bfc_allocator.h
+++ b/tensorflow/core/common_runtime/bfc_allocator.h
@@ -23,6 +23,7 @@ limitations under the License.
 #include <vector>
 
 #include "tensorflow/core/common_runtime/allocator_retry.h"
+#include "tensorflow/core/common_runtime/shared_counter.h"
 #include "tensorflow/core/framework/allocator.h"
 #include "tensorflow/core/lib/gtl/stl_util.h"
 #include "tensorflow/core/lib/strings/strcat.h"
@@ -50,9 +51,14 @@ class BFCAllocator : public Allocator {
   ~BFCAllocator() override;
 
   string Name() override { return name_; }
-  void* AllocateRaw(size_t alignment, size_t num_bytes) override;
+
+  void* AllocateRaw(size_t alignment, size_t num_bytes) override {
+    return AllocateRaw(alignment, num_bytes, AllocationAttributes());
+  }
+
   void* AllocateRaw(size_t alignment, size_t num_bytes,
                     const AllocationAttributes& allocation_attr) override;
+
   void DeallocateRaw(void* ptr) override;
 
   bool TracksAllocationSizes() override;
@@ -67,11 +73,19 @@ class BFCAllocator : public Allocator {
 
   void ClearStats() override;
 
+  void SetTimingCounter(SharedCounter* sc) { timing_counter_ = sc; }
+
  private:
   struct Bin;
 
   void* AllocateRawInternal(size_t alignment, size_t num_bytes,
-                            bool dump_log_on_failure);
+                            bool dump_log_on_failure,
+                            uint64 freed_before_count);
+
+  void* AllocateRawInternalWithRetry(
+      size_t alignment, size_t num_bytes,
+      const AllocationAttributes& allocation_attr);
+
   void DeallocateRawInternal(void* ptr);
 
   // A ChunkHandle is an index into the chunks_ vector in BFCAllocator
@@ -126,6 +140,9 @@ class BFCAllocator : public Allocator {
     // What bin are we in?
     BinNum bin_num = kInvalidBinNum;
 
+    // Optional count when this chunk was most recently made free.
+    uint64 freed_count = 0;
+
     bool in_use() const { return allocation_id != -1; }
 
     string DebugString(BFCAllocator* a,
@@ -314,8 +331,8 @@ class BFCAllocator : public Allocator {
 
   // Returns a pointer to an underlying allocated chunk of size
   // 'rounded_bytes'.
-  void* FindChunkPtr(BinNum bin_num, size_t rounded_bytes, size_t num_bytes)
-      EXCLUSIVE_LOCKS_REQUIRED(lock_);
+  void* FindChunkPtr(BinNum bin_num, size_t rounded_bytes, size_t num_bytes,
+                     uint64 freed_before) EXCLUSIVE_LOCKS_REQUIRED(lock_);
 
   // Splits the chunk specified by 'h' into two chunks, one at least
   // of size 'num_bytes'.
@@ -420,6 +437,7 @@ class BFCAllocator : public Allocator {
 
   std::unique_ptr<SubAllocator> sub_allocator_;
   string name_;
+  SharedCounter* timing_counter_ = nullptr;
 
   // Structures mutable after construction
   mutable mutex lock_;
diff --git a/tensorflow/core/common_runtime/gpu/gpu_device.cc b/tensorflow/core/common_runtime/gpu/gpu_device.cc
index 010fdff4e90..80d221af361 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_device.cc
+++ b/tensorflow/core/common_runtime/gpu/gpu_device.cc
@@ -276,6 +276,28 @@ BaseGPUDevice::BaseGPUDevice(const SessionOptions& options, const string& name,
       sync_every_op_(sync_every_op),
       max_streams_(max_streams) {
   GPUProcessState::singleton()->EnableGPUDevice();
+  pending_cap_ = options.config.gpu_options().experimental().pending_cap();
+  timestamped_allocator_ =
+      options.config.gpu_options().experimental().timestamped_allocator();
+  if (timestamped_allocator_ || pending_cap_ > 0) {
+    std::unique_ptr<SharedCounter> timing_counter;
+    if (timestamped_allocator_) {
+      // In this case the SharedCounter was already created and set in the
+      // associated Allocator, with ownership by GPUProcessState.  Here we take
+      // over ownership of that SharedAllocator to transfer it to the
+      // GPUKernelTracker.
+      timing_counter =
+          GPUProcessState::singleton()->ReleaseGPUAllocatorCounter(tf_gpu_id);
+      DCHECK(timing_counter.get());
+    } else {
+      DCHECK_GT(pending_cap_, 0);
+      // In this case we need a SharedCounter to be owned by GPUKernelTracker
+      // but one was not created for use by the Allocator, so we create one.
+      timing_counter.reset(new SharedCounter);
+    }
+    kernel_tracker_.reset(
+        new GPUKernelTracker(Env::Default(), std::move(timing_counter)));
+  }
 }
 
 BaseGPUDevice::~BaseGPUDevice() {
@@ -508,6 +530,10 @@ void BaseGPUDevice::ComputeHelper(OpKernel* op_kernel,
       if (idc->stream() != stream) stream->ThenWaitFor(idc->stream());
     }
   }
+  if (pending_cap_ > 0) {
+    DCHECK(kernel_tracker_);
+    kernel_tracker_->PauseWhilePendingExceeds(pending_cap_);
+  }
   se::cuda::ScopedActivateExecutorContext scoped_activation{stream->parent()};
   op_kernel->Compute(context);
   if (context->status().ok()) {
@@ -525,6 +551,14 @@ void BaseGPUDevice::ComputeHelper(OpKernel* op_kernel,
       VLOG(1) << "GpuDevice::ComputeHelper scheduled "
               << ComputeOpKernelDebugString(*op_kernel, stream_id);
     }
+    if (kernel_tracker_) {
+      GPUKernelTracker* tracker = kernel_tracker_.get();
+      DCHECK(tracker);
+      uint64 queued_count = tracker->RecordQueued();
+      em_->ThenExecute(stream, [op_kernel, tracker, queued_count]() {
+        tracker->RecordTerminated(queued_count);
+      });
+    }
   } else {
     if (vlog_1) {
       VLOG(1) << "GpuDevice::ComputeHelper failed to schedule "
@@ -721,8 +755,8 @@ Status ParseVisibleDeviceList(const string& visible_device_list,
       if (!strings::safe_strto32(platform_gpu_id_str, &platform_gpu_id)) {
         return errors::InvalidArgument(
             "Could not parse entry in 'visible_device_list': '",
-            platform_gpu_id_str, "'. visible_device_list = ",
-            visible_device_list);
+            platform_gpu_id_str,
+            "'. visible_device_list = ", visible_device_list);
       }
       if (platform_gpu_id < 0 ||
           platform_gpu_id >= gpu_manager->VisibleDeviceCount()) {
@@ -957,15 +991,15 @@ Status BaseGPUDeviceFactory::CreateDevices(
     for (PlatformGpuId platform_gpu_id : valid_platform_gpu_ids) {
       err = cudaSetDevice(platform_gpu_id.value());
       if (err != cudaSuccess) {
-        return errors::Internal("cudaSetDevice() on GPU:",
-                                platform_gpu_id.value(), " failed. Status: ",
-                                cudaGetErrorString(err));
+        return errors::Internal(
+            "cudaSetDevice() on GPU:", platform_gpu_id.value(),
+            " failed. Status: ", cudaGetErrorString(err));
       }
       err = cudaFree(nullptr);
       if (err != cudaSuccess) {
         return errors::Internal("CUDA runtime implicit initialization on GPU:",
-                                platform_gpu_id.value(), " failed. Status: ",
-                                cudaGetErrorString(err));
+                                platform_gpu_id.value(),
+                                " failed. Status: ", cudaGetErrorString(err));
       }
     }
     // Reset to the original device.
@@ -1517,6 +1551,115 @@ Status BaseGPUDeviceFactory::GetValidDeviceIds(
   return Status::OK();
 }
 
+uint64 BaseGPUDevice::SafeAllocFrontier() {
+  if (timestamped_allocator_) {
+    return kernel_tracker_->LastTerminatedCount();
+  } else {
+    return 0;
+  }
+}
+
+int BaseGPUDevice::PendingKernels() {
+  if (kernel_tracker_) {
+    return kernel_tracker_->NumPending();
+  }
+  return 0;
+}
+
+uint64 GPUKernelTracker::RecordQueued() {
+  mutex_lock l(mu_);
+  uint64 queued_count = timing_counter_->next();
+  VLOG(2) << "RecordQueued queued_count=" << queued_count
+          << " first_available_=" << first_available_
+          << " last_completed_=" << last_completed_
+          << " num_pending_=" << num_pending_;
+  pending_kernels_[first_available_].queued_count = queued_count;
+  pending_kernels_[first_available_].terminated = false;
+  ++first_available_;
+  ++num_pending_;
+  if (first_available_ >= pending_kernels_.size()) {
+    first_available_ = 0;
+  }
+  if (first_available_ == last_completed_) {
+    // Ring buffer is full: double it.  All of the same valid PendingKernel
+    // entries exist after the copy, they are just shifted to begin
+    // at index 0 in the new array.
+    std::vector<PendingKernel> new_buffer(pending_kernels_.size() * 2);
+    for (int i = 0; i < pending_kernels_.size(); ++i) {
+      int j = (i + last_completed_) % pending_kernels_.size();
+      new_buffer[i] = pending_kernels_[j];
+    }
+    last_completed_ = 0;
+    first_available_ = pending_kernels_.size();
+    pending_kernels_.swap(new_buffer);
+    VLOG(1) << "last_completed_=" << last_completed_
+            << " first_available_=" << first_available_
+            << " num_pending_=" << num_pending_;
+  }
+  DCHECK_NE(first_available_, last_completed_) << "exhausted pending_kernels";
+  return queued_count;
+}
+
+void GPUKernelTracker::RecordTerminated(uint64 queued_count) {
+  mutex_lock l(mu_);
+  VLOG(2) << "RecordTerminated queued_count=" << queued_count
+          << " first_available_=" << first_available_
+          << " last_completed_=" << last_completed_
+          << " num_pending_=" << num_pending_ << " LC="
+          << ((last_completed_ >= 0)
+                  ? pending_kernels_[last_completed_].queued_count
+                  : -1);
+  DCHECK_NE(first_available_, last_completed_);
+  DCHECK_GT(num_pending_, 0);
+  // Starting just past the last completed entry, find the entry with
+  // this queued_count and mark it done.
+  int index = (last_completed_ + 1) % pending_kernels_.size();
+  while (true) {
+    if (index == first_available_) {
+      // This should never happen.
+      LOG(FATAL) << "Failed to find " << queued_count  // Crash OK
+                 << " in queue";
+    }
+    if (pending_kernels_[index].queued_count == queued_count) {
+      pending_kernels_[index].terminated = true;
+      break;
+    }
+    index = (index + 1) % pending_kernels_.size();
+  }
+  // Next move last_completed_ forward past all completed kernels.  In theory
+  // kernels should always complete in queued order so we should be able to
+  // advance the completed frontier to the last queued PendingKernel.  In
+  // practice we occassionally see the termination callbacks arrive out of order
+  // probably because of thread scheduling.  Eventually we may support out-of-
+  // order completion involving multple compute streams so here we follow a
+  // conservative approach and wait for every single callback to arrive before
+  // advancing the frontier.
+  while (true) {
+    int next_index = (last_completed_ + 1) % pending_kernels_.size();
+    if (next_index == first_available_) break;
+    if (pending_kernels_[next_index].terminated) {
+      last_completed_ = next_index;
+    } else {
+      break;
+    }
+  }
+  // Last decrease num_pending before maybe waking a waiter.
+  --num_pending_;
+  pending_decreased_.notify_one();
+}
+
+uint64 GPUKernelTracker::LastTerminatedCount() {
+  mutex_lock l(mu_);
+  if (last_completed_ < 0) {
+    // This is an edge case that can be encountered only at the beginning of
+    // execution.  There's not yet a safe threshold count. We don't want to
+    // return 0 since that bypasses the count mechanism in BFCAllocator, so
+    // return the least non-zero value.
+    return 1;
+  }
+  return pending_kernels_[last_completed_].queued_count;
+}
+
 }  // namespace tensorflow
 
 #endif  // GOOGLE_CUDA
diff --git a/tensorflow/core/common_runtime/gpu/gpu_device.h b/tensorflow/core/common_runtime/gpu/gpu_device.h
index d002d02c51d..33f0585b4d1 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_device.h
+++ b/tensorflow/core/common_runtime/gpu/gpu_device.h
@@ -34,6 +34,7 @@ limitations under the License.
 #include "tensorflow/core/common_runtime/gpu_device_context.h"
 #include "tensorflow/core/common_runtime/local_device.h"
 #include "tensorflow/core/common_runtime/scoped_allocator_mgr.h"
+#include "tensorflow/core/common_runtime/shared_counter.h"
 #include "tensorflow/core/framework/allocator.h"
 #include "tensorflow/core/framework/device_base.h"
 #include "tensorflow/core/framework/op_kernel.h"
@@ -46,6 +47,7 @@ limitations under the License.
 #include "tensorflow/core/public/session_options.h"
 
 namespace tensorflow {
+class GPUKernelTracker;
 
 class BaseGPUDevice : public LocalDevice {
  public:
@@ -114,6 +116,17 @@ class BaseGPUDevice : public LocalDevice {
     return scoped_allocator_mgr_.get();
   }
 
+  // The following two functions always return 0 unless one of the
+  // related experimental config options has been specified.
+
+  // If returned value is > 0 then GPU Memory chunks freed before this count
+  // are guaranteed not to be in use by any kernel pending on this device.
+  uint64 SafeAllocFrontier() override;
+
+  // Returns the number of kernels that have been queued for execution on
+  // the compute stream and are not yet known to have completed.
+  int PendingKernels();
+
  protected:
   Allocator* gpu_allocator_;  // not owned
   Allocator* cpu_allocator_;  // not owned
@@ -141,6 +154,9 @@ class BaseGPUDevice : public LocalDevice {
   const int32 max_streams_;
   std::unique_ptr<EventMgr> em_;
   std::unique_ptr<thread::ThreadPool> thread_pool_;
+  std::unique_ptr<GPUKernelTracker> kernel_tracker_;
+  int pending_cap_ = 0;
+  bool timestamped_allocator_ = false;
 
   // Initialize scractch buffers used by Eigen.
   Status InitScratchBuffers();
@@ -163,6 +179,75 @@ class BaseGPUDevice : public LocalDevice {
                               StatusCallback done);
 };
 
+// A per-compute-stream utility that keeps track of kernels that have been
+// queued for execution but may not yet have terminated, and also the queued
+// time of the most recently terminated kernel.
+class GPUKernelTracker {
+ public:
+  explicit GPUKernelTracker(Env* env,
+                            std::unique_ptr<SharedCounter> timing_counter)
+      : env_(env),
+        timing_counter_(std::move(timing_counter)),
+        pending_kernels_(64) {}
+
+  // Record that a GPU kernel has just been enqueued on the compute stream.
+  // Inserts a new timing counter value in a new PendingKernel record appended
+  // to the end of the ring buffer then returns that same count.
+  uint64 RecordQueued();
+
+  // Takes a count value returned by RecordQueued and finds the corresponding
+  // PendingKernel record in the ring buffer.  Marks the kernel as completed and
+  // advances the completion frontier accordingly.
+  void RecordTerminated(uint64 at_count);
+
+  // Returns the largest timing count such that all kernels queued no
+  // later than that count are known to have terminated.
+  uint64 LastTerminatedCount();
+
+  // Returns the number of kernels enqueued that are not yet known to
+  // have terminated.
+  int NumPending() {
+    mutex_lock l(mu_);
+    return num_pending_;
+  }
+
+  // Yield current thread until number of pending kernels no longer
+  // exceeds the cap.
+  void PauseWhilePendingExceeds(int cap) {
+    mutex_lock l(mu_);
+    while (num_pending_ > cap) {
+      pending_decreased_.wait(l);
+    }
+  }
+
+ private:
+  Env* env_;
+  std::unique_ptr<SharedCounter> timing_counter_;
+
+  // Records when a kernel was queued for execution.  Kernel launches are
+  // identified by a unique count value from a per-GPU device timing counter.
+  struct PendingKernel {
+    uint64 queued_count;
+    bool terminated;
+    PendingKernel(const PendingKernel& pk)
+        : queued_count(pk.queued_count), terminated(pk.terminated) {}
+    PendingKernel() : queued_count(0), terminated(false) {}
+  };
+  mutex mu_;
+  // Ring buffer of PendingKernel records.
+  std::vector<PendingKernel> pending_kernels_ GUARDED_BY(mu_);
+  // Next unused slot in pending_kernels_.
+  int first_available_ GUARDED_BY(mu_) = 0;
+  // Last completed PendingKernel such that all prior PendingKernels are
+  // also completed.  With out-of-order completion there may be a mixture
+  // of completed and uncompleted entries between last_completed_ and
+  // first_available_, hence num_pending_ is not guaranteed equal to
+  // their differerence.
+  int last_completed_ GUARDED_BY(mu_) = -1;
+  int num_pending_ GUARDED_BY(mu_) = 0;
+  condition_variable pending_decreased_ GUARDED_BY(mu_);
+};
+
 class BaseGPUDeviceFactory : public DeviceFactory {
  public:
   Status CreateDevices(const SessionOptions& options, const string& name_prefix,
diff --git a/tensorflow/core/common_runtime/gpu/gpu_device_test.cc b/tensorflow/core/common_runtime/gpu/gpu_device_test.cc
index ae623b2adbe..fba937ac487 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_device_test.cc
+++ b/tensorflow/core/common_runtime/gpu/gpu_device_test.cc
@@ -24,6 +24,7 @@ limitations under the License.
 #include "tensorflow/core/lib/core/status.h"
 #include "tensorflow/core/lib/core/status_test_util.h"
 #include "tensorflow/core/lib/gtl/stl_util.h"
+#include "tensorflow/core/lib/random/random.h"
 #include "tensorflow/core/platform/test.h"
 
 namespace tensorflow {
@@ -276,6 +277,71 @@ TEST_F(GPUDeviceTest, UnifiedMemoryAllocation) {
   allocator->DeallocateRaw(ptr);
 }
 
+class GPUKernelTrackerTest : public ::testing::Test {
+ protected:
+  void SetUp() {
+    std::unique_ptr<SharedCounter> counter(new SharedCounter);
+    timing_counter_ = counter.get();
+    kernel_tracker_.reset(
+        new GPUKernelTracker(Env::Default(), std::move(counter)));
+  }
+
+  std::unique_ptr<GPUKernelTracker> kernel_tracker_;
+  SharedCounter* timing_counter_ = nullptr;
+};
+
+TEST_F(GPUKernelTrackerTest, basic) {
+  EXPECT_EQ(0, kernel_tracker_->NumPending());
+  // 1 is the expected value when no kernels have yet terminated.
+  EXPECT_EQ(1, kernel_tracker_->LastTerminatedCount());
+
+  std::deque<int64> queued_counts;
+  for (int i = 0; i < 32; ++i) {
+    queued_counts.push_back(kernel_tracker_->RecordQueued());
+  }
+  EXPECT_EQ(32, kernel_tracker_->NumPending());
+  EXPECT_EQ(1, kernel_tracker_->LastTerminatedCount());
+
+  // Mature the kernels in order until empty.
+  while (!queued_counts.empty()) {
+    int64 x = queued_counts.front();
+    queued_counts.pop_front();
+    kernel_tracker_->RecordTerminated(x);
+    EXPECT_EQ(queued_counts.size(), kernel_tracker_->NumPending());
+    EXPECT_EQ(x, kernel_tracker_->LastTerminatedCount());
+  }
+  EXPECT_EQ(timing_counter_->get(), kernel_tracker_->LastTerminatedCount());
+
+  // Next inject so many kernel events that the ring buffer needs
+  // to grow a couple of times, while maturing a few in random order
+  // to introduce gaps between last_completed_ and first_available_.
+  int64 lower_bound = timing_counter_->get();
+  for (int i = 0; i < 1111; ++i) {
+    queued_counts.push_back(kernel_tracker_->RecordQueued());
+    int64 upper_bound = timing_counter_->get();
+    if (0 == (i % 16)) {
+      size_t index = (random::New64() % queued_counts.size());
+      kernel_tracker_->RecordTerminated(queued_counts[index]);
+      queued_counts.erase(queued_counts.begin() + index);
+      EXPECT_LE(lower_bound, kernel_tracker_->LastTerminatedCount());
+      EXPECT_GE(upper_bound, kernel_tracker_->LastTerminatedCount());
+    }
+  }
+
+  // Next mature the remaining kernels in order until empty.
+  while (!queued_counts.empty()) {
+    int64 x = queued_counts.front();
+    queued_counts.pop_front();
+    kernel_tracker_->RecordTerminated(x);
+    EXPECT_EQ(queued_counts.size(), kernel_tracker_->NumPending());
+    // There may be a gap here where we find a kernel that got terminated
+    // out of order, earlier, so the LastTerminatedCount can actually
+    // jump past x.
+    EXPECT_LE(x, kernel_tracker_->LastTerminatedCount());
+  }
+  EXPECT_EQ(timing_counter_->get(), kernel_tracker_->LastTerminatedCount());
+}
+
 }  // namespace tensorflow
 
 #endif
diff --git a/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc b/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc
index 3c1c31aa732..6531d6d367b 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc
+++ b/tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc
@@ -241,7 +241,9 @@ void EventMgr::QueueInUse(se::Stream* stream, InUse iu) {
 // events have recorded, and then retire them.  Initial observations
 // suggest that typical behavior in a TensorFlow program is to have
 // 0-3 events pending most of the time, but there are occasionally
-// spikes of up to several hundred outstanding.
+// spikes of up to several hundred outstanding.  (If GPUKernelTracker
+// is used to cap pending kernels there should never be more than
+// that many.)
 //
 // NOTE: If all events are on the same stream, no later event will
 // complete before an earlier event, except possibly if the earlier
@@ -249,13 +251,10 @@ void EventMgr::QueueInUse(se::Stream* stream, InUse iu) {
 // looking past the first kPending event.  However, if we're using
 // multiple streams there may be some gain in looking deeper.
 // As a compromise, PollEvent() calls that are triggered by the queueing
-// of a single event never look past the first kPending event.  Calls
-// coming from the dedicated polling thread always sweep the full queue.
-//
-// Note that allowing the queue to grow very long could cause overall
-// GPU memory use to spike needlessly.  An alternative strategy would
-// be to throttle new Op execution until the pending event queue
-// clears.
+// of a single event never look past the first kPending event.  Consequently
+// those calls do an expected constant amount of work, unaffected by the
+// length of the pending queue.  Calls coming from the dedicated
+// polling thread always sweep the full queue.
 void EventMgr::PollEvents(bool is_dedicated_poller,
                           gtl::InlinedVector<InUse, 4>* to_free) {
   VLOG(2) << "PollEvents  free_events_ " << free_events_.size()
diff --git a/tensorflow/core/common_runtime/gpu/gpu_process_state.cc b/tensorflow/core/common_runtime/gpu/gpu_process_state.cc
index 909f14a17b6..7804596d1ac 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_process_state.cc
+++ b/tensorflow/core/common_runtime/gpu/gpu_process_state.cc
@@ -27,6 +27,7 @@ limitations under the License.
 #include "tensorflow/core/common_runtime/gpu/gpu_id_utils.h"
 #include "tensorflow/core/common_runtime/gpu/gpu_init.h"
 #include "tensorflow/core/common_runtime/pool_allocator.h"
+#include "tensorflow/core/common_runtime/shared_counter.h"
 #include "tensorflow/core/framework/allocator.h"
 #include "tensorflow/core/framework/log_memory.h"
 #include "tensorflow/core/framework/tracking_allocator.h"
@@ -110,9 +111,15 @@ Allocator* GPUProcessState::GetGPUAllocator(const GPUOptions& options,
         (options.per_process_gpu_memory_fraction() > 1.0 ||
          options.experimental().use_unified_memory()),
         gpu_visitors_[bus_id], {});
-    Allocator* gpu_allocator =
+    GPUBFCAllocator* gpu_bfc_allocator =
         new GPUBFCAllocator(sub_allocator, total_bytes, options,
                             strings::StrCat("GPU_", tf_gpu_id.value(), "_bfc"));
+    Allocator* gpu_allocator = gpu_bfc_allocator;
+    SharedCounter* timing_counter = nullptr;
+    if (options.experimental().timestamped_allocator()) {
+      timing_counter = new SharedCounter;
+      gpu_bfc_allocator->SetTimingCounter(timing_counter);
+    }
 
     // If true, checks for memory overwrites by writing
     // distinctive patterns on both ends of allocated memory.
@@ -137,7 +144,9 @@ Allocator* GPUProcessState::GetGPUAllocator(const GPUOptions& options,
       recording_allocator = new internal::RecordingAllocator(
           &process_state_->mem_desc_map_, gpu_allocator, md, &mu_);
     }
-    allocator_parts = {std::unique_ptr<Allocator>(gpu_allocator), sub_allocator,
+    allocator_parts = {std::unique_ptr<Allocator>(gpu_allocator),
+                       std::unique_ptr<SharedCounter>(timing_counter),
+                       sub_allocator,
                        std::unique_ptr<Allocator>(recording_allocator)};
   }
   if (process_state_->ProcessState::FLAGS_brain_gpu_record_mem_types) {
@@ -151,6 +160,23 @@ Allocator* GPUProcessState::GetGPUAllocator(const GPUOptions& options,
 #endif  // GOOGLE_CUDA
 }
 
+std::unique_ptr<SharedCounter> GPUProcessState::ReleaseGPUAllocatorCounter(
+    TfGpuId tf_gpu_id) {
+  DCHECK(process_state_);
+#if GOOGLE_CUDA
+  GpuIdUtil::CheckValidTfGpuId(tf_gpu_id);
+  mutex_lock l(mu_);
+  if (tf_gpu_id.value() >= static_cast<int64>(gpu_allocators_.size())) {
+    return nullptr;
+  }
+
+  AllocatorParts& allocator_parts = gpu_allocators_[tf_gpu_id.value()];
+  return std::move(allocator_parts.counter);
+#else
+  return nullptr;
+#endif
+}
+
 Allocator* GPUProcessState::GetCUDAHostAllocator(int numa_node) {
   CHECK(process_state_);
   if (!HasGPUDevice() ||
@@ -224,6 +250,7 @@ Allocator* GPUProcessState::GetCUDAHostAllocator(int numa_node) {
       allocator = new TrackingAllocator(allocator, true);
     }
     cuda_host_allocators_.push_back({std::unique_ptr<Allocator>(allocator),
+                                     std::unique_ptr<SharedCounter>(nullptr),
                                      sub_allocator,
                                      std::unique_ptr<Allocator>(nullptr)});
     AllocatorParts& allocator_parts = cuda_host_allocators_.back();
diff --git a/tensorflow/core/common_runtime/gpu/gpu_process_state.h b/tensorflow/core/common_runtime/gpu/gpu_process_state.h
index df51c10c806..c7c9f3a6b35 100644
--- a/tensorflow/core/common_runtime/gpu/gpu_process_state.h
+++ b/tensorflow/core/common_runtime/gpu/gpu_process_state.h
@@ -23,6 +23,7 @@ limitations under the License.
 
 #include "tensorflow/core/common_runtime/gpu/gpu_id.h"
 #include "tensorflow/core/common_runtime/process_state.h"
+#include "tensorflow/core/common_runtime/shared_counter.h"
 #include "tensorflow/core/framework/allocator.h"
 #include "tensorflow/core/platform/mutex.h"
 #include "tensorflow/core/platform/thread_annotations.h"
@@ -33,6 +34,7 @@ namespace tensorflow {
 
 class Allocator;
 class PoolAllocator;
+class SharedCounter;
 
 // Singleton that manages per-process state when GPUs are present.
 class GPUProcessState {
@@ -108,6 +110,8 @@ class GPUProcessState {
   // Returns bus_id for the given GPU id.
   virtual int BusIdForGPU(TfGpuId tf_gpu_id);
 
+  std::unique_ptr<SharedCounter> ReleaseGPUAllocatorCounter(TfGpuId tf_gpu_id);
+
  protected:
   // GPUProcessState is a singleton that should not normally be deleted except
   // at process shutdown.
@@ -132,6 +136,7 @@ class GPUProcessState {
 
   struct AllocatorParts {
     std::unique_ptr<Allocator> allocator;
+    std::unique_ptr<SharedCounter> counter;
     SubAllocator* sub_allocator;  // owned by allocator
     std::unique_ptr<Allocator> recording_allocator;
   };
diff --git a/tensorflow/core/common_runtime/shared_counter.h b/tensorflow/core/common_runtime/shared_counter.h
new file mode 100644
index 00000000000..5e378524b20
--- /dev/null
+++ b/tensorflow/core/common_runtime/shared_counter.h
@@ -0,0 +1,31 @@
+/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef TENSORFLOW_CORE_COMMON_RUNTIME_SHARED_COUNTER_H_
+#define TENSORFLOW_CORE_COMMON_RUNTIME_SHARED_COUNTER_H_
+
+namespace tensorflow {
+// A lightweight thread-safe monotone counter for establishing
+// temporal ordering.
+class SharedCounter {
+ public:
+  int64 get() { return value_; }
+  int64 next() { return ++value_; }
+
+ private:
+  std::atomic<int64> value_{0};
+};
+
+}  // namespace tensorflow
+#endif  // TENSORFLOW_CORE_COMMON_RUNTIME_SHARED_COUNTER_H_
diff --git a/tensorflow/core/distributed_runtime/tensor_coding.cc b/tensorflow/core/distributed_runtime/tensor_coding.cc
index fe2d1a12934..6d20e7cfcad 100644
--- a/tensorflow/core/distributed_runtime/tensor_coding.cc
+++ b/tensorflow/core/distributed_runtime/tensor_coding.cc
@@ -68,13 +68,14 @@ Status TensorResponse::InitFrom(RecvTensorResponse* response) {
   return s;
 }
 
-void TensorResponse::InitPartial(const RecvTensorResponse& response) {
+void TensorResponse::InitPartial(const RecvTensorResponse& response,
+                                 const AllocationAttributes& allocation_attr) {
   // Everything except content is present in *response.  Content will
   // arrive later; allocate a Tensor with appropriate storage for that
   // content.
   meta_ = response;
   TensorShape shape(meta_.tensor().tensor_shape());
-  Tensor t(allocator_, meta_.tensor().dtype(), shape);
+  Tensor t(allocator_, meta_.tensor().dtype(), shape, allocation_attr);
   tensor_ = std::move(t);
 }
 
diff --git a/tensorflow/core/distributed_runtime/tensor_coding.h b/tensorflow/core/distributed_runtime/tensor_coding.h
index 4c34297990d..86d95a30631 100644
--- a/tensorflow/core/distributed_runtime/tensor_coding.h
+++ b/tensorflow/core/distributed_runtime/tensor_coding.h
@@ -76,7 +76,8 @@ class TensorResponse {
 
   // Initialize tensor metadata from response and allocate
   // uninitialized backing storage for actual contents.
-  void InitPartial(const RecvTensorResponse& response);
+  void InitPartial(const RecvTensorResponse& response,
+                   const AllocationAttributes& allocation_attr);
 
   // Return a reference to the parsed tensor.  The tensor will remain
   // live only until *this is destroyed or modified.
diff --git a/tensorflow/core/framework/allocator.h b/tensorflow/core/framework/allocator.h
index 3ded86e8e93..4d0c6d4b19f 100644
--- a/tensorflow/core/framework/allocator.h
+++ b/tensorflow/core/framework/allocator.h
@@ -46,6 +46,10 @@ struct AllocationAttributes {
   // which Op is performing the allocation, and sets this flag to
   // true.
   bool allocation_will_be_logged = false;
+  // EXPERIMENTAL: If provided, then evaluates to a timing count such that only
+  // a memory chunk whose last-freed count is at this value or earlier may be
+  // returned.
+  std::function<uint64()> freed_by_func = nullptr;
 };
 
 // Runtime statistics collected by an allocator.
diff --git a/tensorflow/core/framework/device_base.h b/tensorflow/core/framework/device_base.h
index 321947aca8e..89ba662b69b 100644
--- a/tensorflow/core/framework/device_base.h
+++ b/tensorflow/core/framework/device_base.h
@@ -246,6 +246,15 @@ class DeviceBase {
     return errors::Internal("Device does not implement MakeTensorFromProto()");
   }
 
+  // Some devices (i.e. GPUs) may free device memory prior to its actual use
+  // being completed on the assumption that subsequent allocations can only be
+  // used serially with respect to pending uses.  If this function returns a
+  // non-zero value it is the value of a device-specific counter such that any
+  // device memory tagged with an earlier freed-at count is really unencumbered
+  // by pending uses.  For this to be useful the device memory allocator must
+  // be tagging deallocated memory chunks using the same counter.
+  virtual uint64 SafeAllocFrontier() { return 0; }
+
  protected:
   // Does not take ownership.
   void set_tensorflow_device_thread_pool(thread::ThreadPool* thread_pool) {
diff --git a/tensorflow/core/protobuf/config.proto b/tensorflow/core/protobuf/config.proto
index 2f3dbad661e..44e98542ec0 100644
--- a/tensorflow/core/protobuf/config.proto
+++ b/tensorflow/core/protobuf/config.proto
@@ -156,6 +156,16 @@ message GPUOptions {
     // CollectiveReduce, and serves as an override to automatic ring order
     // generation in OrderTaskDeviceMap() during CollectiveParam resolution.
     string collective_ring_order = 4;
+
+    // If true then extra work is done by GPUDevice and GPUBFCAllocator to
+    // keep track of when GPU memory is freed and when kernels actually
+    // complete so that we can know when a nominally free memory chunk
+    // is really not subject to pending use.
+    bool timestamped_allocator = 5;
+
+    // If > 0 limit the number of pending kernels on any compute
+    // stream to this number.
+    int32 pending_cap = 6;
   }
 
   // Everything inside experimental is subject to change and is not subject
diff --git a/tensorflow/tools/api/golden/v1/tensorflow.-g-p-u-options.pbtxt b/tensorflow/tools/api/golden/v1/tensorflow.-g-p-u-options.pbtxt
index a2cc07483a4..6c528dd1620 100644
--- a/tensorflow/tools/api/golden/v1/tensorflow.-g-p-u-options.pbtxt
+++ b/tensorflow/tools/api/golden/v1/tensorflow.-g-p-u-options.pbtxt
@@ -84,6 +84,18 @@ tf_proto {
         label: LABEL_OPTIONAL
         type: TYPE_STRING
       }
+      field {
+        name: "timestamped_allocator"
+        number: 5
+        label: LABEL_OPTIONAL
+        type: TYPE_BOOL
+      }
+      field {
+        name: "pending_cap"
+        number: 6
+        label: LABEL_OPTIONAL
+        type: TYPE_INT32
+      }
       nested_type {
         name: "VirtualDevices"
         field {

commit 0f39e8a552018eba3bd2916c0a72467617bebf88
Author: Haoyu Zhang <haoyuzhang@google.com>
Date:   Thu Jan 17 10:41:18 2019 -0800

    Reuse existing dataset iterators in Keras for better memory efficiency.
    
    * In Keras model iteration, create only one iterator for training dataset and
      one iterator for validation datasets.
    * If training requires dataset to be reset at each epoch, reinitialize the
      training iterator;
    * If we run multiple validation passes between training epochs, reinitialize the validation
      iterator.
    
    Compared to recreating iterators, this approach reduces host memory usage and
    prevents memory leak.
    
    PiperOrigin-RevId: 229776545

diff --git a/tensorflow/python/keras/engine/distributed_training_utils.py b/tensorflow/python/keras/engine/distributed_training_utils.py
index 27ad6a5c0ba..d13e0b6fa90 100644
--- a/tensorflow/python/keras/engine/distributed_training_utils.py
+++ b/tensorflow/python/keras/engine/distributed_training_utils.py
@@ -34,6 +34,7 @@ from tensorflow.python.keras import callbacks
 from tensorflow.python.keras import metrics as metrics_module
 from tensorflow.python.keras import optimizers
 from tensorflow.python.keras.optimizer_v2 import optimizer_v2
+from tensorflow.python.ops import control_flow_ops
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import variables
 from tensorflow.python.platform import tf_logging as logging
@@ -528,10 +529,15 @@ def list_to_tuple(maybe_list):
 def get_iterator(dataset, distribution_strategy):
   with distribution_strategy.scope():
     iterator = distribution_strategy.make_dataset_iterator(dataset)
-    init_op = iterator.initialize()
+  initialize_iterator(iterator, distribution_strategy)
+  return iterator
+
+
+def initialize_iterator(iterator, distribution_strategy):
+  with distribution_strategy.scope():
+    init_op = control_flow_ops.group(iterator.initialize())
     if not context.executing_eagerly():
       K.get_session().run(init_op)
-  return iterator
 
 
 def _get_input_from_iterator(iterator, model):
diff --git a/tensorflow/python/keras/engine/training_arrays.py b/tensorflow/python/keras/engine/training_arrays.py
index 3b6a13839a1..d0881c11a50 100644
--- a/tensorflow/python/keras/engine/training_arrays.py
+++ b/tensorflow/python/keras/engine/training_arrays.py
@@ -24,6 +24,7 @@ import functools
 import numpy as np
 
 from tensorflow.python.data.ops import dataset_ops
+from tensorflow.python.data.ops import iterator_ops
 from tensorflow.python.eager import context
 from tensorflow.python.framework import errors
 from tensorflow.python.keras import backend as K
@@ -119,18 +120,22 @@ def model_iteration(model,
 
   # In case we were passed a dataset, we extract symbolic tensors from it.
   reset_dataset_after_each_epoch = False
-  original_dataset = None
+  input_iterator = None
   is_dataset = isinstance(inputs,
                           (dataset_ops.DatasetV1, dataset_ops.DatasetV2))
   # TODO(fchollet): consider moving `steps_per_epoch` inference to
   # _standardize_user_data and set reset_dataset_after_each_epoch as an
   # attribute on the dataset instance.
   if is_dataset:
-    original_dataset = inputs
     if steps_per_epoch is None:
       reset_dataset_after_each_epoch = True
       steps_per_epoch = training_utils.infer_steps_for_dataset(
           inputs, steps_per_epoch, epochs=epochs, steps_name=steps_name)
+    input_iterator = _get_iterator(inputs, model._distribution_strategy)
+
+  val_iterator = None
+  if isinstance(val_inputs, (dataset_ops.DatasetV1, dataset_ops.DatasetV2)):
+    val_iterator = _get_iterator(val_inputs, model._distribution_strategy)
 
   if mode == ModeKeys.TRAIN:
     _print_train_info(inputs, val_inputs, steps_per_epoch, verbose)
@@ -150,6 +155,7 @@ def model_iteration(model,
       convert_eager_tensors_to_numpy((inputs, targets, sample_weights))
 
   # Prepare input data.
+  inputs = input_iterator or inputs
   ins = _prepare_feed_values(model, inputs, targets, sample_weights, mode)
   if not is_dataset:
     num_samples_or_steps = _get_num_samples_or_steps(ins, batch_size,
@@ -230,7 +236,7 @@ def model_iteration(model,
           actual_inputs = ins() if callable(ins) else ins
           batch_outs = f(actual_inputs)
         except errors.OutOfRangeError:
-          if original_dataset is None:
+          if not is_dataset:
             # We ran out of batches while the user passed an iterator (legacy).
             logging.warning(
                 'Your dataset iterator ran out of data; '
@@ -332,6 +338,7 @@ def model_iteration(model,
     if (do_validation and
         training_utils.should_run_validation(validation_freq, epoch) and
         not callbacks.model.stop_training):
+      val_inputs = val_iterator or val_inputs
       val_results = model_iteration(
           model,
           val_inputs,
@@ -348,15 +355,18 @@ def model_iteration(model,
         val_results = [val_results]
       epoch_logs = cbks.make_logs(
           model, epoch_logs, val_results, mode, prefix='val_')
+      if val_iterator and epoch < epochs - 1:
+        _reinitialize_iterator(val_iterator, model._distribution_strategy)
 
     if mode == ModeKeys.TRAIN:
       # Epochs only apply to `fit`.
       callbacks.on_epoch_end(epoch, epoch_logs)
     progbar.on_epoch_end(epoch, epoch_logs)
 
-    # Recreate dataset iterator for the next epoch.
+    # Reinitialize dataset iterator for the next epoch.
     if reset_dataset_after_each_epoch and epoch < epochs - 1:
-      ins = _prepare_feed_values(model, original_dataset, None, None, mode)
+      _reinitialize_iterator(input_iterator, model._distribution_strategy)
+      ins = _prepare_feed_values(model, input_iterator, None, None, mode)
 
   callbacks._call_end_hook(mode)
 
@@ -430,7 +440,8 @@ def _prepare_feed_values(model, inputs, targets, sample_weights, mode):
     else:
       return get_distributed_inputs()
 
-  if isinstance(inputs, (dataset_ops.DatasetV1, dataset_ops.DatasetV2)):
+  if isinstance(inputs, (dataset_ops.DatasetV1, dataset_ops.DatasetV2,
+                         iterator_ops.Iterator)):
     inputs, targets, sample_weights = model._standardize_user_data(
         inputs,
         extract_tensors_from_dataset=True)
@@ -445,6 +456,21 @@ def _prepare_feed_values(model, inputs, targets, sample_weights, mode):
   return ins
 
 
+def _get_iterator(inputs, distribution_strategy=None):
+  if distribution_strategy:
+    return distributed_training_utils.get_iterator(
+        inputs, distribution_strategy)
+  return training_utils.get_iterator(inputs)
+
+
+def _reinitialize_iterator(iterator, distribution_strategy=None):
+  if distribution_strategy:
+    distributed_training_utils.initialize_iterator(
+        iterator, distribution_strategy)
+  else:
+    training_utils.initialize_iterator(iterator)
+
+
 def _make_execution_function(model, mode):
   """Makes function to run one step of model execution."""
   if model._distribution_strategy:
diff --git a/tensorflow/python/keras/engine/training_utils.py b/tensorflow/python/keras/engine/training_utils.py
index 3affb841477..7e5bc08e5e4 100644
--- a/tensorflow/python/keras/engine/training_utils.py
+++ b/tensorflow/python/keras/engine/training_utils.py
@@ -1232,6 +1232,19 @@ def is_dataset_or_iterator(data):
                            iterator_ops.Iterator))
 
 
+def get_iterator(dataset):
+  """Create and initialize an iterator from a dataset."""
+  iterator = dataset_ops.make_initializable_iterator(dataset)
+  initialize_iterator(iterator)
+  return iterator
+
+
+def initialize_iterator(iterator):
+  init_op = iterator.initializer
+  if not context.executing_eagerly():
+    K.get_session().run(init_op)
+
+
 def extract_tensors_from_dataset(dataset):
   """Extract a tuple of tensors `inputs, targets, sample_weight` from a dataset.
 
@@ -1241,10 +1254,7 @@ def extract_tensors_from_dataset(dataset):
   Returns:
     Tuple of tensors `x, y, weights`. `y` and `weights` entry may be None.
   """
-  iterator = dataset_ops.make_initializable_iterator(dataset)
-  init_op = iterator.initializer
-  if not context.executing_eagerly():
-    K.get_session().run(init_op)
+  iterator = get_iterator(dataset)
   inputs, targets, sample_weight = unpack_iterator_input(iterator)
   return inputs, targets, sample_weight
 

commit f348ee8a0931c5048a3ed4f8d16b2f47a7466d39
Author: Eddie Zhou <eddz@google.com>
Date:   Fri Oct 26 13:42:24 2018 -0700

    Group the warm-starting of no-vocab variables into one init_from_checkpoint call for efficiency.
    
    PiperOrigin-RevId: 218906890

diff --git a/tensorflow/python/training/warm_starting_util.py b/tensorflow/python/training/warm_starting_util.py
index 5d7f81d36ee..78dbb465b55 100644
--- a/tensorflow/python/training/warm_starting_util.py
+++ b/tensorflow/python/training/warm_starting_util.py
@@ -146,22 +146,19 @@ def _infer_var_name(var):
   return list(name_to_var_dict.keys())[0]
 
 
-def _warm_start_var(var, prev_ckpt, prev_tensor_name=None):
-  """Warm-starts given variable from `prev_tensor_name` tensor in `prev_ckpt`.
+def _get_var_info(var, prev_tensor_name=None):
+  """Helper method for standarizing Variable and naming.
 
   Args:
     var: Current graph's variable that needs to be warm-started (initialized).
-      Can be either of the following:
-      (i) `Variable`
-      (ii) `ResourceVariable`
+      Can be either of the following: (i) `Variable` (ii) `ResourceVariable`
       (iii) list of `Variable`: The list must contain slices of the same larger
-        variable.
-      (iv) `PartitionedVariable`
-    prev_ckpt: A string specifying the directory with checkpoint file(s) or path
-      to checkpoint. The given checkpoint must have tensor with name
-      `prev_tensor_name` (if not None) or tensor with name same as given `var`.
+        variable. (iv) `PartitionedVariable`
     prev_tensor_name: Name of the tensor to lookup in provided `prev_ckpt`. If
       None, we lookup tensor with same name as given `var`.
+
+  Returns:
+    A tuple of the Tensor name and var.
   """
   if checkpoint_utils._is_variable(var):  # pylint: disable=protected-access
     current_var_name = _infer_var_name([var])
@@ -178,7 +175,8 @@ def _warm_start_var(var, prev_ckpt, prev_tensor_name=None):
   if not prev_tensor_name:
     # Assume tensor name remains the same.
     prev_tensor_name = current_var_name
-  checkpoint_utils.init_from_checkpoint(prev_ckpt, {prev_tensor_name: var})
+
+  return prev_tensor_name, var
 
 
 # pylint: disable=protected-access
@@ -427,6 +425,8 @@ def warm_start(ckpt_to_initialize_from,
   prev_var_name_used = set()
   vocab_info_used = set()
 
+  # Group the vocabless vars into one call to init_from_checkpoint.
+  vocabless_vars = {}
   for var_name, variable in six.iteritems(grouped_variables):
     prev_var_name = var_name_to_prev_var_name.get(var_name)
     if prev_var_name:
@@ -469,8 +469,10 @@ def warm_start(ckpt_to_initialize_from,
         # for init_from_checkpoint logic to work correctly.
         if len(variable) == 1:
           variable = variable[0]
-        _warm_start_var(variable, ckpt_to_initialize_from, prev_var_name)
+        prev_tensor_name, var = _get_var_info(variable, prev_var_name)
+        vocabless_vars[prev_tensor_name] = var
 
+  checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars)
   prev_var_name_not_used = set(
       var_name_to_prev_var_name.keys()) - prev_var_name_used
   vocab_info_not_used = set(var_name_to_vocab_info.keys()) - vocab_info_used
diff --git a/tensorflow/python/training/warm_starting_util_test.py b/tensorflow/python/training/warm_starting_util_test.py
index f368fb9180b..91a0b53b3a8 100644
--- a/tensorflow/python/training/warm_starting_util_test.py
+++ b/tensorflow/python/training/warm_starting_util_test.py
@@ -30,6 +30,7 @@ from tensorflow.python.ops import init_ops
 from tensorflow.python.ops import variable_scope
 from tensorflow.python.ops import variables
 from tensorflow.python.platform import test
+from tensorflow.python.training import checkpoint_utils
 from tensorflow.python.training import saver as saver_lib
 from tensorflow.python.training import warm_starting_util as ws_util
 
@@ -121,7 +122,9 @@ class WarmStartingUtilTest(test.TestCase):
       with self.session(graph=g) as sess:
         fruit_weights = variable_scope.get_variable(
             "fruit_weights", initializer=[[0.], [0.], [0.], [0.]])
-        ws_util._warm_start_var(fruit_weights, self.get_temp_dir())
+        prev_tensor_name, var = ws_util._get_var_info(fruit_weights)
+        checkpoint_utils.init_from_checkpoint(self.get_temp_dir(),
+                                              {prev_tensor_name: var})
         sess.run(variables.global_variables_initializer())
         self.assertAllClose(prev_val, fruit_weights.eval(sess))
 
@@ -137,7 +140,9 @@ class WarmStartingUtilTest(test.TestCase):
       with self.session(graph=g) as sess:
         fruit_weights = variable_scope.get_variable(
             "fruit_weights", initializer=[[0.], [0.], [0.], [0.]])
-        ws_util._warm_start_var(fruit_weights, self.get_temp_dir())
+        prev_tensor_name, var = ws_util._get_var_info(fruit_weights)
+        checkpoint_utils.init_from_checkpoint(self.get_temp_dir(),
+                                              {prev_tensor_name: var})
         sess.run(variables.global_variables_initializer())
         self.assertAllClose(prev_val, fruit_weights.eval(sess))
 
@@ -154,7 +159,9 @@ class WarmStartingUtilTest(test.TestCase):
             partitioner=lambda shape, dtype: [2, 1])
         self.assertTrue(
             isinstance(fruit_weights, variables.PartitionedVariable))
-        ws_util._warm_start_var(fruit_weights, self.get_temp_dir())
+        prev_tensor_name, var = ws_util._get_var_info(fruit_weights)
+        checkpoint_utils.init_from_checkpoint(self.get_temp_dir(),
+                                              {prev_tensor_name: var})
         sess.run(variables.global_variables_initializer())
         fruit_weights = fruit_weights._get_variable_list()
         new_val = np.concatenate(
@@ -178,10 +185,10 @@ class WarmStartingUtilTest(test.TestCase):
             partitioner=lambda shape, dtype: [2, 1])
         self.assertTrue(
             isinstance(fruit_weights, variables.PartitionedVariable))
-        ws_util._warm_start_var(
-            fruit_weights,
-            self.get_temp_dir(),
-            prev_tensor_name="old_scope/fruit_weights")
+        prev_tensor_name, var = ws_util._get_var_info(
+            fruit_weights, prev_tensor_name="old_scope/fruit_weights")
+        checkpoint_utils.init_from_checkpoint(self.get_temp_dir(),
+                                              {prev_tensor_name: var})
         sess.run(variables.global_variables_initializer())
         fruit_weights = fruit_weights._get_variable_list()
         new_val = np.concatenate(

commit 9b44e8bd7b116503f9f1fe605d8ff43ef6ee132e
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Mon Aug 13 03:50:52 2018 -0700

    Fix 2 bugs in the logic of the ODE, impacting efficiency:
    1) if either of the side is always zero, you never want to do the multiplication
    2) because of the zero at the end, the if clause was never hit, not saving those flops.
    
    PiperOrigin-RevId: 208457125

diff --git a/tensorflow/contrib/integrate/python/ops/odes.py b/tensorflow/contrib/integrate/python/ops/odes.py
index 61f78febfc0..7b7ac4f347e 100644
--- a/tensorflow/contrib/integrate/python/ops/odes.py
+++ b/tensorflow/contrib/integrate/python/ops/odes.py
@@ -73,7 +73,7 @@ def _scaled_dot_product(scale, xs, ys, name=None):
     # _possibly_nonzero lets us avoid wasted computation.
     return math_ops.add_n(
         [(scale * x) * y for x, y in zip(xs, ys)
-         if _possibly_nonzero(x) or _possibly_nonzero(y)],
+         if _possibly_nonzero(x) and _possibly_nonzero(y)],
         name=scope)
 
 
@@ -122,7 +122,7 @@ def _runge_kutta_step(func,
       yi = y0 + _scaled_dot_product(dt_cast, beta_i, k)
       k.append(func(yi, ti))
 
-    if not (tableau.c_sol[-1] == 0 and tableau.c_sol == tableau.beta[-1]):
+    if not (tableau.c_sol[-1] == 0 and tableau.c_sol[:-1] == tableau.beta[-1]):
       # This property (true for Dormand-Prince) lets us save a few FLOPs.
       yi = y0 + _scaled_dot_product(dt_cast, tableau.c_sol, k)
 

commit 23f826271a5956982df17980bca3ac7513ec4ee4
Author: Cao Zongyan <zongyan.cao@alibaba-inc.com>
Date:   Thu Jul 26 11:27:40 2018 +0800

    A faster BatchSelectFunctor for tf.where on CPU.
    
    Op 'tf.where(c, t, e)' supports that 't' and 'e' are N-D tensors
    while 'c' is a 1D tensor, which would call BatchSelectFunctor to
    get the result. But its basic implementation broadcasts 'c' to the
    same dimension with 't' and 'e', which would get bad efficiency on
    CPU for large tensors. Here a loop-based implementation would be
    adopted to make this operation faster on CPU.

diff --git a/tensorflow/core/kernels/cwise_op_select.cc b/tensorflow/core/kernels/cwise_op_select.cc
index e259daaba47..0d6d83fc3ae 100644
--- a/tensorflow/core/kernels/cwise_op_select.cc
+++ b/tensorflow/core/kernels/cwise_op_select.cc
@@ -22,6 +22,7 @@ limitations under the License.
 #include "tensorflow/core/framework/register_types.h"
 #include "tensorflow/core/kernels/bounds_check.h"
 #include "tensorflow/core/kernels/cwise_ops_common.h"
+#include "tensorflow/core/platform/prefetch.h"
 
 namespace tensorflow {
 
@@ -254,9 +255,48 @@ struct BatchSelectFunctorBase {
   }
 };
 
+// A fast implementation on CPU, using loop to get rid of broadcasting.
 template <typename T>
-struct BatchSelectFunctor<CPUDevice, T> : BatchSelectFunctorBase<CPUDevice, T> {
+struct BatchSelectFunctor<CPUDevice, T> {
+  void operator()(const CPUDevice& d,
+                  typename TTypes<T>::Matrix output_flat_outer_dims,
+                  TTypes<bool>::ConstVec cond_vec,
+                  typename TTypes<T>::ConstMatrix then_flat_outer_dims,
+                  typename TTypes<T>::ConstMatrix else_flat_outer_dims) {
+    const size_t batch = cond_vec.size();
+    const size_t batch_size = then_flat_outer_dims.size() / batch;
+    T* output = output_flat_outer_dims.data();
+    const bool* c = cond_vec.data();
+    const T* t = then_flat_outer_dims.data();
+    const T* e = else_flat_outer_dims.data();
+
+    auto work = [batch_size, output, c, t, e](int64 start, int64 end) {
+      for (size_t i = start; i < end; ++i) {
+        size_t offset = i * batch_size;
+        port::prefetch<port::PREFETCH_HINT_NTA>(
+            reinterpret_cast<const void*>(&t[offset + batch_size]));
+        port::prefetch<port::PREFETCH_HINT_NTA>(
+            reinterpret_cast<const void*>(&e[offset + batch_size]));
+        port::prefetch<port::PREFETCH_HINT_NTA>(
+            reinterpret_cast<const void*>(&c[i + 1]));
+        if (c[i]) {
+          for (size_t j = 0; j < batch_size; ++j) {
+            output[offset + j] = t[offset + j];
+          }
+        } else {
+          for (size_t j = 0; j < batch_size; ++j) {
+            output[offset + j] = e[offset + j];
+          }
+        }
+      }
+    };
+    auto cost = Eigen::TensorOpCost(sizeof(T) * batch_size * 2, // ld bytes
+                                    sizeof(T) * batch_size,     // st bytes
+                                    batch_size); // compute cycles
+    d.parallelFor(batch, cost, work);
+  }
 };
+
 #ifdef TENSORFLOW_USE_SYCL
 template <typename T>
 struct BatchSelectFunctor<SYCLDevice, T>
diff --git a/tensorflow/python/kernel_tests/where_op_test.py b/tensorflow/python/kernel_tests/where_op_test.py
index 17575da6f1b..53324d5b208 100644
--- a/tensorflow/python/kernel_tests/where_op_test.py
+++ b/tensorflow/python/kernel_tests/where_op_test.py
@@ -135,6 +135,15 @@ class WhereOpTest(test.TestCase):
       tf_val = array_ops.where(constant_op.constant(x) > 0, x * x, -x).eval()
     self.assertAllEqual(tf_val, np_val)
 
+  def testBatchSelect(self):
+    x = np.array([[-2, 3, -1] * 64, [1, -3, -3] * 64] * 8192)  # [16384, 192]
+    c_mat = np.array([[False] * 192, [True] * 192] * 8192)  # [16384, 192]
+    c_vec = np.array([False, True] * 8192)  # [16384]
+    np_val = np.where(c_mat, x * x, -x)
+    for use_gpu in [True, False]:
+      with self.test_session(use_gpu=use_gpu):
+        tf_val = array_ops.where(c_vec, x * x, -x).eval()
+      self.assertAllEqual(tf_val, np_val)
 
 class WhereBenchmark(test.Benchmark):
 
@@ -163,5 +172,33 @@ class WhereBenchmark(test.Benchmark):
                 "Throughput: %0.03g GB/s" % (name, r["wall_time"], throughput))
           sys.stdout.flush()
 
+  def benchmarkBatchSelect(self):
+    for (m, n, use_gpu) in itertools.product(
+        [1000, 10000, 100000],
+        [10, 100, 1000],
+        [False, True]):
+      name = "m_%d_n_%d_use_gpu_%s" % (m, n, use_gpu)
+      device = "/%s:0" % ("gpu" if use_gpu else "cpu")
+      with ops.Graph().as_default():
+        with ops.device(device):
+          x_gen = random_ops.random_uniform([m, n], dtype=dtypes.float32)
+          y_gen = random_ops.random_uniform([m, n], dtype=dtypes.float32)
+          c_gen = random_ops.random_uniform([m], dtype=dtypes.float32) <= 0.5
+          x = resource_variable_ops.ResourceVariable(x_gen)
+          y = resource_variable_ops.ResourceVariable(y_gen)
+          c = resource_variable_ops.ResourceVariable(c_gen)
+          op = array_ops.where(c, x, y)
+        with session.Session() as sess:
+          x.initializer.run()
+          y.initializer.run()
+          c.initializer.run()
+          r = self.run_op_benchmark(sess, op, min_iters=100, name=name)
+          # approximate size of output: m*n*2 floats for each axis.
+          gb_processed = m * n * 8 / 1.0e9
+          throughput = gb_processed / r["wall_time"]
+          print("Benchmark: %s \t wall_time: %0.03g s \t "
+                "Throughput: %0.03g GB/s" % (name, r["wall_time"], throughput))
+          sys.stdout.flush()
+
 if __name__ == "__main__":
   test.main()

commit 0b522fd22b986704d1056254961cc7988ae182eb
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri May 25 12:54:49 2018 -0700

    Add ScopedAllocatorOptimizer in support of CollectiveReduce.
    
    The efficiency of CollectiveReduce is greatly improved by merging
    multiple parallel reductions over smaller tensors into a single
    reduction over a larger tensor that is the concatentation of the
    smaller tensors.  Because CollectiveReduce is essentially an
    element-wise array operation which operates on a 1-D reshape of
    the input tensor it is eligible for a ScopedAllocation optimization.
    
    The optimization works by looking for serially independent instances
    of CollectiveReduce that lie within the same name-scope tier and
    have the same control-flow (e.g. loop) embedding structure.  Where
    two or more such nodes are found the upstream nodes that generate
    their inputs are modified to write their outputs into consecutive
    regions of a single tensor buffer maintained by a ScopedAllocator.
    The multiple CollectiveReduce nodes are then replaced by a single
    CollectiveReduce that operates in-place on the backing buffer.
    
    The effectiveness of the optimization depends on there being candidate
    CollectiveReduce nodes with these characteristics that become eligible
    for execution at close to the same time.  If the name scope is too
    large, and includes nodes that become execution eligible at very different
    times, this graph rewrite could result in a slowdown.
    
    Note that this optimization is experimental: it is not guaranteed to
    work, especially for ops other than CollectiveReduce.
    
    PiperOrigin-RevId: 198089642

diff --git a/tensorflow/core/common_runtime/scoped_allocator_mgr.cc b/tensorflow/core/common_runtime/scoped_allocator_mgr.cc
index c045596a69b..8ac6adc2e48 100644
--- a/tensorflow/core/common_runtime/scoped_allocator_mgr.cc
+++ b/tensorflow/core/common_runtime/scoped_allocator_mgr.cc
@@ -160,13 +160,18 @@ Status ScopedAllocatorMgr::AddScopedAllocator(
                                  expected_call_count);
 }
 
-void ScopedAllocatorMgr::PopulateFields(
+/*static*/
+size_t ScopedAllocatorMgr::PopulateFields(
     int32 scope_id, const gtl::ArraySlice<TensorShape>& shapes,
     const DataType dtype, std::vector<ScopedAllocator::Field>* fields) {
   const int32 num_fields = static_cast<int32>(shapes.size());
   fields->resize(num_fields);
   size_t offset = 0;
   for (int32 i = 0; i < num_fields; ++i) {
+    size_t overshoot = offset % Allocator::kAllocatorAlignment;
+    if (overshoot > 0) {
+      offset += (Allocator::kAllocatorAlignment - overshoot);
+    }
     size_t bytes = shapes[i].num_elements() * DataTypeSize(dtype);
     (*fields)[i].scope_id = scope_id + 1 + i;
     (*fields)[i].bytes = bytes;
@@ -175,11 +180,8 @@ void ScopedAllocatorMgr::PopulateFields(
             << " bytes=" << (*fields)[i].bytes
             << " offset=" << (*fields)[i].offset;
     offset += bytes;
-    size_t overshoot = offset % Allocator::kAllocatorAlignment;
-    if (overshoot > 0) {
-      offset += (Allocator::kAllocatorAlignment - overshoot);
-    }
   }
+  return offset;
 }
 
 }  // namespace tensorflow
diff --git a/tensorflow/core/common_runtime/scoped_allocator_mgr.h b/tensorflow/core/common_runtime/scoped_allocator_mgr.h
index effc5f2d775..8c5e853472d 100644
--- a/tensorflow/core/common_runtime/scoped_allocator_mgr.h
+++ b/tensorflow/core/common_runtime/scoped_allocator_mgr.h
@@ -89,10 +89,13 @@ class ScopedAllocatorMgr {
 
   // Populate the bytes and offset members of Field.  Instance allocaters get
   // consecutive scope_id values following that of the base ScopedAllocator.
-  static void PopulateFields(int32 scope_id,
-                             const gtl::ArraySlice<TensorShape>& shapes,
-                             const DataType dtype,
-                             std::vector<ScopedAllocator::Field>* fields);
+  // Returns the total number of bytes required to be allocated in the
+  // backing tensor, for convenience.  (The same value can be obtained
+  // by summing offset and bytes in the last field.)
+  static size_t PopulateFields(int32 scope_id,
+                               const gtl::ArraySlice<TensorShape>& shapes,
+                               const DataType dtype,
+                               std::vector<ScopedAllocator::Field>* fields);
 
   const string& device_name() const { return device_name_; }
 
diff --git a/tensorflow/core/grappler/op_types.cc b/tensorflow/core/grappler/op_types.cc
index fe0fad91482..2a47a4c4958 100644
--- a/tensorflow/core/grappler/op_types.cc
+++ b/tensorflow/core/grappler/op_types.cc
@@ -78,6 +78,12 @@ bool IsCheckNumerics(const NodeDef& node) {
   return node.op() == "CheckNumerics";
 }
 
+bool IsCollective(const NodeDef& node) {
+  return node.op() == "CollectiveReduce" ||
+         node.op() == "CollectiveBcastSend" ||
+         node.op() == "CollectiveBcastRecv";
+}
+
 bool IsComplex(const NodeDef& node) { return node.op() == "Complex"; }
 
 bool IsComplexAbs(const NodeDef& node) { return node.op() == "ComplexAbs"; }
diff --git a/tensorflow/core/grappler/op_types.h b/tensorflow/core/grappler/op_types.h
index 915da21fad7..e7f39981c00 100644
--- a/tensorflow/core/grappler/op_types.h
+++ b/tensorflow/core/grappler/op_types.h
@@ -38,6 +38,7 @@ bool IsBiasAddGrad(const NodeDef& node);
 bool IsBitcast(const NodeDef& node);
 bool IsCast(const NodeDef& node);
 bool IsCheckNumerics(const NodeDef& node);
+bool IsCollective(const NodeDef& node);
 bool IsComplex(const NodeDef& node);
 bool IsComplexAbs(const NodeDef& node);
 bool IsConj(const NodeDef& node);
diff --git a/tensorflow/core/grappler/optimizers/BUILD b/tensorflow/core/grappler/optimizers/BUILD
index f6860695ecd..c90667abade 100644
--- a/tensorflow/core/grappler/optimizers/BUILD
+++ b/tensorflow/core/grappler/optimizers/BUILD
@@ -517,6 +517,7 @@ cc_library(
         ":memory_optimizer",
         ":model_pruner",
         ":remapper",
+        ":scoped_allocator_optimizer",
         ":shape_optimizer",
         "//tensorflow/core:core_cpu_base",
         "//tensorflow/core:framework",
@@ -762,3 +763,47 @@ tf_cuda_cc_test(
         "//tensorflow/core/grappler/utils:grappler_test",
     ],
 )
+
+cc_library(
+    name = "scoped_allocator_optimizer",
+    srcs = ["scoped_allocator_optimizer.cc"],
+    hdrs = [
+        "scoped_allocator_optimizer.h",
+    ],
+    visibility = ["//visibility:public"],
+    deps = [
+        ":graph_optimizer",
+        "//tensorflow/core:core_cpu_lib",
+        "//tensorflow/core:framework",
+        "//tensorflow/core:lib",
+        "//tensorflow/core:lib_internal",
+        "//tensorflow/core:protos_all_cc",
+        "//tensorflow/core:scoped_allocator_ops_op_lib",
+        "//tensorflow/core/grappler:grappler_item",
+        "//tensorflow/core/grappler:op_types",
+        "//tensorflow/core/grappler:utils",
+        "//tensorflow/core/grappler/costs:graph_properties",
+        "//tensorflow/core/grappler/utils:frame",
+    ],
+)
+
+tf_cc_test(
+    name = "scoped_allocator_optimizer_test",
+    size = "small",
+    srcs = ["scoped_allocator_optimizer_test.cc"],
+    deps = [
+        ":scoped_allocator_optimizer",
+        "//tensorflow/cc:cc_ops",
+        "//tensorflow/core:all_kernels",
+        "//tensorflow/core:core_cpu",
+        "//tensorflow/core:direct_session",
+        "//tensorflow/core:lib",
+        "//tensorflow/core:protos_all_cc",
+        "//tensorflow/core:test",
+        "//tensorflow/core:test_main",
+        "//tensorflow/core:testlib",
+        "//tensorflow/core/grappler:grappler_item",
+        "//tensorflow/core/grappler:utils",
+        "//tensorflow/core/grappler/inputs:trivial_test_graph_input_yielder",
+    ],
+)
diff --git a/tensorflow/core/grappler/optimizers/meta_optimizer.cc b/tensorflow/core/grappler/optimizers/meta_optimizer.cc
index a92727535d7..e6622486eb9 100644
--- a/tensorflow/core/grappler/optimizers/meta_optimizer.cc
+++ b/tensorflow/core/grappler/optimizers/meta_optimizer.cc
@@ -29,6 +29,7 @@ limitations under the License.
 #include "tensorflow/core/grappler/optimizers/memory_optimizer.h"
 #include "tensorflow/core/grappler/optimizers/model_pruner.h"
 #include "tensorflow/core/grappler/optimizers/remapper.h"
+#include "tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.h"
 #include "tensorflow/core/grappler/optimizers/shape_optimizer.h"
 #include "tensorflow/core/grappler/utils/colocation.h"
 #include "tensorflow/core/grappler/utils/functions.h"
@@ -88,6 +89,8 @@ std::unique_ptr<GraphOptimizer> MetaOptimizer::MakeNewOptimizer(
   MK_OPT("loop", new LoopOptimizer(cfg_.loop_optimization()));
   MK_OPT("dependency", new DependencyOptimizer(cfg_.dependency_optimization()));
   MK_OPT("debug_stripper", new DebugStripper());
+  MK_OPT("scoped_allocator",
+         new ScopedAllocatorOptimizer(cfg_.scoped_allocator_opts()));
 
   return std::unique_ptr<GraphOptimizer>();
 }
@@ -145,6 +148,10 @@ Status MetaOptimizer::InitializeOptimizers(
     optimizers->emplace_back(
         new AutoParallel(cfg_.auto_parallel().num_replicas()));
   }
+  if (cfg_.scoped_allocator_optimization()) {
+    optimizers->emplace_back(
+        new ScopedAllocatorOptimizer(cfg_.scoped_allocator_opts()));
+  }
   return Status::OK();
 }
 
@@ -211,12 +218,32 @@ Status MetaOptimizer::OptimizeGraph(Cluster* cluster, const GrapplerItem& item,
   bool is_optimized = false;
   GraphOptimizationResult optimization_result(item.id);
 
+  // ScopedAllocatorOptimizer must run last, so move it to the
+  // end of optimizers and run only on the last iteration.
+  {
+    int sa_index = 0;
+    for (; sa_index < optimizers.size(); ++sa_index) {
+      if (optimizers[sa_index]->name() == "scoped_allocator_optimizer") {
+        break;
+      }
+    }
+    const int last_index = optimizers.size() - 1;
+    if (sa_index < last_index) {
+      optimizers[last_index].swap(optimizers[sa_index]);
+    }
+  }
+
+  const int last_iteration = NumIterations(cfg_) - 1;
   for (int iteration = 0; iteration < NumIterations(cfg_); ++iteration) {
     VLOG(4) << "Starting optimization iteration " << iteration + 1;
 
     for (const auto& optimizer : optimizers) {
       // Some optimizers can run only once.
       if (iteration > 0 && IsRunOnceOptimizer(optimizer->name())) continue;
+      // Some must run only on the last iteration.
+      if (optimizer->name() == "scoped_allocator_optimizer" &&
+          iteration != last_iteration)
+        continue;
 
       uint64 start_us = Env::Default()->NowMicros();
       // This swaps the current optimized_graph into optimized item and
@@ -361,6 +388,7 @@ bool MetaOptimizerEnabled(const RewriterConfig& cfg) {
          cfg.auto_parallel().enable() ||
          cfg.memory_optimization() != RewriterConfig::NO_MEM_OPT ||
          cfg.debug_stripper() == RewriterConfig::ON ||
+         cfg.scoped_allocator_optimization() == RewriterConfig::ON ||
          !cfg.optimizers().empty() || !cfg.custom_optimizers().empty();
 }
 
diff --git a/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc b/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc
new file mode 100644
index 00000000000..cceef4098df
--- /dev/null
+++ b/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc
@@ -0,0 +1,929 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include "tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.h"
+
+#include "tensorflow/core/common_runtime/scoped_allocator.h"
+#include "tensorflow/core/common_runtime/scoped_allocator_mgr.h"
+#include "tensorflow/core/framework/graph.pb.h"
+#include "tensorflow/core/framework/node_def_builder.h"
+#include "tensorflow/core/framework/node_def_util.h"
+#include "tensorflow/core/graph/graph.h"
+#include "tensorflow/core/grappler/costs/graph_properties.h"
+#include "tensorflow/core/grappler/grappler_item.h"
+#include "tensorflow/core/grappler/op_types.h"
+#include "tensorflow/core/grappler/utils/frame.h"
+#include "tensorflow/core/lib/gtl/inlined_vector.h"
+
+// Like TF_RETURN_IF_ERROR, but also logs a WARNING.
+#define LOG_WARNING_AND_RETURN_IF_ERROR(...)            \
+  do {                                                  \
+    const ::tensorflow::Status _status = (__VA_ARGS__); \
+    if (TF_PREDICT_FALSE(!_status.ok())) {              \
+      LOG(WARNING) << "error: " << _status;             \
+      return _status;                                   \
+    }                                                   \
+  } while (0)
+
+namespace tensorflow {
+namespace grappler {
+
+namespace {
+// Node names often have some kind of name_scope prefix, with slashes,
+// and a _nn numeric suffix.  Returns true if the main part of the node_name
+// matches op_name, i.e. it looks from the name like this node is
+// of that op type.
+bool HasOpName(const string& node_name, const string& op_name) {
+  size_t begin = node_name.rfind("/");
+  if (begin == string::npos) {
+    begin = 0;
+  } else {
+    ++begin;
+  }
+  size_t end = node_name.rfind("_");
+  if (end != string::npos) {
+    size_t p = end + 1;
+    while (p < node_name.size()) {
+      if (!isdigit(node_name[p])) {
+        end = node_name.size();
+        break;
+      }
+      ++p;
+    }
+  } else {
+    end = node_name.size();
+  }
+  return node_name.substr(begin, end - begin) == op_name;
+}
+
+// After shape inference has been done each op should be annotated
+// with its output shape(s).  This function iterates over a collection
+// of ops that are a potential application of a ScopedAllocator.  It
+// verifies whether they all have the same output type and if so
+// gathers a vector of their output shapes.  It returns an error if
+// any of the ops doesn't have type or shape data, or if it has more
+// than one output, of if the output type of all ops is not the same.
+// If it returns OK then *type and *shapes should be correctly populated.
+Status CheckTypesAndGetShapes(const GraphProperties& graph_properties,
+                              const std::vector<NodeDef*>& ops, DataType* type,
+                              std::vector<TensorShape>* shapes) {
+  VLOG(1) << "CheckTypesAndGetShapes";
+  *type = DT_INVALID;
+  for (NodeDef* n : ops) {
+    AttrSlice n_attrs = AttrSlice(*n);
+    DataType dtype;
+    LOG_WARNING_AND_RETURN_IF_ERROR(GetNodeAttr(n_attrs, "T", &dtype));
+    VLOG(2) << "op " << n->name() << " has type " << dtype << " shapes.size() "
+            << shapes->size();
+    if (!graph_properties.HasOutputProperties(n->name())) {
+      LOG(ERROR) << "Node " << n->DebugString() << " lacks output shape.";
+      return errors::Internal("Node ", n->name(), " lacks output shape.");
+    }
+    const std::vector<OpInfo::TensorProperties>& prop_list =
+        graph_properties.GetOutputProperties(n->name());
+    if (prop_list.size() != 1) {
+      return errors::Internal("Node ", n->name(),
+                              " does not have exactly one output as expected "
+                              "by ScopedAllocatorOptimizer");
+    }
+    const OpInfo::TensorProperties& props = prop_list[0];
+    if (shapes->empty()) {
+      *type = props.dtype();
+    } else if (*type != props.dtype()) {
+      return errors::Internal("Group ops don't all have same type");
+    } else if (!TensorShape::IsValid(props.shape())) {
+      return errors::Internal("Complete shape not known for ", n->name());
+    }
+    VLOG(2) << "Adding shape " << props.shape().DebugString();
+    shapes->push_back(TensorShape(props.shape()));
+  }
+  return Status::OK();
+}
+
+// Describes an existing input edge in the graph.
+struct InputDesc {
+  NodeDef* from_node_def;
+  int output_slot;
+  NodeDef* to_node_def;
+  InputDesc(NodeDef* f, int os, NodeDef* t)
+      : from_node_def(f), output_slot(os), to_node_def(t) {}
+};
+
+// Populates *inputs with all of the non-control inputs of ops.
+// Returns error if it fails to find exactly one input for each op,
+// or if some input is not of type dtype.
+Status GetInputs(NodeMap* node_map, const std::vector<NodeDef*>& ops,
+                 DataType dtype, std::vector<InputDesc>* inputs) {
+  VLOG(1) << "Getinputs";
+  for (NodeDef* n : ops) {
+    NodeDef* inode = nullptr;
+    int position = 0;
+    VLOG(2) << "for node " << n->name();
+    for (const auto& input_name : n->input()) {
+      if (!IsControlInput(input_name)) {
+        if (inode) {
+          return errors::Internal("Found more than one input for node ",
+                                  n->name());
+        }
+        ParseNodeName(input_name, &position);
+        inode = node_map->GetNode(input_name);
+        CHECK(inode) << input_name;
+        VLOG(2) << "inode " << inode->DebugString();
+      }
+    }
+    AttrSlice inode_attrs = AttrSlice(*inode);
+    DataType inode_dtype;
+    LOG_WARNING_AND_RETURN_IF_ERROR(
+        GetNodeAttr(inode_attrs, "T", &inode_dtype));
+    if (inode_dtype != dtype) {
+      return errors::Internal("ScopedAllocatorOptimizer expected input type ",
+                              dtype, " but found ", inode_dtype);
+    }
+    // inputs->push_back(InputDesc(inode, position, n));
+    inputs->emplace_back(inode, position, n);
+  }
+  return Status::OK();
+}
+
+// Remove the NodeDef nd from node_map and graph.  It must be the case
+// that nd no longer has any input or output edges, though that is not
+// checked.
+void RemoveNode(NodeDef* nd, GraphDef* graph, NodeMap* node_map) {
+  node_map->RemoveNode(nd->name());
+  // TODO(tucker): The efficiency of this routine is poor.
+  // Change to accumulate and do a bulk removal, maybe refactoring
+  // some code from dependency_optimizer.
+  protobuf::RepeatedPtrField<NodeDef>* nodes = graph->mutable_node();
+  for (int i = 0; i < nodes->size(); ++i) {
+    if (nd->name() == (*nodes)[i].name()) {
+      nodes->SwapElements(i, nodes->size() - 1);
+      nodes->RemoveLast();
+      return;
+    }
+  }
+  LOG(FATAL) << "Failed to find node " << nd->name() << " in graph";
+}
+
+// Removes a named edge from between two nodes.
+Status RemoveEdge(const string& input_edge_name, const string& from_node_name,
+                  NodeDef* to_node, NodeMap* node_map) {
+  if (node_map) {
+    node_map->RemoveOutput(from_node_name, to_node->name());
+  }
+  protobuf::RepeatedPtrField<string>* inputs = to_node->mutable_input();
+  int edge_index = -1;
+  for (edge_index = 0; edge_index < inputs->size(); ++edge_index) {
+    VLOG(2) << " consider edge " << (*inputs)[edge_index];
+    if ((*inputs)[edge_index] == input_edge_name) {
+      break;
+    }
+  }
+  if (edge_index >= inputs->size()) {
+    return errors::Internal("Could not find input name ", input_edge_name,
+                            " at node ", to_node->name());
+  }
+  inputs->DeleteSubrange(edge_index, 1);
+  return Status::OK();
+}
+}  // namespace
+
+void ScopedAllocatorOptimizer::ExtendNodeAttr(StringPiece name,
+                                              const std::vector<int32>& values,
+                                              NodeDef* node_def) {
+  if (HasNodeAttr(*node_def, name)) {
+    VLOG(2) << "extending";
+    AttrValue* existing = &(*node_def->mutable_attr())[name.ToString()];
+    for (int32 i : values) {
+      existing->mutable_list()->add_i(i);
+    }
+  } else {
+    VLOG(2) << "setting new attr value";
+    AddNodeAttr(name, values, node_def);
+  }
+}
+
+class UnaryElementwiseRewriter : public ScopedAllocatorOptimizer::Rewriter {
+ public:
+  ~UnaryElementwiseRewriter() override {}
+
+  // Return non-OK if any input is already committed to a ScopedAllocator.
+  Status CheckExistingScopedAllocator(const std::vector<InputDesc>& inputs) {
+    for (const InputDesc& nd : inputs) {
+      VLOG(2) << "get attrs for " << nd.from_node_def->name();
+      AttrSlice n_attrs = AttrSlice(*nd.from_node_def);
+      int sa_id;
+      Status ss = GetNodeAttr(n_attrs, "sa_id", &sa_id);
+      if (ss.ok()) {
+        LOG(INFO) << "Abandoning PARewriter because input "
+                  << nd.from_node_def->name() << " is already assigned "
+                  << "to ScopedAllocator " << sa_id;
+        return errors::Internal(
+            "Abandoning PARewriter because input ", nd.from_node_def->name(),
+            " is already assigned to ScopedAllocator ", sa_id);
+      }
+    }
+    return Status::OK();
+  }
+
+  // Return non-OK if any input is a member of op_set.
+  Status CheckInternalDataDependency(const std::set<string>& op_set,
+                                     const std::vector<InputDesc>& inputs) {
+    for (const InputDesc& nd : inputs) {
+      if (op_set.find(nd.from_node_def->name()) != op_set.end()) {
+        if (nd.output_slot != tensorflow::Graph::kControlSlot) {
+          return errors::Internal("Data edge exists bewtween ",
+                                  nd.from_node_def->name(),
+                                  " and another "
+                                  "node in the set");
+        }
+      }
+    }
+    return Status::OK();
+  }
+
+  // Remove all control edges between members of ops.
+  void ClearInternalControlInputs(const std::set<string>& op_set,
+                                  const std::vector<NodeDef*>& ops,
+                                  NodeMap* node_map) {
+    for (NodeDef* n : ops) {
+      for (const auto& input_name : n->input()) {
+        if (IsControlInput(input_name)) {
+          int position = 0;
+          string input_node_name = ParseNodeName(input_name, &position);
+          CHECK_EQ(position, -1);
+          if (op_set.find(input_node_name) != op_set.end()) {
+            // This is an internal control edge.  Remove it.
+            VLOG(1) << "Remove control output from " << input_node_name
+                    << " via edge " << input_name << " to " << n->name();
+            TF_CHECK_OK(RemoveEdge(input_name, input_node_name, n, node_map));
+          }
+        }
+      }
+    }
+  }
+
+  // Examine the input set of an op set, gathering their shapes and types
+  // and checking whether there are any considerations that prevent use
+  // of a single ScopedAllocator for all of those inputs.
+  Status AnalyzeInputs(ScopedAllocatorOptimizer* sa_opti, NodeMap* node_map,
+                       const std::vector<NodeDef*>& ops,
+                       const std::set<string>& op_instance_names,
+                       string* device_name, DataType* dtype,
+                       std::vector<TensorShape>* input_shapes,
+                       std::vector<InputDesc>* inputs, TensorShape* sa_shape) {
+    CHECK(graph_properties_);
+    LOG_WARNING_AND_RETURN_IF_ERROR(
+        CheckTypesAndGetShapes(*graph_properties_, ops, dtype, input_shapes));
+    LOG_WARNING_AND_RETURN_IF_ERROR(
+        GetInputs(sa_opti->node_map(), ops, *dtype, inputs));
+    LOG_WARNING_AND_RETURN_IF_ERROR(CheckExistingScopedAllocator(*inputs));
+    LOG_WARNING_AND_RETURN_IF_ERROR(
+        CheckInternalDataDependency(op_instance_names, *inputs));
+    ClearInternalControlInputs(op_instance_names, ops, node_map);
+    *device_name = ops[0]->device();
+    CHECK(!device_name->empty());
+    CHECK(!input_shapes->empty());
+    CHECK_EQ(0, Allocator::kAllocatorAlignment % DataTypeSize(*dtype))
+        << "ScopedAllocatorOptimizer only applies to types that evenly "
+        << "divide kAllocatorAlignment";
+    std::vector<ScopedAllocator::Field> sa_fields;
+    // Calculate the field embedding boundaries and thereby the
+    // required size of the backing tensor.
+    int64 num_bytes = ScopedAllocatorMgr::PopulateFields(
+        0 /*scope_id*/, *input_shapes, *dtype, &sa_fields);
+    int64 num_elts = num_bytes / DataTypeSize(*dtype);
+    VLOG(2) << "num_bytes " << num_bytes << " num_elts=" << num_elts;
+    *sa_shape = TensorShape({num_elts});
+    return Status::OK();
+  }
+
+  // Build the ScopedAllocator node that will be assigned to allocate
+  // the output tensors of the input node set.
+  Status ConstructScopedAllocatorNode(
+      ScopedAllocatorOptimizer* sa_opti, GraphDef* graph, NodeMap* node_map,
+      const std::vector<NodeDef*>& ops, const string& device_name,
+      DataType dtype, int sa_id, const string& sa_name,
+      const std::vector<TensorShape>& input_shapes,
+      const std::vector<InputDesc>& inputs, const TensorShape& sa_shape) {
+    VLOG(2) << "ConstructScopedAllocatorNode " << sa_name;
+    NodeDefBuilder sa_builder(sa_name, "_ScopedAllocator");
+    sa_builder.Device(device_name);
+    sa_builder.Attr("sa_name", sa_name);
+    sa_builder.Attr("T", dtype);
+    sa_builder.Attr("id", sa_id);
+    sa_builder.Attr("shapes", input_shapes);
+    sa_builder.Attr("shape", sa_shape);
+    sa_builder.Attr("expected_call_count", static_cast<int64>(ops.size()));
+    NodeDef* sa_node = graph->add_node();
+    LOG_WARNING_AND_RETURN_IF_ERROR(sa_builder.Finalize(sa_node));
+    node_map->AddNode(sa_name, sa_node);
+
+    // Add control edges from the ScopedAllocatorOp to all of the
+    // input nodes and mark them for allocation from backing tensor.
+    for (int i = 0; i < inputs.size(); ++i) {
+      auto& nd = inputs[i];
+      VLOG(2) << "To input " << i << ": " << nd.from_node_def->name()
+              << " add control input "
+              << "^" << sa_name;
+      nd.from_node_def->add_input(strings::StrCat("^", sa_name));
+      // This attribute says: allocate output_slot from
+      // ScopedAllocator instance sa_id + 1 + i.
+      ScopedAllocatorOptimizer::ExtendNodeAttr("_scoped_allocator",
+                                               {nd.output_slot, sa_id + 1 + i},
+                                               nd.from_node_def);
+      node_map->AddOutput(sa_name, nd.from_node_def->name());
+    }
+    return Status::OK();
+  }
+
+  Status BuildSAConcatNode(GraphDef* graph, NodeMap* node_map,
+                           const std::vector<NodeDef*>& ops,
+                           const std::set<string>& op_instance_names,
+                           const string& device_name, DataType dtype, int sa_id,
+                           const string& sa_name, const string& sac_name,
+                           const TensorShape& sa_shape,
+                           std::vector<NodeDefBuilder::NodeOut>* sac_inputs) {
+    VLOG(2) << "BuildSAConcatNode " << sac_name;
+    std::set<string> sac_ctl_inputs;
+    for (int i = 0; i < ops.size(); ++i) {
+      NodeDef* old_op = ops[i];
+      for (const string& old_op_input : old_op->input()) {
+        int position = 0;
+        string input_name = ParseNodeName(old_op_input, &position);
+        if (position == -1) {
+          // A control input: drop if from another member of the op set.
+          if (op_instance_names.find(old_op_input) == op_instance_names.end()) {
+            sac_ctl_inputs.insert(old_op_input);
+          }
+        } else {
+          // TODO(tucker): remove redundant check.
+          // A data input: illegal if from another member of the op set.
+          if (op_instance_names.find(old_op_input) != op_instance_names.end()) {
+            LOG(ERROR) << "Data edge between " << old_op_input << " and "
+                       << old_op->name() << " cannot build ScopedAllocator.";
+            return errors::Internal("Data edge between ", old_op_input, " and ",
+                                    old_op->name(),
+                                    " cannot build ScopedAllocator.");
+          }
+          sac_inputs->push_back(
+              NodeDefBuilder::NodeOut(old_op_input, 0, dtype));
+        }
+        VLOG(3) << "from op " << i << ": " << old_op->name()
+                << " sac_inputs append " << old_op_input;
+      }
+    }
+    NodeDefBuilder sac_builder(sac_name, "_ScopedAllocatorConcat");
+    VLOG(2) << "New sac_name " << sac_name << " shape "
+            << sa_shape.DebugString();
+    sac_builder.Device(device_name);
+    sac_builder.Attr("sa_name", sa_name);
+    sac_builder.Attr("id", sa_id);
+    sac_builder.Attr("T", dtype);
+    sac_builder.Attr("shape", sa_shape);
+    sac_builder.Attr("N", static_cast<int>(sac_inputs->size()));
+    sac_builder.Input(NodeDefBuilder::NodeOut(sa_name, 0, dtype));
+    sac_builder.Input(*sac_inputs);
+    NodeDef* sac_node = graph->add_node();
+    LOG_WARNING_AND_RETURN_IF_ERROR(sac_builder.Finalize(sac_node));
+    node_map->AddNode(sac_name, sac_node);
+    node_map->AddOutput(sa_name, sac_name);
+
+    // Attach the old control inputs to the new sac node.
+    for (const string& ctl_input : sac_ctl_inputs) {
+      sac_node->add_input(ctl_input);
+    }
+    return Status::OK();
+  }
+
+  Status BuildReplacementOp(GraphDef* graph, NodeMap* node_map,
+                            const std::vector<NodeDef*>& ops,
+                            const string& device_name, DataType dtype,
+                            const string& op_name, const string& sac_name,
+                            const string& sa_op_name) {
+    VLOG(2) << "BuildReplacementOp " << sa_op_name;
+    NodeDefBuilder op_builder(sa_op_name, op_name);
+    op_builder.Device(device_name);
+
+    // Transfer the Node Attr from the first replaced Node to the new
+    // Node.  TODO(tucker): In principle we should verify that
+    // the Attr are consistent and compatible across all op instances.
+    // Unfortunately that will probably require op-specific tests, so
+    // punt on that for the time being.
+    AttrSlice first_slice(*ops[0]);
+    for (auto& it : first_slice) {
+      op_builder.Attr(it.first, it.second);
+    }
+    op_builder.Attr("_forward_input", {0, 0});
+    op_builder.Input(sac_name, 0, dtype);
+    NodeDef* sa_op_node = graph->add_node();
+    LOG_WARNING_AND_RETURN_IF_ERROR(op_builder.Finalize(sa_op_node));
+    node_map->AddNode(sa_op_name, sa_op_node);
+    node_map->AddOutput(sac_name, sa_op_name);
+    return Status::OK();
+  }
+
+  Status BuildSplitNode(GraphDef* graph, NodeMap* node_map,
+                        const std::vector<NodeDef*>& ops,
+                        const std::vector<TensorShape>& input_shapes,
+                        const std::vector<NodeDefBuilder::NodeOut>& sac_inputs,
+                        const string& device_name, DataType dtype,
+                        const string& op_name, int sa_id,
+                        const string& sas_name, const string& sa_name,
+                        const string& sa_op_name) {
+    VLOG(2) << "new ScopedAllocatorSplit " << sas_name;
+    NodeDefBuilder sas_builder(sas_name, "_ScopedAllocatorSplit");
+    sas_builder.Device(device_name);
+    sas_builder.Attr("sa_name", sa_name);
+    sas_builder.Attr("id", sa_id);
+    sas_builder.Attr("T", dtype);
+    sas_builder.Attr("shapes", input_shapes);
+    std::vector<NodeDefBuilder::NodeOut> sas_inputs = sac_inputs;
+    sas_builder.Attr("N", static_cast<int>(sas_inputs.size()));
+    sas_builder.Input(NodeDefBuilder::NodeOut(sa_op_name, 0, dtype));
+    sas_builder.Input(sas_inputs);
+    NodeDef* sas_node = graph->add_node();
+    LOG_WARNING_AND_RETURN_IF_ERROR(sas_builder.Finalize(sas_node));
+    node_map->AddNode(sas_name, sas_node);
+    node_map->AddOutput(sa_op_name, sas_name);
+    return Status::OK();
+  }
+
+  // After the new ScopedAllocator and its corresponding Concat and
+  // Split nodes have been built, and a new single Op instance
+  // constructed, rewire the graph: Remove input edges to the old Op
+  // nodes and replace the old Op node outputs with the corresponding
+  // ScopedAllocatorSplit node outputs.  After this the old Op nodes
+  // should no longer have any input or output edges and they can be
+  // removed from the graph.
+  Status RewireSubgraph(GraphDef* graph, NodeMap* node_map,
+                        const std::vector<NodeDef*>& ops,
+                        const std::set<string>& op_instance_names,
+                        const string& op_name, const string& sas_name) {
+    VLOG(2) << "RewireSubgraph";
+    for (int op_idx = 0; op_idx < ops.size(); ++op_idx) {
+      NodeDef* old_op = ops[op_idx];
+      // Copy the output node set since we'll be modifying the version
+      // maintained by NodeMap in the loop.
+      std::set<NodeDef*> output_nodes = node_map->GetOutputs(old_op->name());
+      VLOG(3) << "old_op " << old_op->name() << " had " << output_nodes.size()
+              << " outputs.  Moving them to the PASplit node.";
+      if (VLOG_IS_ON(2)) {
+        for (NodeDef* n : output_nodes) {
+          VLOG(3) << "    output: " << n->name();
+        }
+      }
+      for (NodeDef* n : output_nodes) {
+        VLOG(3) << "really checking old output " << n->name()
+                << " for corresponding input.";
+        if (op_instance_names.find(n->name()) != op_instance_names.end()) {
+          // If this output node is a member of the ops set, it must have
+          // been an internal control edge so drop it.
+          VLOG(3) << "Dropping control output from " << old_op->name() << " to "
+                  << n->name();
+          // However, we may already have dropped it at the clear() below,
+          // so if we fail to find it, that's okay.
+          Status ignore = RemoveEdge(strings::StrCat("^", old_op->name()),
+                                     old_op->name(), n, node_map);
+          continue;
+        }
+        bool found = false;
+        VLOG(3) << "about to iterate over " << n->input_size() << " inputs";
+        for (int i = 0; i < n->input_size(); ++i) {
+          VLOG(3) << "input " << n->input(i);
+          int position = 0;
+          string input_node = ParseNodeName(n->input(i), &position);
+          if (input_node == old_op->name()) {
+            found = true;
+            VLOG(3) << "match pos=" << position;
+            if (position == -1) {
+              // It was a control edge
+              *n->mutable_input(i) = strings::StrCat("^", sas_name);
+            } else {
+              CHECK_EQ(0, position)
+                  << "name " << n->input(i) << " pos " << position;
+              *n->mutable_input(i) = strings::StrCat(sas_name, ":", op_idx);
+            }
+            node_map->RemoveOutput(old_op->name(), n->name());
+            node_map->AddOutput(sas_name, n->name());
+            VLOG(3) << "breaking on success";
+            break;
+          } else {
+            VLOG(3) << "other input " << n->input(i);
+          }
+        }
+        // In general it's required that we found the output node's old
+        // input and replaced it, but one exception is if the output node
+        // is of the same type being coalesced and the edge is a control
+        // input.  In that case it probably got eliminated in an earlier
+        // pass.
+        VLOG(3) << "before HasOp";
+        if (!HasOpName(n->name(), op_name)) {
+          CHECK(found) << "old_op " << old_op->name() << " node "
+                       << " could not find input edge on " << n->DebugString()
+                       << " to replace."
+                       << " " << op_name << " not in " << n->name();
+        }
+        VLOG(3) << "bottom of for output_nodes";
+      }
+      VLOG(3) << "Clearing all inputs of " << old_op->name();
+      node_map->RemoveInputs(old_op->name());
+      old_op->clear_input();
+      node_map->RemoveOutputs(old_op->name());
+      VLOG(3) << "after clear: " << old_op->DebugString();
+      // old_op should be dead, with no further inputs or outputs.
+      // It needs to be removed altogether before the graph is generated,
+      // but we need to leave it around until this Optimizer is done,
+      // because there may be some
+      // Remove.
+      RemoveNode(old_op, graph, node_map);
+    }
+    return Status::OK();
+  }
+
+  // Given a collection of instances of op_name, presumed to be
+  // logically parallel and operating on tensors of the same type,
+  // replace them by a single instance.  First find the upstream Ops
+  // generating their inputs. Create a new ScopedAllocatorOp that
+  // outputs a single backing_tensor pre-arranged for sub-allocation
+  // of all of those input tensors.  Then insert a new
+  // ScopedAllocatorConcatOp below the upstream Ops to make explicit
+  // the materialization of a concatenation of their outputs.  Put the
+  // new op_name instance below the new concat op and follow with a
+  // ScopedAllocatorSplitOp that restores the correct shape outputs
+  // for the consumers of the old op_name instances.
+  //
+  // There must be no non-control edges between Nodes in 'ops'.
+  // Control edges among these nodes will be dropped.
+  Status Rewrite(ScopedAllocatorOptimizer* sa_opti, GraphDef* graph,
+                 const string& op_name, const std::vector<NodeDef*>& ops,
+                 bool* applied) override {
+    if (VLOG_IS_ON(1)) {
+      VLOG(1) << "Rewrite";
+      string op_names;
+      for (auto& nd : ops) {
+        strings::StrAppend(&op_names, nd->name(), ", ");
+      }
+      VLOG(1) << "UnaryElementwiseRewriter::Rewrite " << op_name
+              << " to: " << op_names;
+    }
+    NodeMap* node_map = sa_opti->node_map();
+
+    // Make a set of the node names for faster membership testing.
+    std::set<string> op_instance_names;
+    for (auto& nd : ops) {
+      op_instance_names.insert(nd->name());
+      VLOG(2) << "op_instance_name " << nd->name();
+    }
+    DataType dtype;
+    std::vector<TensorShape> input_shapes;
+    std::vector<InputDesc> inputs;
+    TensorShape sa_shape;
+    string device_name;
+
+    TF_RETURN_IF_ERROR(AnalyzeInputs(sa_opti, node_map, ops, op_instance_names,
+                                     &device_name, &dtype, &input_shapes,
+                                     &inputs, &sa_shape));
+
+    int sa_id = sa_opti->NewScopedAllocatorId(input_shapes.size());
+    string sa_name = strings::StrCat("scoped_allocator_", sa_id);
+    TF_RETURN_IF_ERROR(ConstructScopedAllocatorNode(
+        sa_opti, graph, node_map, ops, device_name, dtype, sa_id, sa_name,
+        input_shapes, inputs, sa_shape));
+
+    // TODO(tucker): Maybe add control edges to delay execution of the
+    // ScopedAllocatorOp until just before first use in order to
+    // conserve memory.  What would be correct?  Let I0...In be the
+    // input nodes that are all going to alloc from SA.  If we make
+    // SA wait until all of these are ready, that might be too slow.
+    // It should probably wait until at least one is ready, but which
+    // one?  Maybe just pick the first.
+    // {
+    //   auto& nd = inputs[0];
+    //   std::vector<InputDesc> inputs_to_first;
+    //   LOG_WARNING_AND_RETURN_IF_ERROR(GetInputs(sa_opti->node_map(),
+    //   {nd.from_node_def},
+    //                                dtype, &inputs_to_first));
+    //   for (int i = 0; i < inputs_to_first.size(); ++i) {
+    //     sa_node->add_input(
+    //         strings::StrCat("^", inputs_to_first[i].from_node_def->name()));
+    //   }
+    // }
+
+    // Build a ScopedAllocatorConcat below all of the input nodes.
+    std::vector<NodeDefBuilder::NodeOut> sac_inputs;
+    string sac_name = strings::StrCat("scoped_allocator_concat_", sa_id);
+    TF_RETURN_IF_ERROR(BuildSAConcatNode(
+        graph, node_map, ops, op_instance_names, device_name, dtype, sa_id,
+        sa_name, sac_name, sa_shape, &sac_inputs));
+
+    // Construct a new instance of the parallel op and insert it
+    // immediately below the new ScopedAllocatorConcat.
+    string sa_op_name = strings::StrCat(sa_name, "_", op_name);
+    TF_RETURN_IF_ERROR(BuildReplacementOp(graph, node_map, ops, device_name,
+                                          dtype, op_name, sac_name,
+                                          sa_op_name));
+
+    // Build a ScopedAllocatorSplit split below the new Op.
+    string sas_name = strings::StrCat("scoped_allocator_split_", sa_id);
+    TF_RETURN_IF_ERROR(BuildSplitNode(graph, node_map, ops, input_shapes,
+                                      sac_inputs, device_name, dtype, op_name,
+                                      sa_id, sas_name, sa_name, sa_op_name));
+
+    // Rewire the graph.
+    TF_RETURN_IF_ERROR(RewireSubgraph(graph, node_map, ops, op_instance_names,
+                                      op_name, sas_name));
+
+    *applied = true;
+    return Status::OK();
+  }
+};
+
+ScopedAllocatorOptimizer::ScopedAllocatorOptimizer(
+    const ScopedAllocatorOptions& opts) {
+  VLOG(1) << "ScopedAllocatorOptimizer::ScopedAllocatorOptimizer";
+  Rewriter* r = new UnaryElementwiseRewriter();
+  to_delete_.push_back(r);
+  if (opts.enable_op_size() == 0) {
+    // Opts handled by default:
+    for (const auto& op_name : {"CollectiveReduce"}) {
+      op_name_set_.insert(op_name);
+      rewriters_[op_name] = r;
+    }
+  } else {
+    for (const auto& op_name : opts.enable_op()) {
+      op_name_set_.insert(op_name);
+      rewriters_[op_name] = r;
+    }
+  }
+}
+
+Status ScopedAllocatorOptimizer::Optimize(Cluster* /*cluster*/,
+                                          const GrapplerItem& item,
+                                          GraphDef* optimized_graph) {
+  *optimized_graph = item.graph;
+  // Nodes that cannot be removed from the graph without damaging correctness,
+  // typically fetch nodes.
+  nodes_to_preserve_ = item.NodesToPreserve();
+
+  GraphProperties graph_properties(item);
+  const bool assume_valid_feeds = opt_level_ == RewriterConfig::AGGRESSIVE;
+  LOG_WARNING_AND_RETURN_IF_ERROR(
+      graph_properties.InferStatically(assume_valid_feeds));
+  node_map_.reset(new NodeMap(optimized_graph));
+
+  LOG_WARNING_AND_RETURN_IF_ERROR(ScopedAllocatorOptimizer::ProcessGraphDef(
+      optimized_graph, graph_properties));
+
+  VLOG(1) << "ScopedAllocatorOptimizer::Optimize() done";
+  return Status::OK();
+}
+
+ScopedAllocatorOptimizer::Rewriter* ScopedAllocatorOptimizer::GetRewriter(
+    const string& op_name) {
+  auto it = rewriters_.find(op_name);
+  if (it != rewriters_.end()) {
+    return it->second;
+  }
+  return nullptr;
+}
+
+int ScopedAllocatorOptimizer::NewScopedAllocatorId(int num_fields) {
+  CHECK_GT(num_fields, 0);
+  int id = next_sa_id_;
+  next_sa_id_ += (num_fields + 1);
+  CHECK_GT(next_sa_id_, 0);
+  return id;
+}
+
+ScopedAllocatorOptimizer::~ScopedAllocatorOptimizer() {
+  for (auto ptr : to_delete_) {
+    delete ptr;
+  }
+}
+
+void ScopedAllocatorOptimizer::FindOpOccurrences(GraphDef* graph,
+                                                 const OpNameSet& op_names,
+                                                 GraphOpOccurrences* occs) {
+  VLOG(1) << "FindOpOccurrences ";
+  for (const auto& it : op_names) {
+    VLOG(1) << "search target " << it;
+  }
+  for (int ni = 0; ni < graph->node_size(); ++ni) {
+    NodeDef* node = graph->mutable_node(ni);
+    const string& op_name = node->op();
+    if (op_names.find(op_name) != op_names.end()) {
+      VLOG(1) << "found " << op_name << " on dev " << node->device();
+      (*occs)[node->device()][op_name].push_back(node);
+    }
+  }
+}
+
+namespace {
+struct OpNameOrder {
+  bool operator()(const NodeDef* a, const NodeDef* b) {
+    return a->name() <= b->name();
+  }
+};
+
+class Tree {
+ public:
+  Tree(const string& edge, int depth) : edge_(edge), depth_(depth) {}
+  ~Tree() {
+    for (auto it : subtrees_) delete it.second;
+  }
+
+  Tree* GetSubTree(const string& edge) {
+    auto it = subtrees_.find(edge);
+    if (it != subtrees_.end()) {
+      return it->second;
+    }
+    Tree* t = new Tree(edge, depth_ + 1);
+    subtrees_[edge] = t;
+    return t;
+  }
+
+  void InsertNode(NodeDef* n) { nodes_.push_back(n); }
+
+  string edge_;
+  int depth_;
+  std::vector<NodeDef*> nodes_;
+  std::unordered_map<string, Tree*> subtrees_;
+};
+
+// Applies a function to every Tree in DFS order.  Terminates early
+// on any non-OK Status.
+Status ApplyToAll(Tree* tree, const std::function<Status(Tree*)>& func) {
+  Status s;
+  for (auto it : tree->subtrees_) {
+    s = ApplyToAll(it.second, func);
+    if (!s.ok()) return s;
+  }
+  s = func(tree);
+  return s;
+}
+
+Tree* ComputeScopeTree(const string& op_name,
+                       const std::vector<NodeDef*>& node_vec) {
+  Tree* root = new Tree("", 0);
+  for (NodeDef* n : node_vec) {
+    std::vector<string> pieces = str_util::Split(n->name(), "/");
+    // last piece is node name proper.
+    int depth = pieces.size() - 1;
+    Tree* subtree = root;
+    for (int i = 0; i < depth; ++i) {
+      subtree = subtree->GetSubTree(pieces[i]);
+    }
+    subtree->InsertNode(n);
+  }
+  return root;
+}
+
+void PartitionByLoopStructure(const FrameMap& frame_map,
+                              std::vector<NodeDef*> nodes,
+                              std::vector<std::vector<NodeDef*>>* loop_groups) {
+  // It is assumed that two nodes with identical loop containment have
+  // identical integer vectors.  Represent those by 64 bit hashes.
+  std::unordered_map<uint64, std::vector<NodeDef*>> loop_sets;
+  for (NodeDef* nd : nodes) {
+    uint64 hash = 0;
+    const auto& it = frame_map.find(nd);
+    if (it != frame_map.end()) {
+      const std::vector<int>& loop_ids = it->second;
+      for (int id : loop_ids) {
+        hash = Hash64Combine(hash, static_cast<uint64>(id));
+      }
+    }
+    loop_sets[hash].push_back(nd);
+  }
+  for (auto it : loop_sets) {
+    loop_groups->push_back(std::move(it.second));
+  }
+}
+
+}  // namespace
+
+Status ScopedAllocatorOptimizer::ProcessGraphDef(
+    GraphDef* graph, const GraphProperties& graph_properties) {
+  VLOG(1) << "ProcessGraphDef";
+  Status status;
+  GraphOpOccurrences occ;
+  FindOpOccurrences(graph, op_name_set_, &occ);
+  if (!occ.empty()) {
+    FrameMap frame_map;
+    int num_frames;
+    LOG_WARNING_AND_RETURN_IF_ERROR(
+        IdentifyFramesWithNodeMap(*graph, *node_map_, &frame_map, &num_frames));
+    for (auto& dt : occ) {
+      VLOG(2) << "Processing device " << dt.first;
+      const DevOpOccurrences& dev_occ = dt.second;
+      for (auto& it : dev_occ) {
+        string op_name = it.first;
+        VLOG(1) << "Processing " << op_name << " set size " << it.second.size();
+        Rewriter* rewriter = GetRewriter(op_name);
+        if (!rewriter) {
+          LOG(ERROR) << "Failed to find PARewriter for op_name " << op_name;
+          continue;
+        }
+        rewriter->SetGraphProperties(graph_properties);
+        std::unique_ptr<Tree> root(ComputeScopeTree(it.first, it.second));
+        // Nodes with a common depth and root path are now grouped
+        // in the same Tree struct.  Split those groups into subgroups that
+        // share identical loop nesting.
+        status = ApplyToAll(
+            root.get(), [this, rewriter, graph, &frame_map, &op_name](Tree* t) {
+              VLOG(2) << "applied to tree node " << t->edge_ << " at depth "
+                      << t->depth_ << " of size " << t->nodes_.size();
+              if (t->nodes_.size() > 1) {
+                std::vector<std::vector<NodeDef*>> loop_groups;
+                PartitionByLoopStructure(frame_map, t->nodes_, &loop_groups);
+                for (auto& lg : loop_groups) {
+                  if (lg.size() > 1) {
+                    bool applied = false;
+                    Status s = OrderNodeSet(&lg);
+                    TF_RETURN_IF_ERROR(s);
+                    VLOG(1) << "Applying Rewriter for " << op_name;
+                    s = rewriter->Rewrite(this, graph, op_name, lg, &applied);
+                    LOG_WARNING_AND_RETURN_IF_ERROR(s);
+                  }
+                }
+              }
+              return Status::OK();
+            });
+        if (!status.ok()) {
+          break;
+        }
+      }
+      if (!status.ok()) {
+        break;
+      }
+    }
+  }
+  VLOG(1) << "ScopedAllocatorOptimizer returning " << status;
+  if (!status.ok()) {
+    LOG(ERROR) << "ScopedAllocatorOptimizer: " << status;
+  }
+  return status;
+}
+
+namespace {
+struct InstanceKeyLess {
+  bool operator()(const NodeDef* a, const NodeDef* b) const {
+    AttrSlice a_attrs = AttrSlice(*a);
+    AttrSlice b_attrs = AttrSlice(*b);
+    int32 a_key = -1;
+    int32 b_key = -1;
+    Status s = GetNodeAttr(a_attrs, "instance_key", &a_key);
+    CHECK(s.ok());
+    s = GetNodeAttr(b_attrs, "instance_key", &b_key);
+    CHECK(s.ok());
+    return a_key < b_key;
+  }
+};
+
+struct NameLess {
+  bool operator()(const NodeDef* a, const NodeDef* b) const {
+    return a->name() < b->name();
+  }
+};
+
+bool IsCollectiveNode(const NodeDef& n) {
+  AttrSlice attrs = AttrSlice(n);
+  int key = -1;
+  if (!IsCollective(n)) return false;
+  Status s = GetNodeAttr(attrs, "instance_key", &key);
+  if (s.ok() && key >= 0) {
+    return true;
+  }
+  return false;
+}
+}  // namespace
+
+Status ScopedAllocatorOptimizer::OrderNodeSet(
+    std::vector<NodeDef*>* nodes) const {
+  // Nodes should be identical type.  Default order is by name but for
+  // collectives we order by increasing instance_key so each group gets
+  // the same instance_key.
+  if (nodes->size() <= 1) return Status::OK();
+  if (IsCollectiveNode(*nodes->at(0))) {
+    sort(nodes->begin(), nodes->end(), InstanceKeyLess());
+  } else {
+    sort(nodes->begin(), nodes->end(), NameLess());
+  }
+  return Status::OK();
+}
+
+}  // namespace grappler
+}  // namespace tensorflow
+
+#undef LOG_WARNING_AND_RETURN_IF_ERROR
diff --git a/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.h b/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.h
new file mode 100644
index 00000000000..ab4d444595f
--- /dev/null
+++ b/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.h
@@ -0,0 +1,107 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef TENSORFLOW_CORE_GRAPPLER_OPTIMIZERS_SCOPED_ALLOCATOR_OPTIMIZER_H_
+#define TENSORFLOW_CORE_GRAPPLER_OPTIMIZERS_SCOPED_ALLOCATOR_OPTIMIZER_H_
+
+#include <unordered_map>
+#include <unordered_set>
+#include <vector>
+#include "tensorflow/core/grappler/optimizers/graph_optimizer.h"
+#include "tensorflow/core/protobuf/rewriter_config.pb.h"
+
+namespace tensorflow {
+namespace grappler {
+class Graph;
+class GraphProperties;
+class NodeMap;
+class ScopedAllocatorOptimizer;
+
+// An Optimizer that introduces ScopedAllocators in order to reduce data
+// movement and consolidate some kinds of Ops.
+class ScopedAllocatorOptimizer : public GraphOptimizer {
+ public:
+  explicit ScopedAllocatorOptimizer(const ScopedAllocatorOptions& opts);
+  ~ScopedAllocatorOptimizer() override;
+
+  string name() const override { return "scoped_allocator_optimizer"; }
+
+  Status Optimize(Cluster* cluster, const GrapplerItem& item,
+                  GraphDef* optimized_graph) override;
+
+  void Feedback(Cluster* cluster, const GrapplerItem& item,
+                const GraphDef& optimized_graph, double result) override {}
+
+  // Map from an Op name to a vector of Nodes with that Op.
+  typedef std::unordered_map<string, std::vector<NodeDef*>> DevOpOccurrences;
+  // Map from a device name to a DevOpOccurrences map.
+  typedef std::unordered_map<string, DevOpOccurrences> GraphOpOccurrences;
+  typedef std::unordered_set<string> OpNameSet;
+
+  Status ProcessGraphDef(GraphDef* graph,
+                         const GraphProperties& graph_properties);
+
+  // Populates *occs by grouping Nodes with common Ops, according to
+  // their assigned devices.
+  void FindOpOccurrences(GraphDef* graph, const OpNameSet& op_names,
+                         GraphOpOccurrences* occs);
+
+  // Returns a new, unused scope_id to be assigned to a ScopedAllocator that
+  // will allocate num_fields (> 0) separate tensors.
+  int NewScopedAllocatorId(int num_fields);
+
+  NodeMap* node_map() { return node_map_.get(); }
+
+  // Appends values to the attr value under name in node_def, if present.
+  // If not present does an assignment.
+  static void ExtendNodeAttr(StringPiece name, const std::vector<int32>& values,
+                             NodeDef* node_def);
+
+  // Class that knows how to do graph rewriting for a particular kind of Op in
+  // order to take advantage of a ScopedAllocator.
+  class Rewriter {
+   public:
+    virtual ~Rewriter() {}
+
+    virtual Status Rewrite(ScopedAllocatorOptimizer* paopti, GraphDef* graph,
+                           const string& op_name,
+                           const std::vector<NodeDef*>& nodes,
+                           bool* applied) = 0;
+
+    void SetGraphProperties(const GraphProperties& graph_properties) {
+      graph_properties_ = &graph_properties;
+      CHECK(graph_properties_);
+    }
+
+   protected:
+    const GraphProperties* graph_properties_;
+  };
+
+ private:
+  Rewriter* GetRewriter(const string& op_name);
+
+  Status OrderNodeSet(std::vector<NodeDef*>* nodes) const;
+
+  RewriterConfig::Toggle opt_level_;
+  std::unordered_set<string> nodes_to_preserve_;
+  OpNameSet op_name_set_;
+  std::unordered_map<string, Rewriter*> rewriters_;
+  std::vector<Rewriter*> to_delete_;
+  int next_sa_id_ = 1;
+  std::unique_ptr<NodeMap> node_map_;
+};
+
+}  // namespace grappler
+}  // namespace tensorflow
+#endif  // TENSORFLOW_CORE_GRAPPLER_OPTIMIZERS_SCOPED_ALLOCATOR_OPTIMIZER_H_
diff --git a/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer_test.cc b/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer_test.cc
new file mode 100644
index 00000000000..3a2859dc5f0
--- /dev/null
+++ b/tensorflow/core/grappler/optimizers/scoped_allocator_optimizer_test.cc
@@ -0,0 +1,243 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include "tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.h"
+
+#include <unordered_set>
+
+#include "tensorflow/cc/ops/standard_ops.h"
+#include "tensorflow/core/framework/node_def.pb.h"
+#include "tensorflow/core/framework/tensor_shape.pb.h"
+#include "tensorflow/core/framework/tensor_testutil.h"
+#include "tensorflow/core/graph/testlib.h"
+#include "tensorflow/core/grappler/grappler_item.h"
+#include "tensorflow/core/grappler/utils.h"
+#include "tensorflow/core/lib/core/status_test_util.h"
+#include "tensorflow/core/lib/strings/strcat.h"
+#include "tensorflow/core/platform/test.h"
+#include "tensorflow/core/protobuf/config.pb.h"
+#include "tensorflow/core/public/session.h"
+#include "tensorflow/core/public/session_options.h"
+
+namespace tensorflow {
+namespace grappler {
+namespace {
+
+class ScopedAllocatorOptimizerTest : public ::testing::Test {
+ public:
+  std::unique_ptr<Session> CreateSession(const GraphDef& graph,
+                                         const ConfigProto& config) {
+    SessionOptions options;
+    options.config = config;
+    (*options.config.mutable_device_count())["CPU"] = 2;
+    Session* session = NewSession(options);
+    TF_CHECK_OK(session->Create(graph));
+    return std::unique_ptr<Session>(session);
+  }
+
+  std::vector<Tensor> EvaluateNodes(const GraphDef& graph,
+                                    const std::vector<string>& fetch) {
+    SessionOptions options;
+    std::unique_ptr<tensorflow::Session> session(NewSession(options));
+    TF_CHECK_OK(session->Create(graph));
+    RunOptions run_options;
+    std::vector<Tensor> output_tensors;
+    TF_CHECK_OK(
+        session->Run(run_options, {}, fetch, fetch, &output_tensors, nullptr));
+    TF_CHECK_OK(session->Close());
+    return output_tensors;
+  }
+
+  // Constructs the following graph.
+  // (Flow is top to bottom, like nature intends.)
+  //
+  // The intended optimization is to have s1 and s2 allocate from
+  // an new ScopedAllocator, then replace a1 and a2 with a3 that
+  // reads from the backing buffer.
+  /*
+        a    b    c
+         \  / \  /
+          s1   s2
+          |    |
+          a1   a2
+          |    |
+          r1   r2
+  */
+  void BuildAbsGraph(GraphDef* graph_def) {
+    tensorflow::Scope s = tensorflow::Scope::NewRootScope();
+    s = s.WithDevice("/job:localhost/replica:0/task:0/device:CPU:0");
+
+    Output a =
+        ops::Const<float>(s.WithOpName("a"), {1.0, 0.0, 0.0, -1.0}, {2, 2});
+    Output b =
+        ops::Const<float>(s.WithOpName("b"), {1.0, -2.0, 3.0, 4.0}, {2, 2});
+    Output c =
+        ops::Const<float>(s.WithOpName("c"), {-5.0, -2.0, 0.0, -2.0}, {2, 2});
+    Output s1 = ops::Add(s.WithOpName("s1"), a, b);
+    Output s2 = ops::Add(s.WithOpName("s2"), b, c);
+    Output a1 = ops::Abs(s.WithOpName("a1"), s1);
+    Output a2 = ops::Abs(s.WithOpName("a2"), s2);
+    Output r1 = ops::Reshape(s.WithOpName("r1"), a1, {1, 4});
+    Output r2 = ops::Reshape(s.WithOpName("r2"), a2, {4, 1});
+    TF_CHECK_OK(s.ToGraphDef(graph_def));
+  }
+
+  void SetShapes(GraphDef* graph_def) {
+    TensorShapeProto shape_proto;
+    shape_proto.add_dim()->set_size(2);
+    shape_proto.add_dim()->set_size(2);
+
+    for (NodeDef& n : *graph_def->mutable_node()) {
+      if (n.op() == "Add" || n.op() == "Abs") {
+        AddNodeAttr("_output_shapes", {shape_proto}, &n);
+      }
+    }
+  }
+};
+
+TEST_F(ScopedAllocatorOptimizerTest, UnaryRewriteOnly) {
+  // Tests that Rewrite of program with parallel unary Ops is done as
+  // anticipated.
+  GrapplerItem item;
+  BuildAbsGraph(&item.graph);
+  SetShapes(&item.graph);
+
+  ScopedAllocatorOptions opts;
+  opts.add_enable_op("Abs");
+  ScopedAllocatorOptimizer sao(opts);
+  ScopedAllocatorOptimizer::OpNameSet ons;
+  ons.insert("Abs");
+
+  GraphDef optimized_graph;
+  TF_ASSERT_OK(sao.Optimize(nullptr /*cluster*/, item, &optimized_graph));
+
+  // Examine the resulting graph def.
+  NodeMap node_map(&optimized_graph);
+  NodeDef* nd = node_map.GetNode("scoped_allocator_1");
+  ASSERT_TRUE(nd);
+  {
+    auto& nd_set = node_map.GetOutputs(nd->name());
+    ASSERT_EQ(3, nd_set.size());
+    std::unordered_set<string> expected = {"scoped_allocator_concat_1", "s1",
+                                           "s2"};
+    for (auto it : nd_set) {
+      ASSERT_NE(expected.find(it->name()), expected.end())
+          << "Failed to find " << it->name();
+    }
+  }
+  {
+    auto& nd_set = node_map.GetOutputs("scoped_allocator_concat_1");
+    ASSERT_EQ(1, nd_set.size());
+    for (auto it : nd_set) {
+      ASSERT_EQ("scoped_allocator_1_Abs", it->name());
+    }
+  }
+  {
+    auto& nd_set = node_map.GetOutputs("scoped_allocator_1_Abs");
+    ASSERT_EQ(1, nd_set.size());
+    for (auto it : nd_set) {
+      ASSERT_EQ("scoped_allocator_split_1", it->name());
+    }
+  }
+  {
+    auto& nd_set = node_map.GetOutputs("scoped_allocator_split_1");
+    ASSERT_EQ(2, nd_set.size());
+    std::unordered_set<string> name_set;
+    for (auto it : nd_set) {
+      name_set.insert(it->name());
+    }
+    ASSERT_TRUE(name_set.find("r1") != name_set.end());
+    ASSERT_TRUE(name_set.find("r2") != name_set.end());
+  }
+}
+
+TEST_F(ScopedAllocatorOptimizerTest, UnaryExecute) {
+  // Constructs the same graph as UnaryRewriteOnly, but actually executes it.
+  GrapplerItem item;
+  BuildAbsGraph(&item.graph);
+
+  // Turn off all optimization except the ScopedAllocatorOptimizer
+  // to avoid anything that would alter the expected graph input/output,
+  // e.g. by constant folding away all calculations.
+  ConfigProto config;
+  GraphOptions* gopt = config.mutable_graph_options();
+  OptimizerOptions* opts = gopt->mutable_optimizer_options();
+  opts->set_do_common_subexpression_elimination(false);
+  opts->set_do_constant_folding(false);
+  opts->set_do_function_inlining(false);
+  opts->set_opt_level(OptimizerOptions::L0);
+  RewriterConfig* rwcfg = gopt->mutable_rewrite_options();
+  rwcfg->clear_optimizers();
+  (*rwcfg->add_optimizers()) = "scoped_allocator";
+  rwcfg->mutable_scoped_allocator_opts()->add_enable_op("Abs");
+  std::unique_ptr<Session> session(CreateSession(item.graph, config));
+
+  std::vector<std::pair<string, Tensor>> inputs;
+
+  // Request two targets: one fetch output and one non-fetched output.
+  std::vector<string> output_names = {"r1:0", "r2:0",
+                                      "scoped_allocator_1_Abs:0"};
+  std::vector<string> target_nodes = {};
+  std::vector<Tensor> outputs;
+  Status s = session->Run(inputs, output_names, target_nodes, &outputs);
+  TF_ASSERT_OK(s);
+  ASSERT_EQ(outputs.size(), 3);
+  std::vector<float> expected_r1({2, 2, 3, 3});
+  std::vector<float> expected_r2({4, 4, 3, 2});
+  // a + b == 2, -2, 3, 3
+  // b + c == -4, -4, 3, 2
+  for (int oi = 0; oi < outputs.size(); ++oi) {
+    for (int i = 0; i < outputs[oi].NumElements(); ++i) {
+      VLOG(0) << "output vec " << oi << " index " << i << " = "
+              << outputs[oi].flat<float>()(i);
+    }
+    if (oi == 0) {
+      ASSERT_EQ(expected_r1.size(), outputs[oi].NumElements());
+      for (int i = 0; i < expected_r1.size(); ++i) {
+        EXPECT_EQ(expected_r1[i], outputs[oi].flat<float>()(i));
+      }
+    } else if (oi == 1) {
+      ASSERT_EQ(expected_r2.size(), outputs[oi].NumElements());
+      for (int i = 0; i < expected_r2.size(); ++i) {
+        EXPECT_EQ(expected_r2[i], outputs[oi].flat<float>()(i));
+      }
+    }
+  }
+}
+
+// Tests static ScopedAllocatorOptimizer::ExtendNodeAttr.
+// Maybe this should be moved elsewhere?
+TEST_F(ScopedAllocatorOptimizerTest, Extend) {
+  NodeDef nd;
+  ScopedAllocatorOptimizer::ExtendNodeAttr("_scoped_allocator", {0, 2}, &nd);
+  ScopedAllocatorOptimizer::ExtendNodeAttr("_scoped_allocator", {6, 7}, &nd);
+  ScopedAllocatorOptimizer::ExtendNodeAttr("_scoped_allocator", {2, 3}, &nd);
+  VLOG(0) << "nd: " << nd.DebugString();
+  std::vector<int> scoped_allocator_attrs;
+  AttrSlice slice(nd);
+  Status sa_status =
+      GetNodeAttr(slice, "_scoped_allocator", &scoped_allocator_attrs);
+  for (int i : scoped_allocator_attrs) {
+    VLOG(0) << "extracted: " << i;
+  }
+  NodeDef nd2;
+  AddNodeAttr("_scoped_allocator", {0, 2}, &nd2);
+  AddNodeAttr("_scoped_allocator", {6, 7}, &nd2);
+  AddNodeAttr("_scoped_allocator", {2, 3}, &nd2);
+  VLOG(0) << "nd2: " << nd2.DebugString();
+}
+
+}  // namespace
+}  // namespace grappler
+}  // namespace tensorflow
diff --git a/tensorflow/core/kernels/scoped_allocator_ops_test.cc b/tensorflow/core/kernels/scoped_allocator_ops_test.cc
index 019c6619ee1..bb0129fa6f7 100644
--- a/tensorflow/core/kernels/scoped_allocator_ops_test.cc
+++ b/tensorflow/core/kernels/scoped_allocator_ops_test.cc
@@ -37,10 +37,12 @@ namespace tensorflow {
 
 class ScopedAllocatorOpTest : public OpsTestBase {
  protected:
-  void MakeOp(const gtl::ArraySlice<TensorShape>& shapes, DataType dtype,
+  void MakeOp(const TensorShape& shape,
+              const gtl::ArraySlice<TensorShape>& shapes, DataType dtype,
               const string& name, int32 id, int32 expected_call_count) {
     TF_EXPECT_OK(NodeDefBuilder("scoped_allocator_op", "_ScopedAllocator")
                      .Attr("T", dtype)
+                     .Attr("shape", shape)
                      .Attr("shapes", shapes)
                      .Attr("sa_name", name)
                      .Attr("id", id)
@@ -61,12 +63,14 @@ class ScopedAllocatorOpTest : public OpsTestBase {
 };
 
 TEST_F(ScopedAllocatorOpTest, Simple) {
-  MakeOp({TensorShape({8})}, DT_FLOAT, "test", 120, 1);
-  MakeOp({TensorShape({32, 32})}, DT_DOUBLE, "test1", 130, 1);
-  MakeOp({TensorShape({64}), TensorShape({3, 3}), TensorShape({5, 5, 5})},
+  MakeOp(TensorShape({8}), {TensorShape({8})}, DT_FLOAT, "test", 120, 1);
+  MakeOp(TensorShape({1024}), {TensorShape({32, 32})}, DT_DOUBLE, "test1", 130,
+         1);
+  MakeOp(TensorShape({204}),
+         {TensorShape({64}), TensorShape({3, 3}), TensorShape({5, 5, 5})},
          DT_HALF, "test2", 140, 3);
-  MakeOp({TensorShape({512}), TensorShape({64, 8})}, DT_UINT32, "test3", 150,
-         2);
+  MakeOp(TensorShape({1024}), {TensorShape({512}), TensorShape({64, 8})},
+         DT_UINT32, "test3", 150, 2);
 }
 
 // PrepOp is common to ConcatOp tests and SplitOpTests.
@@ -249,23 +253,26 @@ TEST_F(ScopedAllocatorConcatOpTest, FailBounds) {
 
 class ScopedAllocatorSplitOpTest : public OpsTestBase {
  protected:
-  void BuildNodeDef(const TensorShape& shape, DataType dtype,
-                    const string& name, int32 id, int32 num_tensors) {
+  void BuildNodeDef(const TensorShape& in_shape, DataType dtype,
+                    const string& name, int32 id, int32 num_tensors,
+                    const std::vector<TensorShape>& out_shapes) {
     TF_EXPECT_OK(
         NodeDefBuilder("scoped_allocator_split_op", "_ScopedAllocatorSplit")
             .Attr("T", dtype)
             .Attr("N", num_tensors)
             .Attr("sa_name", name)
             .Attr("id", id)
+            .Attr("shapes", out_shapes)
             .Input(FakeInput(dtype))  // backing tensor and input
             .Input(
                 FakeInput(num_tensors, dtype))  // list of subtensors to forward
             .Finalize(node_def()));
   }
 
-  void MakeOp(const TensorShape& shape, DataType dtype, const string& name,
-              int32 id, int32 num_tensors) {
-    BuildNodeDef(shape, dtype, name, id, num_tensors);
+  void MakeOp(const TensorShape& in_shape, DataType dtype, const string& name,
+              int32 id, int32 num_tensors,
+              const std::vector<TensorShape>& out_shapes) {
+    BuildNodeDef(in_shape, dtype, name, id, num_tensors, out_shapes);
     TF_EXPECT_OK(InitOp());
   }
 
@@ -305,33 +312,33 @@ class ScopedAllocatorSplitOpTest : public OpsTestBase {
 };
 
 TEST_F(ScopedAllocatorSplitOpTest, Success1) {
-  MakeOp({32}, DT_FLOAT, "test", 120, 2);
+  MakeOp({32}, DT_FLOAT, "test", 120, 2, {{16}, {16}});
   ExecOp(DT_FLOAT, 120, {{16}, {16}});
 }
 
 TEST_F(ScopedAllocatorSplitOpTest, Success2) {
-  MakeOp({2, 2, 2}, DT_DOUBLE, "test", 120, 2);
+  MakeOp({2, 2, 2}, DT_DOUBLE, "test", 120, 2, {{2, 2}, {2, 2}});
   ExecOp(DT_DOUBLE, 120, {{2, 2}, {2, 2}});
 }
 
 TEST_F(ScopedAllocatorSplitOpTest, Success3) {
-  MakeOp({3, 3, 3}, DT_HALF, "test", 120, 3);
+  MakeOp({3, 3, 3}, DT_HALF, "test", 120, 3, {{3, 3}, {3, 3}, {3, 3}});
   ExecOp(DT_HALF, 120, {{3, 3}, {3, 3}, {3, 3}});
 }
 
 TEST_F(ScopedAllocatorSplitOpTest, FailNLessThan2) {
-  BuildNodeDef({4, 4}, DT_FLOAT, "test", 120, 1);
+  BuildNodeDef({4, 4}, DT_FLOAT, "test", 120, 1, {{4, 4}});
   Status s = InitOp();
   EXPECT_EQ(s.code(), error::INVALID_ARGUMENT);
 }
 
 TEST_F(ScopedAllocatorSplitOpTest, FailDtypeCheck) {
-  MakeOp({8}, DT_FLOAT, "test", 120, 2);
+  MakeOp({8}, DT_FLOAT, "test", 120, 2, {{4}, {4}});
   EXPECT_DEATH(ExecOp(DT_HALF, 120, {{4}, {4}}), "");
 }
 
 TEST_F(ScopedAllocatorSplitOpTest, FailBounds) {
-  MakeOp({8}, DT_DOUBLE, "test", 120, 2);
+  MakeOp({8}, DT_DOUBLE, "test", 120, 2, {{4}, {4}});
   AddInputFromArray<double>({8}, {0, 1, 2, 3, 4, 5, 6, 7});
   AddInputFromArray<double>({4}, {0, 1, 2, 3});
   AddInputFromArray<double>({4}, {4, 5, 6, 7});
diff --git a/tensorflow/core/ops/scoped_allocator_ops.cc b/tensorflow/core/ops/scoped_allocator_ops.cc
index 1e0dcdac96c..359b4d8756f 100644
--- a/tensorflow/core/ops/scoped_allocator_ops.cc
+++ b/tensorflow/core/ops/scoped_allocator_ops.cc
@@ -21,6 +21,7 @@ namespace tensorflow {
 REGISTER_OP("_ScopedAllocator")
     .Output("output: T")
     .Attr("shapes: list(shape)")
+    .Attr("shape: shape")
     .Attr("T: type")
     .Attr("sa_name: string")
     .Attr("id: int")
@@ -35,6 +36,16 @@ Returns a reference to this value.
 
 This is an experimental op for internal use only.  It is possible to use this
 op in unsafe ways.
+
+'shapes' is a list of the shapes of the tensors that are to be allocated
+by this ScopedAllocator.
+'shape' is the shape of the output of this Op, i.e. the 1D backing tensor
+from which the individual allocated tensors are aliased.
+'sa_name' is the name assigned to the Node, for connectivity specification
+and debugging.
+'id' is a non-negative integer 'scope_id' handled by the ScopedAllocatorMgr.
+'expected_call_count' is the number of individual tensors expected to
+be allocated from the backing tensor.
 )doc");
 
 REGISTER_OP("_ScopedAllocatorConcat")
@@ -57,6 +68,18 @@ reference to that ScopedAllocator's backing tensor.
 
 This is an experimental op for internal use only.  It is possible to use this
 op in unsafe ways.
+
+'backing' is the backing tensor, i.e. the output of an upstream ScopedAllocator.
+'inputs' is a list of nominal input tensors, all of which must be aliases
+to regions of the backing tensor.  These will be outputs of upstream nodes
+that allocate their outputs from the same ScopedAllocator.
+'shape' is the shape of the output, which will usually be the same shape as
+the input backing tensor.
+'reshape' is true iff the output shape is to be different from that of
+the input backing tensor.
+'sa_name' is the Node name of the upstream ScopedAllocator.
+'id' is the scope_id identifying the upstream ScopedAllocator.
+'N' is the number of nominal inputs to be concatenated.
 )doc");
 
 REGISTER_OP("_ScopedAllocatorSplit")
@@ -67,8 +90,9 @@ REGISTER_OP("_ScopedAllocatorSplit")
     .Attr("sa_name: string")
     .Attr("id: int")
     .Attr("N: int >= 2")
+    .Attr("shapes: list(shape)")
     .SetIsStateful()
-    .SetShapeFn(shape_inference::ExplicitShape)
+    .SetShapeFn(shape_inference::ExplicitShapes)
     .Doc(R"doc(
 Acts roughly like a SplitV Op that splits one tensor into multiple tensors
 but must only be used in conjunction with corresponding ScopedAllocator
@@ -79,6 +103,17 @@ second list.
 
 This is an experimental op for internal use only.  It is possible to use this
 op in unsafe ways.
+
+'concat' is the single output produced by an upstream ScopedAllocatorConcat
+node.  This is actually the backing tensor from a ScopedAllocator node
+upstream of the ScopedAllocatorConcat.
+'split' is a list of tensors aliased from the backing tensor.  It will
+become the output of this ScopedAllocatorSplit node.
+'type' is the common DataType of all of the input and output tensors.
+'sa_name' is the Node name of the upstream ScopedAllocator.
+'id' is the scope_id identifying the upstream ScopedAllocator.
+'N' is the number of split tensors.
+'shapes' is a list of the split tensor shapes.
 )doc");
 
 }  // end namespace tensorflow
diff --git a/tensorflow/core/protobuf/rewriter_config.proto b/tensorflow/core/protobuf/rewriter_config.proto
index 45e57594e4d..bbb25d6f3f7 100644
--- a/tensorflow/core/protobuf/rewriter_config.proto
+++ b/tensorflow/core/protobuf/rewriter_config.proto
@@ -14,6 +14,11 @@ message AutoParallelOptions {
   int32 num_replicas = 2;
 }
 
+message ScopedAllocatorOptions {
+  // If present, only perform optimization for these ops.
+  repeated string enable_op = 1;
+}
+
 message RewriterConfig {
   // Graph rewriting is experimental and subject to change, not covered by any
   // API stability guarantees.
@@ -67,6 +72,9 @@ message RewriterConfig {
   Toggle debug_stripper = 11;
   // If true, don't remove unnecessary ops from the graph
   bool disable_model_pruning = 2;
+  // Try to allocate some independent Op outputs contiguously in order to
+  // merge or eliminate downstream Ops (off by default).
+  Toggle scoped_allocator_optimization = 15;
 
   // Controls how many times we run the optimizers in meta optimizer (default
   // is once).
@@ -115,6 +123,8 @@ message RewriterConfig {
   // meta-optimizer or when manually specified through the optimizers field.
   AutoParallelOptions auto_parallel = 5;
 
+  ScopedAllocatorOptions scoped_allocator_opts = 16;
+
   // If non-empty, will use this as an alternative way to specify a list of
   // optimizations to turn on and the order of the optimizations (replacing the
   // meta-optimizer).

commit d9df4313a98fdc62187a94c5ab6d8955b699e9f2
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Feb 6 12:02:18 2018 -0800

    Improve side_effect_guards to (1) avoid aliasing non-tensors and (2) combine aliasing with re-indenting. Move the renaming visitor into a generic utility API.
    This loses potential efficiency by risking sequencing ops where there is no risk of a race. On the other hand it's still not entirely robust, and we need to raise an error where we can't guarantee that.
    
    PiperOrigin-RevId: 184717456

diff --git a/tensorflow/contrib/py2tf/converters/control_flow.py b/tensorflow/contrib/py2tf/converters/control_flow.py
index a256c074a80..6a942581033 100644
--- a/tensorflow/contrib/py2tf/converters/control_flow.py
+++ b/tensorflow/contrib/py2tf/converters/control_flow.py
@@ -21,6 +21,7 @@ from __future__ import print_function
 import gast
 
 from tensorflow.contrib.py2tf.pyct import anno
+from tensorflow.contrib.py2tf.pyct import ast_util
 from tensorflow.contrib.py2tf.pyct import templates
 from tensorflow.contrib.py2tf.pyct import transformer
 from tensorflow.contrib.py2tf.pyct.static_analysis.annos import NodeAnno
@@ -42,25 +43,6 @@ class SymbolNamer(object):
     raise NotImplementedError()
 
 
-class SymbolRenamer(gast.NodeTransformer):
-  """Transformer that can rename symbols to a simple names."""
-
-  def __init__(self, name_map):
-    self.name_map = name_map
-
-  def _process(self, node):
-    qn = anno.getanno(node, anno.Basic.QN)
-    if qn in self.name_map:
-      return gast.Name(self.name_map[qn], node.ctx, None)
-    return node
-
-  def visit_Name(self, node):
-    return self._process(node)
-
-  def visit_Attribute(self, node):
-    return self._process(node)
-
-
 class ControlFlowTransformer(transformer.Base):
   """Transforms control flow structures like loops an conditionals."""
 
@@ -99,10 +81,8 @@ class ControlFlowTransformer(transformer.Base):
         self.context.namer.new_symbol(s.ssf(), all_referenced)
         for s in aliased_orig_names)
     alias_map = dict(zip(aliased_orig_names, aliased_new_names))
-    node_body = node.body
-    node_body = [SymbolRenamer(alias_map).visit(n) for n in node_body]
-    node_orelse = node.orelse
-    node_orelse = [SymbolRenamer(alias_map).visit(n) for n in node_orelse]
+    node_body = ast_util.rename_symbols(node.body, alias_map)
+    node_orelse = ast_util.rename_symbols(node.orelse, alias_map)
 
     if len(all_modified) == 1:
       results = all_modified[0]
@@ -179,11 +159,8 @@ class ControlFlowTransformer(transformer.Base):
     else:
       state_ast_tuple = gast.Tuple([n.ast() for n in state], None)
 
-    node_body = node.body
-    node_body = [SymbolRenamer(ssf_map).visit(n) for n in node_body]
-
-    test = node.test
-    test = SymbolRenamer(ssf_map).visit(test)
+    node_body = ast_util.rename_symbols(node.body, ssf_map)
+    test = ast_util.rename_symbols(node.test, ssf_map)
 
     template = """
       def test_name(state_ssf):
diff --git a/tensorflow/contrib/py2tf/converters/converter_test_base.py b/tensorflow/contrib/py2tf/converters/converter_test_base.py
index bcb96c81ae7..5b23db33e16 100644
--- a/tensorflow/contrib/py2tf/converters/converter_test_base.py
+++ b/tensorflow/contrib/py2tf/converters/converter_test_base.py
@@ -18,6 +18,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import imp
+
 from tensorflow.contrib.py2tf.pyct import context
 from tensorflow.contrib.py2tf.pyct import parser
 from tensorflow.contrib.py2tf.pyct import qual_names
@@ -28,6 +30,17 @@ from tensorflow.python.platform import test
 
 
 class TestCase(test.TestCase):
+  """Base class for unit tests in this module. Contains relevant utilities."""
+
+  def make_fake_tf(self, *symbols):
+    fake_tf = imp.new_module('fake_tf')
+    for s in symbols:
+      setattr(fake_tf, s.__name__, s)
+    return fake_tf
+
+  def attach_namespace(self, module, **ns):
+    for k, v in ns.items():
+      setattr(module, k, v)
 
   def parse_and_analyze(self,
                         test_fn,
diff --git a/tensorflow/contrib/py2tf/converters/side_effect_guards.py b/tensorflow/contrib/py2tf/converters/side_effect_guards.py
index c73388d06e4..948cb96c3fe 100644
--- a/tensorflow/contrib/py2tf/converters/side_effect_guards.py
+++ b/tensorflow/contrib/py2tf/converters/side_effect_guards.py
@@ -37,6 +37,8 @@ from __future__ import print_function
 import gast
 
 from tensorflow.contrib.py2tf.pyct import anno
+from tensorflow.contrib.py2tf.pyct import ast_util
+from tensorflow.contrib.py2tf.pyct import qual_names
 from tensorflow.contrib.py2tf.pyct import templates
 from tensorflow.contrib.py2tf.pyct import transformer
 from tensorflow.contrib.py2tf.pyct.static_analysis.annos import NodeAnno
@@ -62,45 +64,56 @@ class SideEffectGuardTransformer(transformer.Base):
 
   def __init__(self, context):
     super(SideEffectGuardTransformer, self).__init__(context)
-    self.indent_next = False
-    self.next_indent_owner = None
 
   # pylint:disable=invalid-name
 
   def _visit_and_reindent(self, nodes):
     new_nodes = []
     current_dest = new_nodes
+    alias_map = {}
+    reindent_requested = False
     for n in nodes:
       n = self.visit(n)
+      # NOTE: the order in which these statements execute is important; in
+      # particular, watch out for ending up with cycles in the AST.
+      if alias_map:
+        n = ast_util.rename_symbols(n, alias_map)
       if isinstance(n, (list, tuple)):
         current_dest.extend(n)
       else:
         current_dest.append(n)
-      if self.indent_next:
-        assert self.next_indent_owner is not None
-        current_dest.append(self.next_indent_owner)
-        current_dest = self.next_indent_owner.body
-        self.next_indent_owner = None
-        self.indent_next = False
-    if not current_dest:
+      if anno.hasanno(n, anno.Basic.INDENT_BLOCK_REMAINDER):
+        reindent_requested = True
+        new_dest, new_alias_map = anno.getanno(
+            n, anno.Basic.INDENT_BLOCK_REMAINDER)
+        anno.delanno(n, anno.Basic.INDENT_BLOCK_REMAINDER)
+        new_alias_map.update(alias_map)
+        alias_map = new_alias_map
+        current_dest = new_dest
+    if reindent_requested and not current_dest:
       # TODO(mdan): There may still be something that could be done.
       raise ValueError('Unable to insert statement into the computation flow: '
-                       'it is not followed by any computation that can we can '
-                       'condition on the statement.')
+                       'it is not followed by any computation which '
+                       'the statement could gate.')
     return new_nodes
 
   def visit_FunctionDef(self, node):
     node.body = self._visit_and_reindent(node.body)
     return node
 
-  def _gate_symbols(self, guard_statement, guarded_args):
-    # TODO(mdan): This won't work for variables.
-    template = """
-      (args,) = (tf.identity(a) for a in (args,))
-    """
-    guards = templates.replace(template, args=tuple(guarded_args))
-    guard_statement.body.extend(guards)
-    return guard_statement
+  def visit_With(self, node):
+    node.body = self._visit_and_reindent(node.body)
+    return node
+
+  def visit_If(self, node):
+    node.body = self._visit_and_reindent(node.body)
+    node.orelse = self._visit_and_reindent(node.orelse)
+    return node
+
+  def visit_While(self, node):
+    node.body = self._visit_and_reindent(node.body)
+    node.orelse = self._visit_and_reindent(node.orelse)
+    return node
 
   def visit_Expr(self, node):
     self.generic_visit(node)
@@ -109,30 +122,62 @@ class SideEffectGuardTransformer(transformer.Base):
       #   opt.minimize(loss)
       # or:
       #   tf.py_func(...)
-      template = """
-        with py2tf_utils.control_dependency_on_returns(tf, call):
-          # TODO(mdan): Also insert ops to re-fetch if variables are involved?
-          pass  # Will be removed below.
-      """
-      # TODO(mdan): This is brittle. Reorganize the mechanism.
-      statements = templates.replace(template, call=node.value)
-      control_deps_guard = statements[-1]
-      control_deps_guard.body = []
 
       # First, attempt to gate future evaluation of args. If that's not
       # possible, gate all remaining statements (and that may fail too, see
       # _visit_and_reindent.
       args_scope = anno.getanno(node.value, NodeAnno.ARGS_SCOPE)
-      guarded_args = tuple(args_scope.used & (args_scope.parent.modified
-                                              | args_scope.parent.returned))
+      # NOTE: We can't guard object attributes because they may not be writable.
+      guarded_args = tuple(
+          s for s in args_scope.used if not s.is_composite())
+
+      # TODO(mdan): Include all arguments which depended on guarded_args too.
+      # For example, the following will still cause a race:
+      #   tf.assign(a, a + 1)
+      #   b = a + 1
+      #   tf.assign(a, a + 1)  # Control deps here should include `b`
+      #   c = b + 1
+      # Or maybe we should just raise an "unsafe assign" error?
+
       if guarded_args:
-        node = tuple(statements[:-1]) + (
-            self._gate_symbols(control_deps_guard, guarded_args),)
+        # The aliases may need new names to avoid incorrectly making them local.
+        # TODO(mdan): This is brutal. It will even rename modules - any fix?
+        need_alias = tuple(
+            s for s in guarded_args if s not in args_scope.parent.modified)
+        aliased_new_names = tuple(
+            qual_names.QN(
+                self.context.namer.new_symbol(
+                    s.ssf(), args_scope.parent.referenced)) for s in need_alias)
+        alias_map = dict(zip(need_alias, aliased_new_names))
+        if len(guarded_args) == 1:
+          s, = guarded_args
+          aliased_guarded_args = alias_map.get(s, s)
+        else:
+          aliased_guarded_args = gast.Tuple(
+              [alias_map.get(s, s).ast() for s in guarded_args], None)
+
+        template = """
+          with py2tf_utils.control_dependency_on_returns(tf, call):
+            aliased_guarded_args = py2tf_utils.alias_tensors(tf, guarded_args)
+        """
+        control_deps_guard = templates.replace(
+            template,
+            call=node.value,
+            aliased_guarded_args=aliased_guarded_args,
+            guarded_args=guarded_args)[-1]
       else:
-        node = tuple(statements[:-1])
-        # The mechanism will insert the guard statement later.
-        self.indent_next = True
-        self.next_indent_owner = control_deps_guard
+        alias_map = {}
+
+        template = """
+          with py2tf_utils.control_dependency_on_returns(tf, call):
+            pass
+        """
+        control_deps_guard = templates.replace(template, call=node.value)[-1]
+        control_deps_guard.body = []
+
+      node = control_deps_guard
+      anno.setanno(node, anno.Basic.INDENT_BLOCK_REMAINDER,
+                   (node.body, alias_map))
     return node
 
   # pylint:enable=invalid-name
diff --git a/tensorflow/contrib/py2tf/converters/side_effect_guards_test.py b/tensorflow/contrib/py2tf/converters/side_effect_guards_test.py
index dea09ecc3ff..409c8b02c59 100644
--- a/tensorflow/contrib/py2tf/converters/side_effect_guards_test.py
+++ b/tensorflow/contrib/py2tf/converters/side_effect_guards_test.py
@@ -22,8 +22,12 @@ from tensorflow.contrib.py2tf import utils
 from tensorflow.contrib.py2tf.converters import converter_test_base
 from tensorflow.contrib.py2tf.converters import side_effect_guards
 from tensorflow.contrib.py2tf.pyct import compiler
+from tensorflow.python.framework import constant_op
+from tensorflow.python.framework import errors_impl
 from tensorflow.python.framework import ops
 from tensorflow.python.ops import array_ops
+from tensorflow.python.ops import control_flow_ops
+from tensorflow.python.ops import gen_math_ops
 from tensorflow.python.ops import state_ops
 from tensorflow.python.ops import variables
 from tensorflow.python.platform import test
@@ -32,32 +36,135 @@ from tensorflow.python.platform import test
 class TestNamer(side_effect_guards.SymbolNamer):
 
   def new_symbol(self, name_root, _):
-    return name_root
+    return 'renamed_%s' % name_root
 
 
 class SideEffectGuardsTest(converter_test_base.TestCase):
 
-  def test_transform(self):
+  def _transform_and_compile(self, test_fn):
+    ns = {
+        'control_flow_ops': control_flow_ops,
+        'constant_op': constant_op,
+        'gen_math_ops': gen_math_ops,
+        'ops': ops,
+        'state_ops': state_ops,
+    }
+    node = self.parse_and_analyze(
+        test_fn, ns,
+        namer=TestNamer())
+    node = side_effect_guards.transform(node, self.ctx)
+    result = compiler.ast_to_object(node)
+    self.attach_namespace(result, **ns)
+    result.tf = self.make_fake_tf(array_ops.identity, control_flow_ops.Assert,
+                                  gen_math_ops.greater,
+                                  ops.control_dependencies, ops.Tensor)
+    result.py2tf_utils = utils
+    return result.test_fn, node
+
+  def test_side_effect_on_return_only_variable(self):
 
     def test_fn(a):
       state_ops.assign(a, a + 1)
       return a
 
-    node = self.parse_and_analyze(
-        test_fn, {'state_ops': state_ops}, namer=TestNamer())
-    node = side_effect_guards.transform(node, self.ctx)
-    result = compiler.ast_to_object(node)
-    setattr(result, 'state_ops', state_ops)
-    setattr(result, 'py2tf_utils', utils)
+    tf_test_fn, node = self._transform_and_compile(test_fn)
+
+    self.assertEqual(len(node.body[0].body), 1)
+    with self.test_session() as sess:
+      v = variables.Variable(2)
+      sess.run(v.initializer)
+      # NOTE: We don't expect the assignment to execute in this case, because
+      # variables cannot be reliably guarded.
+      self.assertEqual(2, sess.run(tf_test_fn(v)))
+
+  def test_side_effect_on_used_variable(self):
+
+    def test_fn(a):
+      state_ops.assign(a, a + 1)
+      return a + 1
+
+    tf_test_fn, node = self._transform_and_compile(test_fn)
+
+    self.assertEqual(len(node.body[0].body), 1)
+    with self.test_session() as sess:
+      v = variables.Variable(2)
+      sess.run(v.initializer)
+      # NOTE: Unlike test_side_effect_on_return_only_variable, the variable was
+      # used in the local scope and so we could catch the assign's side effect.
+      self.assertEqual(4, sess.run(tf_test_fn(v)))
+
+  def test_side_effect_on_tensor(self):
+
+    def test_fn(a):
+      control_flow_ops.Assert(gen_math_ops.greater(a, 0), ['expected in throw'])
+      return a
+
+    tf_test_fn, node = self._transform_and_compile(test_fn)
+
+    self.assertEqual(len(node.body[0].body), 1)
+    with self.test_session() as sess:
+      # NOTE: In this case we can also capture the side effect because the
+      # argument is a tensor ans we can wrap it inside an identity.
+      with self.assertRaisesRegexp(errors_impl.InvalidArgumentError,
+                                   'expected in throw'):
+        sess.run(tf_test_fn(constant_op.constant(-1)))
+
+  def test_multiline_block(self):
+
+    def test_fn(a):
+      state_ops.assign(a, a + 1)
+      b = a + 1
+      state_ops.assign(a, b + 1)
+      c = b + 1
+      d = c + 1
+      return d
+
+    tf_test_fn, node = self._transform_and_compile(test_fn)
+
+    self.assertEqual(len(node.body[0].body), 1)
+    with self.test_session() as sess:
+      v = variables.Variable(2)
+      sess.run(v.initializer)
+      self.assertEqual(6, sess.run(tf_test_fn(v)))
+
+  def test_multiline_nested_block(self):
+
+    def test_fn(a):
+      with ops.name_scope('foo'):
+        state_ops.assign(a, a + 1)
+        b = a + 1
+        # state_ops.assign(a, b + 1)
+        c = b + 1
+        d = c + 1
+      return d
+
+    tf_test_fn, node = self._transform_and_compile(test_fn)
+
+    self.assertEqual(len(node.body[0].body[0].body), 1)
+    with self.test_session() as sess:
+      v = variables.Variable(2)
+      sess.run(v.initializer)
+      self.assertEqual(6, sess.run(tf_test_fn(v)))
+
+  def test_multiline_block_unsafe(self):
+
+    def test_fn(a):
+      state_ops.assign(a, a + 1)
+      b = a + 1
+      state_ops.assign(a, a + 1)
+      c = b + 1
+      d = c + 1
+      return d
 
-    # TODO(mdan): Configure the namespaces instead of doing these hacks.
-    ops.identity = array_ops.identity
-    setattr(result, 'tf', ops)
+    tf_test_fn, node = self._transform_and_compile(test_fn)
 
+    self.assertEqual(len(node.body[0].body), 1)
     with self.test_session() as sess:
       v = variables.Variable(2)
       sess.run(v.initializer)
-      self.assertEqual(3, sess.run(result.test_fn(v)))
+      # NOTE: This intentionally highlights the flakiness. The test should be
+      # tightened down once that is solved.
+      self.assertTrue(sess.run(tf_test_fn(v)) in (6, 7))
 
 
 if __name__ == '__main__':
diff --git a/tensorflow/contrib/py2tf/pyct/BUILD b/tensorflow/contrib/py2tf/pyct/BUILD
index 054eb17fb6a..91054fe61df 100644
--- a/tensorflow/contrib/py2tf/pyct/BUILD
+++ b/tensorflow/contrib/py2tf/pyct/BUILD
@@ -1,5 +1,7 @@
 licenses(["notice"])  # Apache 2.0
 
+exports_files(["LICENSE"])
+
 load("//tensorflow:tensorflow.bzl", "py_test")
 
 filegroup(
@@ -19,9 +21,9 @@ py_library(
     srcs = [
         "__init__.py",
         "anno.py",
+        "ast_util.py",
         "compiler.py",
         "context.py",
-        "copier.py",
         "parser.py",
         "pretty_printer.py",
         "qual_names.py",
@@ -49,8 +51,8 @@ py_test(
 )
 
 py_test(
-    name = "compiler_test",
-    srcs = ["compiler_test.py"],
+    name = "ast_util_test",
+    srcs = ["ast_util_test.py"],
     srcs_version = "PY2AND3",
     deps = [
         ":pyct",
@@ -60,8 +62,8 @@ py_test(
 )
 
 py_test(
-    name = "copier_test",
-    srcs = ["copier_test.py"],
+    name = "compiler_test",
+    srcs = ["compiler_test.py"],
     srcs_version = "PY2AND3",
     deps = [
         ":pyct",
diff --git a/tensorflow/contrib/py2tf/pyct/anno.py b/tensorflow/contrib/py2tf/pyct/anno.py
index c6d41f9e128..7a0528b6d0b 100644
--- a/tensorflow/contrib/py2tf/pyct/anno.py
+++ b/tensorflow/contrib/py2tf/pyct/anno.py
@@ -39,6 +39,11 @@ class Basic(NoValue):
   QN = 'Qualified name, as it appeared in the code.'
   SKIP_PROCESSING = (
       'This node should be preserved as is and not processed any further.')
+  INDENT_BLOCK_REMAINDER = (
+      'When a node is annotated with this, the remainder of the block should '
+      'be indented below it. The annotation contains a tuple '
+      '(new_body, name_map), where `new_body` is the new indented block and '
+      '`name_map` allows renaming symbols.')
 
 
 def getanno(node, key, field_name='___pyct_anno'):
@@ -57,3 +62,11 @@ def setanno(node, key, value, field_name='___pyct_anno'):
   # So that the annotations survive gast_to_ast() and ast_to_gast()
   if field_name not in node._fields:
     node._fields += (field_name,)
+
+
+def delanno(node, key, field_name='___pyct_anno'):
+  annotations = getattr(node, field_name)
+  del annotations[key]
+  if not annotations:
+    delattr(node, field_name)
+    node._fields = tuple(f for f in node._fields if f != field_name)
diff --git a/tensorflow/contrib/py2tf/pyct/anno_test.py b/tensorflow/contrib/py2tf/pyct/anno_test.py
index 19e3b457621..ff40bfe1f50 100644
--- a/tensorflow/contrib/py2tf/pyct/anno_test.py
+++ b/tensorflow/contrib/py2tf/pyct/anno_test.py
@@ -37,6 +37,11 @@ class AnnoTest(test.TestCase):
     self.assertTrue(anno.hasanno(node, 'foo'))
     self.assertEqual(3, anno.getanno(node, 'foo'))
 
+    anno.delanno(node, 'foo')
+    self.assertFalse(anno.hasanno(node, 'foo'))
+    with self.assertRaises(AttributeError):
+      anno.getanno(node, 'foo')
+
 
 if __name__ == '__main__':
   test.main()
diff --git a/tensorflow/contrib/py2tf/pyct/copier.py b/tensorflow/contrib/py2tf/pyct/ast_util.py
similarity index 73%
rename from tensorflow/contrib/py2tf/pyct/copier.py
rename to tensorflow/contrib/py2tf/pyct/ast_util.py
index 41598fdc995..f916775b9cf 100644
--- a/tensorflow/contrib/py2tf/pyct/copier.py
+++ b/tensorflow/contrib/py2tf/pyct/ast_util.py
@@ -66,3 +66,31 @@ def copy_clean(node):
     return tuple(copier.visit(n) for n in node)
   else:
     return copier.visit(node)
+
+
+class SymbolRenamer(gast.NodeTransformer):
+  """Transformer that can rename symbols to a simple names."""
+
+  def __init__(self, name_map):
+    self.name_map = name_map
+
+  def _process(self, node):
+    qn = anno.getanno(node, anno.Basic.QN)
+    if qn in self.name_map:
+      return gast.Name(str(self.name_map[qn]), node.ctx, None)
+    return self.generic_visit(node)
+
+  def visit_Name(self, node):
+    return self._process(node)
+
+  def visit_Attribute(self, node):
+    return self._process(node)
+
+
+def rename_symbols(node, name_map):
+  renamer = SymbolRenamer(name_map)
+  if isinstance(node, list):
+    return [renamer.visit(n) for n in node]
+  elif isinstance(node, tuple):
+    return tuple(renamer.visit(n) for n in node)
+  return renamer.visit(node)
diff --git a/tensorflow/contrib/py2tf/pyct/copier_test.py b/tensorflow/contrib/py2tf/pyct/ast_util_test.py
similarity index 57%
rename from tensorflow/contrib/py2tf/pyct/copier_test.py
rename to tensorflow/contrib/py2tf/pyct/ast_util_test.py
index a6b35eda149..e0b00c17816 100644
--- a/tensorflow/contrib/py2tf/pyct/copier_test.py
+++ b/tensorflow/contrib/py2tf/pyct/ast_util_test.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""Tests for copier module."""
+"""Tests for ast_util module."""
 
 from __future__ import absolute_import
 from __future__ import division
@@ -20,11 +20,37 @@ from __future__ import print_function
 
 import ast
 
-from tensorflow.contrib.py2tf.pyct import copier
+from tensorflow.contrib.py2tf.pyct import ast_util
+from tensorflow.contrib.py2tf.pyct import qual_names
 from tensorflow.python.platform import test
 
 
-class CopierTest(test.TestCase):
+class AstUtilTest(test.TestCase):
+
+  def test_rename_symbols(self):
+    node = ast.Tuple([
+        ast.Name('a', ast.Load()),
+        ast.Name('b', ast.Load()),
+        ast.Attribute(ast.Name('b', None), 'c', ast.Store()),
+        ast.Attribute(
+            ast.Attribute(ast.Name('b', None), 'c', ast.Load()), 'd',
+            None)
+    ], None)
+    node = qual_names.resolve(node)
+    node = ast_util.rename_symbols(
+        node,
+        {
+            qual_names.QN('a'): qual_names.QN('renamed_a'),
+            qual_names.QN('b.c'): qual_names.QN('renamed_b_c'),
+        })
+
+    self.assertEqual(node.elts[0].id, 'renamed_a')
+    self.assertTrue(isinstance(node.elts[0].ctx, ast.Load))
+    self.assertEqual(node.elts[1].id, 'b')
+    self.assertEqual(node.elts[2].id, 'renamed_b_c')
+    self.assertTrue(isinstance(node.elts[2].ctx, ast.Store))
+    self.assertEqual(node.elts[3].value.id, 'renamed_b_c')
+    self.assertTrue(isinstance(node.elts[3].value.ctx, ast.Load))
 
   def test_copy_clean(self):
     ret = ast.Return(
@@ -43,7 +69,7 @@ class CopierTest(test.TestCase):
         body=[ret],
         decorator_list=[],
         returns=None)
-    new_node = copier.copy_clean(node)
+    new_node = ast_util.copy_clean(node)
     self.assertFalse(node is new_node)
     self.assertFalse(ret is new_node.body[0])
     self.assertFalse(hasattr(new_node.body[0], '__foo'))
diff --git a/tensorflow/contrib/py2tf/pyct/templates.py b/tensorflow/contrib/py2tf/pyct/templates.py
index 1039fc87139..5fd5252619f 100644
--- a/tensorflow/contrib/py2tf/pyct/templates.py
+++ b/tensorflow/contrib/py2tf/pyct/templates.py
@@ -26,7 +26,7 @@ import textwrap
 
 import gast
 
-from tensorflow.contrib.py2tf.pyct import copier
+from tensorflow.contrib.py2tf.pyct import ast_util
 from tensorflow.contrib.py2tf.pyct import parser
 from tensorflow.contrib.py2tf.pyct import qual_names
 
@@ -77,7 +77,7 @@ class ReplaceTransformer(gast.NodeTransformer):
     if node.id not in self.replacements:
       return node
 
-    new_nodes = copier.copy_clean(self.replacements[node.id])
+    new_nodes = ast_util.copy_clean(self.replacements[node.id])
     if isinstance(new_nodes, gast.AST):
       new_nodes = [new_nodes]
 
diff --git a/tensorflow/contrib/py2tf/utils/BUILD b/tensorflow/contrib/py2tf/utils/BUILD
index 01804aa8834..502720047ef 100644
--- a/tensorflow/contrib/py2tf/utils/BUILD
+++ b/tensorflow/contrib/py2tf/utils/BUILD
@@ -1,5 +1,7 @@
 licenses(["notice"])  # Apache 2.0
 
+exports_files(["LICENSE"])
+
 load("//tensorflow:tensorflow.bzl", "py_test")
 
 filegroup(
@@ -19,6 +21,7 @@ py_library(
     srcs = [
         "__init__.py",
         "context_managers.py",
+        "misc.py",
     ],
     srcs_version = "PY2AND3",
     visibility = ["//tensorflow:__subpackages__"],
@@ -35,3 +38,13 @@ py_test(
         "//tensorflow/python:client_testlib",
     ],
 )
+
+py_test(
+    name = "misc_test",
+    srcs = ["misc_test.py"],
+    srcs_version = "PY2AND3",
+    deps = [
+        ":utils",
+        "//tensorflow/python:client_testlib",
+    ],
+)
diff --git a/tensorflow/contrib/py2tf/utils/__init__.py b/tensorflow/contrib/py2tf/utils/__init__.py
index bca33e89e99..2ab0d7b9fdf 100644
--- a/tensorflow/contrib/py2tf/utils/__init__.py
+++ b/tensorflow/contrib/py2tf/utils/__init__.py
@@ -19,3 +19,4 @@ from __future__ import division
 from __future__ import print_function
 
 from tensorflow.contrib.py2tf.utils.context_managers import control_dependency_on_returns
+from tensorflow.contrib.py2tf.utils.misc import alias_tensors
diff --git a/tensorflow/contrib/py2tf/utils/misc.py b/tensorflow/contrib/py2tf/utils/misc.py
new file mode 100644
index 00000000000..ee5cedd0805
--- /dev/null
+++ b/tensorflow/contrib/py2tf/utils/misc.py
@@ -0,0 +1,48 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Miscellaneous utilities that don't fit anywhere else."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+
+def alias_tensors(tf, *args):
+  """Wrap any Tensor arguments with an identity op.
+
+  Any other argument, including Variables, is returned unchanged.
+
+  Args:
+    tf: The TensorFlow module.
+    *args: Any arguments. Must contain at least one element.
+
+  Returns:
+    Same as *args, with Tensor instances replaced as described.
+
+  Raises:
+    ValueError: If args doesn't meet the requirements.
+  """
+
+  def alias_if_tensor(a):
+    return tf.identity(a) if isinstance(a, tf.Tensor) else a
+
+  # TODO(mdan): Recurse into containers?
+  # TODO(mdan): Anything we can do about variables? Fake a scope reuse?
+  if len(args) > 1:
+    return (alias_if_tensor(a) for a in args)
+  elif len(args) == 1:
+    return alias_if_tensor(args[0])
+
+  raise ValueError('at least one argument required')
diff --git a/tensorflow/contrib/py2tf/utils/misc_test.py b/tensorflow/contrib/py2tf/utils/misc_test.py
new file mode 100644
index 00000000000..5613cc052a8
--- /dev/null
+++ b/tensorflow/contrib/py2tf/utils/misc_test.py
@@ -0,0 +1,64 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for misc module."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import imp
+
+from tensorflow.contrib.py2tf.utils import misc
+from tensorflow.python.framework import constant_op
+from tensorflow.python.framework import ops
+from tensorflow.python.ops import array_ops
+from tensorflow.python.ops import variables
+from tensorflow.python.platform import test
+
+
+class ContextManagersTest(test.TestCase):
+
+  def test_alias_single_tensor(self):
+    a = constant_op.constant(1)
+    fake_tf = imp.new_module('fake_tf')
+    fake_tf.identity = array_ops.identity
+    fake_tf.Tensor = ops.Tensor
+
+    new_a = misc.alias_tensors(fake_tf, a)
+    self.assertFalse(new_a is a)
+    with self.test_session() as sess:
+      self.assertEqual(1, sess.run(new_a))
+
+  def test_alias_tensors(self):
+    a = constant_op.constant(1)
+    v = variables.Variable(2)
+    s = 'a'
+    l = [1, 2, 3]
+    fake_tf = imp.new_module('fake_tf')
+    fake_tf.identity = array_ops.identity
+    fake_tf.Tensor = ops.Tensor
+
+    new_a, new_v, new_s, new_l = misc.alias_tensors(fake_tf, a, v, s, l)
+
+    self.assertFalse(new_a is a)
+    self.assertTrue(new_v is v)
+    self.assertTrue(new_s is s)
+    self.assertTrue(new_l is l)
+    with self.test_session() as sess:
+      self.assertEqual(1, sess.run(new_a))
+
+
+if __name__ == '__main__':
+  test.main()

commit 4e1a7a74b61aa02bc9c3104706afb2153faefddf
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Dec 12 11:18:28 2017 -0800

    Add CompositeNodeManager for Grappler VirtualScheduler.
    
    CompositeNodeManager has per-device LIFO manager, FirstReadyManagers for _Send
    and _Recv ops, and chooses FirstReady among the ops from per-device LIFOManager
    and _Send and _Recv FirstReadyManagers.
    
    This one can maximizes producer-consumer locality within a device (with LIFO),
    but does not introduce previously reported scheduling inefficiency w.r.t.
    multi-device execution with separately managing _Send and _Recv ops and global
    FirstReady policy across devices.
    
    It's implemented, but not enabled; VirtualScheduler still uses
    FirstReadyManager.
    
    PiperOrigin-RevId: 178787352

diff --git a/tensorflow/core/grappler/costs/virtual_scheduler.cc b/tensorflow/core/grappler/costs/virtual_scheduler.cc
index 1554aeb3c07..1e3da6f525a 100644
--- a/tensorflow/core/grappler/costs/virtual_scheduler.cc
+++ b/tensorflow/core/grappler/costs/virtual_scheduler.cc
@@ -91,6 +91,152 @@ struct RecvNodeDescriptorEqual {
 };
 }  // namespace
 
+// ReadyNodeManager
+const NodeDef* LIFOManager::GetCurrNode() {
+  CHECK(!nodes_.empty()) << "GetCurrNode(), but there's no ready node";
+  if (curr_pos_ == nodes_.end()) {
+    curr_pos_ = --(nodes_.rbegin().base());  // Last one in the list.
+  }
+  // Once curr_pos_ is set to a valid entry in the list, we keep using the
+  // cached curr_pos_ until RemoveCurrNode() is called. AddNode() will not
+  // change the GetCurrNode() return value.
+  return *curr_pos_;
+}
+
+void LIFOManager::RemoveCurrNode() {
+  // Make sure we have curr_pos_ ready to be removed.
+  GetCurrNode();
+  // Note curr_pos_ may not be pointing the last element if some nodes are
+  // added.
+  nodes_.erase(curr_pos_);
+
+  curr_pos_ = nodes_.end();  // Reset curr_pos_.
+}
+
+FirstReadyManager::FirstReadyManager(
+    const std::unordered_map<const NodeDef*, NodeState>* node_state)
+    : ReadyNodeManager(), node_state_(node_state) {
+  std::make_heap(nodes_.begin(), nodes_.end());
+  greater_ = [this](const NodeDef* a, const NodeDef* b) -> bool {
+    // Note: we need a node with minimum time_ready, not
+    // maximum; hence, using a > b for comparison function.
+    return node_state_->at(a).time_ready > node_state_->at(b).time_ready;
+  };
+}
+
+const NodeDef* FirstReadyManager::GetCurrNode() {
+  if (nodes_.empty()) {
+    // Nothing in the node_; probably, the very first call. Move
+    // waiting_queue_ to node_.
+    DrainWaitingQueue();
+    CHECK(!nodes_.empty()) << "GetCurrNode(), but there's no ready node";
+  }
+  return nodes_.front();
+}
+
+void FirstReadyManager::RemoveCurrNode() {
+  if (nodes_.empty()) {
+    // Make sure that there is a node to be removed at the front of nodes_.
+    GetCurrNode();
+  }
+  std::pop_heap(nodes_.begin(), nodes_.end(), greater_);
+  nodes_.pop_back();
+  DrainWaitingQueue();
+}
+
+bool FirstReadyManager::Empty() const {
+  return nodes_.empty() && waiting_queue_.empty();
+}
+
+void FirstReadyManager::DrainWaitingQueue() {
+  for (const auto* node : waiting_queue_) {
+    // push_heap in AddNode() and pop_heap in RemoveCurrNode() guarantees that
+    // the first element is the node with minimum time_ready.
+    nodes_.push_back(node);
+    std::push_heap(nodes_.begin(), nodes_.end(), greater_);
+  }
+  waiting_queue_.clear();
+}
+
+CompositeNodeManager::CompositeNodeManager(
+    const std::unordered_map<const NodeDef*, NodeState>* node_state)
+    : ReadyNodeManager(),
+      send_manager_(node_state),
+      recv_manager_(node_state),
+      node_state_(node_state) {
+  curr_node_ = nullptr;
+}
+
+void CompositeNodeManager::AddNode(const NodeDef* node) {
+  if (IsSend(*node)) {
+    send_manager_.AddNode(node);
+  } else if (IsRecv(*node)) {
+    recv_manager_.AddNode(node);
+  } else {
+    const auto& device = node_state_->at(node).device_name;
+    ops_lifo_map_[device].AddNode(node);
+  }
+}
+
+const NodeDef* CompositeNodeManager::GetCurrNode() {
+  if (curr_node_) return curr_node_;
+
+  // Locally (normal ops, not _Send / _Recv) LIFO,
+  // Globally (among the LIFO-selected ops from each device and _Send and
+  // _Recv) FirstReady.
+  std::vector<std::pair<const NodeDef*, Costs::Duration>> candidates;
+  for (auto& ops_lifo : ops_lifo_map_) {
+    if (!ops_lifo.second.Empty()) {
+      const auto* op = ops_lifo.second.GetCurrNode();
+      candidates.emplace_back(op, node_state_->at(op).time_ready);
+    }
+  }
+  if (!send_manager_.Empty()) {
+    const auto* send = send_manager_.GetCurrNode();
+    candidates.emplace_back(send, node_state_->at(send).time_ready);
+  }
+  if (!recv_manager_.Empty()) {
+    const auto* recv = recv_manager_.GetCurrNode();
+    candidates.emplace_back(recv, node_state_->at(recv).time_ready);
+  }
+  CHECK(!candidates.empty());
+  auto first_ready =
+      std::min_element(candidates.begin(), candidates.end(),
+                       [](const std::pair<const NodeDef*, Costs::Duration>& a,
+                          const std::pair<const NodeDef*, Costs::Duration>& b) {
+                         return a.second < b.second;
+                       });
+  // Next time we call GetCurrNode(), it just returns the cached one,
+  // curr_node_ until we call RemovCurrNode().
+  curr_node_ = first_ready->first;
+
+  return curr_node_;
+}
+
+void CompositeNodeManager::RemoveCurrNode() {
+  const auto* node = GetCurrNode();
+  if (IsSend(*node)) {
+    send_manager_.RemoveCurrNode();
+  } else if (IsRecv(*node)) {
+    recv_manager_.RemoveCurrNode();
+  } else {
+    const auto device = node_state_->at(node).device_name;
+    ops_lifo_map_[device].RemoveCurrNode();
+  }
+  // Reset curr_node_ so that GetCurrNode() finds another node.
+  curr_node_ = nullptr;
+}
+
+bool CompositeNodeManager::Empty() const {
+  // Empty if all the ready managers are empty.
+  bool empty = true;
+  for (const auto& ops_lifo : ops_lifo_map_) {
+    empty &= ops_lifo.second.Empty();
+  }
+  return empty && send_manager_.Empty() && recv_manager_.Empty();
+}
+
+// VirtualScheduler
 VirtualScheduler::VirtualScheduler(const GrapplerItem* grappler_item,
                                    const bool use_static_shapes,
                                    Cluster* cluster)
@@ -112,6 +258,8 @@ ReadyNodeManager* VirtualScheduler::ReadyNodeManagerFactory(
     return new LIFOManager();
   } else if (ready_node_manager == "FirstReady") {
     return new FirstReadyManager(GetNodeStates());
+  } else if (ready_node_manager == "Composite") {
+    return new CompositeNodeManager(GetNodeStates());
   }
   LOG(FATAL) << "Not a valid ready node manager: " << ready_node_manager;
 }
diff --git a/tensorflow/core/grappler/costs/virtual_scheduler.h b/tensorflow/core/grappler/costs/virtual_scheduler.h
index 3018e3509a2..74088780cb3 100644
--- a/tensorflow/core/grappler/costs/virtual_scheduler.h
+++ b/tensorflow/core/grappler/costs/virtual_scheduler.h
@@ -158,25 +158,8 @@ class LIFOManager : public ReadyNodeManager {
   LIFOManager() : ReadyNodeManager() {}
   ~LIFOManager() override {}
   void AddNode(const NodeDef* node) override { nodes_.push_back(node); }
-  const NodeDef* GetCurrNode() override {
-    CHECK(!nodes_.empty()) << "GetCurrNode(), but there's no ready node";
-    if (curr_pos_ == nodes_.end()) {
-      curr_pos_ = --(nodes_.rbegin().base());  // Last one in the list.
-    }
-    // Once curr_pos_ is set to a valid entry in the list, we keep using the
-    // cached curr_pos_ until RemoveCurrNode() is called. AddNode() will not
-    // change the GetCurrNode() return value.
-    return *curr_pos_;
-  }
-  void RemoveCurrNode() override {
-    // Make sure we have curr_pos_ ready to be removed.
-    GetCurrNode();
-    // Note curr_pos_ may not be pointing the last element if some nodes are
-    // added.
-    nodes_.erase(curr_pos_);
-
-    curr_pos_ = nodes_.end();  // Reset curr_pos_.
-  }
+  const NodeDef* GetCurrNode() override;
+  void RemoveCurrNode() override;
   bool Empty() const override { return nodes_.empty(); }
 
  private:
@@ -194,54 +177,16 @@ class LIFOManager : public ReadyNodeManager {
 class FirstReadyManager : public ReadyNodeManager {
  public:
   FirstReadyManager(
-      const std::unordered_map<const NodeDef*, NodeState>* node_state)
-      : ReadyNodeManager(), node_state_(node_state) {
-    std::make_heap(nodes_.begin(), nodes_.end());
-    greater_ = [this](const NodeDef* a, const NodeDef* b) -> bool {
-      // Note: we need a node with minimum time_ready, not
-      // maximum; hence, using a > b for comparison function.
-      return node_state_->at(a).time_ready > node_state_->at(b).time_ready;
-    };
-  }
+      const std::unordered_map<const NodeDef*, NodeState>* node_state);
   ~FirstReadyManager() override {}
-
   void AddNode(const NodeDef* node) override { waiting_queue_.push_back(node); }
-
-  const NodeDef* GetCurrNode() override {
-    if (nodes_.empty()) {
-      // Nothing in the node_; probably, the very first call. Move
-      // waiting_queue_ to node_.
-      _DrainWaitingQueue();
-      CHECK(!nodes_.empty()) << "GetCurrNode(), but there's no ready node";
-    }
-    return nodes_.front();
-  }
-
-  void RemoveCurrNode() override {
-    if (nodes_.empty()) {
-      // Make sure that there is a node to be removed at the front of nodes_.
-      GetCurrNode();
-    }
-    std::pop_heap(nodes_.begin(), nodes_.end(), greater_);
-    nodes_.pop_back();
-    _DrainWaitingQueue();
-  }
-
-  bool Empty() const override {
-    return nodes_.empty() && waiting_queue_.empty();
-  }
+  const NodeDef* GetCurrNode() override;
+  void RemoveCurrNode() override;
+  bool Empty() const override;
 
  private:
   // Move all the nodes in the waiting_queue_ to nodes_.
-  void _DrainWaitingQueue() {
-    for (const auto* node : waiting_queue_) {
-      // push_heap in AddNode() and pop_heap in RemoveCurrNode() guarantees that
-      // the first element is the node with minimum time_ready.
-      nodes_.push_back(node);
-      std::push_heap(nodes_.begin(), nodes_.end(), greater_);
-    }
-    waiting_queue_.clear();
-  }
+  void DrainWaitingQueue();
 
   // nodes_ is the main queue, where we construct heap, and the front is the
   // current node.
@@ -259,6 +204,41 @@ class FirstReadyManager : public ReadyNodeManager {
   const std::unordered_map<const NodeDef*, NodeState>* node_state_;
 };
 
+// CompositeNodeManager has a few other NodeManagers: per-device LIFO for normal
+// ops (neither _Send nor _Recv) and FirstyReadyManagers for _Send ops and _Recv
+// ops, and then it chooses FirstReady among the ops chosen from each
+// internal NodeManagers. The objective is to maximize producer-consumer
+// locality within device, while processing nodes across devices, including
+// _Send and _Recv, fairly, in terms of their time_ready.
+class CompositeNodeManager : public ReadyNodeManager {
+ public:
+  CompositeNodeManager(
+      const std::unordered_map<const NodeDef*, NodeState>* node_state);
+  ~CompositeNodeManager() override {}
+
+  void AddNode(const NodeDef* node) override;
+  const NodeDef* GetCurrNode() override;
+  void RemoveCurrNode() override;
+  bool Empty() const override;
+
+ private:
+  // Internal ready node managers:
+  // LIFO for normal ops to maximize producer consumer locality.
+  // One LIFO per device.
+  std::unordered_map<string, LIFOManager> ops_lifo_map_;
+  // FirstReady for send and recv. Handle send and recv separately ensures that
+  // send and recv do not block previously read ops with LIFO schedule.
+  FirstReadyManager send_manager_;
+  FirstReadyManager recv_manager_;
+
+  // NodeState structure from VirtualScheduler to get time_ready of ready nodes.
+  // Not owned by FirstReadyManager.
+  const std::unordered_map<const NodeDef*, NodeState>* node_state_;
+
+  // Cached curr node. Set back to nullptr from RemoveCurrNode().
+  const NodeDef* curr_node_;
+};
+
 // The virtual scheduler emulates execution of nodes in a graph, considering
 // dependencies, device, etc.
 class VirtualScheduler {
diff --git a/tensorflow/core/grappler/costs/virtual_scheduler_test.cc b/tensorflow/core/grappler/costs/virtual_scheduler_test.cc
index 412b494be73..c5e6aa89894 100644
--- a/tensorflow/core/grappler/costs/virtual_scheduler_test.cc
+++ b/tensorflow/core/grappler/costs/virtual_scheduler_test.cc
@@ -44,8 +44,15 @@ class VirtualSchedulerTest : public ::testing::Test {
   NodeDef node1_, node2_, node3_, node4_, node5_, node6_;
   std::unordered_map<const NodeDef*, NodeState> node_states_;
 
+  // Device names:
   const string kCPU0 = "/job:localhost/replica:0/task:0/cpu:0";
   const string kCPU1 = "/job:localhost/replica:0/task:0/cpu:1";
+  const string kChannelFrom0To1 = "Channel from CPU0 to CPU1";
+  const string kChannelFrom1To0 = "Channel from CPU1 to CPU0";
+  // Op names:
+  const string kSend = "_Send";
+  const string kRecv = "_Recv";
+  const string kConv2D = "Conv2D";
 
   DeviceProperties GetDummyCPUDevice() {
     // Create CPU with 2 cores, 4 Ghz freq, 2 GB/s mem bandwidth.
@@ -59,29 +66,26 @@ class VirtualSchedulerTest : public ::testing::Test {
     return cpu_device;
   }
 
+  void NodeSetUp(const string& name, const string& op_name,
+                 const string& device_name, const uint64 time_ready,
+                 NodeDef* node) {
+    node->set_name(name);
+    node->set_op(op_name);
+    node->set_device(device_name);
+
+    node_states_[node] = NodeState();
+    node_states_[node].time_ready = time_ready;
+    node_states_[node].device_name = device_name;
+  }
+
   void SetUp() override {
-    // Initializes nodes for manager
-    node1_.set_name("Node1");
-    node2_.set_name("Node2");
-    node3_.set_name("Node3");
-    node4_.set_name("Node4");
-    node5_.set_name("Node5");
-    node6_.set_name("Node6");
-
-    // Initialize node_states, with time_ready in reverse order.
-    node_states_[&node1_] = NodeState();
-    node_states_[&node2_] = NodeState();
-    node_states_[&node3_] = NodeState();
-    node_states_[&node4_] = NodeState();
-    node_states_[&node5_] = NodeState();
-    node_states_[&node6_] = NodeState();
-
-    node_states_[&node6_].time_ready = 1000;
-    node_states_[&node5_].time_ready = 2000;
-    node_states_[&node4_].time_ready = 3000;
-    node_states_[&node3_].time_ready = 4000;
-    node_states_[&node2_].time_ready = 5000;
-    node_states_[&node1_].time_ready = 6000;
+    // node1_ to node6_ on kCPU0, with time_ready in reverse_order.
+    NodeSetUp("Node1", kConv2D, kCPU0, 6000, &node1_);
+    NodeSetUp("Node2", kConv2D, kCPU0, 5000, &node2_);
+    NodeSetUp("Node3", kConv2D, kCPU0, 4000, &node3_);
+    NodeSetUp("Node4", kConv2D, kCPU0, 3000, &node4_);
+    NodeSetUp("Node5", kConv2D, kCPU0, 2000, &node5_);
+    NodeSetUp("Node6", kConv2D, kCPU0, 1000, &node6_);
 
     // Initializes cluster_ and placer_.
     std::unordered_map<string, DeviceProperties> devices;
@@ -1207,15 +1211,9 @@ TEST_F(VirtualSchedulerTest, GetCurrNodeFirstReadyManager) {
   NodeDef node7;
   NodeDef node8;
   NodeDef node9;
-  node7.set_name("Node7");
-  node8.set_name("Node8");
-  node9.set_name("Node9");
-  node_states_[&node7] = NodeState();
-  node_states_[&node8] = NodeState();
-  node_states_[&node9] = NodeState();
-  node_states_[&node7].time_ready = 5;
-  node_states_[&node8].time_ready = 4;
-  node_states_[&node9].time_ready = 3;
+  NodeSetUp("Node7", kConv2D, kCPU0, 5, &node7);
+  NodeSetUp("Node8", kConv2D, kCPU0, 4, &node8);
+  NodeSetUp("Node9", kConv2D, kCPU0, 3, &node9);
 
   manager.AddNode(&node7);
   EXPECT_EQ("Node6", manager.GetCurrNode()->name());
@@ -1249,6 +1247,132 @@ TEST_F(VirtualSchedulerTest, GetCurrNodeFirstReadyManager) {
   EXPECT_TRUE(manager.Empty());
 }
 
+TEST_F(VirtualSchedulerTest, RemoveSingleNodeCompositeNodeManager) {
+  CompositeNodeManager manager = CompositeNodeManager(&node_states_);
+
+  manager.AddNode(&node1_);
+  manager.RemoveCurrNode();
+  EXPECT_TRUE(manager.Empty());
+}
+
+TEST_F(VirtualSchedulerTest, RemoveSingleNodeComopsiteNodeManager) {
+  CompositeNodeManager manager = CompositeNodeManager(&node_states_);
+
+  manager.AddNode(&node1_);
+  manager.RemoveCurrNode();
+  EXPECT_TRUE(manager.Empty());
+}
+
+TEST_F(VirtualSchedulerTest, GetAndRemoveMultipleComopsiteNodeManager) {
+  CompositeNodeManager manager = CompositeNodeManager(&node_states_);
+
+  // Add the nodes to LIFOManager.
+  manager.AddNode(&node1_);
+  manager.AddNode(&node2_);
+  manager.AddNode(&node3_);
+  manager.AddNode(&node4_);
+
+  // Keep checking current node as nodes are removed and added.
+  EXPECT_EQ("Node4", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_EQ("Node3", manager.GetCurrNode()->name());
+  manager.AddNode(&node5_);
+  // GetCurrNode()  should return the same node even if some nodes are added,
+  // until RemoveCurrNode() is called.
+  EXPECT_EQ("Node3", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_EQ("Node5", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_EQ("Node2", manager.GetCurrNode()->name());
+  manager.AddNode(&node6_);
+  EXPECT_EQ("Node2", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_EQ("Node6", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_EQ("Node1", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_TRUE(manager.Empty());
+}
+
+TEST_F(VirtualSchedulerTest, MultiDeviceSendRecvComopsiteNodeManager) {
+  CompositeNodeManager manager = CompositeNodeManager(&node_states_);
+
+  // Additional nodes on kCPU1
+  NodeDef node7;
+  NodeDef node8;
+  NodeDef node9;
+  NodeSetUp("Node7", kConv2D, kCPU1, 1001, &node7);
+  NodeSetUp("Node8", kConv2D, kCPU1, 2001, &node8);
+  NodeSetUp("Node9", kConv2D, kCPU1, 3001, &node9);
+
+  // Send and Recv nodes.
+  NodeDef send1;
+  NodeDef send2;
+  NodeDef recv1;
+  NodeDef recv2;
+  NodeSetUp("Send1", kSend, kChannelFrom0To1, 2002, &send1);
+  NodeSetUp("Send2", kSend, kChannelFrom1To0, 2005, &send2);
+  NodeSetUp("Recv1", kRecv, kCPU0, 2003, &recv1);
+  NodeSetUp("Recv2", kRecv, kCPU1, 2003, &recv2);
+
+  // Insert nodes.
+  manager.AddNode(&node1_);
+  manager.AddNode(&node2_);
+  manager.AddNode(&node3_);
+  manager.AddNode(&node4_);
+  manager.AddNode(&node5_);
+  manager.AddNode(&node6_);
+  manager.AddNode(&node7);
+  manager.AddNode(&node8);
+  manager.AddNode(&node9);
+  manager.AddNode(&send1);
+  manager.AddNode(&send2);
+  manager.AddNode(&recv1);
+  manager.AddNode(&recv2);
+
+  // on kCPU0; last one is node6_, on kCPU1: last one is node9;
+  // so choose one that has earliest time_ready among node6_, node9,
+  // Send1, Send2, Recv1, and Recv2.
+  EXPECT_EQ("Node6", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Then, the next one on kCPU0 is node5_; choose the earliest time_ready node
+  // among node5_, node9, Send1, Send2, Recv1, and Recv2.
+  EXPECT_EQ("Node5", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Next, choose among node4_, node9, Send1, Send2, Recv1, and Recv2.
+  EXPECT_EQ("Send1", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Next, choose among node4_, node9, Sen2, Recv1, and Recv2.
+  EXPECT_EQ("Recv1", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Next, choose among node4_, node9, Send2, and Recv2.
+  EXPECT_EQ("Recv2", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Next, choose among node4_, node9, and Send2.
+  EXPECT_EQ("Send2", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Next, choose between node4_, node9.
+  EXPECT_EQ("Node4", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Next, choose between node3_, node9.
+  EXPECT_EQ("Node9", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Next, choose between node3_, node8.
+  EXPECT_EQ("Node8", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Next, choose between node3_, node7.
+  EXPECT_EQ("Node7", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  // Then, just the nodes on kCPU1 -- LIFO.
+  EXPECT_EQ("Node3", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_EQ("Node2", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_EQ("Node1", manager.GetCurrNode()->name());
+  manager.RemoveCurrNode();
+  EXPECT_TRUE(manager.Empty());
+}
+
 // Create small graph, run predict costs on it, make sure the costs from the
 // summary match the hand-calculated costs.
 TEST_F(VirtualSchedulerTest, SummaryCostTest) {
@@ -1634,20 +1758,20 @@ TEST_F(VirtualSchedulerTest, InterDeviceTransfer) {
     const auto& name = x.first;
     const auto& node_info = x.second;
     const auto& op = node_info.op_info.op();
-    if (op == "_Recv") {
+    if (op == kRecv) {
       recv_op_names[get_port_num(name)] = name;
-    } else if (op == "_Send") {
+    } else if (op == kSend) {
       send_op_names[get_port_num(name)] = name;
     }
     op_count[op]++;
   }
 
   // Same number of _Send and _Recv.
-  EXPECT_EQ(op_count.at("_Send"), op_count.at("_Recv"));
+  EXPECT_EQ(op_count.at(kSend), op_count.at(kRecv));
 
   // Expect 4 Send and Recvs each: port 0, 1, and, 2, and control dependency.
-  EXPECT_EQ(op_count.at("_Recv"), 4);
-  EXPECT_EQ(op_count.at("_Send"), 4);
+  EXPECT_EQ(op_count.at(kRecv), 4);
+  EXPECT_EQ(op_count.at(kSend), 4);
 
   // Helper lambda for extracting output Tensor size.
   auto get_output_size = [this, ops_executed](const string& name) -> int64 {

commit 0ba2a1f6db399cbb5be3e71acdad1123af29348a
Author: Jeffrey A. Dean <jeff@google.com>
Date:   Fri Aug 4 14:55:48 2017 -0700

    Various compiler speedups.  Improves compilation of an image model from
    35.6 seconds to 33.3 seconds (average of three runs) (+6.5% improvement).
    
    (1) Avoid extra hash table lookups in HeapSimulator by holding onto the pointer
    to the hash table value, rather than looking it up from the same key multiple
    times.
    
    (2) In HeapSimulator, reuse operand_buffers_to_free and dead_buffers_to_free
    across all instructions, rather than allocating new vectors on every
    instructions.
    
    (3) Avoid use of Printf and improve efficiency of string generation for
    HloInstruction::ToString and ShapeUtil::HumanStringWithLayout.
    PiperOrigin-RevId: 164314222

diff --git a/tensorflow/compiler/xla/service/heap_simulator.cc b/tensorflow/compiler/xla/service/heap_simulator.cc
index 726ed5c86a2..4d6c4a54ca7 100644
--- a/tensorflow/compiler/xla/service/heap_simulator.cc
+++ b/tensorflow/compiler/xla/service/heap_simulator.cc
@@ -110,6 +110,8 @@ Status HeapSimulator::RunComputation(
   FlatSet<const LogicalBuffer*> output_source_buffers =
       points_to_analysis.GetPointsToSet(root).CreateFlattenedSet();
 
+  std::vector<const LogicalBuffer*> dead_buffers_to_free;
+  std::vector<const LogicalBuffer*> operand_buffers_to_free;
   for (const HloInstruction* instruction : instruction_sequence) {
     const TuplePointsToAnalysis::BufferDefinitionVector&
         buffers_defined_by_instruction =
@@ -125,17 +127,21 @@ Status HeapSimulator::RunComputation(
     // dependencies have already been accounted for in the ordering of the given
     // 'instruction_sequence', and should not otherwise artificially extend the
     // lifetime of buffers that aren't already connected by a data dependency.
-    std::vector<const LogicalBuffer*> dead_buffers_to_free;
+    dead_buffers_to_free.clear();
     for (const LogicalBuffer* buffer : buffers_defined_by_instruction) {
       if (IgnoreBuffer(buffer)) {
         continue;
       }
+      FlatSet<const HloInstruction*>* live_set = nullptr;
       for (const BufferAlias& alias :
            points_to_analysis.GetBufferAliases(*buffer)) {
         const std::vector<HloInstruction*>& users =
             alias.instruction()->users();
         if (!users.empty()) {
-          live_buffers[buffer].insert(users.begin(), users.end());
+          if (live_set == nullptr) {
+            live_set = &live_buffers[buffer];
+          }
+          live_set->insert(users.begin(), users.end());
         }
       }
 
@@ -161,15 +167,17 @@ Status HeapSimulator::RunComputation(
     // all source buffers of all operands of this instruction.  Buffers that
     // have no instructions left to visit are moved from live_buffers to
     // operand_buffers_to_free.
-    std::vector<const LogicalBuffer*> operand_buffers_to_free;
+    operand_buffers_to_free.clear();
     for (const LogicalBuffer* operand_buffer :
          UniqueOperandSourceBuffers(instruction, points_to_analysis)) {
       if (IgnoreBuffer(operand_buffer)) {
         continue;
       }
-      live_buffers[operand_buffer].erase(instruction);
-      if (live_buffers[operand_buffer].empty()) {
-        live_buffers.erase(operand_buffer);
+      auto it = live_buffers.find(operand_buffer);
+      FlatSet<const HloInstruction*>* live_set = &it->second;
+      live_set->erase(instruction);
+      if (live_set->empty()) {
+        live_buffers.erase(it);
         operand_buffers_to_free.push_back(operand_buffer);
       }
     }
diff --git a/tensorflow/compiler/xla/service/hlo_instruction.cc b/tensorflow/compiler/xla/service/hlo_instruction.cc
index 99c56c74d6a..5e1c5191166 100644
--- a/tensorflow/compiler/xla/service/hlo_instruction.cc
+++ b/tensorflow/compiler/xla/service/hlo_instruction.cc
@@ -39,13 +39,11 @@ limitations under the License.
 #include "tensorflow/core/lib/gtl/flatmap.h"
 #include "tensorflow/core/lib/strings/str_util.h"
 #include "tensorflow/core/lib/strings/strcat.h"
-#include "tensorflow/core/lib/strings/stringprintf.h"
 #include "tensorflow/core/platform/logging.h"
 
 namespace xla {
 
 using ::tensorflow::str_util::Join;
-using ::tensorflow::strings::Printf;
 using ::tensorflow::strings::StrAppend;
 using ::tensorflow::strings::StrCat;
 
@@ -1543,7 +1541,7 @@ string HloInstruction::ToString(bool compact_operands,
       operands = "{...}";
     }
   } else if (opcode() == HloOpcode::kParameter) {
-    operands = Printf("%lld", parameter_number_);
+    StrAppend(&operands, parameter_number_);
   } else {
     tensorflow::gtl::ArraySlice<HloInstruction*> slice(operands_);
     const int64 kMaxOperandsToShowIfCompact = 4;
@@ -1608,19 +1606,17 @@ string HloInstruction::ToString(bool compact_operands,
     StrAppend(&extra, " # metadata=", metadata_.ShortDebugString());
   }
 
-  return Printf("%s = %s %s(%s)%s", name().c_str(),
-                ShapeUtil::HumanStringWithLayout(shape()).c_str(),
-                ExtendedOpcodeStr().c_str(), operands.c_str(), extra.c_str());
+  return StrCat(name(), " = ", ShapeUtil::HumanStringWithLayout(shape()), " ",
+                ExtendedOpcodeStr(), "(", operands, ")", extra);
 }
 
 string HloInstruction::ToShortString() const {
-  return Printf("%s = %s(%s)", name().c_str(),
-                HloOpcodeString(opcode()).c_str(),
+  return StrCat(name(), " = ", HloOpcodeString(opcode()), "(",
                 Join(operands_, ", ",
                      [](string* out, HloInstruction* operand) {
                        StrAppend(out, operand->name());
-                     })
-                    .c_str());
+                     }),
+                ")");
 }
 
 HloInstructionProto HloInstruction::ToProto() const {
diff --git a/tensorflow/compiler/xla/shape_util.cc b/tensorflow/compiler/xla/shape_util.cc
index 745491e485f..2100ef5c1da 100644
--- a/tensorflow/compiler/xla/shape_util.cc
+++ b/tensorflow/compiler/xla/shape_util.cc
@@ -362,6 +362,35 @@ bool CompareShapes(const Shape& lhs, const Shape& rhs, bool compare_layouts) {
   }
 }
 
+namespace {
+
+// Class to memoize the computation of
+//   tensorflow::str_util::Lowercase(PrimitiveType_Name(p))
+// for all PrimitiveType values "p"
+class PrimitiveTypeNameGenerator {
+ public:
+  PrimitiveTypeNameGenerator() {
+    for (int i = 0; i < PrimitiveType_ARRAYSIZE; i++) {
+      if (PrimitiveType_IsValid(i)) {
+        lowercase_name_[i] = tensorflow::str_util::Lowercase(
+            PrimitiveType_Name(static_cast<PrimitiveType>(i)));
+      }
+    }
+  }
+  const string& LowercaseName(PrimitiveType t) {
+    return lowercase_name_[static_cast<int>(t)];
+  }
+
+ private:
+  string lowercase_name_[PrimitiveType_ARRAYSIZE];
+};
+
+const string& LowercasePrimitiveTypeName(PrimitiveType s) {
+  static PrimitiveTypeNameGenerator* gen = new PrimitiveTypeNameGenerator();
+  return gen->LowercaseName(s);
+}
+}  // namespace
+
 /* static */ string ShapeUtil::HumanStringWithLayout(const Shape& shape) {
   if (shape.element_type() == TUPLE) {
     string text = "(";
@@ -374,18 +403,22 @@ bool CompareShapes(const Shape& lhs, const Shape& rhs, bool compare_layouts) {
     text += ")";
     return text;
   } else {
-    string layout;
+    string result = tensorflow::strings::StrCat(
+        LowercasePrimitiveTypeName(shape.element_type()), "[");
+    for (int i = 0; i < shape.dimensions().size(); i++) {
+      tensorflow::strings::StrAppend(&result, (i > 0) ? "," : "",
+                                     shape.dimensions(i));
+    }
+    result += "]";
     if (!IsScalar(shape) && !IsOpaque(shape)) {
       if (LayoutUtil::HasLayout(shape)) {
-        layout = LayoutUtil::HumanString(shape.layout());
+        tensorflow::strings::StrAppend(&result,
+                                       LayoutUtil::HumanString(shape.layout()));
       } else {
-        layout = "{no layout}";
+        tensorflow::strings::StrAppend(&result, "{no layout}");
       }
     }
-    return tensorflow::strings::StrCat(
-        tensorflow::str_util::Lowercase(
-            PrimitiveType_Name(shape.element_type())),
-        "[", tensorflow::str_util::Join(shape.dimensions(), ","), "]", layout);
+    return result;
   }
 }
 

commit 9fb91045a4186bdae15fc84272da1af11b3eea3c
Author: nolan liu <nolan.liou@gmail.com>
Date:   Tue Jul 25 06:53:50 2017 +0800

    Add mark_flag_as_required functions to make the APIs compatible with … (#11568)
    
    * Add mark_flag_as_required functions to make the APIs compatible with python-gflags.(#11195)
    
    * Add review advises for efficiency
    
    * Cleaning up some lint, renaming one of the private method names.
    
    * Import as underscore.

diff --git a/tensorflow/python/platform/flags.py b/tensorflow/python/platform/flags.py
index 60ec4f84c44..138a0ced97b 100644
--- a/tensorflow/python/platform/flags.py
+++ b/tensorflow/python/platform/flags.py
@@ -20,6 +20,7 @@ from __future__ import print_function
 
 import argparse as _argparse
 
+from tensorflow.python.platform import tf_logging as _logging
 from tensorflow.python.util.all_util import remove_undocumented
 
 _global_parser = _argparse.ArgumentParser()
@@ -34,12 +35,14 @@ class _FlagValues(object):
   def __init__(self):
     self.__dict__['__flags'] = {}
     self.__dict__['__parsed'] = False
+    self.__dict__['__required_flags'] = set()
 
   def _parse_flags(self, args=None):
     result, unparsed = _global_parser.parse_known_args(args=args)
     for flag_name, val in vars(result).items():
       self.__dict__['__flags'][flag_name] = val
     self.__dict__['__parsed'] = True
+    self._assert_all_required()
     return unparsed
 
   def __getattr__(self, name):
@@ -60,6 +63,19 @@ class _FlagValues(object):
     if not self.__dict__['__parsed']:
       self._parse_flags()
     self.__dict__['__flags'][name] = value
+    self._assert_required(name)
+
+  def _add_required_flag(self, item):
+    self.__dict__['__required_flags'].add(item)
+
+  def _assert_required(self, flag_name):
+    if (flag_name not in self.__dict__['__flags'] or
+        self.__dict__['__flags'][flag_name] is None):
+      raise AttributeError('Flag --%s must be specified.' % flag_name)
+
+  def _assert_all_required(self):
+    for flag_name in self.__dict__['__required_flags']:
+      self._assert_required(flag_name)
 
 
 def _define_helper(flag_name, default_value, docstring, flagtype):
@@ -136,6 +152,51 @@ def DEFINE_float(flag_name, default_value, docstring):
   """
   _define_helper(flag_name, default_value, docstring, float)
 
+
+def mark_flag_as_required(flag_name):
+  """Ensures that flag is not None during program execution.
+  
+  It is recommended to call this method like this:
+  
+    if __name__ == '__main__':
+      tf.flags.mark_flag_as_required('your_flag_name')
+      tf.app.run()
+  
+  Args:
+    flag_name: string, name of the flag to mark as required.
+ 
+  Raises:
+    AttributeError: if flag_name is not registered as a valid flag name.
+      NOTE: The exception raised will change in the future. 
+  """
+  if _global_parser.get_default(flag_name) is not None:
+    _logging.warn(
+        'Flag %s has a non-None default value; therefore, '
+        'mark_flag_as_required will pass even if flag is not specified in the '
+        'command line!' % flag_name)
+  FLAGS._add_required_flag(flag_name)
+
+
+def mark_flags_as_required(flag_names):
+  """Ensures that flags are not None during program execution.
+  
+  Recommended usage:
+  
+    if __name__ == '__main__':
+      tf.flags.mark_flags_as_required(['flag1', 'flag2', 'flag3'])
+      tf.app.run()
+  
+  Args:
+    flag_names: a list/tuple of flag names to mark as required.
+
+  Raises:
+    AttributeError: If any of flag name has not already been defined as a flag.
+      NOTE: The exception raised will change in the future.
+  """
+  for flag_name in flag_names:
+    mark_flag_as_required(flag_name)
+
+
 _allowed_symbols = [
     # We rely on gflags documentation.
     'DEFINE_bool',
@@ -144,5 +205,7 @@ _allowed_symbols = [
     'DEFINE_integer',
     'DEFINE_string',
     'FLAGS',
+    'mark_flag_as_required',
+    'mark_flags_as_required',
 ]
 remove_undocumented(__name__, _allowed_symbols)
diff --git a/tensorflow/python/platform/flags_test.py b/tensorflow/python/platform/flags_test.py
index 8b990975ddf..c6bdd94a763 100644
--- a/tensorflow/python/platform/flags_test.py
+++ b/tensorflow/python/platform/flags_test.py
@@ -35,6 +35,8 @@ flags.DEFINE_boolean("bool_a", False, "HelpString")
 flags.DEFINE_boolean("bool_c", False, "HelpString")
 flags.DEFINE_boolean("bool_d", True, "HelpString")
 flags.DEFINE_bool("bool_e", True, "HelpString")
+flags.DEFINE_string("string_foo_required", "default_val", "HelpString")
+flags.DEFINE_string("none_string_foo_required", None, "HelpString")
 
 FLAGS = flags.FLAGS
 
@@ -84,6 +86,18 @@ class FlagsTest(unittest.TestCase):
     copied = copy.copy(FLAGS)
     self.assertEqual(copied.__dict__, FLAGS.__dict__)
 
+  def testStringRequired(self):
+    res = FLAGS.string_foo_required
+    self.assertEqual(res, "default_val")
+    FLAGS.string_foo_required = "bar"
+    self.assertEqual("bar", FLAGS.string_foo_required)
+
+  def testNoneStringRequired(self):
+    res = FLAGS.none_string_foo_required
+    self.assertEqual(res, "default_val")
+    FLAGS.none_string_foo_required = "bar"
+    self.assertEqual("bar", FLAGS.none_string_foo_required)
+
 
 def main(_):
   # unittest.main() tries to interpret the unknown flags, so use the
@@ -97,7 +111,9 @@ if __name__ == "__main__":
   # Test command lines
   sys.argv.extend([
       "--bool_a", "--nobool_negation", "--bool_c=True", "--bool_d=False",
-      "and_argument"
+      "and_argument",
+      "--none_string_foo_required=default_val"
   ])
-
+  flags.mark_flag_as_required('string_foo_required')
+  flags.mark_flags_as_required(['none_string_foo_required'])
   app.run()

commit 624f0d402383ba4476e6131cbe1d421e3710c1bd
Author: Shanqing Cai <cais@google.com>
Date:   Thu Apr 13 12:47:18 2017 -0800

    Improve efficiency of get_session_handle
    
    Change get_session_handle() back to use v1 of GetSessionHandleOp, instead of v2. The v1 op returns a string, instead of an encoded ResourceHandle, which gets rid of the need to call ParseFromString() in Python.
    Change: 153101229

diff --git a/tensorflow/core/common_runtime/direct_session.cc b/tensorflow/core/common_runtime/direct_session.cc
index 768c2f6f753..e660624e282 100644
--- a/tensorflow/core/common_runtime/direct_session.cc
+++ b/tensorflow/core/common_runtime/direct_session.cc
@@ -757,7 +757,8 @@ Status DirectSession::ResourceHandleToInputTensor(const Tensor& resource_tensor,
 
   ResourceHandle resource_handle = resource_tensor.scalar<ResourceHandle>()();
 
-  if (resource_handle.hash_code() == MakeTypeIndex<Tensor>().hash_code()) {
+  if (resource_handle.container() ==
+      SessionState::kTensorHandleResourceTypeName) {
     return session_state_.GetTensor(resource_handle.name(), retrieved_tensor);
   } else {
     return errors::InvalidArgument(strings::StrCat(
diff --git a/tensorflow/core/ops/data_flow_ops.cc b/tensorflow/core/ops/data_flow_ops.cc
index 7e7d499f888..cb886ae8fa2 100644
--- a/tensorflow/core/ops/data_flow_ops.cc
+++ b/tensorflow/core/ops/data_flow_ops.cc
@@ -2154,7 +2154,13 @@ REGISTER_OP("GetSessionHandle")
     .Output("handle: string")
     .Attr("T: type")
     .SetShapeFn(shape_inference::ScalarShape)
-    .Deprecated(23, "Use GetSessionHandleV2");
+    .Doc(R"doc(
+Store the input tensor in the state of the current session.
+
+value: The tensor to be stored.
+handle: The handle for the tensor stored in the session state, represented
+  as a string.
+)doc");
 
 REGISTER_OP("GetSessionHandleV2")
     .Input("value: T")
diff --git a/tensorflow/python/client/tf_session.i b/tensorflow/python/client/tf_session.i
index 3331f172179..7c6f1cdd5e1 100644
--- a/tensorflow/python/client/tf_session.i
+++ b/tensorflow/python/client/tf_session.i
@@ -18,6 +18,7 @@ limitations under the License.
 %{
 
 #include "tensorflow/python/client/tf_session_helper.h"
+#include "tensorflow/core/framework/session_state.h"
 #include "tensorflow/core/lib/core/errors.h"
 #include "tensorflow/core/public/version.h"
 
@@ -57,6 +58,9 @@ tensorflow::ImportNumpy();
   }
 }
 
+// Constants used by TensorHandle (get_session_handle).
+%constant const char* TENSOR_HANDLE_KEY = tensorflow::SessionState::kTensorHandleResourceTypeName;
+
 ////////////////////////////////////////////////////////////////////////////////
 // BEGIN TYPEMAPS FOR tensorflow::TF_Run_wrapper()
 ////////////////////////////////////////////////////////////////////////////////
diff --git a/tensorflow/python/ops/session_ops.py b/tensorflow/python/ops/session_ops.py
index 3d038cfd8a0..de43b562f9f 100644
--- a/tensorflow/python/ops/session_ops.py
+++ b/tensorflow/python/ops/session_ops.py
@@ -26,11 +26,10 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import sys
-
 import numpy as np
 
 from tensorflow.core.framework import resource_handle_pb2
+from tensorflow.python import pywrap_tensorflow_internal
 from tensorflow.python.framework import device as pydev
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
@@ -39,16 +38,6 @@ from tensorflow.python.ops import gen_data_flow_ops
 from tensorflow.python.util import compat
 
 
-def decode_resource_handle(encoded):
-  """Decode a ResourceHandle proto encoded as custom numpy struct type."""
-  resource_handle = resource_handle_pb2.ResourceHandle()
-  if sys.version_info.major < 3:
-    resource_handle.ParseFromString("".join([chr(ch[0]) for ch in encoded]))
-  else:
-    resource_handle.ParseFromString(bytes([ch[0] for ch in encoded]))
-  return resource_handle
-
-
 def encode_resource_handle(resource_handle):
   """Encode a ResourceHandle proto as custom numpy struct type."""
   return np.asarray(bytearray(resource_handle.SerializeToString()),
@@ -69,8 +58,8 @@ class TensorHandle(object):
       dtype: The data type of the tensor represented by `handle`.
       session: The session in which the tensor is produced.
     """
-    self._resource_handle = decode_resource_handle(handle)
-    self._handle = compat.as_str_any(self._resource_handle.name)
+    self._handle = compat.as_str_any(handle)
+    self._resource_handle = None
     self._dtype = dtype
     self._session = session
     self._auto_gc_enabled = True
@@ -82,9 +71,14 @@ class TensorHandle(object):
   def __str__(self):
     return self._handle
 
-  @property
-  def resource_handle(self):
+  def _get_resource_handle(self):
     """The ResourceHandle representation of this handle."""
+    if not self._resource_handle:
+      self._resource_handle = resource_handle_pb2.ResourceHandle()
+      self._resource_handle.device = self._handle.split(";")[-1]
+      self._resource_handle.container = (
+          pywrap_tensorflow_internal.TENSOR_HANDLE_KEY)
+      self._resource_handle.name = self._handle
     return self._resource_handle
 
   def to_numpy_array(self):
@@ -94,7 +88,7 @@ class TensorHandle(object):
       A numpy array of a custom struct type that can be used as a feed value
       to run().
     """
-    return encode_resource_handle(self.resource_handle)
+    return encode_resource_handle(self._get_resource_handle())
 
   @property
   def handle(self):
@@ -186,7 +180,7 @@ def get_session_handle(data, name=None):
 
   # Colocate this operation with data.
   with ops.colocate_with(data):
-    return gen_data_flow_ops._get_session_handle_v2(data, name=name)  # pylint: disable=protected-access
+    return gen_data_flow_ops._get_session_handle(data, name=name)  # pylint: disable=protected-access
 
 
 def get_session_tensor(handle, dtype, name=None):
@@ -291,7 +285,7 @@ def _get_handle_mover(graph, feeder, handle):
     # Create mover if we haven't done it.
     holder, reader = _get_handle_reader(graph, handle, dtype)
     with graph.as_default(), graph.device(feeder.op.device):
-      mover = gen_data_flow_ops._get_session_handle_v2(reader)  # pylint: disable=protected-access
+      mover = gen_data_flow_ops._get_session_handle(reader)  # pylint: disable=protected-access
     result = (holder, mover)
     graph._handle_movers[graph_key] = result
   return result

commit de604cbaaa093765f5dbccaf2a1097e83ca7fe32
Author: Shanqing Cai <cais@google.com>
Date:   Fri Mar 10 10:12:32 2017 -0800

    tfdbg: Let NodeStepper use TensorHandles as direct feeds
    
    CL/149672538 has made TensorHandles directly feedable for Session.run() calls. This eliminates the need to call TensorHandles.eval() to get the value of a Tensor and feed it back in during tfdbg NodeStepper's cont() (i.e., continue-to) calls.
    
    This change improves the memory efficiency and performance of NodeStepper.cont() by eliminating unnecessary Tensor-numpy and numpy-Tensor copying.
    Change: 149769853

diff --git a/tensorflow/python/debug/lib/stepper.py b/tensorflow/python/debug/lib/stepper.py
index 0240013111d..c814520b7e7 100644
--- a/tensorflow/python/debug/lib/stepper.py
+++ b/tensorflow/python/debug/lib/stepper.py
@@ -623,7 +623,7 @@ class NodeStepper(object):
         elif (can_feed and inp not in feeds and
               use_tensor_handles and inp.name in self._tensor_handles):
           # Tensor handle found in cache.
-          feeds[inp] = self._tensor_handles[inp.name].eval()
+          feeds[inp] = self._tensor_handles[inp.name]
           self._last_feed_types[inp.name] = self.FEED_TYPE_HANDLE
         elif (can_feed and inp not in feeds and
               use_dumped_intermediates and

commit cdecf416365c85f8274393e097ecab163cbea7c3
Author: Shanqing Cai <cais@google.com>
Date:   Thu Mar 9 11:13:46 2017 -0800

    Enable the direct use of TensorHandles as feed values through ResourceHandles
    
    This is motivated by, among other goals, the need to enhance memory efficiency during TFDBG's stepper operations. The stepper caches TensorHandles to already-continued-to tensors and use them as feeds if later continue-to actions depend on the tensors as transitive inputs. However, previously the TensorHandles had to be converted to Numpy arrays by calling eval() and the Numpy arrays were then fed back to next Session.run() calls. This mode of operation involved at least two unnecessary tensor-numpy and numpy-tensor copying.
    
    This CL makes it possible to use the ResourceHandle representations TensorHandles directly as feed values, eliminating the need for the aforementioned copying.
    
    To this end, the following changes are made
    1) the underlying representations of TensorHandles are changed from string to ResourceHandle. A custom numpy struct type is created to allow ResourceHandle of the TensorHandle subtype to be fed during Session.run() calls.
    2) added GetSessionHandleOpV2, which deprecates GetSessionHandleOp. The V2 op outputs a DT_RESOURCE Tensor, instead of a string Tensor in the deprecated version.
    Change: 149672538

diff --git a/tensorflow/core/common_runtime/direct_session.cc b/tensorflow/core/common_runtime/direct_session.cc
index 18bc8fb6347..c4b2b6c12a5 100644
--- a/tensorflow/core/common_runtime/direct_session.cc
+++ b/tensorflow/core/common_runtime/direct_session.cc
@@ -739,6 +739,26 @@ Status DirectSession::PRun(const string& handle, const NamedTensorList& inputs,
   return s;
 }
 
+Status DirectSession::ResourceHandleToInputTensor(const Tensor& resource_tensor,
+                                                  Tensor* retrieved_tensor) {
+  if (resource_tensor.dtype() != DT_RESOURCE) {
+    return errors::InvalidArgument(strings::StrCat(
+        "ResourceHandleToInputTensor() received non-DT_RESOURCE Tensor: ",
+        resource_tensor.dtype()));
+  }
+
+  ResourceHandle resource_handle = resource_tensor.scalar<ResourceHandle>()();
+
+  if (resource_handle.hash_code() == MakeTypeIndex<Tensor>().hash_code()) {
+    return session_state_.GetTensor(resource_handle.name(), retrieved_tensor);
+  } else {
+    return errors::InvalidArgument(strings::StrCat(
+        "Invalid resource type hash code: ", resource_handle.hash_code(),
+        "(name: ", resource_handle.name(),
+        " type: ", resource_handle.maybe_type_name(), ")"));
+  }
+}
+
 Status DirectSession::SendInputs(const NamedTensorList& inputs,
                                  const ExecutorsAndKeys* executors_and_keys,
                                  IntraProcessRendezvous* rendez) {
@@ -759,7 +779,16 @@ Status DirectSession::SendInputs(const NamedTensorList& inputs,
       return s;
     }
 
-    s = rendez->Send(parsed, Rendezvous::Args(), input.second, false);
+    if (input.second.dtype() == DT_RESOURCE) {
+      Tensor tensor_from_handle;
+      s = ResourceHandleToInputTensor(input.second, &tensor_from_handle);
+      if (s.ok()) {
+        s = rendez->Send(parsed, Rendezvous::Args(), tensor_from_handle, false);
+      }
+    } else {
+      s = rendez->Send(parsed, Rendezvous::Args(), input.second, false);
+    }
+
     if (!s.ok()) {
       rendez->StartAbort(s);
       return s;
diff --git a/tensorflow/core/common_runtime/direct_session.h b/tensorflow/core/common_runtime/direct_session.h
index 3e3a5eaa8f4..1495648631e 100644
--- a/tensorflow/core/common_runtime/direct_session.h
+++ b/tensorflow/core/common_runtime/direct_session.h
@@ -192,6 +192,9 @@ class DirectSession : public Session {
   ::tensorflow::Status ExtendLocked(const GraphDef& graph)
       EXCLUSIVE_LOCKS_REQUIRED(graph_def_lock_);
 
+  ::tensorflow::Status ResourceHandleToInputTensor(
+      const Tensor& resource_tensor, Tensor* retrieved_tensor);
+
   // Feeds more inputs to the executors, triggering further execution.
   ::tensorflow::Status SendInputs(
       const std::vector<std::pair<string, Tensor>>& inputs,
diff --git a/tensorflow/core/common_runtime/direct_session_test.cc b/tensorflow/core/common_runtime/direct_session_test.cc
index 9e717dfc234..c8b8a09b8e8 100644
--- a/tensorflow/core/common_runtime/direct_session_test.cc
+++ b/tensorflow/core/common_runtime/direct_session_test.cc
@@ -627,7 +627,7 @@ TEST(DirectSessionTest, RunHandleTest) {
   value1.scalar<float>()() = 2.0;
   Node* const1 = test::graph::Constant(&g, value1);
   Node* node3 = test::graph::Add(&g, identity0, const1);
-  Node* node4 = test::graph::Unary(&g, "GetSessionHandle", node3);
+  Node* node4 = test::graph::Unary(&g, "GetSessionHandleV2", node3);
 
   Tensor value2(DT_STRING, TensorShape({}));
   Node* const2 = test::graph::Constant(&g, value2);
@@ -648,17 +648,21 @@ TEST(DirectSessionTest, RunHandleTest) {
   ASSERT_TRUE(s.ok());
   ASSERT_EQ(1, outputs.size());
 
+  ResourceHandle resource_handle = outputs[0].scalar<ResourceHandle>()();
+  Tensor string_handle(DT_STRING, {});
+  string_handle.flat<string>().setConstant(resource_handle.name());
+
   // Second run call: Use a handle.
   std::vector<Tensor> outputs1;
-  s = session->Run({{const2->name(), outputs[0]}}, {node6->name() + ":0"}, {},
-                   &outputs1);
+  s = session->Run({{const2->name(), string_handle}}, {node6->name() + ":0"},
+                   {}, &outputs1);
   ASSERT_TRUE(s.ok());
   ASSERT_EQ(1, outputs1.size());
   ASSERT_EQ(5.0, outputs1[0].flat<float>()(0));
 
   // Third run call: Delete a handle.
   std::vector<Tensor> outputs2;
-  s = session->Run({{const2->name(), outputs[0]}}, {}, {node7->name()},
+  s = session->Run({{const2->name(), string_handle}}, {}, {node7->name()},
                    &outputs2);
   ASSERT_TRUE(s.ok());
 }
diff --git a/tensorflow/core/common_runtime/session_state.cc b/tensorflow/core/common_runtime/session_state.cc
index 2c80c4d1123..7e7200070d0 100644
--- a/tensorflow/core/common_runtime/session_state.cc
+++ b/tensorflow/core/common_runtime/session_state.cc
@@ -18,6 +18,8 @@ limitations under the License.
 
 namespace tensorflow {
 
+const char* SessionState::kTensorHandleResourceTypeName = "TensorHandle";
+
 Status SessionState::GetTensor(const string& handle, Tensor* tensor) {
   mutex_lock l(state_lock_);
   auto it = tensors_.find(handle);
diff --git a/tensorflow/core/framework/session_state.h b/tensorflow/core/framework/session_state.h
index a3eafcf4745..8fbe940f6ae 100644
--- a/tensorflow/core/framework/session_state.h
+++ b/tensorflow/core/framework/session_state.h
@@ -41,6 +41,8 @@ class SessionState {
 
   int64 GetNewId();
 
+  static const char* kTensorHandleResourceTypeName;
+
  private:
   mutex state_lock_;
 
diff --git a/tensorflow/core/graph/graph.cc b/tensorflow/core/graph/graph.cc
index 509c67c11ff..6d9b114e90b 100644
--- a/tensorflow/core/graph/graph.cc
+++ b/tensorflow/core/graph/graph.cc
@@ -98,6 +98,7 @@ void Node::Initialize(int id, int cost_id, Properties* props) {
   SET_CLASS(NC_VARIABLE, ts, "VariableV2", "");
   SET_CLASS(NC_IDENTITY, ts, "Identity", "RefIdentity");
   SET_CLASS(NC_GET_SESSION_HANDLE, ts, "GetSessionHandle", "");
+  SET_CLASS(NC_GET_SESSION_HANDLE, ts, "GetSessionHandleV2", "");
   SET_CLASS(NC_GET_SESSION_TENSOR, ts, "GetSessionTensor", "");
   SET_CLASS(NC_DELETE_SESSION_TENSOR, ts, "DeleteSessionTensor", "");
   if (class_ == NC_UNINITIALIZED) {
diff --git a/tensorflow/core/kernels/session_ops.cc b/tensorflow/core/kernels/session_ops.cc
index 59fb225b928..54eca4a20a0 100644
--- a/tensorflow/core/kernels/session_ops.cc
+++ b/tensorflow/core/kernels/session_ops.cc
@@ -41,13 +41,24 @@ class GetSessionHandleOp : public OpKernel {
       : OpKernel(context) {}
 
   void Compute(OpKernelContext* ctx) override {
-    const Tensor& val = ctx->input(0);
+    Tensor val = ctx->input(0);
     int64 id = ctx->session_state()->GetNewId();
     TensorStore::TensorAndKey tk{val, id, def().device()};
     OP_REQUIRES_OK(ctx, ctx->tensor_store()->AddTensor(def().name(), tk));
+
     Tensor* handle = nullptr;
     OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &handle));
-    handle->flat<string>().setConstant(tk.GetHandle(def().name()));
+    if (ctx->expected_output_dtype(0) == DT_RESOURCE) {
+      ResourceHandle resource_handle = MakeResourceHandle<Tensor>(
+          ctx, SessionState::kTensorHandleResourceTypeName,
+          tk.GetHandle(def().name()));
+      resource_handle.set_maybe_type_name(
+          SessionState::kTensorHandleResourceTypeName);
+      handle->scalar<ResourceHandle>()() = resource_handle;
+    } else {
+      // Legacy behavior in V1.
+      handle->flat<string>().setConstant(tk.GetHandle(def().name()));
+    }
   }
 
   TF_DISALLOW_COPY_AND_ASSIGN(GetSessionHandleOp);
@@ -55,12 +66,19 @@ class GetSessionHandleOp : public OpKernel {
 
 REGISTER_KERNEL_BUILDER(Name("GetSessionHandle").Device(DEVICE_CPU),
                         GetSessionHandleOp);
+REGISTER_KERNEL_BUILDER(Name("GetSessionHandleV2").Device(DEVICE_CPU),
+                        GetSessionHandleOp);
 
 #define REGISTER_GPU_KERNEL(type)                         \
   REGISTER_KERNEL_BUILDER(Name("GetSessionHandle")        \
                               .Device(DEVICE_GPU)         \
                               .HostMemory("handle")       \
                               .TypeConstraint<type>("T"), \
+                          GetSessionHandleOp)             \
+  REGISTER_KERNEL_BUILDER(Name("GetSessionHandleV2")      \
+                              .Device(DEVICE_GPU)         \
+                              .HostMemory("handle")       \
+                              .TypeConstraint<type>("T"), \
                           GetSessionHandleOp)
 
 TF_CALL_NUMBER_TYPES(REGISTER_GPU_KERNEL);
@@ -73,12 +91,17 @@ REGISTER_GPU_KERNEL(bool);
                               .Device(DEVICE_SYCL)        \
                               .HostMemory("handle")       \
                               .TypeConstraint<type>("T"), \
+                          GetSessionHandleOp)             \
+  REGISTER_KERNEL_BUILDER(Name("GetSessionHandleV2")      \
+                              .Device(DEVICE_SYCL)        \
+                              .HostMemory("handle")       \
+                              .TypeConstraint<type>("T"), \
                           GetSessionHandleOp)
 
 TF_CALL_NUMBER_TYPES(REGISTER_SYCL_KERNEL);
 REGISTER_SYCL_KERNEL(bool);
 #undef REGISTER_SYCL_KERNEL
-#endif // TENSORFLOW_USE_SYCL
+#endif  // TENSORFLOW_USE_SYCL
 
 class GetSessionTensorOp : public OpKernel {
  public:
@@ -147,5 +170,5 @@ REGISTER_KERNEL_BUILDER(
 REGISTER_KERNEL_BUILDER(
     Name("DeleteSessionTensor").Device(DEVICE_SYCL).HostMemory("handle"),
     DeleteSessionTensorOp);
-#endif // TENSORFLOW_USE_SYCL
+#endif  // TENSORFLOW_USE_SYCL
 }  // namespace tensorflow
diff --git a/tensorflow/core/ops/data_flow_ops.cc b/tensorflow/core/ops/data_flow_ops.cc
index 365716b3725..f2a78956e3c 100644
--- a/tensorflow/core/ops/data_flow_ops.cc
+++ b/tensorflow/core/ops/data_flow_ops.cc
@@ -2152,11 +2152,19 @@ REGISTER_OP("GetSessionHandle")
     .Output("handle: string")
     .Attr("T: type")
     .SetShapeFn(shape_inference::ScalarShape)
+    .Deprecated(23, "Use GetSessionHandleV2");
+
+REGISTER_OP("GetSessionHandleV2")
+    .Input("value: T")
+    .Output("handle: resource")
+    .Attr("T: type")
+    .SetShapeFn(shape_inference::ScalarShape)
     .Doc(R"doc(
 Store the input tensor in the state of the current session.
 
 value: The tensor to be stored.
-handle: The handle for the tensor stored in the session state.
+handle: The handle for the tensor stored in the session state, represented
+  as a ResourceHandle object.
 )doc");
 
 REGISTER_OP("GetSessionTensor")
diff --git a/tensorflow/python/client/session.py b/tensorflow/python/client/session.py
index aa06d0ee70c..7429bafff61 100644
--- a/tensorflow/python/client/session.py
+++ b/tensorflow/python/client/session.py
@@ -422,7 +422,9 @@ class _FetchHandler(object):
         self._fetches.append(fetch_name)
         self._ops.append(False)
       # Remember the fetch if it is for a tensor handle.
-      if isinstance(fetch, ops.Tensor) and fetch.op.type == 'GetSessionHandle':
+      if (isinstance(fetch, ops.Tensor) and
+          (fetch.op.type == 'GetSessionHandle' or
+           fetch.op.type == 'GetSessionHandleV2')):
         self._fetch_handles[fetch_name] = fetch.op.inputs[0].dtype
     self._final_fetches = [x for x in self._fetches if x not in feeds]
 
@@ -926,7 +928,7 @@ class BaseSession(SessionInterface):
           if isinstance(subfeed_val, ops.Tensor):
             raise TypeError('The value of a feed cannot be a tf.Tensor object. '
                             'Acceptable feed values include Python scalars, '
-                            'strings, lists, or numpy ndarrays.')
+                            'strings, lists, numpy ndarrays, or TensorHandles.')
 
           subfeed_dtype = subfeed_t.dtype.as_numpy_dtype
           if isinstance(subfeed_val,
@@ -937,9 +939,15 @@ class BaseSession(SessionInterface):
                 ' Try explicitly setting the type of the feed tensor'
                 ' to a larger type (e.g. int64).')
 
-          np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
+          is_tensor_handle_feed = isinstance(subfeed_val,
+                                             session_ops.TensorHandle)
+          if is_tensor_handle_feed:
+            np_val = subfeed_val.to_numpy_array()
+          else:
+            np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
 
-          if not subfeed_t.get_shape().is_compatible_with(np_val.shape):
+          if (not is_tensor_handle_feed and
+              not subfeed_t.get_shape().is_compatible_with(np_val.shape)):
             raise ValueError(
                 'Cannot feed value of shape %r for Tensor %r, '
                 'which has shape %r'
diff --git a/tensorflow/python/client/tf_session_helper.cc b/tensorflow/python/client/tf_session_helper.cc
index a69c56368fb..a370b904b6c 100644
--- a/tensorflow/python/client/tf_session_helper.cc
+++ b/tensorflow/python/client/tf_session_helper.cc
@@ -50,7 +50,11 @@ Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,
   PyObject* value;
   Py_ssize_t pos = 0;
   if (PyDict_Next(descr->fields, &pos, &key, &value)) {
-    const char* key_string = PyBytes_AsString(key);
+    // In Python 3, the keys of numpy custom struct types are unicode, unlike
+    // Python 2, where the keys are bytes.
+    const char* key_string =
+        PyBytes_Check(key) ? PyBytes_AsString(key)
+                           : PyBytes_AsString(PyUnicode_AsASCIIString(key));
     if (!key_string) {
       return errors::Internal("Corrupt numpy type descriptor");
     }
@@ -69,6 +73,8 @@ Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,
       *out_tf_datatype = TF_QUINT16;
     } else if (key == "qint32") {
       *out_tf_datatype = TF_QINT32;
+    } else if (key == "resource") {
+      *out_tf_datatype = TF_RESOURCE;
     } else {
       return errors::Internal("Unsupported numpy data type");
     }
@@ -125,6 +131,8 @@ Status PyArray_TYPE_to_TF_DataType(PyArrayObject* array,
       // Quantized types are currently represented as custom struct types.
       // PyArray_TYPE returns NPY_VOID for structs, and we should look into
       // descr to derive the actual type.
+      // Direct feeds of certain types of ResourceHandles are represented as a
+      // custom struct type.
       return PyArrayDescr_to_TF_DataType(descr, out_tf_datatype);
     default:
       // TODO(mrry): Support these.
@@ -175,6 +183,9 @@ Status TF_DataType_to_PyArray_TYPE(TF_DataType tf_datatype,
     case TF_STRING:
       *out_pyarray_type = NPY_OBJECT;
       break;
+    case TF_RESOURCE:
+      *out_pyarray_type = NPY_VOID;
+      break;
     // TODO(keveman): These should be changed to NPY_VOID, and the type used for
     // the resulting numpy array should be the custom struct types that we
     // expect for quantized types.
@@ -322,6 +333,61 @@ static Status CopyStringToPyArrayElement(PyArrayObject* pyarray, void* i_ptr,
   return Status::OK();
 }
 
+// Determine the dimensions of a numpy ndarray to be created to represent an
+// output Tensor.
+gtl::InlinedVector<npy_intp, 4> GetPyArrayDimensionsForTensor(
+    const TF_Tensor* tensor, tensorflow::int64* nelems) {
+  if (TF_TensorType(tensor) == TF_RESOURCE) {
+    gtl::InlinedVector<npy_intp, 4> dims(1);
+    ResourceHandle* resource_handle =
+        reinterpret_cast<ResourceHandle*>(TF_TensorData(tensor));
+    dims[0] = resource_handle->SerializeAsString().size();
+    *nelems = dims[0];
+
+    return dims;
+  } else {
+    const int ndims = TF_NumDims(tensor);
+    gtl::InlinedVector<npy_intp, 4> dims(ndims);
+    *nelems = 1;
+    for (int i = 0; i < ndims; ++i) {
+      dims[i] = TF_Dim(tensor, i);
+      *nelems *= dims[i];
+    }
+
+    return dims;
+  }
+}
+
+// Determine the type description (PyArray_Descr) of a numpy ndarray to be
+// created to represent an output Tensor.
+Status GetPyArrayDescrForTensor(const TF_Tensor* tensor,
+                                PyArray_Descr** descr) {
+  if (TF_TensorType(tensor) == TF_RESOURCE) {
+    PyObject* field = PyTuple_New(3);
+#if PY_MAJOR_VERSION < 3
+    PyTuple_SetItem(field, 0, PyBytes_FromString("resource"));
+#else
+    PyTuple_SetItem(field, 0, PyUnicode_FromString("resource"));
+#endif
+    PyTuple_SetItem(field, 1, PyArray_TypeObjectFromType(NPY_UBYTE));
+    PyTuple_SetItem(field, 2, PyLong_FromLong(1));
+    PyObject* fields = PyList_New(1);
+    PyList_SetItem(fields, 0, field);
+    int convert_result = PyArray_DescrConverter(fields, descr);
+    if (convert_result != 1) {
+      return errors::Internal("Failed to create numpy array description for ",
+                              "TF_RESOURCE-type tensor");
+    }
+  } else {
+    int type_num = -1;
+    TF_RETURN_IF_ERROR(
+        TF_DataType_to_PyArray_TYPE(TF_TensorType(tensor), &type_num));
+    *descr = PyArray_DescrFromType(type_num);
+  }
+
+  return Status::OK();
+}
+
 // Converts the given TF_Tensor to a Numpy array.
 // If the returned status is OK, the caller becomes the owner of *out_array.
 Status TF_Tensor_to_PyObject(TF_Tensor* tensor, PyObject** out_array) {
@@ -333,26 +399,20 @@ Status TF_Tensor_to_PyObject(TF_Tensor* tensor, PyObject** out_array) {
     return Status::OK();
   }
 
-  const int ndims = TF_NumDims(tensor);
-  gtl::InlinedVector<npy_intp, 4> dims(ndims);
-  tensorflow::int64 nelems = 1;
-  for (int i = 0; i < ndims; ++i) {
-    dims[i] = TF_Dim(tensor, i);
-    nelems *= dims[i];
-  }
+  tensorflow::int64 nelems = -1;
+  gtl::InlinedVector<npy_intp, 4> dims =
+      GetPyArrayDimensionsForTensor(tensor, &nelems);
 
   // Convert TensorFlow dtype to numpy type descriptor.
-  int type_num = -1;
-  TF_RETURN_IF_ERROR(
-      TF_DataType_to_PyArray_TYPE(TF_TensorType(tensor), &type_num));
-  PyArray_Descr* descr = PyArray_DescrFromType(type_num);
+  PyArray_Descr* descr = nullptr;
+  TF_RETURN_IF_ERROR(GetPyArrayDescrForTensor(tensor, &descr));
 
   // Copy the TF_TensorData into a newly-created ndarray and return it.
   // TODO(mrry): Perhaps investigate zero-copy approaches. This would involve
   // creating an ndarray-like object that wraps the TF_Tensor buffer, and
   // maps its destructor to TF_DeleteTensor.
   Safe_PyObjectPtr safe_out_array =
-      tensorflow::make_safe(PyArray_Empty(ndims, dims.data(), descr, 0));
+      tensorflow::make_safe(PyArray_Empty(dims.size(), dims.data(), descr, 0));
   if (!safe_out_array) {
     return errors::Internal("Could not allocate ndarray");
   }
@@ -371,6 +431,12 @@ Status TF_Tensor_to_PyObject(TF_Tensor* tensor, PyObject** out_array) {
         }
         PyArray_ITER_NEXT(iter.get());
       }
+    } else if (TF_TensorType(tensor) == TF_RESOURCE) {
+      ResourceHandle* resource_handle =
+          reinterpret_cast<ResourceHandle*>(TF_TensorData(tensor));
+      memcpy(PyArray_DATA(py_array),
+             resource_handle->SerializeAsString().c_str(),
+             PyArray_NBYTES(py_array));
     } else {
       return errors::Internal("ndarray was ", PyArray_NBYTES(py_array),
                               " bytes but TF_Tensor was ",
@@ -418,6 +484,8 @@ void TF_Run_wrapper_helper(TF_DeprecatedSession* session, const char* handle,
   Py_ssize_t pos = 0;
   int index = 0;
   Status s;
+
+  gtl::InlinedVector<std::shared_ptr<ResourceHandle>, 4> resource_handles;
   while (PyDict_Next(feed_dict, &pos, &key, &value)) {
     char* key_string = PyBytes_AsString(key);
     if (!key_string) {
@@ -457,7 +525,19 @@ void TF_Run_wrapper_helper(TF_DeprecatedSession* session, const char* handle,
     // type, this steals a reference to array, which will be relinquished when
     // the underlying buffer is deallocated. For string, a new temporary buffer
     // is allocated into which the strings are encoded.
-    if (dtype != TF_STRING) {
+    if (dtype == TF_RESOURCE) {
+      const string serialized(reinterpret_cast<char*>(PyArray_DATA(array)),
+                              PyArray_NBYTES(array));
+      std::shared_ptr<ResourceHandle> resource_handle(new ResourceHandle());
+      resource_handle->ParseFromString(serialized);
+      resource_handles.emplace_back(resource_handle);
+      TF_Tensor* tensor =
+          TF_AllocateTensor(dtype, {}, 0, sizeof(ResourceHandle));
+      std::memcpy(TF_TensorData(tensor),
+                  reinterpret_cast<void*>(resource_handle.get()),
+                  sizeof(ResourceHandle));
+      inputs_safe.emplace_back(make_safe(tensor));
+    } else if (dtype != TF_STRING) {
       // NOTE(mrry): We currently copy the numpy array into a new
       // buffer to avoid possible issues on deallocation (such as
       // having to acquire the Python Global Interpreter Lock).
diff --git a/tensorflow/python/debug/lib/debug_data.py b/tensorflow/python/debug/lib/debug_data.py
index baaa15abca8..af1256ee321 100644
--- a/tensorflow/python/debug/lib/debug_data.py
+++ b/tensorflow/python/debug/lib/debug_data.py
@@ -26,6 +26,7 @@ import numpy as np
 from six.moves import xrange  # pylint: disable=redefined-builtin
 
 from tensorflow.core.framework import graph_pb2
+from tensorflow.core.framework import types_pb2
 from tensorflow.core.util import event_pb2
 from tensorflow.python.framework import tensor_util
 from tensorflow.python.platform import gfile
@@ -77,10 +78,14 @@ def load_tensor_from_event(event):
   if (event.summary.value[0].tensor.tensor_content or
       event.summary.value[0].tensor.string_val):
     # Initialized tensor.
-    try:
-      tensor_value = tensor_util.MakeNdarray(event.summary.value[0].tensor)
-    except KeyError:
-      tensor_value = None
+    tensor_proto = event.summary.value[0].tensor
+    if tensor_proto.dtype == types_pb2.DT_RESOURCE:
+      return None
+    else:
+      try:
+        tensor_value = tensor_util.MakeNdarray(tensor_proto)
+      except KeyError:
+        tensor_value = None
   else:
     # Uninitialized tensor or tensor of unconvertible data type.
     tensor_value = None
diff --git a/tensorflow/python/framework/dtypes.py b/tensorflow/python/framework/dtypes.py
index 7564cbcb058..d373bac47a2 100644
--- a/tensorflow/python/framework/dtypes.py
+++ b/tensorflow/python/framework/dtypes.py
@@ -458,6 +458,9 @@ _np_qint16 = np.dtype([("qint16", np.int16, 1)])
 _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
 _np_qint32 = np.dtype([("qint32", np.int32, 1)])
 
+# Custom struct dtype for directly-fed ResourceHandles of supported type(s).
+np_resource = np.dtype([("resource", np.ubyte, 1)])
+
 # Standard mappings between types_pb2.DataType values and numpy.dtypes.
 _NP_TO_TF = frozenset([
     (np.float16, float16),
diff --git a/tensorflow/python/kernel_tests/session_ops_test.py b/tensorflow/python/kernel_tests/session_ops_test.py
index 25d60c52590..41b678feb9a 100644
--- a/tensorflow/python/kernel_tests/session_ops_test.py
+++ b/tensorflow/python/kernel_tests/session_ops_test.py
@@ -22,6 +22,8 @@ from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import session_ops
+from tensorflow.python.ops import state_ops
+from tensorflow.python.ops import variables
 from tensorflow.python.platform import test
 
 
@@ -229,6 +231,45 @@ class SessionOpsTest(test.TestCase):
                      b_p: b_handle.handle})
       self.assertEqual(3.0, c_handle.eval())
 
+  def testFeedOneHandleDirectly(self):
+    with self.test_session() as sess:
+      a = constant_op.constant(10.0)
+      b = constant_op.constant(5.0)
+      c = math_ops.multiply(a, b)
+      d = math_ops.multiply(c, c)
+
+      h_c = sess.run(session_ops.get_session_handle(c))
+
+      self.assertAllClose(2500.0, sess.run(d, feed_dict={c: h_c}))
+
+  def testFeedTwoHandlesDirectly(self):
+    with self.test_session() as sess:
+      a = constant_op.constant(10.0)
+      b = constant_op.constant(5.0)
+      c = math_ops.multiply(a, b)
+      d = math_ops.div(a, b)
+      e = math_ops.subtract(c, d)
+
+      h_c = sess.run(session_ops.get_session_handle(c))
+      h_d = sess.run(session_ops.get_session_handle(d))
+
+      self.assertAllClose(48.0, sess.run(e, feed_dict={c: h_c, d: h_d}))
+      self.assertAllClose(-48.0, sess.run(e, feed_dict={c: h_d, d: h_c}))
+
+  def testFeedHandleToVariableDirectly(self):
+    with self.test_session() as sess:
+      a = variables.Variable(12.0)
+      inc_a = state_ops.assign_add(a, 2.0)
+      b = math_ops.add(a, 5.0)
+      sess.run(a.initializer)
+
+      h_a_read = sess.run(session_ops.get_session_handle(a.read_value()))
+      self.assertAllClose(12.0, sess.run(a))
+
+      self.assertAllClose(17.0, sess.run(b, feed_dict={a: h_a_read}))
+      sess.run(inc_a)
+      self.assertAllClose(19.0, sess.run(b, feed_dict={a: h_a_read}))
+
 
 if __name__ == "__main__":
   test.main()
diff --git a/tensorflow/python/ops/data_flow_grad.py b/tensorflow/python/ops/data_flow_grad.py
index 95c15f334da..79e94dace01 100644
--- a/tensorflow/python/ops/data_flow_grad.py
+++ b/tensorflow/python/ops/data_flow_grad.py
@@ -78,5 +78,6 @@ ops.NotDifferentiable("StackPop")
 ops.NotDifferentiable("StackClose")
 
 ops.NotDifferentiable("GetSessionHandle")
+ops.NotDifferentiable("GetSessionHandleV2")
 ops.NotDifferentiable("GetSessionTensor")
 ops.NotDifferentiable("DeleteSessionTensor")
diff --git a/tensorflow/python/ops/hidden_ops.txt b/tensorflow/python/ops/hidden_ops.txt
index 4937f1a50a8..fbfd7bb7d6a 100644
--- a/tensorflow/python/ops/hidden_ops.txt
+++ b/tensorflow/python/ops/hidden_ops.txt
@@ -60,6 +60,7 @@ FakeQueue
 FIFOQueue
 FIFOQueueV2
 GetSessionHandle
+GetSessionHandleV2
 GetSessionTensor
 HashTable
 InitializeTable
diff --git a/tensorflow/python/ops/session_ops.py b/tensorflow/python/ops/session_ops.py
index ff5d5a2b2f8..0a06982ad7c 100644
--- a/tensorflow/python/ops/session_ops.py
+++ b/tensorflow/python/ops/session_ops.py
@@ -16,6 +16,7 @@
 """Tensor Handle Operations. See the @{python/session_ops} guide.
 
 @@get_session_handle
+@@get_session_handle_v2
 @@get_session_tensor
 @@delete_session_tensor
 """
@@ -25,6 +26,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import sys
+
+import numpy as np
+
+from tensorflow.core.framework import resource_handle_pb2
 from tensorflow.python.framework import device as pydev
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
@@ -33,6 +39,22 @@ from tensorflow.python.ops import gen_data_flow_ops
 from tensorflow.python.util import compat
 
 
+def decode_resource_handle(encoded):
+  """Decode a ResourceHandle proto encoded as custom numpy struct type."""
+  resource_handle = resource_handle_pb2.ResourceHandle()
+  if sys.version_info.major < 3:
+    resource_handle.ParseFromString("".join([chr(ch[0]) for ch in encoded]))
+  else:
+    resource_handle.ParseFromString(bytes([ch[0] for ch in encoded]))
+  return resource_handle
+
+
+def encode_resource_handle(resource_handle):
+  """Encode a ResourceHandle proto as custom numpy struct type."""
+  return np.asarray(bytearray(resource_handle.SerializeToString()),
+                    dtype=dtypes.np_resource)
+
+
 class TensorHandle(object):
   """Represents a handle for a live tensor in a session."""
 
@@ -47,7 +69,8 @@ class TensorHandle(object):
       dtype: The data type of the tensor represented by `handle`.
       session: The session in which the tensor is produced.
     """
-    self._handle = compat.as_str_any(handle)
+    self._resource_handle = decode_resource_handle(handle)
+    self._handle = compat.as_str_any(self._resource_handle.name)
     self._dtype = dtype
     self._session = session
     self._auto_gc_enabled = True
@@ -59,6 +82,20 @@ class TensorHandle(object):
   def __str__(self):
     return self._handle
 
+  @property
+  def resource_handle(self):
+    """The ResourceHandle representation of this handle."""
+    return self._resource_handle
+
+  def to_numpy_array(self):
+    """Convert a TensorHandle object to a feedable numpy value.
+
+    Returns:
+      A numpy array of a custom struct type that can be used as a feed value
+      to run().
+    """
+    return encode_resource_handle(self.resource_handle)
+
   @property
   def handle(self):
     """The string representation of this handle."""
@@ -154,7 +191,7 @@ def get_session_handle(data, name=None):
 
   # Colocate this operation with data.
   with ops.colocate_with(data):
-    return gen_data_flow_ops._get_session_handle(data, name=name)
+    return gen_data_flow_ops._get_session_handle_v2(data, name=name)  # pylint: disable=protected-access
 
 
 def get_session_tensor(handle, dtype, name=None):
@@ -259,7 +296,7 @@ def _get_handle_mover(graph, feeder, handle):
     # Create mover if we haven't done it.
     holder, reader = _get_handle_reader(graph, handle, dtype)
     with graph.as_default(), graph.device(feeder.op.device):
-      mover = gen_data_flow_ops._get_session_handle(reader)
+      mover = gen_data_flow_ops._get_session_handle_v2(reader)  # pylint: disable=protected-access
     result = (holder, mover)
     graph._handle_movers[graph_key] = result
   return result

commit 6dc7f989f10ceb4cc6b19e627ab742e02a0787b7
Author: Yaroslav Bulatov <yaroslavvb@gmail.com>
Date:   Mon Dec 12 22:34:22 2016 -0800

    replace AsProtoField with AsProtoTensorContent for efficiency (#6257)

diff --git a/tensorflow/core/distributed_runtime/master_session.cc b/tensorflow/core/distributed_runtime/master_session.cc
index 1918eae8751..0baff5c1904 100644
--- a/tensorflow/core/distributed_runtime/master_session.cc
+++ b/tensorflow/core/distributed_runtime/master_session.cc
@@ -438,7 +438,7 @@ static bool CopyIfNeeded(TensorProto* in, TensorProto* out) {
   } else {
     Tensor t(in->dtype());
     if (!t.FromProto(cpu_allocator(), *in)) return false;
-    t.AsProtoField(out);
+    t.AsProtoTensorContent(out);
   }
   return true;
 }
diff --git a/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc b/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc
index 3bee20623b6..13aeb9f9c72 100644
--- a/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc
+++ b/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc
@@ -377,7 +377,7 @@ class GrpcWorkerService : public AsyncServiceInterface {
               recv->set_key(key);
               // TODO(zhifengc): Deal with gpu -> cpu copy.
               TensorProto* proto = recv->mutable_val();
-              val.AsProtoField(proto);
+              val.AsProtoTensorContent(proto);
             }
           }
           delete collector;

commit d26c33bd3720fe492c023e368c68f56b763fa1d3
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Oct 6 17:09:00 2016 -0800

    Adding an optimized implementation of concat on GPUs.  Large efficiency gains
    over current code when there are many tensors that are being combined.
    
    One piece of fixing b/30377985.  The next step is to implement a split that
    can output variable sizes, then the gradient of concat will be one (fast) op
    instead of many slower ones.
    Change: 135429927

diff --git a/tensorflow/core/kernels/BUILD b/tensorflow/core/kernels/BUILD
index 845de179e2f..a8164830397 100644
--- a/tensorflow/core/kernels/BUILD
+++ b/tensorflow/core/kernels/BUILD
@@ -88,14 +88,19 @@ tf_kernel_library(
 
 tf_kernel_library(
     name = "concat_lib",
-    srcs = ["concat_lib_cpu.cc"],
+    srcs = [
+        "concat_lib_cpu.cc",
+        "concat_lib_gpu.cc",
+    ],
     hdrs = [
         "concat_lib.h",
         "concat_lib_cpu.h",
     ],
     gpu_srcs = [
-        "concat_lib_gpu.cu.cc",
+        "concat_lib_gpu_impl.cu.cc",
         "concat_lib.h",
+        "cuda_device_array.h",
+        "cuda_device_array_gpu.h",
     ],
     deps = [
         ":bounds_check",
diff --git a/tensorflow/core/kernels/concat_lib.h b/tensorflow/core/kernels/concat_lib.h
index 7e02142936f..cef873f804a 100644
--- a/tensorflow/core/kernels/concat_lib.h
+++ b/tensorflow/core/kernels/concat_lib.h
@@ -29,22 +29,15 @@ void ConcatCPU(DeviceBase* d,
                const std::vector<
                    std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>& inputs,
                typename TTypes<T, 2>::Matrix* output);
-
-// Assumes all inputs are nonempty
-template <typename T>
-void ConcatGPU32(
-    const Eigen::GpuDevice& d,
-    const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>&
-        inputs,
-    typename TTypes<T, 2>::Matrix* output);
-
+#if GOOGLE_CUDA
 template <typename T>
-void ConcatGPU64(
-    const Eigen::GpuDevice& d,
+void ConcatGPU(
+    OpKernelContext* c,
     const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>&
-        inputs,
-    typename TTypes<T, 2>::Matrix* output);
+        inputs_flat,
+    Tensor* output, typename TTypes<T, 2>::Tensor* output_flat);
 
+#endif  // GOOGLE_CUDA
 }  // namespace tensorflow
 
 #endif  // TENSORFLOW_KERNELS_CONCAT_LIB_H_
diff --git a/tensorflow/core/kernels/concat_lib_gpu.cc b/tensorflow/core/kernels/concat_lib_gpu.cc
new file mode 100644
index 00000000000..592621c52af
--- /dev/null
+++ b/tensorflow/core/kernels/concat_lib_gpu.cc
@@ -0,0 +1,122 @@
+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// See docs in ../ops/array_ops.cc.
+
+#include <vector>
+
+#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"
+#include "tensorflow/core/framework/op_kernel.h"
+#include "tensorflow/core/framework/register_types.h"
+#include "tensorflow/core/framework/tensor.h"
+#include "tensorflow/core/framework/tensor_types.h"
+#include "tensorflow/core/framework/types.h"
+
+#if GOOGLE_CUDA
+
+#include "tensorflow/core/kernels/cuda_device_array.h"
+
+namespace tensorflow {
+
+template <typename T, typename IntType>
+void ConcatGPUSlice(
+    const Eigen::GpuDevice& gpu_device,
+    const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>&
+        inputs_flat,
+    typename TTypes<T, 2>::Matrix* output);
+
+template <typename T, typename IntType>
+void ConcatGPUImpl(const Eigen::GpuDevice& d,
+                   const CudaDeviceArrayStruct<const T*>& input_ptrs,
+                   const CudaDeviceArrayStruct<IntType>& ptr_offsets,
+                   bool same_size, int slice_size,
+                   typename TTypes<T, 2>::Matrix* output);
+
+namespace {
+
+template <typename T, typename IntType>
+void ConcatGPUCall(
+    OpKernelContext* c,
+    const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>&
+        inputs_flat,
+    typename TTypes<T, 2>::Tensor* output_flat) {
+  CudaDeviceArrayOnHost<const T*> input_ptrs(c, inputs_flat.size());
+  OP_REQUIRES_OK(c, input_ptrs.Init());
+  for (int i = 0; i < inputs_flat.size(); ++i) {
+    input_ptrs.Set(i, inputs_flat[i]->data());
+  }
+  OP_REQUIRES_OK(c, input_ptrs.Finalize());
+
+  CudaDeviceArrayOnHost<IntType> output_scan(c, inputs_flat.size() + 1);
+  OP_REQUIRES_OK(c, output_scan.Init());
+  IntType scan = 0;
+  output_scan.Set(0, scan);
+  bool one_size_input = true;
+  for (int i = 0; i < inputs_flat.size(); ++i) {
+    if (one_size_input && i < inputs_flat.size() - 1 &&
+        inputs_flat[i]->dimension(1) != inputs_flat[i + 1]->dimension(1)) {
+      one_size_input = false;
+    }
+    scan += inputs_flat[i]->dimension(1);
+    output_scan.Set(i + 1, scan);
+  }
+  if (!one_size_input) OP_REQUIRES_OK(c, output_scan.Finalize());
+
+  ConcatGPUImpl<T, IntType>(c->eigen_gpu_device(), input_ptrs.data(),
+                            output_scan.data(), one_size_input,
+                            inputs_flat[0]->dimension(1), output_flat);
+}
+
+}  // end namespace
+
+template <typename T>
+void ConcatGPU(
+    OpKernelContext* c,
+    const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>&
+        inputs_flat,
+    Tensor* output, typename TTypes<T, 2>::Tensor* output_flat) {
+  if (inputs_flat.size() < 16) {
+    if (output->NumElements() < std::numeric_limits<int32>::max()) {
+      ConcatGPUSlice<T, int32>(c->eigen_gpu_device(), inputs_flat, output_flat);
+    } else {
+      ConcatGPUSlice<T, int64>(c->eigen_gpu_device(), inputs_flat, output_flat);
+    }
+  } else {
+    // Switching indexing to int64 might cause performance issues.
+    // Hence, we keep int32 indexing in the GPU kernel unless we need to
+    // switch to int64.
+    if (output->NumElements() < std::numeric_limits<int32>::max()) {
+      ConcatGPUCall<T, int32>(c, inputs_flat, output_flat);
+    } else {
+      ConcatGPUCall<T, int64>(c, inputs_flat, output_flat);
+    }
+  }
+}
+
+#define REGISTER(T)                                                           \
+  template void ConcatGPU<T>(                                                 \
+      OpKernelContext * c,                                                    \
+      const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>& \
+          inputs_flat,                                                        \
+      Tensor* output, typename TTypes<T, 2>::Tensor* output_flat);
+
+TF_CALL_GPU_NUMBER_TYPES(REGISTER);
+REGISTER(bfloat16);
+
+#undef REGISTER
+
+}  // namespace tensorflow
+
+#endif  // GOOGLE_CUDA
diff --git a/tensorflow/core/kernels/concat_lib_gpu.cu.cc b/tensorflow/core/kernels/concat_lib_gpu.cu.cc
deleted file mode 100644
index 99b1ca2c13b..00000000000
--- a/tensorflow/core/kernels/concat_lib_gpu.cu.cc
+++ /dev/null
@@ -1,91 +0,0 @@
-/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-==============================================================================*/
-
-#if GOOGLE_CUDA
-
-#define EIGEN_USE_GPU
-
-#include <stdio.h>
-
-#include <memory>
-#include <vector>
-
-#include "tensorflow/core/framework/bfloat16.h"
-#include "tensorflow/core/framework/register_types.h"
-#include "tensorflow/core/framework/tensor_types.h"
-#include "tensorflow/core/kernels/concat_lib.h"
-
-namespace tensorflow {
-
-typedef Eigen::GpuDevice GPUDevice;
-
-template <typename T>
-void ConcatGPU32(
-    const GPUDevice& d,
-    const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>&
-        inputs,
-    typename TTypes<T, 2>::Matrix* output) {
-  Eigen::array<int32, 2> offset{0, 0};
-  for (int i = 0; i < inputs.size(); ++i) {
-    Eigen::array<int32, 2> size;
-    size[0] = inputs[i]->dimension(0);
-    size[1] = inputs[i]->dimension(1);
-    To32Bit(*output).slice(offset, size).device(d) = To32Bit(*inputs[i]);
-    offset[1] += size[1];
-  }
-}
-
-template <typename T>
-void ConcatGPU64(
-    const GPUDevice& d,
-    const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>&
-        inputs,
-    typename TTypes<T, 2>::Matrix* output) {
-  Eigen::array<int64, 2> offset{0, 0};
-  for (int i = 0; i < inputs.size(); ++i) {
-    Eigen::array<int64, 2> size;
-    size[0] = inputs[i]->dimension(0);
-    size[1] = inputs[i]->dimension(1);
-    output->slice(offset, size).device(d) = *inputs[i];
-    offset[1] += size[1];
-  }
-}
-
-#define REGISTER_GPU32(T)                                                     \
-  template void ConcatGPU32<T>(                                               \
-      const GPUDevice& d,                                                     \
-      const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>& \
-          inputs,                                                             \
-      typename TTypes<T, 2>::Matrix* output);
-
-#define REGISTER_GPU64(T)                                                     \
-  template void ConcatGPU64<T>(                                               \
-      const GPUDevice& d,                                                     \
-      const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>& \
-          inputs,                                                             \
-      typename TTypes<T, 2>::Matrix* output);
-
-TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU32);
-REGISTER_GPU32(bfloat16);
-
-TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU64);
-REGISTER_GPU64(bfloat16);
-
-#undef REGISTER_GPU32
-#undef REGISTER_GPU64
-
-}  // end namespace tensorflow
-
-#endif  // GOOGLE_CUDA
diff --git a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc
new file mode 100644
index 00000000000..9037a48580a
--- /dev/null
+++ b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc
@@ -0,0 +1,270 @@
+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#if GOOGLE_CUDA
+
+#define EIGEN_USE_GPU
+
+#include <memory>
+#include <vector>
+
+#include "tensorflow/core/framework/bfloat16.h"
+#include "tensorflow/core/framework/register_types.h"
+#include "tensorflow/core/framework/tensor_types.h"
+#include "tensorflow/core/kernels/cuda_device_array_gpu.h"
+
+namespace tensorflow {
+
+typedef Eigen::GpuDevice GPUDevice;
+
+namespace {
+
+struct Cuda2DLaunchConfig {
+  dim3 virtual_thread_count;
+  dim3 thread_per_block;
+  dim3 block_count;
+};
+
+Cuda2DLaunchConfig GetCuda2DLaunchConfig(int xdim, int ydim,
+                                         const GPUDevice& d) {
+  Cuda2DLaunchConfig config;
+
+  config.virtual_thread_count = dim3(xdim, ydim, 1);
+
+  const int kThreadsPerBlock = 256;
+  int block_cols = std::min(xdim, kThreadsPerBlock);
+  // ok to round down here and just do more loops in the kernel
+  int block_rows = std::max(kThreadsPerBlock / block_cols, 1);
+
+  const int physical_thread_count =
+      d.getNumCudaMultiProcessors() * d.maxCudaThreadsPerMultiProcessor();
+
+  const int max_blocks = std::max(physical_thread_count / kThreadsPerBlock, 1);
+
+  config.thread_per_block = dim3(block_cols, block_rows, 1);
+
+  int grid_x = std::min((xdim + block_cols - 1) / block_cols, max_blocks);
+
+  config.block_count = dim3(
+      grid_x, std::min(max_blocks / grid_x, std::max(ydim / block_rows, 1)), 1);
+
+  return config;
+}
+
+template <typename IntType>
+__device__ IntType upper_bound(IntType* first, IntType count, IntType val) {
+  IntType* orig = first;
+  IntType* it = nullptr;
+  IntType step = 0;
+  while (count > 0) {
+    it = first;
+    step = count / 2;
+    it += step;
+    if (!(val < *it)) {
+      first = ++it;
+      count -= step + 1;
+    } else {
+      count = step;
+    }
+  }
+
+  return first - orig;
+}
+
+template <typename T, typename IntType>
+__global__ void concat_fixed_kernel(
+    CudaDeviceArrayStruct<const T*> input_ptr_data, int split_size,
+    int total_rows, int total_cols, T* output) {
+  const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);
+  IntType gidx = blockIdx.x * blockDim.x + threadIdx.x;
+
+  for (; gidx < total_cols; gidx += blockDim.x * gridDim.x) {
+    IntType gidy = blockIdx.y * blockDim.y + threadIdx.y;
+
+    IntType split = gidx / split_size;
+    const T* input_ptr = input_ptrs[split];
+    IntType col_offset = gidx % split_size;
+#pragma unroll
+    for (; gidy < total_rows; gidy += blockDim.y * gridDim.y) {
+      output[gidy * total_cols + gidx] =
+          input_ptr[gidy * split_size + col_offset];
+    }
+  }
+}
+
+}  // end namespace
+
+// cannot be in anonymous namespace due to extern shared memory
+template <typename T, typename IntType, bool useSmem>
+__global__ void concat_variable_kernel(
+    CudaDeviceArrayStruct<const T*> input_ptr_data,
+    CudaDeviceArrayStruct<IntType> output_scan, IntType total_rows,
+    IntType total_cols, T* output) {
+  const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);
+  IntType* col_scan = GetCudaDeviceArrayOnDevice(&output_scan);
+
+  // do upper_bound on col to find which pointer we should be using
+  IntType gidx = blockIdx.x * blockDim.x + threadIdx.x;
+  IntType num_inputs = input_ptr_data.size;
+
+  // verbose declaration needed due to template
+  extern __shared__ __align__(sizeof(T)) unsigned char smem[];
+  IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);
+
+  if (useSmem) {
+    IntType lidx = threadIdx.y * blockDim.x + threadIdx.x;
+    IntType blockSize = blockDim.x * blockDim.y;
+
+    for (IntType i = lidx; i < output_scan.size; i += blockSize) {
+      smem_col_scan[i] = col_scan[i];
+    }
+
+    __syncthreads();
+
+    col_scan = smem_col_scan;
+  }
+
+  // do an initial binary search and then scan linearly from there
+  // works well when there are many small segments and when the
+  // segments are much longer
+  IntType segment = upper_bound<IntType>(col_scan, num_inputs, gidx) - 1;
+
+  IntType curr_offset = col_scan[segment];
+  IntType curr_segment = segment;
+  for (; gidx < total_cols; gidx += blockDim.x * gridDim.x) {
+    IntType curr_col_offset;
+    while ((curr_col_offset = col_scan[curr_segment + 1]) <= gidx) {
+      curr_offset = curr_col_offset;
+      ++curr_segment;
+    }
+
+    IntType local_col = gidx - curr_offset;
+    IntType segment_width = curr_col_offset - curr_offset;
+    const T* input_ptr = input_ptrs[curr_segment];
+
+    IntType gidy = blockIdx.y * blockDim.y + threadIdx.y;
+    for (; gidy < total_rows; gidy += blockDim.y * gridDim.y)
+      output[gidy * total_cols + gidx] =
+          input_ptr[gidy * segment_width + local_col];
+  }
+}
+
+template <typename T, typename IntType>
+void ConcatGPUSlice(
+    const Eigen::GpuDevice& gpu_device,
+    const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>&
+        inputs_flat,
+    typename TTypes<T, 2>::Matrix* output) {
+  Eigen::array<IntType, 2> offset{0, 0};
+  for (int i = 0; i < inputs_flat.size(); ++i) {
+    Eigen::array<IntType, 2> size;
+    size[0] = inputs_flat[i]->dimension(0);
+    size[1] = inputs_flat[i]->dimension(1);
+    if (std::is_same<IntType, int32>::value) {
+      To32Bit(*output).slice(offset, size).device(gpu_device) =
+          To32Bit(*inputs_flat[i]);
+    } else {
+      output->slice(offset, size).device(gpu_device) = *inputs_flat[i];
+    }
+
+    offset[1] += size[1];
+  }
+}
+
+template <typename T, typename IntType>
+void ConcatGPUImpl(const Eigen::GpuDevice& gpu_device,
+                   const CudaDeviceArrayStruct<const T*>& input_ptrs,
+                   const CudaDeviceArrayStruct<IntType>& output_scan,
+                   bool fixed_size, int split_size,
+                   typename TTypes<T, 2>::Matrix* output) {
+  auto config = GetCuda2DLaunchConfig(output->dimension(1),
+                                      output->dimension(0), gpu_device);
+
+  if (fixed_size) {
+    concat_fixed_kernel<T, IntType><<<
+        config.block_count, config.thread_per_block, 0, gpu_device.stream()>>>(
+        input_ptrs, split_size, output->dimension(0), output->dimension(1),
+        output->data());
+  } else {
+    IntType smem_max = gpu_device.sharedMemPerBlock();
+    IntType smem_usage = output_scan.size * sizeof(IntType);
+    // performance crossover is less than using maximum available shared memory
+    // on most processors
+    // possibly due to decreasing occupancy
+    // 4096 inputs is a lot, most code will take the smem path
+    const int32 kMaxSmemBytesPerformance = 16384;
+    if (smem_usage < smem_max && smem_usage < kMaxSmemBytesPerformance)
+      concat_variable_kernel<
+          T, IntType, true><<<config.block_count, config.thread_per_block,
+                              smem_usage, gpu_device.stream()>>>(
+          input_ptrs, output_scan, output->dimension(0), output->dimension(1),
+          output->data());
+    else
+      concat_variable_kernel<
+          T, IntType, false><<<config.block_count, config.thread_per_block, 0,
+                               gpu_device.stream()>>>(
+          input_ptrs, output_scan, output->dimension(0), output->dimension(1),
+          output->data());
+  }
+}
+
+#define REGISTER_GPUCONCAT32(T)                                               \
+  template void ConcatGPUSlice<T, int32>(                                     \
+      const Eigen::GpuDevice& gpu_device,                                     \
+      const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>& \
+          inputs_flat,                                                        \
+      typename TTypes<T, 2>::Matrix* output);
+
+#define REGISTER_GPUCONCAT64(T)                                               \
+  template void ConcatGPUSlice<T, int64>(                                     \
+      const Eigen::GpuDevice& gpu_device,                                     \
+      const std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>& \
+          inputs_flat,                                                        \
+      typename TTypes<T, 2>::Matrix* output);
+
+#define REGISTER_GPU32(T)                                               \
+  template void ConcatGPUImpl<T, int32>(                                \
+      const Eigen::GpuDevice& d,                                        \
+      const CudaDeviceArrayStruct<const T*>& input_ptrs,                \
+      const CudaDeviceArrayStruct<int32>& ptr_offsets, bool fixed_size, \
+      int split_size, typename TTypes<T, 2>::Matrix* output);
+
+#define REGISTER_GPU64(T)                                               \
+  template void ConcatGPUImpl<T, int64>(                                \
+      const Eigen::GpuDevice& d,                                        \
+      const CudaDeviceArrayStruct<const T*>& input_ptrs,                \
+      const CudaDeviceArrayStruct<int64>& ptr_offsets, bool fixed_size, \
+      int split_size, typename TTypes<T, 2>::Matrix* output);
+
+TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPUCONCAT32);
+REGISTER_GPUCONCAT32(bfloat16);
+
+TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPUCONCAT64);
+REGISTER_GPUCONCAT64(bfloat16);
+
+TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU32);
+REGISTER_GPU32(bfloat16);
+
+TF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU64);
+REGISTER_GPU64(bfloat16);
+
+#undef REGISTER_GPUCONCAT32
+#undef REGISTER_GPUCONCAT64
+#undef REGISTER_GPU32
+#undef REGISTER_GPU64
+
+}  // end namespace tensorflow
+
+#endif  // GOOGLE_CUDA
diff --git a/tensorflow/core/kernels/concat_op.cc b/tensorflow/core/kernels/concat_op.cc
index f12527364b2..fbd38dff1f7 100644
--- a/tensorflow/core/kernels/concat_op.cc
+++ b/tensorflow/core/kernels/concat_op.cc
@@ -123,14 +123,7 @@ class ConcatOp : public OpKernel {
       auto output_flat = output->shaped<T, 2>({inputs_flat_dim0, output_dim1});
 #if GOOGLE_CUDA
       if (std::is_same<Device, GPUDevice>::value) {
-        // Switching indexing to int64 might cause performance issues.
-        // Hence, we keep int32 indexing in the GPU kernel unless we need to
-        // switch to int64.
-        if (output->NumElements() < std::numeric_limits<int32>::max()) {
-          ConcatGPU32<T>(c->eigen_gpu_device(), inputs_flat, &output_flat);
-        } else {
-          ConcatGPU64<T>(c->eigen_gpu_device(), inputs_flat, &output_flat);
-        }
+        ConcatGPU<T>(c, inputs_flat, output, &output_flat);
         return;
       }
 #endif  // GOOGLE_CUDA
diff --git a/tensorflow/core/kernels/constant_op.cc b/tensorflow/core/kernels/constant_op.cc
index 699e9e01420..be7a7a41a41 100644
--- a/tensorflow/core/kernels/constant_op.cc
+++ b/tensorflow/core/kernels/constant_op.cc
@@ -57,6 +57,7 @@ REGISTER_KERNEL_BUILDER(Name("Const").Device(DEVICE_CPU), ConstantOp);
       Name("Const").Device(DEVICE_##D).TypeConstraint<TYPE>("dtype"), \
       ConstantOp);
 REGISTER_KERNEL(GPU, Eigen::half);
+REGISTER_KERNEL(GPU, bfloat16);
 REGISTER_KERNEL(GPU, float);
 REGISTER_KERNEL(GPU, double);
 REGISTER_KERNEL(GPU, uint8);
diff --git a/tensorflow/core/kernels/pack_op.cc b/tensorflow/core/kernels/pack_op.cc
index fc255557886..e072eb36b34 100644
--- a/tensorflow/core/kernels/pack_op.cc
+++ b/tensorflow/core/kernels/pack_op.cc
@@ -111,14 +111,7 @@ class PackOp : public OpKernel {
       }
 #if GOOGLE_CUDA
       if (std::is_same<Device, GPUDevice>::value) {
-        // Switching indexing to int64 might cause performance issues.
-        // Hence, we keep int32 indexing in the GPU kernel unless we need to
-        // switch to int64.
-        if (output_size < std::numeric_limits<int32>::max()) {
-          ConcatGPU32<T>(c->eigen_gpu_device(), inputs_flat, &output_flat);
-        } else {
-          ConcatGPU64<T>(c->eigen_gpu_device(), inputs_flat, &output_flat);
-        }
+        ConcatGPU<T>(c, inputs_flat, output, &output_flat);
         return;
       }
 #endif  // GOOGLE_CUDA
diff --git a/tensorflow/core/kernels/tensor_array_ops.cc b/tensorflow/core/kernels/tensor_array_ops.cc
index cbb7f1cca79..a1ccadc6ad1 100644
--- a/tensorflow/core/kernels/tensor_array_ops.cc
+++ b/tensorflow/core/kernels/tensor_array_ops.cc
@@ -521,16 +521,7 @@ class TensorArrayPackOrGatherOp : public OpKernel {
 
 #if GOOGLE_CUDA
     if (std::is_same<Device, GPUDevice>::value) {
-      // Switching indexing to int64 might cause performance issues.
-      // Hence, we keep int32 indexing in the GPU kernel unless we need to
-      // switch to int64.
-      if (output_shape.num_elements() < std::numeric_limits<int32>::max()) {
-        ConcatGPU32<T>(ctx->eigen_gpu_device(), input_tensors_flat,
-                       &output_flat);
-      } else {
-        ConcatGPU64<T>(ctx->eigen_gpu_device(), input_tensors_flat,
-                       &output_flat);
-      }
+      ConcatGPU<T>(ctx, input_tensors_flat, output_tensor, &output_flat);
       return;
     }
 #endif  // GOOGLE_CUDA
@@ -722,16 +713,7 @@ class TensorArrayConcatOp : public OpKernel {
           output_tensor->shaped<T, 2>({1, output_shape.num_elements()});
 #if GOOGLE_CUDA
       if (std::is_same<Device, GPUDevice>::value) {
-        // Switching indexing to int64 might cause performance issues.
-        // Hence, we keep int32 indexing in the GPU kernel unless we need to
-        // switch to int64.
-        if (output_shape.num_elements() < std::numeric_limits<int32>::max()) {
-          ConcatGPU32<T>(ctx->eigen_gpu_device(), input_tensors_flat,
-                         &output_flat);
-        } else {
-          ConcatGPU64<T>(ctx->eigen_gpu_device(), input_tensors_flat,
-                         &output_flat);
-        }
+        ConcatGPU<T>(ctx, input_tensors_flat, output_tensor, &output_flat);
         return;
       }
 #endif  // GOOGLE_CUDA
diff --git a/tensorflow/python/BUILD b/tensorflow/python/BUILD
index c9f7dfdc5c1..13dd45b9a0b 100644
--- a/tensorflow/python/BUILD
+++ b/tensorflow/python/BUILD
@@ -2168,3 +2168,15 @@ cuda_py_test(
     ],
     main = "ops/batch_norm_benchmark.py",
 )
+
+cuda_py_test(
+    name = "concat_benchmark",
+    srcs = [
+        "ops/concat_benchmark.py",
+    ],
+    additional_deps = [
+        ":nn_ops",
+        "//tensorflow:tensorflow_py",
+    ],
+    main = "ops/concat_benchmark.py",
+)
diff --git a/tensorflow/python/kernel_tests/concat_op_test.py b/tensorflow/python/kernel_tests/concat_op_test.py
index 9be6fbbb56a..79f9a94dac4 100644
--- a/tensorflow/python/kernel_tests/concat_op_test.py
+++ b/tensorflow/python/kernel_tests/concat_op_test.py
@@ -131,6 +131,7 @@ class ConcatOpTest(tf.test.TestCase):
 
   def testRandom(self):
     self._testRandom(tf.float32)
+    self._testRandom(tf.float32, use_gpu=True)
     self._testRandom(tf.int16)
     self._testRandom(tf.int32, use_gpu=True)
     self._testRandom(tf.bfloat16)
@@ -238,7 +239,7 @@ class ConcatOpTest(tf.test.TestCase):
     # Random dims of rank 5
     input_shape = np.random.randint(1, 5, size=5)
     # Random number of tensors
-    num_tensors = np.random.randint(1, 10)
+    num_tensors = np.random.randint(12, 20)
     # Random dim to concat on
     concat_dim = np.random.randint(5)
     concat_dim_sizes = np.random.randint(1, 5, size=num_tensors)
@@ -428,6 +429,42 @@ class ConcatOpTest(tf.test.TestCase):
       # not just non-crashingness, once other large tensor fixes have gone in.
       _ = onezeros.eval()
 
+  # important as gpu implementation could fail if
+  # shared memory is not large for all the inputs
+  def testConcatLargeNumberOfTensors(self):
+    with self.test_session(use_gpu=True):
+      for concat_dim in range(2):
+        params = {}
+        p = []
+        shape = np.array([7, 13])
+        if tf.test.is_gpu_available():
+          num_tensors = 10000
+        else:
+          num_tensors = 1000
+        for i in np.arange(num_tensors):
+          input_shape = shape
+          placeholder = tf.placeholder(tf.float32, shape=input_shape)
+          p.append(placeholder)
+
+          params[placeholder] = np.random.rand(*input_shape).astype(np.float32)
+
+        concat_inputs = p
+        c = tf.concat(concat_dim, concat_inputs)
+        result = c.eval(feed_dict=params)
+
+        self.assertEqual(result.shape, c.get_shape())
+        cur_offset = 0
+
+        for i in np.arange(num_tensors):
+          # The index into the result is the ':' along all dimensions
+          # except the concat_dim. slice(0, size) is used for ':', and
+          # a list of slices is used to index into result.
+          index = [slice(0, params[p[i]].shape[j]) for j in np.arange(2)]
+          index[concat_dim] = slice(cur_offset,
+                                    cur_offset + params[p[i]].shape[concat_dim])
+          cur_offset += params[p[i]].shape[concat_dim]
+          self.assertAllEqual(result[index], params[p[i]])
+
 
 class ConcatOffsetTest(tf.test.TestCase):
 
diff --git a/tensorflow/python/ops/concat_benchmark.py b/tensorflow/python/ops/concat_benchmark.py
new file mode 100644
index 00000000000..9130a35e073
--- /dev/null
+++ b/tensorflow/python/ops/concat_benchmark.py
@@ -0,0 +1,142 @@
+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Benchmark for split and grad of split."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import itertools
+import random
+import time
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+tf.app.flags.DEFINE_boolean("use_gpu", True, """Run GPU benchmarks.""")
+
+
+def build_graph(device, input_shape, variable, num_inputs, axis, grad):
+  """Build a graph containing a sequence of batch normalizations.
+
+  Args:
+    device: string, the device to run on.
+    input_shape: shape of the input tensors.
+    variable: whether or not to randomize the input shape
+    num_inputs: the number of inputs to concat
+    axis: axis to be concat'ed
+    grad: if True compute the gradient
+
+  Returns:
+    An array of tensors to run()
+  """
+  with tf.device("/%s:0" % device):
+    if not variable:
+      inputs = [tf.zeros(input_shape) for _ in range(num_inputs)]
+    else:
+      if axis == 1:
+        inputs = [
+            tf.zeros([
+                input_shape[0],
+                random.randint(max(1, input_shape[1] - 5), input_shape[1] + 5)
+            ]) for _ in range(num_inputs)
+        ]
+      else:
+        inputs = [
+            tf.zeros([
+                random.randint(max(1, input_shape[0] - 5), input_shape[0] + 5),
+                input_shape[1]
+            ]) for _ in range(num_inputs)
+        ]
+
+    outputs = [tf.concat(axis, inputs) for _ in range(100)]
+    if grad:
+      return tf.group(*list(
+          itertools.chain.from_iterable(
+              [tf.gradients(output, inputs) for output in outputs])))
+    else:
+      return tf.group(*outputs)
+
+
+class ConcatBenchmark(tf.test.Benchmark):
+  """Benchmark batch normalization."""
+
+  def _run_graph(self, device, input_shape, variable, num_inputs, axis, grad,
+                 num_iters):
+    """Run the graph and print its execution time.
+
+    Args:
+      device: string, the device to run on.
+      input_shape: shape of the input tensors.
+      variable: whether or not the input shape should be fixed
+      num_inputs: the number of inputs to concat
+      axis: axis to be concat'ed
+      grad: if True compute the gradient
+      num_iters: number of steps to run.
+
+    Returns:
+      The duration of the run in seconds.
+    """
+    graph = tf.Graph()
+    with graph.as_default():
+      outputs = build_graph(device, input_shape, variable, num_inputs, axis,
+                            grad)
+    config = tf.ConfigProto(graph_options=tf.GraphOptions(
+        optimizer_options=tf.OptimizerOptions(
+            opt_level=tf.OptimizerOptions.L0)))
+    with tf.Session(graph=graph, config=config) as session:
+      tf.initialize_all_variables().run()
+      _ = session.run(outputs)  # warm up.
+      start_time = time.time()
+      for _ in range(num_iters):
+        _ = session.run(outputs)
+      duration = time.time() - start_time
+      print("%s shape:%d/%d var: %r #inputs:%d axis:%d grad:%r - %f secs - %f "
+            "GB/sec" % (device, input_shape[0], input_shape[1], variable,
+                        num_inputs, axis, grad, duration / num_iters,
+                        num_inputs * input_shape[0] * input_shape[1] * 4 * 2 *
+                        100 / (duration / num_iters) / 1e9))
+
+    name_template = (
+        "concat_bench_{device}_input_shape_{shape}_variable_{variable}"
+        "_num_inputs_{num_inputs}_axis_{axis}_grad_{grad}")
+
+    self.report_benchmark(name=name_template.format(
+        device=device,
+        num_inputs=num_inputs,
+        variable=variable,
+        grad=grad,
+        shape=str(input_shape).replace(" ", ""),
+        axis=str(axis),
+        iters=num_iters))
+
+    return duration
+
+  def benchmark_concat(self):
+    print("Forward vs backward concat")
+    shapes = [[2000, 8], [8, 2000], [100, 18], [1000, 18], [10000, 18],
+              [100, 97], [1000, 97], [10000, 97], [10000, 1], [1, 10000],
+              [100000, 1], [1, 100000]]
+    axis_ = [0, 1]
+    num_inputs = 100
+    num_iters = [20] * len(shapes)
+    variable = [False, True]  # fixed input size or not
+    for shape, iters in zip(shapes, num_iters):
+      for axis in axis_:
+        for v in variable:
+          self._run_graph("gpu", shape, v, num_inputs, axis, False, iters)
+
+
+if __name__ == "__main__":
+  tf.test.main()

commit 426e36d6f351dd556ccb1c8defa1ddd88015942d
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Fri Aug 26 07:33:33 2016 -0800

    Reduce subclass boilerplate by embracing duck-typing and adopting a "public-calls-private" design pattern. This pattern reduces lines of Distribution sub-class code by ~33%.
    
    Specifically, this change:
    
    1. Makes BaseDistribution purely virtual.
    2. The new idiom is that all public methods are implemented in the base class, and all possible argument checking is done there. Subclasses implement _methods called by the base class; if they fail to implement it will fire a NotImplementedError during tf graph construction.
    3. Fix style inconsistencies, eg, function order, remove unnecessary indirection, use tf.ones instead of tf.constant, etc.
    4. Fix efficiency when I noticed a problem. Example Dirichlet._variance.
    Change: 131404360

diff --git a/tensorflow/contrib/bayesflow/python/ops/stochastic_graph.py b/tensorflow/contrib/bayesflow/python/ops/stochastic_graph.py
index 499c91ea6ff..7d29ae23d19 100644
--- a/tensorflow/contrib/bayesflow/python/ops/stochastic_graph.py
+++ b/tensorflow/contrib/bayesflow/python/ops/stochastic_graph.py
@@ -43,6 +43,7 @@ import threading
 
 import six
 
+from tensorflow.contrib import distributions
 from tensorflow.contrib.bayesflow.python.ops import stochastic_gradient_estimators as sge
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
@@ -333,7 +334,7 @@ class DistributionTensor(StochasticTensor):
     `MeanValueType` or if `loss_fn=None`.
 
     Args:
-      dist_cls: a class deriving from `BaseDistribution`.
+      dist_cls: a `Distribution` class.
       name: a name for this `DistributionTensor` and its ops.
       dist_value_type: a `_StochasticValueType`, which will determine what the
           `value` of this `DistributionTensor` will be. If not provided, the
@@ -346,7 +347,13 @@ class DistributionTensor(StochasticTensor):
           module for additional loss functions and baselines.
       **dist_args: keyword arguments to be passed through to `dist_cls` on
           construction.
+
+    Raises:
+      TypeError: if `dist_cls` is not a `Distribution`.
+      TypeError: if `loss_fn` is not `callable`.
     """
+    if not issubclass(dist_cls, distributions.Distribution):
+      raise TypeError("dist_cls must be a subclass of Distribution")
     self._dist_cls = dist_cls
     self._dist_args = dist_args
     if dist_value_type is None:
@@ -395,12 +402,12 @@ class DistributionTensor(StochasticTensor):
     if isinstance(self._value_type, MeanValue):
       value_tensor = self._dist.mean()
     elif isinstance(self._value_type, SampleValue):
-      value_tensor = self._dist.sample_n(self._value_type.n)
+      value_tensor = self._dist.sample(self._value_type.n)
     elif isinstance(self._value_type, SampleAndReshapeValue):
       if self._value_type.n == 1:
         value_tensor = self._dist.sample()
       else:
-        samples = self._dist.sample_n(self._value_type.n)
+        samples = self._dist.sample(self._value_type.n)
         samples_shape = array_ops.shape(samples)
         samples_static_shape = samples.get_shape()
         new_batch_size = samples_shape[0] * samples_shape[1]
diff --git a/tensorflow/contrib/distributions/python/kernel_tests/bernoulli_test.py b/tensorflow/contrib/distributions/python/kernel_tests/bernoulli_test.py
index 82f77fbfd1e..565cb7f76c4 100644
--- a/tensorflow/contrib/distributions/python/kernel_tests/bernoulli_test.py
+++ b/tensorflow/contrib/distributions/python/kernel_tests/bernoulli_test.py
@@ -198,6 +198,20 @@ class BernoulliTest(tf.test.TestCase):
       self.assertAllClose(p, np.mean(sample_values, axis=0), atol=1e-2)
       self.assertEqual(set([0, 1]), set(sample_values.flatten()))
 
+  def testSampleActsLikeSampleN(self):
+    with self.test_session() as sess:
+      p = [0.2, 0.6]
+      dist = tf.contrib.distributions.Bernoulli(p=p)
+      n = 1000
+      seed = 42
+      self.assertAllEqual(dist.sample(n, seed).eval(),
+                          dist.sample_n(n, seed).eval())
+      n = tf.placeholder(tf.int32)
+      sample, sample_n = sess.run([dist.sample(n, seed),
+                                   dist.sample_n(n, seed)],
+                                  feed_dict={n: 1000})
+      self.assertAllEqual(sample, sample_n)
+
   def testMean(self):
     with self.test_session():
       p = np.array([[0.2, 0.7], [0.5, 0.4]], dtype=np.float32)
diff --git a/tensorflow/contrib/distributions/python/kernel_tests/categorical_test.py b/tensorflow/contrib/distributions/python/kernel_tests/categorical_test.py
index f91f2c33ac8..4fbcc1d812f 100644
--- a/tensorflow/contrib/distributions/python/kernel_tests/categorical_test.py
+++ b/tensorflow/contrib/distributions/python/kernel_tests/categorical_test.py
@@ -70,8 +70,10 @@ class CategoricalTest(tf.test.TestCase):
     self.assertEqual(dist.dtype, dist.mode().dtype)
     self.assertEqual(dist.logits.dtype, tf.float32)
     self.assertEqual(dist.logits.dtype, dist.entropy().dtype)
-    self.assertEqual(dist.logits.dtype, dist.pmf(0).dtype)
-    self.assertEqual(dist.logits.dtype, dist.log_pmf(0).dtype)
+    self.assertEqual(dist.logits.dtype, dist.pmf(
+        np.array(0, dtype=np.int64)).dtype)
+    self.assertEqual(dist.logits.dtype, dist.log_pmf(
+        np.array(0, dtype=np.int64)).dtype)
 
   def testUnknownShape(self):
     with self.test_session():
diff --git a/tensorflow/contrib/distributions/python/ops/bernoulli.py b/tensorflow/contrib/distributions/python/ops/bernoulli.py
index 148e8b8562a..49a4e6f8fcf 100644
--- a/tensorflow/contrib/distributions/python/ops/bernoulli.py
+++ b/tensorflow/contrib/distributions/python/ops/bernoulli.py
@@ -24,7 +24,6 @@ from tensorflow.contrib.distributions.python.ops import kullback_leibler
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import nn
@@ -67,55 +66,18 @@ class Bernoulli(distribution.Distribution):
     Raises:
       ValueError: If p and logits are passed, or if neither are passed.
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._name = name
-    self._dtype = dtype
-    self._validate_args = validate_args
     self._logits, self._p = distribution_util.get_logits_and_prob(
         name=name, logits=logits, p=p, validate_args=validate_args)
     with ops.name_scope(name):
       with ops.name_scope("q"):
         self._q = 1. - self._p
-    self._batch_shape = array_ops.shape(self._logits)
-    self._event_shape = array_ops.constant([], dtype=dtypes.int32)
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def dtype(self):
-    return self._dtype
-
-  @property
-  def is_reparameterized(self):
-    return False
-
-  def batch_shape(self, name="batch_shape"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._batch_shape]):
-        return array_ops.identity(self._batch_shape)
-
-  def get_batch_shape(self):
-    return self._logits.get_shape()
-
-  def event_shape(self, name="event_shape"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._batch_shape]):
-        return array_ops.constant([], dtype=self._batch_shape.dtype)
-
-  def get_event_shape(self):
-    return tensor_shape.scalar()
+        super(Bernoulli, self).__init__(
+            dtype=dtype,
+            parameters={"p": self._p, "q": self._q, "logits": self._logits},
+            is_continuous=False,
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def logits(self):
@@ -130,143 +92,69 @@ class Bernoulli(distribution.Distribution):
     """1-p."""
     return self._q
 
-  def prob(self, event, name="prob"):
-    """Probability mass function.
+  def _batch_shape(self):
+    return array_ops.shape(self._logits)
 
-    Args:
-      event: `int32` or `int64` binary Tensor; must be broadcastable with `p`.
-      name: A name for this operation.
+  def _get_batch_shape(self):
+    return self._logits.get_shape()
 
-    Returns:
-      The probabilities of the events.
-    """
-    return super(Bernoulli, self).prob(event, name)
+  def _event_shape(self):
+    return array_ops.constant([], dtype=dtypes.int32)
 
-  def log_prob(self, event, name="log_prob"):
-    """Log of the probability mass function.
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
 
-    Args:
-      event: `int32` or `int64` binary Tensor.
-      name: A name for this operation (optional).
+  def _sample_n(self, n, seed=None):
+    new_shape = array_ops.concat(0, ([n], self.batch_shape()))
+    uniform = random_ops.random_uniform(
+        new_shape, seed=seed, dtype=dtypes.float32)
+    sample = math_ops.less(uniform, self.p)
+    return math_ops.cast(sample, self.dtype)
 
-    Returns:
-      The log-probabilities of the events.
-    """
+  def _log_prob(self, event):
     # TODO(jaana): The current sigmoid_cross_entropy_with_logits has
     # inconsistent  behavior for logits = inf/-inf.
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.logits, event]):
-        event = ops.convert_to_tensor(event, name="event")
-        event = math_ops.cast(event, self.logits.dtype)
-        logits = self.logits
-        # sigmoid_cross_entropy_with_logits doesn't broadcast shape,
-        # so we do this here.
-        # TODO(b/30637701): Check dynamic shape, and don't broadcast if the
-        # dynamic shapes are the same.
-        if (not event.get_shape().is_fully_defined() or
-            not logits.get_shape().is_fully_defined() or
-            event.get_shape() != logits.get_shape()):
-          logits = array_ops.ones_like(event) * logits
-          event = array_ops.ones_like(logits) * event
-        return -nn.sigmoid_cross_entropy_with_logits(logits, event)
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Generate `n` samples.
+    event = ops.convert_to_tensor(event, name="event")
+    event = math_ops.cast(event, self.logits.dtype)
+    logits = self.logits
+    # sigmoid_cross_entropy_with_logits doesn't broadcast shape,
+    # so we do this here.
+    # TODO(b/30637701): Check dynamic shape, and don't broadcast if the
+    # dynamic shapes are the same.
+    if (not event.get_shape().is_fully_defined() or
+        not logits.get_shape().is_fully_defined() or
+        event.get_shape() != logits.get_shape()):
+      logits = array_ops.ones_like(event) * logits
+      event = array_ops.ones_like(logits) * event
+    return -nn.sigmoid_cross_entropy_with_logits(logits, event)
 
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer seed for RNG.
-      name: name to give to the op.
-
-    Returns:
-      samples: a `Tensor` of shape `(n,) + self.batch_shape` with values of type
-          `self.dtype`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.p, n]):
-        n = ops.convert_to_tensor(n, name="n")
-        new_shape = array_ops.concat(0, ([n], self.batch_shape()))
-        uniform = random_ops.random_uniform(
-            new_shape, seed=seed, dtype=dtypes.float32)
-        sample = math_ops.less(uniform, self.p)
-        sample.set_shape(tensor_shape.vector(tensor_util.constant_value(n))
-                         .concatenate(self.get_batch_shape()))
-        return math_ops.cast(sample, self.dtype)
+  def _prob(self, event):
+    return math_ops.exp(self._log_prob(event))
 
-  def entropy(self, name="entropy"):
-    """Entropy of the distribution.
+  def _entropy(self):
+    # TODO(b/31086883): use tf.nn.softplus; fix inconsistent behavior between
+    # cpu and gpu at -inf/inf.
+    return (-self.logits * (math_ops.sigmoid(self.logits) - 1) +
+            math_ops.log(1. + math_ops.exp(-self.logits)))
 
-    Args:
-      name: Name for the op.
+  def _mean(self):
+    return array_ops.identity(self.p)
 
-    Returns:
-      entropy: `Tensor` of the same type and shape as `p`.
-    """
-    # TODO(jaana): fix inconsistent behavior between cpu and gpu at -inf/inf.
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.logits]):
-        return (-self.logits * (math_ops.sigmoid(
-            self.logits) - 1) + math_ops.log(
-                math_ops.exp(-self.logits) + 1))
+  def _variance(self):
+    return self.q * self.p
 
-  def mean(self, name="mean"):
-    """Mean of the distribution.
+  def _std(self):
+    return math_ops.sqrt(self._variance())
 
-    Args:
-      name: Name for the op.
+  def _mode(self):
+    return math_ops.cast(self.p > self.q, self.dtype)
 
-    Returns:
-      mean: `Tensor` of the same type and shape as `p`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.p]):
-        return array_ops.identity(self.p)
 
-  def mode(self, name="mode"):
-    """Mode of the distribution.
+distribution_util.append_class_fun_doc(Bernoulli.mode, doc_str="""
 
+  Specific notes:
     1 if p > 1-p. 0 otherwise.
-
-    Args:
-      name: Name for the op.
-
-    Returns:
-      mode: binary `Tensor` of type self.dtype.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.p, self.q]):
-        return math_ops.cast(self.p > self.q, self.dtype)
-
-  def variance(self, name="variance"):
-    """Variance of the distribution.
-
-    Args:
-      name: Name for the op.
-
-    Returns:
-      variance: `Tensor` of the same type and shape as `p`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.p, self.q]):
-        return self.q * self.p
-
-  def std(self, name="std"):
-    """Standard deviation of the distribution.
-
-    Args:
-      name: Name for the op.
-
-    Returns:
-      std: `Tensor` of the same type and shape as `p`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return math_ops.sqrt(self.variance())
-
-  @property
-  def is_continuous(self):
-    return False
+""")
 
 
 @kullback_leibler.RegisterKL(Bernoulli, Bernoulli)
diff --git a/tensorflow/contrib/distributions/python/ops/beta.py b/tensorflow/contrib/distributions/python/ops/beta.py
index cc6beb110a1..04f9d9acb9b 100644
--- a/tensorflow/contrib/distributions/python/ops/beta.py
+++ b/tensorflow/contrib/distributions/python/ops/beta.py
@@ -18,22 +18,21 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-# pylint: disable=line-too-long
+import numpy as np
 
 from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.contrib.distributions.python.ops import distribution_util
+from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import control_flow_ops
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import random_ops
 
-# pylint: enable=line-too-long
-
 
 class Beta(distribution.Distribution):
   """Beta distribution.
@@ -134,21 +133,19 @@ class Beta(distribution.Distribution):
     with ops.name_scope(name, values=[a, b]):
       with ops.control_dependencies([
           check_ops.assert_positive(a),
-          check_ops.assert_positive(b)] if validate_args else []):
-        a = array_ops.identity(a, name="a")
-        b = array_ops.identity(b, name="b")
-
-      self._a = a
-      self._b = b
-      self._name = name
-
-      # Used for mean/mode/variance/entropy/sampling computations
-      self._a_b_sum = self._a + self._b
-
-      self._get_batch_shape = self._a_b_sum.get_shape()
-      self._get_event_shape = tensor_shape.TensorShape([])
-      self._validate_args = validate_args
-      self._allow_nan_stats = allow_nan_stats
+          check_ops.assert_positive(b),
+      ] if validate_args else []):
+        self._a = array_ops.identity(a, name="a")
+        self._b = array_ops.identity(b, name="b")
+        contrib_tensor_util.assert_same_float_dtype((self._a, self._b))
+        # Used for mean/mode/variance/entropy/sampling computations
+        self._a_b_sum = self._a + self._b
+        super(Beta, self).__init__(
+            dtype=self._a_b_sum.dtype,
+            parameters={"a": self._a, "b": self._b, "a_b_sum": self._a_b_sum},
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def a(self):
@@ -161,246 +158,109 @@ class Beta(distribution.Distribution):
     return self._b
 
   @property
-  def name(self):
-    """Name to prepend to all ops."""
-    return self._name
-
-  @property
-  def dtype(self):
-    """dtype of samples from this distribution."""
-    return self._a_b_sum.dtype
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._a_b_sum]):
-        return array_ops.shape(self._a_b_sum)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      batch shape
-    """
-    return self._get_batch_shape
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
+  def a_b_sum(self):
+    """Sum of parameters."""
+    return self._a_b_sum
+
+  def _batch_shape(self):
+    return array_ops.shape(self.a_b_sum)
+
+  def _get_batch_shape(self):
+    return self.a_b_sum.get_shape()
+
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
+
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
+
+  def _sample_n(self, n, seed=None):
+    a = array_ops.ones_like(self.a_b_sum, dtype=self.dtype) * self.a
+    b = array_ops.ones_like(self.a_b_sum, dtype=self.dtype) * self.b
+    gamma1_sample = random_ops.random_gamma(
+        [n,], a, dtype=self.dtype, seed=seed)
+    gamma2_sample = random_ops.random_gamma(
+        [n,], b, dtype=self.dtype, seed=seed)
+    beta_sample = gamma1_sample / (gamma1_sample + gamma2_sample)
+    return beta_sample
+
+  def _log_prob(self, x):
+    x = self._assert_valid_sample(x)
+    log_unnormalized_prob = ((self.a - 1.) * math_ops.log(x) +
+                             (self.b - 1.) * math_ops.log(1. - x))
+    log_normalization = (math_ops.lgamma(self.a) +
+                         math_ops.lgamma(self.b) -
+                         math_ops.lgamma(self.a_b_sum))
+    return log_unnormalized_prob - log_normalization
+
+  def _prob(self, x):
+    return math_ops.exp(self._log_prob(x))
+
+  def _entropy(self):
+    return (math_ops.lgamma(self.a) -
+            (self.a - 1.) * math_ops.digamma(self.a) +
+            math_ops.lgamma(self.b) -
+            (self.b - 1.) * math_ops.digamma(self.b) -
+            math_ops.lgamma(self.a_b_sum) +
+            (self.a_b_sum - 2.) * math_ops.digamma(self.a_b_sum))
+
+  def _mean(self):
+    return self.a / self.a_b_sum
+
+  def _variance(self):
+    return (self.a * self.b) / (self.a_b_sum**2. * (self.a_b_sum + 1.))
+
+  def _std(self):
+    return math_ops.sqrt(self.variance())
+
+  def _mode(self):
+    mode = (self.a - 1.)/ (self.a_b_sum - 2.)
+    if self.allow_nan_stats:
+      nan = np.array(np.nan, dtype=self.dtype.as_numpy_dtype())
+      return math_ops.select(
+          math_ops.logical_and(
+              math_ops.greater(self.a, 1.),
+              math_ops.greater(self.b, 1.)),
+          mode,
+          array_ops.fill(self.batch_shape(), nan, name="nan"))
+    else:
+      return control_flow_ops.with_dependencies([
+          check_ops.assert_less(
+              array_ops.ones((), dtype=self.dtype), self.a,
+              message="Mode not defined for components of a <= 1."),
+          check_ops.assert_less(
+              array_ops.ones((), dtype=self.dtype), self.b,
+              message="Mode not defined for components of b <= 1."),
+      ], mode)
+
+  def _assert_valid_sample(self, x):
+    """Check x for proper shape, values, then return tensor version."""
+    if not self.validate_args: return x
+    return control_flow_ops.with_dependencies([
+        check_ops.assert_positive(
+            x,
+            message="Negative events lie outside Beta distribution support."),
+        check_ops.assert_less(
+            x, array_ops.ones((), self.dtype),
+            message="Event>=1 lies outside Beta distribution support."),
+    ], x)
 
-    Args:
-      name: name to give to the op
 
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], name=name, dtype=dtypes.int32)
+_prob_note = """
 
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
+    Note that the argument `x` must be a non-negative floating point tensor
+    whose shape can be broadcast with `self.a` and `self.b`.  For fixed leading
+    dimensions, the last dimension represents counts for the corresponding Beta
+    distribution in `self.a` and `self.b`. `x` is only legal if `0 < x < 1`.
+"""
 
-    Same meaning as `event_shape`. May be only partially defined.
+distribution_util.append_class_fun_doc(Beta.log_prob, doc_str=_prob_note)
+distribution_util.append_class_fun_doc(Beta.prob, doc_str=_prob_note)
 
-    Returns:
-      event shape
-    """
-    return self._get_event_shape
-
-  def mean(self, name="mean"):
-    """Mean of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._a, self._a_b_sum]):
-        return self._a / self._a_b_sum
-
-  def variance(self, name="variance"):
-    """Variance of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._a, self._b, self._a_b_sum]):
-        return (self._a * self._b) / (
-            self._a_b_sum **2 * (self._a_b_sum + 1))
-
-  def std(self, name="std"):
-    """Standard deviation of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return math_ops.sqrt(self.variance())
-
-  def mode(self, name="mode"):
-    """Mode of the distribution.
+distribution_util.append_class_fun_doc(Beta.mode, doc_str="""
 
     Note that the mode for the Beta distribution is only defined
     when `a > 1`, `b > 1`. This returns the mode when `a > 1` and `b > 1`,
-    and NaN otherwise. If `self.allow_nan_stats` is `False`, an exception
+    and `NaN` otherwise. If `self.allow_nan_stats` is `False`, an exception
     will be raised rather than returning `NaN`.
-
-    Args:
-      name: The name for this op.
-
-    Returns:
-      Mode of the Beta distribution.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._a, self._b, self._a_b_sum]):
-        a = self._a
-        b = self._b
-        a_b_sum = self._a_b_sum
-        one = constant_op.constant(1, self.dtype)
-        mode = (a - 1)/ (a_b_sum - 2)
-
-        if self.allow_nan_stats:
-          return math_ops.select(
-              math_ops.logical_and(
-                  math_ops.greater(a, 1), math_ops.greater(b, 1)),
-              mode,
-              (constant_op.constant(float("NaN"), dtype=self.dtype) *
-               array_ops.ones_like(a_b_sum, dtype=self.dtype)))
-        else:
-          return control_flow_ops.with_dependencies([
-              check_ops.assert_less(
-                  one, a,
-                  message="mode not defined for components of a <= 1"
-              ),
-              check_ops.assert_less(
-                  one, b,
-                  message="mode not defined for components of b <= 1"
-              )], mode)
-
-  def entropy(self, name="entropy"):
-    """Entropy of the distribution in nats."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._a, self._b, self._a_b_sum]):
-        a = self._a
-        b = self._b
-        a_b_sum = self._a_b_sum
-
-        entropy = math_ops.lgamma(a) - (a - 1) * math_ops.digamma(a)
-        entropy += math_ops.lgamma(b) - (b - 1) * math_ops.digamma(b)
-        entropy += -math_ops.lgamma(a_b_sum) + (
-            a_b_sum - 2) * math_ops.digamma(a_b_sum)
-        return entropy
-
-  def cdf(self, x, name="cdf"):
-    """Cumulative distribution function."""
-    # TODO(srvasude): Implement this once betainc op is checked in.
-    raise NotImplementedError("Beta cdf not implemented.")
-
-  def log_cdf(self, x, name="log_cdf"):
-    """Log CDF."""
-    raise NotImplementedError("Beta cdf not implemented.")
-
-  def log_prob(self, x, name="log_prob"):
-    """`Log(P[counts])`, computed for every batch member.
-
-    Args:
-      x:  Non-negative floating point tensor whose shape can
-        be broadcast with `self.a` and `self.b`.  For fixed leading
-        dimensions, the last dimension represents counts for the corresponding
-        Beta distribution in `self.a` and `self.b`. `x` is only legal if
-        0 < x < 1.
-      name:  Name to give this Op, defaults to "log_prob".
-
-    Returns:
-      Log probabilities for each record, shape `[N1,...,Nm]`.
-    """
-    a = self._a
-    b = self._b
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[a, x]):
-        x = self._check_x(x)
-
-        unnorm_pdf = (a - 1) * math_ops.log(x) + (
-            b - 1) * math_ops.log(1 - x)
-        normalization_factor = -(math_ops.lgamma(a) + math_ops.lgamma(b)
-                                 - math_ops.lgamma(a + b))
-        log_prob = unnorm_pdf + normalization_factor
-
-        return log_prob
-
-  def prob(self, x, name="prob"):
-    """`P[x]`, computed for every batch member.
-
-    Args:
-      x:  Non-negative floating point tensor whose shape can
-        be broadcast with `self.a` and `self.b`.  For fixed leading
-        dimensions, the last dimension represents x for the corresponding Beta
-        distribution in `self.a` and `self.b`. `x` is only legal if is
-        between 0 and 1.
-      name:  Name to give this Op, defaults to "pdf".
-
-    Returns:
-      Probabilities for each record, shape `[N1,...,Nm]`.
-    """
-    return super(Beta, self).prob(x, name=name)
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the Beta Distributions.
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
-
-    Returns:
-      samples: `[n, ...]`, a `Tensor` of `n` samples for each
-        of the distributions determined by broadcasting the hyperparameters.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.a, self.b, n]):
-        a = array_ops.ones_like(self._a_b_sum, dtype=self.dtype) * self.a
-        b = array_ops.ones_like(self._a_b_sum, dtype=self.dtype) * self.b
-        n = ops.convert_to_tensor(n, name="n")
-
-        gamma1_sample = random_ops.random_gamma(
-            [n,], a, dtype=self.dtype, seed=seed)
-        gamma2_sample = random_ops.random_gamma(
-            [n,], b, dtype=self.dtype, seed=seed)
-
-        beta_sample = gamma1_sample / (gamma1_sample + gamma2_sample)
-
-        n_val = tensor_util.constant_value(n)
-        final_shape = tensor_shape.vector(n_val).concatenate(
-            self._a_b_sum.get_shape())
-
-        beta_sample.set_shape(final_shape)
-        return beta_sample
-
-  @property
-  def is_continuous(self):
-    return True
-
-  @property
-  def is_reparameterized(self):
-    return False
-
-  def _check_x(self, x):
-    """Check x for proper shape, values, then return tensor version."""
-    x = ops.convert_to_tensor(x, name="x_before_deps")
-    dependencies = [
-        check_ops.assert_positive(x),
-        check_ops.assert_less(x, constant_op.constant(
-            1, self.dtype))] if self.validate_args else []
-    return control_flow_ops.with_dependencies(dependencies, x)
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/binomial.py b/tensorflow/contrib/distributions/python/ops/binomial.py
index 697fb02495e..fc799d30b40 100644
--- a/tensorflow/contrib/distributions/python/ops/binomial.py
+++ b/tensorflow/contrib/distributions/python/ops/binomial.py
@@ -125,93 +125,23 @@ class Binomial(distribution.Distribution):
     ```
 
     """
-
     self._logits, self._p = distribution_util.get_logits_and_prob(
         name=name, logits=logits, p=p, validate_args=validate_args)
-
     with ops.name_scope(name, values=[n]):
       with ops.control_dependencies([
           check_ops.assert_non_negative(
               n, message="n has negative components."),
           distribution_util.assert_integer_form(
-              n, message="n has non-integer components."
-          )] if validate_args else []):
-        self._n = array_ops.identity(n, name="convert_n")
-
-        self._name = name
-        self._validate_args = validate_args
-        self._allow_nan_stats = allow_nan_stats
-
-        self._get_batch_shape = common_shapes.broadcast_shape(
-            self._n.get_shape(), self._p.get_shape())
-        self._get_event_shape = tensor_shape.TensorShape([])
-
-  @property
-  def name(self):
-    """Name to prepend to all ops."""
-    return self._name
-
-  @property
-  def dtype(self):
-    """dtype of samples from this distribution."""
-    return self._p.dtype
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    return array_ops.shape(self._n + self._p)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      batch shape
-    """
-    return self._get_batch_shape
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], name=name, dtype=dtypes.int32)
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `event_shape`. May be only partially defined.
-
-    Returns:
-      event shape
-    """
-    return self._get_event_shape
+              n, message="n has non-integer components."),
+      ] if validate_args else []):
+        self._n = array_ops.identity(n, name="n")
+        super(Binomial, self).__init__(
+            dtype=self._p.dtype,
+            parameters={"n": self._n, "p": self._p, "logits": self._logits},
+            is_continuous=False,
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def n(self):
@@ -228,101 +158,43 @@ class Binomial(distribution.Distribution):
     """Probability of success."""
     return self._p
 
-  def mean(self, name="mean"):
-    """Mean of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._n, self._p]):
-        return self._n * self._p
-
-  def variance(self, name="variance"):
-    """Variance of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._n, self._p]):
-        return self._n * self._p * (1 - self._p)
-
-  def std(self, name="std"):
-    """Standard deviation of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._n, self._p]):
-        return math_ops.sqrt(self.variance())
-
-  def mode(self, name="mode"):
-    """Mode of the distribution.
-
-    Note that when `(n + 1) * p` is an integer, there are actually two modes.
-    Namely, `(n + 1) * p` and `(n + 1) * p - 1` are both modes. Here we return
-    only the larger of the two modes.
-
-    Args:
-      name: The name for this op.
-
-    Returns:
-      The mode of the Binomial distribution.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._n, self._p]):
-        return math_ops.floor((self._n + 1) * self._p)
-
-  def log_prob(self, counts, name="log_prob"):
-    """`Log(P[counts])`, computed for every batch member.
-
-    For each batch member of counts `k`, `P[counts]` is the probability that
-    after sampling `n` draws from this Binomial distribution, the number of
-    successes is `k`.  Note that different sequences of draws can result in the
-    same counts, thus the probability includes a combinatorial coefficient.
+  def _batch_shape(self):
+    return array_ops.shape(self._n + self._p)
 
-    Args:
-      counts:  Non-negative tensor with dtype `dtype` and whose shape can be
-        broadcast with `self.p` and `self.n`. `counts` is only legal if it is
-        less than or equal to `n` and its components are equal to integer
-        values.
-      name:  Name to give this Op, defaults to "log_prob".
-
-    Returns:
-      Log probabilities for each record, shape `[N1,...,Nm]`.
-    """
-    n = self._n
-    p = self._p
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._n, self._p, counts]):
-        counts = self._check_counts(counts)
+  def _get_batch_shape(self):
+    return common_shapes.broadcast_shape(self.n.get_shape(),
+                                         self.p.get_shape())
 
-        prob_prob = counts * math_ops.log(p) + (
-            n - counts) * math_ops.log(1 - p)
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
 
-        combinations = math_ops.lgamma(n + 1) - math_ops.lgamma(
-            counts + 1) - math_ops.lgamma(n - counts + 1)
-        log_prob = prob_prob + combinations
-        return log_prob
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
 
-  def prob(self, counts, name="prob"):
-    """`P[counts]`, computed for every batch member.
+  def _log_prob(self, counts):
+    counts = self._check_counts(counts)
+    prob_prob = (counts * math_ops.log(self.p) +
+                 (self.n - counts) * math_ops.log(1. - self.p))
+    combinations = (math_ops.lgamma(self.n + 1) -
+                    math_ops.lgamma(counts + 1) -
+                    math_ops.lgamma(self.n - counts + 1))
+    log_prob = prob_prob + combinations
+    return log_prob
 
+  def _prob(self, counts):
+    return math_ops.exp(self._log_prob(counts))
 
-    For each batch member of counts `k`, `P[counts]` is the probability that
-    after sampling `n` draws from this Binomial distribution, the number of
-    successes is `k`.  Note that different sequences of draws can result in the
-    same counts, thus the probability includes a combinatorial coefficient.
+  def _mean(self):
+    return self._n * self._p
 
-    Args:
-      counts:  Non-negative tensor with dtype `dtype` and whose shape can be
-        broadcast with `self.p` and `self.n`. `counts` is only legal if it is
-        less than or equal to `n` and its components are equal to integer
-        values.
-      name:  Name to give this Op, defaults to "prob".
-
-    Returns:
-      Probabilities for each record, shape `[N1,...,Nm]`.
-    """
-    return super(Binomial, self).prob(counts, name=name)
+  def _variance(self):
+    return self._n * self._p * (1 - self._p)
 
-  @property
-  def is_continuous(self):
-    return False
+  def _std(self):
+    return math_ops.sqrt(self._variance())
 
-  @property
-  def is_reparameterized(self):
-    return False
+  def _mode(self):
+    return math_ops.floor((self._n + 1) * self._p)
 
   def _check_counts(self, counts):
     """Check counts for proper shape, values, then return tensor version."""
@@ -336,3 +208,26 @@ class Binomial(distribution.Distribution):
             counts, self._n, message="counts are not less than or equal to n."),
         distribution_util.assert_integer_form(
             counts, message="counts have non-integer components.")], counts)
+
+
+_prob_note = """
+
+    For each batch member of counts `k`, `P[counts]` is the probability that
+    after sampling `n` draws from this Binomial distribution, the number of
+    successes is `k`.  Note that different sequences of draws can result in the
+    same counts, thus the probability includes a combinatorial coefficient.
+
+    counts:  Non-negative tensor with dtype `dtype` and whose shape can be
+      broadcast with `self.p` and `self.n`. `counts` is only legal if it is
+      less than or equal to `n` and its components are equal to integer
+      values.
+"""
+distribution_util.append_class_fun_doc(Binomial.log_prob, doc_str=_prob_note)
+distribution_util.append_class_fun_doc(Binomial.prob, doc_str=_prob_note)
+
+distribution_util.append_class_fun_doc(Binomial.mode, doc_str="""
+
+    Note that when `(n + 1) * p` is an integer, there are actually two modes.
+    Namely, `(n + 1) * p` and `(n + 1) * p - 1` are both modes. Here we return
+    only the larger of the two modes.
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/categorical.py b/tensorflow/contrib/distributions/python/ops/categorical.py
index 02e74e1d548..a84c31d1049 100644
--- a/tensorflow/contrib/distributions/python/ops/categorical.py
+++ b/tensorflow/contrib/distributions/python/ops/categorical.py
@@ -19,10 +19,10 @@ from __future__ import division
 from __future__ import print_function
 
 from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import nn_ops
@@ -58,66 +58,45 @@ class Categorical(distribution.Distribution):
         undefined statistics will return NaN for this statistic.
       name: A name for this distribution (optional).
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._name = name
-    self._dtype = dtype
-    self._validate_args = validate_args
     with ops.name_scope(name, values=[logits]):
       self._logits = ops.convert_to_tensor(logits, name="logits")
-      logits_shape = array_ops.shape(self._logits, name="logits_shape")
-      static_logits_shape = self._logits.get_shape().with_rank_at_least(1)
-      static_logits_rank = static_logits_shape.ndims
-      if static_logits_rank is not None:
+
+      logits_shape_static = self._logits.get_shape().with_rank_at_least(1)
+      if logits_shape_static.ndims is not None:
         self._batch_rank = ops.convert_to_tensor(
-            static_logits_rank - 1, dtype=dtypes.int32,
+            logits_shape_static.ndims - 1,
+            dtype=dtypes.int32,
             name="batch_rank")
       else:
-        self._batch_rank = array_ops.rank(self._logits) - 1
+        with ops.name_scope(name="batch_rank"):
+          self._batch_rank = array_ops.rank(self._logits) - 1
 
-      if static_logits_shape[-1].value is not None:
+      logits_shape = array_ops.shape(self._logits, name="logits_shape")
+      if logits_shape_static[-1].value is not None:
         self._num_classes = ops.convert_to_tensor(
-            static_logits_shape[-1].value,
-            dtype=dtypes.int32, name="num_classes")
+            logits_shape_static[-1].value,
+            dtype=dtypes.int32,
+            name="num_classes")
       else:
-        self._num_classes = array_ops.gather(logits_shape, self._batch_rank)
-
-      self._batch_shape = logits_shape[:-1]
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def dtype(self):
-    return self._dtype
-
-  @property
-  def is_reparameterized(self):
-    return False
-
-  def batch_shape(self, name="batch_shape"):
-    with ops.name_scope(self.name):
-      return array_ops.identity(self._batch_shape, name=name)
-
-  def get_batch_shape(self):
-    return self.logits.get_shape()[:-1]
-
-  def event_shape(self, name="event_shape"):
-    with ops.name_scope(self.name):
-      return array_ops.constant([], dtype=self._batch_shape.dtype, name=name)
-
-  def get_event_shape(self):
-    return tensor_shape.scalar()
+        self._num_classes = array_ops.gather(logits_shape,
+                                             self._batch_rank,
+                                             name="num_classes")
+
+      if logits_shape_static[:-1].is_fully_defined():
+        self._batch_shape_val = constant_op.constant(
+            logits_shape_static[:-1].as_list(),
+            dtype=dtypes.int32,
+            name="batch_shape")
+      else:
+        with ops.name_scope(name="batch_shape"):
+          self._batch_shape_val = logits_shape[:-1]
+      super(Categorical, self).__init__(
+          dtype=dtype,
+          parameters={"logits": self._logits, "num_classes": self._num_classes},
+          is_continuous=False,
+          validate_args=validate_args,
+          allow_nan_stats=allow_nan_stats,
+          name=name)
 
   @property
   def num_classes(self):
@@ -128,90 +107,55 @@ class Categorical(distribution.Distribution):
   def logits(self):
     return self._logits
 
-  def log_prob(self, k, name="log_prob"):
-    """Log-probability of class `k`.
-
-    Args:
-      k: `int32` or `int64` Tensor. Must be broadcastable with a `batch_shape`
-        `Tensor`.
-      name: A name for this operation (optional).
-
-    Returns:
-      The log-probabilities of the classes indexed by `k`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[k, self.logits]):
-        k = ops.convert_to_tensor(k, name="k")
-
-        logits = self.logits * array_ops.ones_like(
-            array_ops.expand_dims(k, -1),
-            dtype=self.logits.dtype)
-        k *= array_ops.ones(
-            array_ops.slice(
-                array_ops.shape(logits), [0], [array_ops.rank(logits) - 1]),
-            dtype=k.dtype)
-        k.set_shape(tensor_shape.TensorShape(logits.get_shape()[:-1]))
-
-        return -nn_ops.sparse_softmax_cross_entropy_with_logits(logits, k)
-
-  def prob(self, k, name="prob"):
-    """Probability of class `k`.
+  def _batch_shape(self):
+    # Use identity to inherit callers "name".
+    return array_ops.identity(self._batch_shape_val)
 
-    Args:
-      k: `int32` or `int64` Tensor. Must be broadcastable with logits.
-      name: A name for this operation (optional).
-
-    Returns:
-      The probabilities of the classes indexed by `k`
-    """
-    return super(Categorical, self).prob(k, name)
+  def _get_batch_shape(self):
+    return self.logits.get_shape()[:-1]
 
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the Categorical distribution.
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
 
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Random seed (optional).
-      name: A name for this operation (optional).
-
-    Returns:
-      An `int64` `Tensor` with shape `[n, batch_shape, event_shape]`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.logits, n]):
-        n = ops.convert_to_tensor(n, name="n")
-        logits_2d = array_ops.reshape(
-            self.logits, array_ops.pack([-1, self.num_classes]))
-        samples = random_ops.multinomial(logits_2d, n, seed=seed)
-        samples = math_ops.cast(samples, self._dtype)
-        ret = array_ops.reshape(
-            array_ops.transpose(samples),
-            array_ops.concat(0, ([n], self.batch_shape())))
-        ret.set_shape(tensor_shape.vector(tensor_util.constant_value(n))
-                      .concatenate(self.get_batch_shape()))
-        return ret
-
-  def entropy(self, name="sample"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        logits_2d = array_ops.reshape(
-            self.logits, array_ops.pack([-1, self.num_classes]))
-        histogram_2d = nn_ops.softmax(logits_2d)
-        ret = array_ops.reshape(
-            nn_ops.softmax_cross_entropy_with_logits(logits_2d, histogram_2d),
-            self.batch_shape())
-        ret.set_shape(self.get_batch_shape())
-        return ret
-
-  def mode(self, name="mode"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        ret = math_ops.argmax(self.logits, dimension=self._batch_rank)
-        ret = math_ops.cast(ret, self._dtype)
-        ret.set_shape(self.get_batch_shape())
-        return ret
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
 
-  @property
-  def is_continuous(self):
-    return False
+  def _sample_n(self, n, seed=None):
+    logits_2d = array_ops.reshape(
+        self.logits, array_ops.pack([-1, self.num_classes]))
+    samples = random_ops.multinomial(logits_2d, n, seed=seed)
+    samples = math_ops.cast(samples, self.dtype)
+    ret = array_ops.reshape(
+        array_ops.transpose(samples),
+        array_ops.concat(0, ([n], self.batch_shape())))
+    return ret
+
+  def _log_prob(self, k):
+    k = ops.convert_to_tensor(k, name="k")
+    logits = self.logits * array_ops.ones_like(
+        array_ops.expand_dims(k, -1),
+        dtype=self.logits.dtype)
+    shape = array_ops.slice(array_ops.shape(logits), [0],
+                            [array_ops.rank(logits) - 1])
+    k *= array_ops.ones(shape, dtype=k.dtype)
+    k.set_shape(tensor_shape.TensorShape(logits.get_shape()[:-1]))
+    return -nn_ops.sparse_softmax_cross_entropy_with_logits(logits, k)
+
+  def _prob(self, k):
+    return math_ops.exp(self._log_prob(k))
+
+  def _entropy(self):
+    logits_2d = array_ops.reshape(
+        self.logits, array_ops.pack([-1, self.num_classes]))
+    histogram_2d = nn_ops.softmax(logits_2d)
+    ret = array_ops.reshape(
+        nn_ops.softmax_cross_entropy_with_logits(logits_2d, histogram_2d),
+        self.batch_shape())
+    ret.set_shape(self.get_batch_shape())
+    return ret
+
+  def _mode(self):
+    ret = math_ops.argmax(self.logits, dimension=self._batch_rank)
+    ret = math_ops.cast(ret, self.dtype)
+    ret.set_shape(self.get_batch_shape())
+    return ret
diff --git a/tensorflow/contrib/distributions/python/ops/chi2.py b/tensorflow/contrib/distributions/python/ops/chi2.py
index 8abb906c8d0..bb1771c3901 100644
--- a/tensorflow/contrib/distributions/python/ops/chi2.py
+++ b/tensorflow/contrib/distributions/python/ops/chi2.py
@@ -58,9 +58,9 @@ class Chi2(gamma.Gamma):
     # allow_nan_stats=False
     # through to the parent class results in unnecessary asserts.
     with ops.name_scope(name, values=[df]):
-      df = ops.convert_to_tensor(df)
+      df = ops.convert_to_tensor(df, name="df")
       self._df = df
-      super(Chi2, self).__init__(alpha=df / 2,
+      super(Chi2, self).__init__(alpha=0.5 * df,
                                  beta=constant_op.constant(0.5, dtype=df.dtype),
                                  validate_args=validate_args,
                                  allow_nan_stats=allow_nan_stats)
diff --git a/tensorflow/contrib/distributions/python/ops/dirichlet.py b/tensorflow/contrib/distributions/python/ops/dirichlet.py
index 2b279541240..899867fe7f5 100644
--- a/tensorflow/contrib/distributions/python/ops/dirichlet.py
+++ b/tensorflow/contrib/distributions/python/ops/dirichlet.py
@@ -17,12 +17,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import numpy as np
+
 from tensorflow.contrib.distributions.python.ops import distribution
 from tensorflow.contrib.distributions.python.ops import distribution_util
-from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import ops
-from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import control_flow_ops
@@ -129,25 +128,21 @@ class Dirichlet(distribution.Distribution):
 
     """
     with ops.name_scope(name, values=[alpha]):
-      alpha = ops.convert_to_tensor(alpha, name="alpha_before_deps")
+      alpha = ops.convert_to_tensor(alpha, name="alpha")
       with ops.control_dependencies([
-          check_ops.assert_positive(alpha), check_ops.assert_rank_at_least(
-              alpha, 1)
+          check_ops.assert_positive(alpha),
+          check_ops.assert_rank_at_least(alpha, 1)
       ] if validate_args else []):
-        alpha = array_ops.identity(alpha, name="alpha")
-
-      self._alpha = alpha
-      self._name = name
-
-      # Used for mean/mode/variance/entropy computations
-      self._alpha_0 = math_ops.reduce_sum(alpha,
-                                          reduction_indices=[-1],
-                                          keep_dims=False)
-
-      self._get_batch_shape = self._alpha_0.get_shape()
-      self._get_event_shape = self._alpha.get_shape().with_rank_at_least(1)[-1:]
-      self._validate_args = validate_args
-      self._allow_nan_stats = allow_nan_stats
+        self._alpha = array_ops.identity(alpha, name="alpha")
+        self._alpha_sum = math_ops.reduce_sum(alpha,
+                                              reduction_indices=[-1],
+                                              keep_dims=False)
+        super(Dirichlet, self).__init__(
+            dtype=self._alpha.dtype,
+            parameters={"alpha": self._alpha, "alpha_sum": self._alpha_sum},
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def alpha(self):
@@ -155,241 +150,108 @@ class Dirichlet(distribution.Distribution):
     return self._alpha
 
   @property
-  def name(self):
-    """Name to prepend to all ops."""
-    return self._name
-
-  @property
-  def dtype(self):
-    """dtype of samples from this distribution."""
-    return self._alpha.dtype
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha]):
-        return array_ops.shape(self._alpha_0)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      batch shape
-    """
-    return self._get_batch_shape
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha]):
-        return array_ops.gather(array_ops.shape(self._alpha),
-                                [array_ops.rank(self._alpha) - 1])
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `event_shape`. May be only partially defined.
-
-    Returns:
-      event shape
-    """
-    return self._get_event_shape
-
-  def mean(self, name="mean"):
-    """Mean of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._alpha_0]):
-        return self._alpha / array_ops.expand_dims(self._alpha_0, -1)
-
-  def variance(self, name="variance"):
-    """Variance of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._alpha_0]):
-        alpha = array_ops.expand_dims(self._alpha, -1)
-        alpha_0 = array_ops.expand_dims(self._alpha_0, -1)
-
-        expanded_alpha_0 = array_ops.expand_dims(alpha_0, -1)
-
-        variance = -math_ops.batch_matmul(alpha, alpha, adj_y=True) / (
-            expanded_alpha_0 ** 2 * (expanded_alpha_0 + 1))
-        diagonal = self._alpha / (alpha_0 * (alpha_0 + 1))
-        variance += array_ops.batch_matrix_diag(diagonal)
-        return variance
-
-  def std(self, name="std"):
-    """Standard deviation of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return math_ops.sqrt(self.variance())
-
-  def mode(self, name="mode"):
-    """Mode of the distribution.
-
-    Note that the mode for the Beta distribution is only defined
-    when `alpha > 1`. This returns the mode when `alpha > 1`,
-    and NaN otherwise. If `self.allow_nan_stats` is `False`, an exception
-    will be raised rather than returning `NaN`.
-
-    Args:
-      name: The name for this op.
-
-    Returns:
-      Mode of the Dirichlet distribution.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._alpha_0]):
-        one = constant_op.constant(1, self.dtype)
-        mode = (self._alpha - 1)/ (
-            array_ops.expand_dims(self._alpha_0, -1) - math_ops.cast(
-                self.event_shape()[0], self.dtype))
-
-        if self.allow_nan_stats:
-          return math_ops.select(
-              math_ops.greater(self._alpha, 1),
-              mode,
-              (constant_op.constant(float("NaN"), dtype=self.dtype) *
-               array_ops.ones_like(self._alpha, dtype=self.dtype)))
-        else:
-          return control_flow_ops.with_dependencies([
-              check_ops.assert_less(
-                  one, self._alpha,
-                  message="mode not defined for components of alpha <= 1")
-          ], mode)
-
-  def entropy(self, name="entropy"):
-    """Entropy of the distribution in nats."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._alpha_0]):
-        alpha = self._alpha
-        alpha_0 = self._alpha_0
-
-        entropy = special_math_ops.lbeta(alpha)
-        entropy += (alpha_0 - math_ops.cast(
-            self.event_shape()[0], self.dtype)) * math_ops.digamma(
-                alpha_0)
-        entropy += -math_ops.reduce_sum(
-            (alpha - 1) * math_ops.digamma(alpha),
-            reduction_indices=[-1],
-            keep_dims=False)
-        return entropy
-
-  def cdf(self, x, name="cdf"):
-    """Cumulative distribution function."""
-    raise NotImplementedError("Dirichlet does not have a well-defined cdf.")
-
-  def log_cdf(self, x, name="log_cdf"):
-    """Log CDF."""
-    raise NotImplementedError("Dirichlet does not have a well-defined cdf.")
-
-  def log_prob(self, x, name="log_prob"):
-    """`Log(P[counts])`, computed for every batch member.
-
-    Args:
-      x:  Non-negative tensor with dtype `dtype` and whose shape can
-        be broadcast with `self.alpha`.  For fixed leading dimensions, the last
-        dimension represents counts for the corresponding Dirichlet distribution
-        in `self.alpha`. `x` is only legal if it sums up to one.
-      name:  Name to give this Op, defaults to "log_prob".
-
-    Returns:
-      Log probabilities for each record, shape `[N1,...,Nm]`.
-    """
-    alpha = self._alpha
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[alpha, x]):
-        x = self._check_x(x)
-
-        unnorm_prob = (alpha - 1) * math_ops.log(x)
-        log_prob = math_ops.reduce_sum(
-            unnorm_prob, reduction_indices=[-1],
-            keep_dims=False) - special_math_ops.lbeta(alpha)
-
-        return log_prob
-
-  def prob(self, x, name="prob"):
-    """`P[x]`, computed for every batch member.
-
-    Args:
-      x:  Non-negative tensor with dtype `dtype` and whose shape can
-        be broadcast with `self.alpha`.  For fixed leading dimensions, the last
-        dimension represents x for the corresponding Dirichlet distribution in
-        `self.alpha` and `self.beta`. `x` is only legal if it sums up to one.
-      name:  Name to give this Op, defaults to "prob".
-
-    Returns:
-      Probabilities for each record, shape `[N1,...,Nm]`.
-    """
-    return super(Dirichlet, self).prob(x, name=name)
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the distributions.
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
-
-    Returns:
-      samples: `[n, ...]`, a `Tensor` of `n` samples for each
-        of the distributions determined by broadcasting the hyperparameters.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.alpha, n]):
-        gamma_sample = random_ops.random_gamma(
-            [n,], self.alpha, dtype=self.dtype, seed=seed)
-        n_val = tensor_util.constant_value(n)
-        final_shape = tensor_shape.vector(n_val).concatenate(
-            self.alpha.get_shape())
-
-        gamma_sample.set_shape(final_shape)
-        return gamma_sample / math_ops.reduce_sum(
-            gamma_sample, reduction_indices=[-1], keep_dims=True)
-
-  @property
-  def is_continuous(self):
-    return True
-
-  @property
-  def is_reparameterized(self):
-    return False
-
-  def _check_x(self, x):
-    """Check x for proper shape, values, then return tensor version."""
-    x = ops.convert_to_tensor(x, name="x_before_deps")
-    candidate_one = math_ops.reduce_sum(x, reduction_indices=[-1])
-    one = constant_op.constant(1., self.dtype)
-    dependencies = [check_ops.assert_positive(x), check_ops.assert_less(
-        x, one, message="x has components greater than or equal to 1"),
-                    distribution_util.assert_close(one, candidate_one)
-                   ] if self.validate_args else []
-    return control_flow_ops.with_dependencies(dependencies, x)
+  def alpha_sum(self):
+    """Sum of shape parameter."""
+    return self._alpha_sum
+
+  def _batch_shape(self):
+    return array_ops.shape(self.alpha_sum)
+
+  def _get_batch_shape(self):
+    return self.alpha_sum.get_shape()
+
+  def _event_shape(self):
+    return array_ops.gather(array_ops.shape(self.alpha),
+                            [array_ops.rank(self.alpha) - 1])
+
+  def _get_event_shape(self):
+    return self.alpha.get_shape().with_rank_at_least(1)[-1:]
+
+  def _sample_n(self, n, seed=None):
+    gamma_sample = random_ops.random_gamma(
+        [n,], self.alpha, dtype=self.dtype, seed=seed)
+    return gamma_sample / math_ops.reduce_sum(
+        gamma_sample, reduction_indices=[-1], keep_dims=True)
+
+  def _log_prob(self, x):
+    x = ops.convert_to_tensor(x, name="x")
+    x = self._assert_valid_sample(x)
+    unnorm_prob = (self.alpha - 1.) * math_ops.log(x)
+    log_prob = math_ops.reduce_sum(
+        unnorm_prob, reduction_indices=[-1],
+        keep_dims=False) - special_math_ops.lbeta(self.alpha)
+    return log_prob
+
+  def _prob(self, x):
+    return math_ops.exp(self._log_prob(x))
+
+  def _entropy(self):
+    entropy = special_math_ops.lbeta(self.alpha)
+    entropy += math_ops.digamma(self.alpha_sum) * (
+        self.alpha_sum - math_ops.cast(self.event_shape()[0], self.dtype))
+    entropy += -math_ops.reduce_sum(
+        (self.alpha - 1.) * math_ops.digamma(self.alpha),
+        reduction_indices=[-1],
+        keep_dims=False)
+    return entropy
+
+  def _mean(self):
+    return self.alpha / array_ops.expand_dims(self.alpha_sum, -1)
+
+  def _variance(self):
+    scale = self.alpha_sum * math_ops.sqrt(1. + self.alpha_sum)
+    alpha = self.alpha / scale
+    outer_prod = -math_ops.batch_matmul(
+        array_ops.expand_dims(alpha, dim=-1),  # column
+        array_ops.expand_dims(alpha, dim=-2))  # row
+    return array_ops.batch_matrix_set_diag(
+        outer_prod, alpha * (self.alpha_sum / scale - alpha))
+
+  def _std(self):
+    return math_ops.sqrt(self._variance())
+
+  def _mode(self):
+    mode = ((self.alpha - 1.) /
+            (array_ops.expand_dims(self.alpha_sum, dim=-1) -
+             math_ops.cast(self.event_shape()[0], self.dtype)))
+    if self.allow_nan_stats:
+      nan = np.array(np.nan, dtype=self.dtype.as_numpy_dtype())
+      shape = array_ops.concat(0, (self.batch_shape(), self.event_shape()))
+      return math_ops.select(
+          math_ops.greater(self.alpha, 1.),
+          mode,
+          array_ops.fill(shape, nan, name="nan"))
+    else:
+      return control_flow_ops.with_dependencies([
+          check_ops.assert_less(
+              array_ops.ones((), dtype=self.dtype), self.alpha,
+              message="mode not defined for components of alpha <= 1")
+      ], mode)
+
+  def _assert_valid_sample(self, x):
+    if not self.validate_args: return x
+    return control_flow_ops.with_dependencies([
+        check_ops.assert_positive(x),
+        distribution_util.assert_close(
+            array_ops.ones((), dtype=self.dtype),
+            math_ops.reduce_sum(x, reduction_indices=[-1])),
+    ], x)
+
+
+_prob_note = """
+
+  Note that the input must be a non-negative tensor with dtype `dtype` and whose
+  shape can be broadcast with `self.alpha`.  For fixed leading dimensions, the
+  last dimension represents counts for the corresponding Dirichlet distribution
+  in `self.alpha`. `x` is only legal if it sums up to one.
+"""
+distribution_util.append_class_fun_doc(Dirichlet.log_prob, doc_str=_prob_note)
+distribution_util.append_class_fun_doc(Dirichlet.prob, doc_str=_prob_note)
+
+distribution_util.append_class_fun_doc(Dirichlet.mode, doc_str="""
+
+  Note that the mode for the Dirichlet distribution is only defined
+  when `alpha > 1`. This returns the mode when `alpha > 1`,
+  and NaN otherwise. If `self.allow_nan_stats` is `False`, an exception
+  will be raised rather than returning `NaN`.
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/dirichlet_multinomial.py b/tensorflow/contrib/distributions/python/ops/dirichlet_multinomial.py
index d6fc8522d09..45e31e5580b 100644
--- a/tensorflow/contrib/distributions/python/ops/dirichlet_multinomial.py
+++ b/tensorflow/contrib/distributions/python/ops/dirichlet_multinomial.py
@@ -139,9 +139,6 @@ class DirichletMultinomial(distribution.Distribution):
     ```
 
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
-    self._name = name
     with ops.name_scope(name, values=[n, alpha]):
       # Broadcasting works because:
       # * The broadcasting convention is to prepend dimensions of size [1], and
@@ -152,16 +149,19 @@ class DirichletMultinomial(distribution.Distribution):
       #   explicitivity.
       #   * All calls involving `counts` eventually require a broadcast between
       #   `counts` and alpha.
-      self._alpha = self._check_alpha(alpha)
-      self._n = self._check_n(n)
-
+      self._alpha = self._assert_valid_alpha(alpha, validate_args)
+      self._n = self._assert_valid_n(n, validate_args)
       self._alpha_sum = math_ops.reduce_sum(
           self._alpha, reduction_indices=[-1], keep_dims=False)
-
-      self._get_batch_shape = self._alpha_sum.get_shape()
-
-      # event shape depends only on alpha, not "n".
-      self._get_event_shape = self._alpha.get_shape().with_rank_at_least(1)[-1:]
+      super(DirichletMultinomial, self).__init__(
+          dtype=self._alpha.dtype,
+          parameters={"alpha": self._alpha,
+                      "alpha_sum": self._alpha_sum,
+                      "n": self._n},
+          is_continuous=False,
+          validate_args=validate_args,
+          allow_nan_stats=allow_nan_stats,
+          name=name)
 
   @property
   def n(self):
@@ -174,195 +174,57 @@ class DirichletMultinomial(distribution.Distribution):
     return self._alpha
 
   @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    """Name to prepend to all ops."""
-    return self._name
-
-  @property
-  def dtype(self):
-    """dtype of samples from this distribution."""
-    return self._alpha.dtype
-
-  def mean(self, name="mean"):
-    """Class means for every batch member."""
-    alpha = self._alpha
-    alpha_sum = self._alpha_sum
-    n = self._n
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[alpha, alpha_sum, n]):
-        mean_no_n = alpha / array_ops.expand_dims(alpha_sum, -1)
-        return array_ops.expand_dims(n, -1) * mean_no_n
-
-  def variance(self, name="mean"):
-    """Class variances for every batch member.
-
-    The variance for each batch member is defined as the following:
-
-    ```
-    Var(X_j) = n * alpha_j / alpha_0 * (1 - alpha_j / alpha_0) *
-      (n + alpha_0) / (1 + alpha_0)
-    ```
-
-    where `alpha_0 = sum_j alpha_j`.
-
-    The covariance between elements in a batch is defined as:
-
-    ```
-    Cov(X_i, X_j) = -n * alpha_i * alpha_j / alpha_0 ** 2 *
-      (n + alpha_0) / (1 + alpha_0)
-    ```
-
-    Args:
-      name: The name for this op.
-
-    Returns:
-      A `Tensor` representing the variances for each batch member.
-    """
-    alpha = self._alpha
-    alpha_sum = self._alpha_sum
-    n = self._n
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[alpha, alpha_sum, n]):
-        expanded_alpha_sum = array_ops.expand_dims(alpha_sum, -1)
-        shared_factor = n * (expanded_alpha_sum + n) / (
-            expanded_alpha_sum + 1) * array_ops.ones_like(alpha)
-
-        mean_no_n = alpha / expanded_alpha_sum
-        expanded_mean_no_n = array_ops.expand_dims(mean_no_n, -1)
-        variance = -math_ops.batch_matmul(
-            expanded_mean_no_n, expanded_mean_no_n, adj_y=True)
-        variance += array_ops.batch_matrix_diag(mean_no_n)
-        variance *= array_ops.expand_dims(shared_factor, -1)
-        return variance
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha_sum]):
-        return array_ops.shape(self._alpha_sum)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      batch shape
-    """
-    return self._get_batch_shape
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha]):
-        return array_ops.reverse(array_ops.shape(self._alpha), [True])[0]
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `event_shape`. May be only partially defined.
-
-    Returns:
-      event shape
-    """
-    return self._get_event_shape
-
-  def cdf(self, x, name="cdf"):
-    raise NotImplementedError(
-        "DirichletMultinomial does not have a well-defined cdf.")
-
-  def log_cdf(self, x, name="log_cdf"):
-    raise NotImplementedError(
-        "DirichletMultinomial does not have a well-defined cdf.")
-
-  def log_prob(self, counts, name="log_prob"):
-    """`Log(P[counts])`, computed for every batch member.
-
-    For each batch of counts `[n_1,...,n_k]`, `P[counts]` is the probability
-    that after sampling `n` draws from this Dirichlet Multinomial
-    distribution, the number of draws falling in class `j` is `n_j`.  Note that
-    different sequences of draws can result in the same counts, thus the
-    probability includes a combinatorial coefficient.
-
-    Args:
-      counts:  Non-negative tensor with dtype `dtype` and whose shape can be
-        broadcast with `self.alpha`.  For fixed leading dimensions, the last
-        dimension represents counts for the corresponding Dirichlet Multinomial
-        distribution in `self.alpha`. `counts` is only legal if it sums up to
-        `n` and its components are equal to integer values.
-      name:  Name to give this Op, defaults to "log_prob".
-
-    Returns:
-      Log probabilities for each record, shape `[N1,...,Nn]`.
-    """
-    n = self._n
-    alpha = self._alpha
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[n, alpha, counts]):
-        counts = self._check_counts(counts)
-
-        ordered_prob = (special_math_ops.lbeta(alpha + counts) -
-                        special_math_ops.lbeta(alpha))
-        log_prob = ordered_prob + distribution_util.log_combinations(
-            n, counts)
-        return log_prob
-
-  def prob(self, counts, name="prob"):
-    """`P[counts]`, computed for every batch member.
-
-    For each batch of counts `[c_1,...,c_k]`, `P[counts]` is the probability
-    that after sampling `sum_j c_j` draws from this Dirichlet Multinomial
-    distribution, the number of draws falling in class `j` is `c_j`.  Note that
-    different sequences of draws can result in the same counts, thus the
-    probability includes a combinatorial coefficient.
-
-    Args:
-      counts:  Non-negative tensor with dtype `dtype` and whose shape can be
-        broadcast with `self.alpha`.  For fixed leading dimensions, the last
-        dimension represents counts for the corresponding Dirichlet Multinomial
-        distribution in `self.alpha`. `counts` is only legal if it sums up to
-        `n` and its components are equal to integer values.
-      name:  Name to give this Op, defaults to "prob".
-
-    Returns:
-      Probabilities for each record, shape `[N1,...,Nn]`.
-    """
-    return super(DirichletMultinomial, self).prob(counts, name=name)
-
-  def _check_counts(self, counts):
+  def alpha_sum(self):
+    """Summation of alpha parameter."""
+    return self._alpha_sum
+
+  def _batch_shape(self):
+    return array_ops.shape(self.alpha_sum)
+
+  def _get_batch_shape(self):
+    return self.alpha_sum.get_shape()
+
+  def _event_shape(self):
+    return array_ops.reverse(array_ops.shape(self.alpha), [True])[0]
+
+  def _get_event_shape(self):
+    # Event shape depends only on alpha, not "n".
+    return self.alpha.get_shape().with_rank_at_least(1)[-1:]
+
+  def _log_prob(self, counts):
+    counts = self._assert_valid_counts(counts)
+    ordered_prob = (special_math_ops.lbeta(self.alpha + counts) -
+                    special_math_ops.lbeta(self.alpha))
+    log_prob = ordered_prob + distribution_util.log_combinations(
+        self.n, counts)
+    return log_prob
+
+  def _prob(self, counts):
+    return math_ops.exp(self._log_prob(counts))
+
+  def _mean(self):
+    normalized_alpha = self.alpha / array_ops.expand_dims(self.alpha_sum, -1)
+    return array_ops.expand_dims(self.n, -1) * normalized_alpha
+
+  def _variance(self):
+    alpha_sum = array_ops.expand_dims(self.alpha_sum, -1)
+    normalized_alpha = self.alpha / alpha_sum
+    variance = -math_ops.batch_matmul(
+        array_ops.expand_dims(normalized_alpha, -1),
+        array_ops.expand_dims(normalized_alpha, -2))
+    variance = array_ops.batch_matrix_set_diag(
+        variance, normalized_alpha * (1. - normalized_alpha))
+    shared_factor = (self.n * (alpha_sum + self.n) /
+                     (alpha_sum + 1) * array_ops.ones_like(self.alpha))
+    variance *= array_ops.expand_dims(shared_factor, -1)
+    return variance
+
+  def _assert_valid_counts(self, counts):
     """Check counts for proper shape, values, then return tensor version."""
     counts = ops.convert_to_tensor(counts, name="counts")
     if not self.validate_args:
       return counts
     candidate_n = math_ops.reduce_sum(counts, reduction_indices=[-1])
-
     return control_flow_ops.with_dependencies([
         check_ops.assert_non_negative(counts),
         check_ops.assert_equal(
@@ -370,26 +232,59 @@ class DirichletMultinomial(distribution.Distribution):
             message="counts do not sum to n"),
         distribution_util.assert_integer_form(counts)], counts)
 
-  def _check_alpha(self, alpha):
+  def _assert_valid_alpha(self, alpha, validate_args):
     alpha = ops.convert_to_tensor(alpha, name="alpha")
-    if not self.validate_args:
+    if not validate_args:
       return alpha
     return control_flow_ops.with_dependencies(
         [check_ops.assert_rank_at_least(alpha, 1),
          check_ops.assert_positive(alpha)], alpha)
 
-  def _check_n(self, n):
+  def _assert_valid_n(self, n, validate_args):
     n = ops.convert_to_tensor(n, name="n")
-    if not self.validate_args:
+    if not validate_args:
       return n
     return control_flow_ops.with_dependencies(
         [check_ops.assert_non_negative(n),
          distribution_util.assert_integer_form(n)], n)
 
-  @property
-  def is_continuous(self):
-    return False
 
-  @property
-  def is_reparameterized(self):
-    return False
+_prob_note = """
+
+  For each batch of counts `[n_1,...,n_k]`, `P[counts]` is the probability
+  that after sampling `n` draws from this Dirichlet Multinomial
+  distribution, the number of draws falling in class `j` is `n_j`.  Note that
+  different sequences of draws can result in the same counts, thus the
+  probability includes a combinatorial coefficient.
+
+  Note that input, "counts", must be a non-negative tensor with dtype `dtype`
+  and whose shape can be broadcast with `self.alpha`.  For fixed leading
+  dimensions, the last dimension represents counts for the corresponding
+  Dirichlet Multinomial distribution in `self.alpha`. `counts` is only legal if
+  it sums up to `n` and its components are equal to integer values.
+"""
+distribution_util.append_class_fun_doc(DirichletMultinomial.log_prob,
+                                       doc_str=_prob_note)
+distribution_util.append_class_fun_doc(DirichletMultinomial.prob,
+                                       doc_str=_prob_note)
+
+distribution_util.append_class_fun_doc(DirichletMultinomial.variance,
+                                       doc_str="""
+
+  The variance for each batch member is defined as the following:
+
+  ```
+  Var(X_j) = n * alpha_j / alpha_0 * (1 - alpha_j / alpha_0) *
+    (n + alpha_0) / (1 + alpha_0)
+  ```
+
+  where `alpha_0 = sum_j alpha_j`.
+
+  The covariance between elements in a batch is defined as:
+
+  ```
+  Cov(X_i, X_j) = -n * alpha_i * alpha_j / alpha_0 ** 2 *
+    (n + alpha_0) / (1 + alpha_0)
+  ```
+
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/distribution.py b/tensorflow/contrib/distributions/python/ops/distribution.py
index 1115d30a4c7..30c79bc75e6 100644
--- a/tensorflow/contrib/distributions/python/ops/distribution.py
+++ b/tensorflow/contrib/distributions/python/ops/distribution.py
@@ -19,8 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import abc
+import contextlib
+import numpy as np
 import six
 
+from tensorflow.contrib.distributions.python.ops import distribution_util
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
@@ -38,89 +41,37 @@ class BaseDistribution(object):
   that want to fulfill a simpler distribution contract.
   """
 
-  @abc.abstractproperty
-  def name(self):
-    """Name to prepend to all ops."""
-    # return self._name.
-    pass
-
   @abc.abstractmethod
-  def prob(self, value, name="prob"):
-    """Probability density/mass function."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[value]):
-        value = ops.convert_to_tensor(value)
-        return math_ops.exp(self.log_prob(value))
+  def sample_n(self, n, seed=None, name="sample"):
+    # See `Distribution.sample_n` for docstring.
+    pass
 
   @abc.abstractmethod
   def log_prob(self, value, name="log_prob"):
-    """Log of the probability density/mass function."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[value]):
-        value = ops.convert_to_tensor(value)
-        return math_ops.log(self.prob(value))
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Generate `n` samples.
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer seed for RNG
-      name: name to give to the op.
-
-    Returns:
-      samples: a `Tensor` with a prepended dimension (n,).
-    """
-    raise NotImplementedError("sample_n not implemented")
-
-  def sample(self, sample_shape=(), seed=None, name="sample"):
-    """Generate samples of the specified shape.
-
-    Note that a call to `sample()` without arguments will generate a single
-    sample.
-
-    Args:
-      sample_shape: Rank 1 `int32` `Tensor`. Shape of the generated samples.
-      seed: Python integer seed for RNG
-      name: name to give to the op.
-
-    Returns:
-      samples: a `Tensor` with prepended dimensions `sample_shape`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[sample_shape]):
-        sample_shape = ops.convert_to_tensor(sample_shape,
-                                             dtype=dtypes.int32,
-                                             name="sample_shape")
-        total = math_ops.reduce_prod(sample_shape)
-        samples = self.sample_n(total, seed)
-        output_shape = array_ops.concat(0, [sample_shape, array_ops.slice(
-            array_ops.shape(samples), [1], [-1])])
-        output = array_ops.reshape(samples, output_shape, name=name)
-        output.set_shape(tensor_util.constant_value_as_shape(
-            sample_shape).concatenate(samples.get_shape()[1:]))
-    return output
+    # See `Distribution.log_prob` for docstring.
+    pass
 
 
-@six.add_metaclass(abc.ABCMeta)
 class Distribution(BaseDistribution):
-  """Fully-featured abstract base class for probability distributions.
+  """A generic probability distribution base class.
 
-  This class defines the API for probability distributions. Users will only ever
-  instantiate subclasses of `Distribution`.
+  `Distribution` is a base class for constructing and organizing properties
+  (e.g., mean, variance) of random variables (e.g, Bernoulli, Gaussian).
 
-  ### API
+  ### Subclassing
 
-  The key methods for probability distributions are defined here.
+  Subclasess are expected to implement a leading-underscore version of the
+  same-named function.  The argument signature should be identical except for
+  the omission of `name="..."`.  For example, to enable `log_prob(value,
+  name="log_prob")` a subclass should implement `_log_prob(value)`.
 
-  To keep ops generated by the distribution tied together by name, subclasses
-  should override `name` and use it to prepend names of ops in other methods
-  (see `cdf` for an example).
+  Subclasses can rewrite/append to public-level docstrings. For example,
+
+  ```python
+  Subclass.prob.__func__.__doc__ += "Some other details."
+  ```
 
-  Subclasses that wish to support `cdf` and `log_cdf` can override `log_cdf`
-  and use the base class's implementation for `cdf`, or vice versa. The same
-  goes for `log_prob` and `prob`.
+  would add the string "Some other details." to the `prob` function docstring.
 
   ### Broadcasting, batching, and shapes
 
@@ -210,6 +161,42 @@ class Distribution(BaseDistribution):
 
   """
 
+  def __init__(self,
+               dtype=None,
+               parameters=None,
+               is_continuous=True,
+               is_reparameterized=False,
+               validate_args=True,
+               allow_nan_stats=False,
+               name=None):
+    """Constructs the `Distribution`.
+
+    Args:
+      dtype: The type of the event samples. `None` implies no type-enforcement.
+      parameters: Python dictionary of parameters used by this `Distribution`.
+      is_continuous: Python boolean, default `True`. If `True` this
+        `Distribution` is continuous over its supported domain.
+      is_reparameterized: Python boolean, default `False`. If `True` this
+        `Distribution` can be reparameterized in terms of some standard
+        distribution with a function whose Jacobian is constant for the support
+        of the standard distribution.
+      validate_args: Whether to validate input with asserts. If `validate_args`
+        is `False`, and the inputs are invalid, correct behavior is not
+        guaranteed.
+      allow_nan_stats: Python boolean, default `False`. If `False`, raise an
+        exception if a statistic (e.g., mean, mode) is undefined for any batch
+        member. If True, batch members with valid parameters leading to
+        undefined statistics will return `NaN` for this statistic.
+      name: A name for this distribution (optional).
+    """
+    self._name = name or type(self).__name__
+    self._dtype = dtype
+    self._parameters = parameters or {}
+    self._is_continuous = is_continuous
+    self._is_reparameterized = is_reparameterized
+    self._allow_nan_stats = allow_nan_stats
+    self._validate_args = validate_args
+
   @classmethod
   def param_shapes(cls, sample_shape, name="DistributionParamShapes"):
     """Shapes of parameters given the desired shape of a call to `sample()`.
@@ -277,8 +264,8 @@ class Distribution(BaseDistribution):
 
     # shapes has a Tensor shape for mu and sigma
     # shapes == {
-    #   'mu': tf.constant([batch_size, 10]),
-    #   'sigma': tf.constant([batch_size, 10]),
+    #   "mu": tf.constant([batch_size, 10]),
+    #   "sigma": tf.constant([batch_size, 10]),
     # }
 
     # Here we parameterize mu and sigma with the output of a linear
@@ -349,70 +336,54 @@ class Distribution(BaseDistribution):
     """
     raise NotImplementedError("_safe_transforms is not implemented")
 
-  @abc.abstractproperty
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    # return self._allow_nan_stats
-    # Notes:
-    #
-    # When it makes sense, return +- infinity for statistics.  E.g. the variance
-    # of a Cauchy distribution would be +infinity.  However, sometimes the
-    # statistic is undefined (e.g. if a distribution's pdf does not achieve a
-    # maximum within the support of the distribution, mode is undefined).
-    # If the mean is undefined, then by definition the variance is undefined.
-    # E.g. the mean for Student's T for df = 1 is undefined (no clear way to say
-    # it is either + or - infinity), so the variance = E[(X - mean)^2] is also
-    # undefined.
-    #
-    # Distributions should be initialized with a kwarg "allow_nan_stats" with
-    # the following docstring (refer to above docstring note on undefined
-    # statistics for more detail).
-    # allow_nan_stats:  Boolean, default False.  If False, raise an exception if
-    #   a statistic (e.g. mean/mode/etc...) is undefined for any batch member.
-    #   If True, batch members with valid parameters leading to undefined
-    #   statistics will return NaN for this statistic.
-    pass
-
-  @abc.abstractproperty
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    # return self._validate_args.
-    pass
+  @property
+  def name(self):
+    """Name prepended to all ops created by this `Distribution`."""
+    return self._name
 
-  @abc.abstractproperty
+  @property
   def dtype(self):
-    """dtype of samples from this distribution."""
-    # return self._dtype
-    pass
+    """The `DType` of `Tensor`s handled by this `Distribution`."""
+    return self._dtype
 
-  @abc.abstractmethod
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
+  @property
+  def parameters(self):
+    """Dictionary of parameters used by this `Distribution`."""
+    return self._parameters
 
-    Args:
-      name: name to give to the op
+  @property
+  def is_continuous(self):
+    return self._is_continuous
 
-    Returns:
-      `Tensor` `event_shape`
-    """
-    # For scalar distributions, constant([], int32)
-    # with ops.name_scope(self.name):
-    #   with ops.name_scope(name, values=[tensor_arguments]):
-    #     Your code here
-    pass
+  @property
+  def is_reparameterized(self):
+    return self._is_reparameterized
 
-  @abc.abstractmethod
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
+  @property
+  def allow_nan_stats(self):
+    """Python boolean describing behavior when a stat is undefined.
 
-    Same meaning as `event_shape`. May be only partially defined.
+    Stats return +/- infinity when it makes sense.  E.g., the variance
+    of a Cauchy distribution is infinity.  However, sometimes the
+    statistic is undefined, e.g., if a distribution's pdf does not achieve a
+    maximum within the support of the distribution, the mode is undefined.
+    If the mean is undefined, then by definition the variance is undefined.
+    E.g. the mean for Student's T for df = 1 is undefined (no clear way to say
+    it is either + or - infinity), so the variance = E[(X - mean)^2] is also
+    undefined.
+
+    Returns:
+      allow_nan_stats: Python boolean.
     """
-    # return self._event_shape
-    pass
+    return self._allow_nan_stats
+
+  @property
+  def validate_args(self):
+    """Python boolean indicated possibly expensive checks are enabled."""
+    return self._validate_args
 
-  @abc.abstractmethod
   def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
+    """Shape of a single sample from a single event index as a 1-D `Tensor`.
 
     The product of the dimensions of the `batch_shape` is the number of
     independent distributions of this kind the instance represents.
@@ -421,119 +392,327 @@ class Distribution(BaseDistribution):
       name: name to give to the op
 
     Returns:
-      `Tensor` `batch_shape`
+      batch_shape: `Tensor`.
     """
-    # with ops.name_scope(self.name):
-    #   with ops.name_scope(name, values=[tensor_arguments]):
-    #     Your code here
-    pass
+    self._check_hasattr(self._batch_shape)
+    with self._name_scope(name):
+      return self._batch_shape()
 
-  @abc.abstractmethod
   def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
+    """Shape of a single sample from a single event index as a `TensorShape`.
 
     Same meaning as `batch_shape`. May be only partially defined.
+
+    Returns:
+      batch_shape: `TensorShape`, possibly unknown.
     """
-    pass
+    self._check_hasattr(self._get_batch_shape)
+    return self._get_batch_shape()
 
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Generate `n` samples.
+  def event_shape(self, name="event_shape"):
+    """Shape of a single sample from a single batch as a 1-D int32 `Tensor`.
 
     Args:
-      n: scalar. Number of samples to draw from each distribution.
-      seed: Python integer seed for RNG
-      name: name to give to the op.
+      name: name to give to the op
 
     Returns:
-      samples: a `Tensor` of shape `(n,) + self.batch_shape + self.event_shape`
-          with values of type `self.dtype`.
+      event_shape: `Tensor`.
     """
-    return super(Distribution, self).sample_n(n, seed, name)
+    self._check_hasattr(self._event_shape)
+    with self._name_scope(name):
+      return self._event_shape()
+
+  def get_event_shape(self):
+    """Shape of a single sample from a single batch as a `TensorShape`.
+
+    Same meaning as `event_shape`. May be only partially defined.
+
+    Returns:
+      event_shape: `TensorShape`, possibly unknown.
+    """
+    self._check_hasattr(self._get_event_shape)
+    return self._get_event_shape()
 
   def sample(self, sample_shape=(), seed=None, name="sample"):
-    """Generate samples of the specified shape for each batched distribution.
+    """Generate samples of the specified shape.
 
     Note that a call to `sample()` without arguments will generate a single
-    sample per batched distribution.
+    sample.
 
     Args:
-      sample_shape: Rank 1 `int32` `Tensor`. Shape of the generated samples.
+      sample_shape: 0D or 1D `int32` `Tensor`. Shape of the generated samples.
       seed: Python integer seed for RNG
       name: name to give to the op.
 
     Returns:
-      samples: a `Tensor` of dtype `self.dtype` and shape
-          `sample_shape + self.batch_shape + self.event_shape`.
+      samples: a `Tensor` with prepended dimensions `sample_shape`.
     """
-    return super(Distribution, self).sample(sample_shape, seed, name)
+    with self._name_scope(name, values=[sample_shape]):
+      sample_shape = ops.convert_to_tensor(
+          sample_shape, dtype=dtypes.int32, name="sample_shape")
+      if sample_shape.get_shape().ndims == 0:
+        return self.sample_n(sample_shape, seed)
+      sample_shape, total = self._expand_sample_shape(sample_shape)
+      samples = self.sample_n(total, seed)
+      output_shape = array_ops.concat(0, [sample_shape, array_ops.slice(
+          array_ops.shape(samples), [1], [-1])])
+      output = array_ops.reshape(samples, output_shape)
+      output.set_shape(tensor_util.constant_value_as_shape(
+          sample_shape).concatenate(samples.get_shape()[1:]))
+      return output
 
-  def cdf(self, value, name="cdf"):
-    """Cumulative distribution function."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[value]):
-        value = ops.convert_to_tensor(value)
-        return math_ops.exp(self.log_cdf(value))
+  def sample_n(self, n, seed=None, name="sample_n"):
+    """Generate `n` samples.
+
+    Args:
+      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
+        observations to sample.
+      seed: Python integer seed for RNG
+      name: name to give to the op.
+
+    Returns:
+      samples: a `Tensor` with a prepended dimension (n,).
+
+    Raises:
+      TypeError: if `n` is not an integer type.
+    """
+    self._check_hasattr(self._sample_n)
+    with self._name_scope(name, values=[n]):
+      n = ops.convert_to_tensor(n, name="n")
+      if not n.dtype.is_integer:
+        raise TypeError("n.dtype=%s is not an integer type" % n.dtype)
+      x = self._sample_n(n, seed)
+
+      # Set shape hints.
+      sample_shape = tensor_shape.TensorShape(
+          tensor_util.constant_value(n))
+      batch_ndims = self.get_batch_shape().ndims
+      event_ndims = self.get_event_shape().ndims
+      if batch_ndims is not None and event_ndims is not None:
+        inferred_shape = sample_shape.concatenate(
+            self.get_batch_shape().concatenate(
+                self.get_event_shape()))
+        x.set_shape(inferred_shape)
+      elif x.get_shape().ndims is not None and x.get_shape().ndims > 0:
+        x.get_shape()[0].merge_with(sample_shape)
+        if batch_ndims is not None and batch_ndims > 0:
+          x.get_shape()[1:1+batch_ndims].merge_with(self.get_batch_shape())
+        if event_ndims is not None and event_ndims > 0:
+          x.get_shape()[-event_ndims:].merge_with(self.get_event_shape())
+
+      return x
+
+  def log_prob(self, value, name="log_prob"):
+    """Log probability density/mass function (depending on `is_continuous`).
+
+    Args:
+      value: `float` or `double` `Tensor`.
+      name: The name to give this op.
+
+    Returns:
+      log_prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
+        values of type `self.dtype`.
+    """
+    self._check_hasattr(self._log_prob)
+    with self._name_scope(name, values=[value]):
+      value = ops.convert_to_tensor(value, name="value")
+      return self._log_prob(value)
+
+  def prob(self, value, name="prob"):
+    """Probability density/mass function (depending on `is_continuous`).
+
+    Args:
+      value: `float` or `double` `Tensor`.
+      name: The name to give this op.
+
+    Returns:
+      prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
+        values of type `self.dtype`.
+    """
+    self._check_hasattr(self._prob)
+    with self._name_scope(name, values=[value]):
+      value = ops.convert_to_tensor(value, name="value")
+      return self._prob(value)
 
   def log_cdf(self, value, name="log_cdf"):
-    """Log CDF."""
-    raise NotImplementedError("log_cdf is not implemented")
+    """Log cumulative distribution function.
+
+    Args:
+      value: `float` or `double` `Tensor`.
+      name: The name to give this op.
+
+    Returns:
+      logcdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
+        values of type `self.dtype`.
+    """
+    self._check_hasattr(self._log_cdf)
+    with self._name_scope(name, values=[value]):
+      value = ops.convert_to_tensor(value, name="value")
+      return self._log_cdf(value)
+
+  def cdf(self, value, name="cdf"):
+    """Cumulative distribution function.
+
+    Args:
+      value: `float` or `double` `Tensor`.
+      name: The name to give this op.
+
+    Returns:
+      cdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
+        values of type `self.dtype`.
+    """
+    self._check_hasattr(self._cdf)
+    with self._name_scope(name, values=[value]):
+      value = ops.convert_to_tensor(value, name="value")
+      return self._cdf(value)
 
   def entropy(self, name="entropy"):
-    """Entropy of the distribution in nats."""
-    raise NotImplementedError("entropy not implemented")
+    """Shanon entropy in nats."""
+    self._check_hasattr(self._entropy)
+    with self._name_scope(name):
+      return self._entropy()
 
   def mean(self, name="mean"):
-    """Mean of the distribution."""
-    raise NotImplementedError("mean not implemented")
+    """Mean."""
+    self._check_hasattr(self._mean)
+    with self._name_scope(name):
+      return self._mean()
 
-  def mode(self, name="mode"):
-    """Mode of the distribution."""
-    raise NotImplementedError("mode not implemented")
+  def variance(self, name="variance"):
+    """Variance."""
+    self._check_hasattr(self._variance)
+    with self._name_scope(name):
+      return self._variance()
 
   def std(self, name="std"):
-    """Standard deviation of the distribution."""
-    raise NotImplementedError("std not implemented")
+    """Standard deviation."""
+    self._check_hasattr(self._std)
+    with self._name_scope(name):
+      return self._std()
 
-  def variance(self, name="variance"):
-    """Variance of the distribution."""
-    raise NotImplementedError("variance not implemented")
+  def mode(self, name="mode"):
+    """Mode."""
+    self._check_hasattr(self._mode)
+    with self._name_scope(name):
+      return self._mode()
 
-  @abc.abstractproperty
-  def is_continuous(self):
-    pass
+  def log_pdf(self, value, name="log_pdf"):
+    """Log probability density function.
 
-  @abc.abstractproperty
-  def is_reparameterized(self):
-    pass
+    Args:
+      value: `float` or `double` `Tensor`.
+      name: The name to give this op.
 
-  def log_pdf(self, value, name="log_pdf"):
-    """Log of the probability density function."""
-    if self.is_continuous:
-      return self.log_prob(value, name=name)
-    else:
-      raise NotImplementedError(
-          "log_pdf is not implemented for non-continuous distributions")
+    Returns:
+      log_prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
+        values of type `self.dtype`.
+
+    Raises:
+      AttributeError: if not `is_continuous`.
+    """
+    if not self.is_continuous:
+      raise AttributeError(
+          "log_pdf is undefined for non-continuous distributions.")
+    return self.log_prob(value, name=name)
 
   def pdf(self, value, name="pdf"):
-    """The probability density function."""
-    if self.is_continuous:
-      return self.prob(value, name=name)
-    else:
-      raise NotImplementedError(
-          "pdf is not implemented for non-continuous distributions")
+    """Probability density function.
+
+    Args:
+      value: `float` or `double` `Tensor`.
+      name: The name to give this op.
+
+    Returns:
+      prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
+        values of type `self.dtype`.
+
+    Raises:
+      AttributeError: if not `is_continuous`.
+    """
+    if not self.is_continuous:
+      raise AttributeError("pdf is undefined for non-continuous distributions.")
+    return self.prob(value, name)
 
   def log_pmf(self, value, name="log_pmf"):
-    """Log of the probability mass function."""
+    """Log probability mass function.
+
+    Args:
+      value: `float` or `double` `Tensor`.
+      name: The name to give this op.
+
+    Returns:
+      log_pmf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
+        values of type `self.dtype`.
+
+    Raises:
+      AttributeError: if `is_continuous`.
+    """
     if self.is_continuous:
-      raise NotImplementedError(
-          "log_pmf is not implemented for continuous distributions")
-    else:
-      return self.log_prob(value, name=name)
+      raise AttributeError("log_pmf is undefined for continuous distributions.")
+    return self.log_prob(value, name=name)
 
   def pmf(self, value, name="pmf"):
-    """The probability mass function."""
+    """Probability mass function.
+
+    Args:
+      value: `float` or `double` `Tensor`.
+      name: The name to give this op.
+
+    Returns:
+      pmf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
+        values of type `self.dtype`.
+
+    Raises:
+      AttributeError: if `is_continuous`.
+    """
     if self.is_continuous:
-      raise NotImplementedError(
-          "pmf is not implemented for continuous distributions")
+      raise AttributeError("pmf is undefined for continuous distributions.")
+    return self.prob(value, name=name)
+
+  @contextlib.contextmanager
+  def _name_scope(self, name=None, values=None):
+    """Helper function to standardize op scope."""
+    with ops.name_scope(self.name):
+      with ops.name_scope(name, values=(
+          (values or []) + list(self.parameters.values()))) as scope:
+        yield scope
+
+  def _check_hasattr(self, func):
+    if hasattr(self, func.__func__.__name__) and callable(func): return
+    raise NotImplementedError(
+        "Subclass %s does not implement %s" %
+        (type(self).__name__, func.__func__.__name__))
+
+  def _expand_sample_shape(self, sample_shape):
+    """Helper to `sample` which ensures sample_shape is 1D."""
+    sample_shape_static_val = tensor_util.constant_value(sample_shape)
+    ndims = sample_shape.get_shape().ndims
+    if sample_shape_static_val is None:
+      if ndims is None or not sample_shape.get_shape().is_fully_defined():
+        ndims = array_ops.rank(sample_shape)
+      expanded_shape = distribution_util.pick_vector(
+          math_ops.equal(ndims, 0),
+          np.array((1,), dtype=dtypes.int32.as_numpy_dtype()),
+          array_ops.shape(sample_shape))
+      sample_shape = array_ops.reshape(sample_shape, expanded_shape)
+      total = math_ops.reduce_prod(sample_shape)  # reduce_prod([]) == 1
     else:
-      return self.prob(value, name=name)
+      if ndims is None:
+        raise ValueError(
+            "Shouldn't be here; ndims cannot be none when we have a "
+            "tf.constant shape.")
+      if ndims == 0:
+        sample_shape_static_val = np.reshape(sample_shape_static_val, [1])
+        sample_shape = ops.convert_to_tensor(
+            sample_shape_static_val,
+            dtype=dtypes.int32,
+            name="sample_shape")
+      total = np.prod(sample_shape_static_val,
+                      dtype=dtypes.int32.as_numpy_dtype())
+    return sample_shape, total
+
+
+distribution_util.append_class_fun_doc(BaseDistribution.sample_n,
+                                       doc_str=Distribution.sample_n.__doc__)
+distribution_util.append_class_fun_doc(BaseDistribution.log_prob,
+                                       doc_str=Distribution.log_prob.__doc__)
diff --git a/tensorflow/contrib/distributions/python/ops/distribution_util.py b/tensorflow/contrib/distributions/python/ops/distribution_util.py
index c6386b905b6..a1ab35f1f0c 100644
--- a/tensorflow/contrib/distributions/python/ops/distribution_util.py
+++ b/tensorflow/contrib/distributions/python/ops/distribution_util.py
@@ -18,6 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import sys
 import numpy as np
 
 from tensorflow.python.framework import constant_op
@@ -374,3 +375,27 @@ def pick_vector(cond,
     return array_ops.slice(array_ops.concat(0, (true_vector, false_vector)),
                            [math_ops.select(cond, 0, n)],
                            [math_ops.select(cond, n, -1)])
+
+
+def append_class_fun_doc(fn, doc_str):
+  """Appends the `doc_str` argument to `fn.__doc__`.
+
+  This function is primarily needed because Python 3 changes how docstrings are
+  programmatically set.
+
+  Args:
+    fn: Class function.
+    doc_str: String
+  """
+  # TODO(b/31100586): Figure out why appending accumulates rather than resets
+  # for each subclass.
+  if sys.version_info.major < 3:
+    if fn.__func__.__doc__ is None:
+      fn.__func__.__doc__ = doc_str
+    # else:
+    #   fn.__func__.__doc__ += doc_str
+  else:
+    if fn.__doc__ is None:
+      fn.__doc__ = doc_str
+    # else:
+    #   fn.__doc__ += doc_str
diff --git a/tensorflow/contrib/distributions/python/ops/exponential.py b/tensorflow/contrib/distributions/python/ops/exponential.py
index 95aedc88a9f..59ac2390606 100644
--- a/tensorflow/contrib/distributions/python/ops/exponential.py
+++ b/tensorflow/contrib/distributions/python/ops/exponential.py
@@ -21,10 +21,7 @@ from __future__ import print_function
 import numpy as np
 
 from tensorflow.contrib.distributions.python.ops import gamma
-from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import ops
-from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import math_ops
 from tensorflow.python.ops import random_ops
@@ -41,8 +38,11 @@ class Exponential(gamma.Gamma):
   distribution, with Exponential(lam) = Gamma(1, lam).
   """
 
-  def __init__(
-      self, lam, validate_args=True, allow_nan_stats=False, name="Exponential"):
+  def __init__(self,
+               lam,
+               validate_args=True,
+               allow_nan_stats=False,
+               name="Exponential"):
     """Construct Exponential distribution with parameter `lam`.
 
     Args:
@@ -62,51 +62,28 @@ class Exponential(gamma.Gamma):
     # allow_nan_stats=False
     # through to the parent class results in unnecessary asserts.
     with ops.name_scope(name, values=[lam]):
-      lam = ops.convert_to_tensor(lam)
-      self._lam = lam
+      self._lam = ops.convert_to_tensor(lam, name="lam")
       super(Exponential, self).__init__(
-          alpha=constant_op.constant(1.0, dtype=lam.dtype),
-          beta=lam,
+          alpha=array_ops.ones((), dtype=self._lam.dtype),
+          beta=self._lam,
           allow_nan_stats=allow_nan_stats,
           validate_args=validate_args)
+      # While the Gamma distribution is not reparameterizeable, the
+      # exponential distribution is.
+      self._is_reparameterized = True
 
   @property
   def lam(self):
     return self._lam
 
-  @property
-  def is_reparameterized(self):
-    # While the Gamma distribution is not reparameterizeable, the
-    # exponential distribution is.
-    return True
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the Exponential Distributions.
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
-
-    Returns:
-      samples: `[n, ...]`, a `Tensor` of `n` samples for each
-        of the distributions determined by the hyperparameters.
-    """
-    broadcast_shape = self._lam.get_shape()
-    with ops.name_scope(name, "ExponentialSample", [self.lam, n]):
-      n = ops.convert_to_tensor(n, name="n")
-      shape = array_ops.concat(0, ([n], array_ops.shape(self._lam)))
-      # Sample uniformly-at-random from the open-interval (0, 1).
-      sampled = random_ops.random_uniform(
-          shape, minval=np.nextafter(
-              self.dtype.as_numpy_dtype(0.), self.dtype.as_numpy_dtype(1.)),
-          maxval=constant_op.constant(1.0, dtype=self.dtype),
-          seed=seed,
-          dtype=self.dtype)
-
-      n_val = tensor_util.constant_value(n)
-      final_shape = tensor_shape.vector(n_val).concatenate(broadcast_shape)
-      sampled.set_shape(final_shape)
-
-      return -math_ops.log(sampled) / self._lam
+  def _sample_n(self, n, seed=None):
+    shape = array_ops.concat(0, ([n], array_ops.shape(self._lam)))
+    # Sample uniformly-at-random from the open-interval (0, 1).
+    sampled = random_ops.random_uniform(
+        shape,
+        minval=np.nextafter(self.dtype.as_numpy_dtype(0.),
+                            self.dtype.as_numpy_dtype(1.)),
+        maxval=array_ops.ones((), dtype=self.dtype),
+        seed=seed,
+        dtype=self.dtype)
+    return -math_ops.log(sampled) / self._lam
diff --git a/tensorflow/contrib/distributions/python/ops/gamma.py b/tensorflow/contrib/distributions/python/ops/gamma.py
index 4363a1b7c5a..a02b21ce103 100644
--- a/tensorflow/contrib/distributions/python/ops/gamma.py
+++ b/tensorflow/contrib/distributions/python/ops/gamma.py
@@ -20,8 +20,9 @@ from __future__ import print_function
 
 import numpy as np
 
-from tensorflow.contrib.distributions.python.ops import distribution  # pylint: disable=line-too-long
-from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util  # pylint: disable=line-too-long
+from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.contrib.distributions.python.ops import distribution_util
+from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util
 from tensorflow.python.framework import common_shapes
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
@@ -88,41 +89,20 @@ class Gamma(distribution.Distribution):
     Raises:
       TypeError: if `alpha` and `beta` are different dtypes.
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
-    with ops.name_scope(name, values=[alpha, beta]) as scope:
-      self._name = scope
-      with ops.control_dependencies([check_ops.assert_positive(
-          alpha), check_ops.assert_positive(beta)] if validate_args else []):
-        alpha = array_ops.identity(alpha, name="alpha")
-        beta = array_ops.identity(beta, name="beta")
-
-    self._get_batch_shape = common_shapes.broadcast_shape(
-        alpha.get_shape(), beta.get_shape())
-    self._get_event_shape = tensor_shape.TensorShape([])
-
-    self._alpha = alpha
-    self._beta = beta
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    """Name to prepend to all ops."""
-    return self._name
-
-  @property
-  def dtype(self):
-    """dtype of samples from this distribution."""
-    return self._alpha.dtype
+    with ops.name_scope(name, values=[alpha, beta]):
+      with ops.control_dependencies([
+          check_ops.assert_positive(alpha),
+          check_ops.assert_positive(beta),
+      ] if validate_args else []):
+        self._alpha = array_ops.identity(alpha, name="alpha")
+        self._beta = array_ops.identity(beta, name="beta")
+        contrib_tensor_util.assert_same_float_dtype((self._alpha, self._beta))
+        super(Gamma, self).__init__(
+            dtype=self._alpha.dtype,
+            parameters={"alpha": self._alpha, "beta": self._beta},
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def alpha(self):
@@ -134,181 +114,88 @@ class Gamma(distribution.Distribution):
     """Inverse scale parameter."""
     return self._beta
 
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta]):
-        return array_ops.shape(self._alpha + self._beta)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      `TensorShape` object.
-    """
-    return self._get_batch_shape
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], dtype=dtypes.int32)
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `event_shape`. May be only partially defined.
-
-    Returns:
-      `TensorShape` object.
-    """
-    return self._get_event_shape
-
-  def mean(self, name="mean"):
-    """Mean of each batch member."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta]):
-        return self._alpha / self._beta
-
-  def mode(self, name="mode"):
-    """Mode of each batch member.
-
-    The mode of a gamma distribution is `(alpha - 1) / beta` when `alpha > 1`,
-    and `NaN` otherwise.  If `self.allow_nan_stats` is `False`, an exception
-    will be raised rather than returning `NaN`.
-
-    Args:
-      name:  A name to give this op.
-
-    Returns:
-      The mode for every batch member, a `Tensor` with same `dtype` as self.
-    """
-    alpha = self._alpha
-    beta = self._beta
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[alpha, beta]):
-        mode_if_defined = (alpha - 1.0) / beta
-        if self.allow_nan_stats:
-          alpha_ge_1 = alpha >= 1.0
-          nan = np.nan * self._ones()
-          return math_ops.select(alpha_ge_1, mode_if_defined, nan)
-        else:
-          one = constant_op.constant(1.0, dtype=self.dtype)
-          return control_flow_ops.with_dependencies(
-              [check_ops.assert_less(
-                  one, alpha,
-                  message="mode not defined for components of alpha <= 1"
-              )], mode_if_defined)
-
-  def variance(self, name="variance"):
-    """Variance of each batch member."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta]):
-        return self._alpha / math_ops.square(self._beta)
-
-  def std(self, name="std"):
-    """Standard deviation of this distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta]):
-        return math_ops.sqrt(self._alpha) / self._beta
-
-  def log_prob(self, x, name="log_prob"):
-    """Log prob of observations in `x` under these Gamma distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `alpha` and `beta`.
-      name: The name to give this op.
-
-    Returns:
-      log_prob: tensor of dtype `dtype`, the log-PDFs of `x`.
-
-    Raises:
-      TypeError: if `x` and `alpha` are different dtypes.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta, x]):
-        alpha = self._alpha
-        beta = self._beta
-        x = ops.convert_to_tensor(x)
-        x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if
-                                               self.validate_args else [], x)
-        contrib_tensor_util.assert_same_float_dtype(tensors=[x,],
-                                                    dtype=self.dtype)
-
-        return (alpha * math_ops.log(beta) + (alpha - 1) * math_ops.log(x) -
-                beta * x - math_ops.lgamma(self._alpha))
-
-  def prob(self, x, name="prob"):
-    """Pdf of observations in `x` under these Gamma distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `alpha` and `beta`.
-      name: The name to give this op.
-
-    Returns:
-      prob: tensor of dtype `dtype`, the PDFs of `x`
-
-    Raises:
-      TypeError: if `x` and `alpha` are different dtypes.
-    """
-    return super(Gamma, self).prob(x, name)
-
-  def log_cdf(self, x, name="log_cdf"):
-    """Log CDF of observations `x` under these Gamma distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `alpha` and `beta`.
-      name: The name to give this op.
-
-    Returns:
-      log_cdf: tensor of dtype `dtype`, the log-CDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta, x]):
-        x = ops.convert_to_tensor(x)
-        x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if
-                                               self.validate_args else [], x)
-        contrib_tensor_util.assert_same_float_dtype(tensors=[x,],
-                                                    dtype=self.dtype)
-        # Note that igamma returns the regularized incomplete gamma function,
-        # which is what we want for the CDF.
-        return math_ops.log(math_ops.igamma(self._alpha, self._beta * x))
-
-  def cdf(self, x, name="cdf"):
-    """CDF of observations `x` under these Gamma distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `alpha` and `beta`.
-      name: The name to give this op.
-
-    Returns:
-      cdf: tensor of dtype `dtype`, the CDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta, x]):
-        return math_ops.igamma(self._alpha, self._beta * x)
-
-  def entropy(self, name="entropy"):
-    """The entropy of Gamma distribution(s).
+  def _batch_shape(self):
+    return array_ops.shape(self.alpha + self.beta)
+
+  def _get_batch_shape(self):
+    return common_shapes.broadcast_shape(self.alpha.get_shape(),
+                                         self.beta.get_shape())
+
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
+
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
+
+  def _sample_n(self, n, seed=None):
+    return random_ops.random_gamma([n],
+                                   self.alpha,
+                                   beta=self.beta,
+                                   dtype=self.dtype,
+                                   seed=seed)
+
+  def _log_prob(self, x):
+    x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if
+                                           self.validate_args else [], x)
+    contrib_tensor_util.assert_same_float_dtype(tensors=[x],
+                                                dtype=self.dtype)
+    return (self.alpha * math_ops.log(self.beta) +
+            (self.alpha - 1.) * math_ops.log(x) -
+            self.beta * x -
+            math_ops.lgamma(self.alpha))
+
+  def _prob(self, x):
+    return math_ops.exp(self._log_prob(x))
+
+  def _log_cdf(self, x):
+    x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if
+                                           self.validate_args else [], x)
+    contrib_tensor_util.assert_same_float_dtype(tensors=[x], dtype=self.dtype)
+    # Note that igamma returns the regularized incomplete gamma function,
+    # which is what we want for the CDF.
+    return math_ops.log(math_ops.igamma(self.alpha, self.beta * x))
+
+  def _cdf(self, x):
+    return math_ops.igamma(self.alpha, self.beta * x)
+
+  def _entropy(self):
+    return (self.alpha -
+            math_ops.log(self.beta) +
+            math_ops.lgamma(self.alpha) +
+            (1. - self.alpha) * math_ops.digamma(self.alpha))
+
+  def _mean(self):
+    return self.alpha / self.beta
+
+  def _variance(self):
+    return self.alpha / math_ops.square(self.beta)
+
+  def _std(self):
+    return math_ops.sqrt(self.alpha) / self.beta
+
+  def _mode(self):
+    mode = (self.alpha - 1.) / self.beta
+    if self.allow_nan_stats:
+      nan = np.array(np.nan, dtype=self.dtype.as_numpy_dtype())
+      return math_ops.select(
+          self.alpha >= 1.,
+          mode,
+          array_ops.fill(self.batch_shape(), nan, name="nan"))
+    else:
+      return control_flow_ops.with_dependencies([
+          check_ops.assert_less(
+              array_ops.ones((), self.dtype),
+              self.alpha,
+              message="mode not defined for components of alpha <= 1"),
+          ], mode)
+
+
+distribution_util.append_class_fun_doc(Gamma.sample_n, doc_str="""
+
+    See the documentation for tf.random_gamma for more details.
+""")
+
+distribution_util.append_class_fun_doc(Gamma.entropy, doc_str="""
 
     This is defined to be
 
@@ -318,50 +205,11 @@ class Gamma(distribution.Distribution):
     ```
 
     where digamma(alpha) is the digamma function.
+""")
 
-    Args:
-      name: The name to give this op.
-
-    Returns:
-      entropy: tensor of dtype `dtype`, the entropy.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.alpha, self._beta]):
-        alpha = self._alpha
-        beta = self._beta
-        return (alpha - math_ops.log(beta) + math_ops.lgamma(alpha) +
-                (1 - alpha) * math_ops.digamma(alpha))
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Draws `n` samples from the Gamma distribution(s).
-
-    See the doc for tf.random_gamma for further detail.
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed for this operation.
-      name: Optional name for the operation.
-
-    Returns:
-      samples: a `Tensor` of shape `(n,) + self.batch_shape + self.event_shape`
-          with values of type `self.dtype`.
-    """
-    with ops.name_scope(self.name, values=[n, self.alpha, self._beta]):
-      return random_ops.random_gamma([n],
-                                     self.alpha,
-                                     beta=self._beta,
-                                     dtype=self.dtype,
-                                     seed=seed,
-                                     name=name)
-
-  @property
-  def is_reparameterized(self):
-    return False
-
-  def _ones(self):
-    return array_ops.ones_like(self._alpha + self._beta, dtype=self.dtype)
+distribution_util.append_class_fun_doc(Gamma.mode, doc_str="""
 
-  @property
-  def is_continuous(self):
-    return True
+    The mode of a gamma distribution is `(alpha - 1) / beta` when `alpha > 1`,
+    and `NaN` otherwise.  If `self.allow_nan_stats` is `False`, an exception
+    will be raised rather than returning `NaN`.
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/inverse_gamma.py b/tensorflow/contrib/distributions/python/ops/inverse_gamma.py
index 869b61bd3e9..58c92a9acae 100644
--- a/tensorflow/contrib/distributions/python/ops/inverse_gamma.py
+++ b/tensorflow/contrib/distributions/python/ops/inverse_gamma.py
@@ -20,8 +20,8 @@ from __future__ import print_function
 
 import numpy as np
 
-from tensorflow.contrib.distributions.python.ops import distribution  # pylint: disable=line-too-long
-from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util  # pylint: disable=line-too-long
+from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.contrib.distributions.python.ops import distribution_util
 from tensorflow.python.framework import common_shapes
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
@@ -87,41 +87,19 @@ class InverseGamma(distribution.Distribution):
     Raises:
       TypeError: if `alpha` and `beta` are different dtypes.
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
-    with ops.name_scope(name, values=[alpha, beta]) as scope:
-      self._name = scope
-      with ops.control_dependencies([check_ops.assert_positive(
-          alpha), check_ops.assert_positive(beta)] if validate_args else []):
-        alpha = array_ops.identity(alpha, name="alpha")
-        beta = array_ops.identity(beta, name="beta")
-
-    self._get_batch_shape = common_shapes.broadcast_shape(
-        alpha.get_shape(), beta.get_shape())
-    self._get_event_shape = tensor_shape.TensorShape([])
-
-    self._alpha = alpha
-    self._beta = beta
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    """Name to prepend to all ops."""
-    return self._name
-
-  @property
-  def dtype(self):
-    """dtype of samples from this distribution."""
-    return self._alpha.dtype
+    with ops.name_scope(name, values=[alpha, beta]):
+      with ops.control_dependencies([
+          check_ops.assert_positive(alpha),
+          check_ops.assert_positive(beta),
+      ] if validate_args else []):
+        self._alpha = array_ops.identity(alpha, name="alpha")
+        self._beta = array_ops.identity(beta, name="beta")
+        super(InverseGamma, self).__init__(
+            dtype=self._alpha.dtype,
+            parameters={"alpha": self._alpha, "beta": self._beta},
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def alpha(self):
@@ -133,210 +111,88 @@ class InverseGamma(distribution.Distribution):
     """Scale parameter."""
     return self._beta
 
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta]):
-        return array_ops.shape(self._alpha + self._beta)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      `TensorShape` object.
-    """
-    return self._get_batch_shape
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], dtype=dtypes.int32)
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `event_shape`. May be only partially defined.
-
-    Returns:
-      `TensorShape` object.
-    """
-    return self._get_event_shape
-
-  def mean(self, name="mean"):
-    """Mean of each batch member.
-
-    The mean of an inverse gamma distribution is `beta / (alpha - 1)`,
-    when `alpha > 1`, and `NaN` otherwise.  If `self.allow_nan_stats` is
-    `False`, an exception will be raised rather than returning `NaN`
-
-    Args:
-      name: A name to give this op.
-
-    Returns:
-      The mean for every batch member, a `Tensor` with same `dtype` as self.
-    """
-    alpha = self._alpha
-    beta = self._beta
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[alpha, beta]):
-        mean_if_defined = beta / (alpha - 1.0)
-        if self.allow_nan_stats:
-          alpha_gt_1 = alpha > 1.0
-          nan = np.nan * self._ones()
-          return math_ops.select(alpha_gt_1, mean_if_defined, nan)
-        else:
-          one = constant_op.constant(1.0, dtype=self.dtype)
-          return control_flow_ops.with_dependencies(
-              [check_ops.assert_less(
-                  one, alpha,
-                  message="mean not defined for components of alpha <= 1")],
-              mean_if_defined)
-
-  def mode(self, name="mode"):
-    """Mode of each batch member.
-
-    The mode of an inverse gamma distribution is `beta / (alpha + 1)`.
-
-    Args:
-      name: A name to give this op.
-
-    Returns:
-      The mode for every batch member, a `Tensor` with same `dtype` as self.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta]):
-        return self._beta / (self._alpha + 1.0)
-
-  def variance(self, name="variance"):
-    """Variance of each batch member.
-
-    Variance for inverse gamma is defined only for `alpha > 2`. If
-    `self.allow_nan_stats` is `False`, an exception will be raised rather
-    than returning `NaN`.
-
-    Args:
-      name: A name to give this op.
-
-    Returns:
-      The variance for every batch member, a `Tensor` with same `dtype` as self.
-    """
-    alpha = self._alpha
-    beta = self._beta
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[alpha, beta]):
-        var_if_defined = (math_ops.square(self._beta) /
-                          (math_ops.square(self._alpha - 1.0) *
-                           (self._alpha - 2.0)))
-        if self.allow_nan_stats:
-          alpha_gt_2 = alpha > 2.0
-          nan = np.nan * self._ones()
-          return math_ops.select(alpha_gt_2, var_if_defined, nan)
-        else:
-          two = constant_op.constant(2.0, dtype=self.dtype)
-          return control_flow_ops.with_dependencies(
-              [check_ops.assert_less(
-                  two, alpha,
-                  message="variance not defined for components of alpha <= 2")],
-              var_if_defined)
-
-  def log_prob(self, x, name="log_prob"):
-    """Log prob of observations in `x` under these InverseGamma distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `alpha` and `beta`.
-      name: The name to give this op.
-
-    Returns:
-      log_prob: tensor of dtype `dtype`, the log-PDFs of `x`.
-
-    Raises:
-      TypeError: if `x` and `alpha` are different dtypes.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta, x]):
-        alpha = self._alpha
-        beta = self._beta
-        x = ops.convert_to_tensor(x)
-        x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if
-                                               self.validate_args else [], x)
-        contrib_tensor_util.assert_same_float_dtype(tensors=[x,],
-                                                    dtype=self.dtype)
-
-        return (alpha * math_ops.log(beta) - math_ops.lgamma(self._alpha) -
-                (alpha + 1) * math_ops.log(x) - beta / x)
-
-  def prob(self, x, name="prob"):
-    """Pdf of observations in `x` under these Gamma distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `alpha` and `beta`.
-      name: The name to give this op.
-
-    Returns:
-      prob: tensor of dtype `dtype`, the PDFs of `x`
-
-    Raises:
-      TypeError: if `x` and `alpha` are different dtypes.
-    """
-    return super(InverseGamma, self).prob(x, name)
-
-  def log_cdf(self, x, name="log_cdf"):
-    """Log CDF of observations `x` under these InverseGamma distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `alpha` and `beta`.
-      name: The name to give this op.
-
-    Returns:
-      log_cdf: tensor of dtype `dtype`, the log-CDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta, x]):
-        x = ops.convert_to_tensor(x)
-        x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if
-                                               self.validate_args else [], x)
-        contrib_tensor_util.assert_same_float_dtype(tensors=[x,],
-                                                    dtype=self.dtype)
-        # Note that igammac returns the upper regularized incomplete gamma
-        # function Q(a, x), which is what we want for the CDF.
-        return math_ops.log(math_ops.igammac(self._alpha, self._beta / x))
-
-  def cdf(self, x, name="cdf"):
-    """CDF of observations `x` under these InverseGamma distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `alpha` and `beta`.
-      name: The name to give this op.
-
-    Returns:
-      cdf: tensor of dtype `dtype`, the CDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta, x]):
-        return math_ops.igammac(self._alpha, self._beta / x)
-
-  def entropy(self, name="entropy"):
-    """The entropy of these InverseGamma distribution(s).
+  def _batch_shape(self):
+    return array_ops.shape(self.alpha + self.beta)
+
+  def _get_batch_shape(self):
+    return common_shapes.broadcast_shape(self.alpha.get_shape(),
+                                         self.beta.get_shape())
+
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
+
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
+
+  def _sample_n(self, n, seed=None):
+    return 1. / random_ops.random_gamma([n], self.alpha, beta=self.beta,
+                                        dtype=self.dtype, seed=seed)
+
+  def _log_prob(self, x):
+    x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if
+                                           self.validate_args else [], x)
+    return (self.alpha * math_ops.log(self.beta) -
+            math_ops.lgamma(self.alpha) -
+            (self.alpha + 1.) * math_ops.log(x) - self.beta / x)
+
+  def _prob(self, x):
+    return math_ops.exp(self._log_prob(x))
+
+  def _log_cdf(self, x):
+    return math_ops.log(self._cdf(x))
+
+  def _cdf(self, x):
+    x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if
+                                           self.validate_args else [], x)
+    # Note that igammac returns the upper regularized incomplete gamma
+    # function Q(a, x), which is what we want for the CDF.
+    return math_ops.igammac(self.alpha, self.beta / x)
+
+  def _entropy(self):
+    return (self.alpha +
+            math_ops.log(self.beta) +
+            math_ops.lgamma(self.alpha) -
+            (1. + self.alpha) * math_ops.digamma(self.alpha))
+
+  def _mean(self):
+    mean = self.beta / (self.alpha - 1.)
+    if self.allow_nan_stats:
+      nan = np.array(np.nan, dtype=self.dtype.as_numpy_dtype())
+      return math_ops.select(
+          self.alpha > 1., mean,
+          array_ops.fill(self.batch_shape(), nan, name="nan"))
+    else:
+      return control_flow_ops.with_dependencies([
+          check_ops.assert_less(
+              array_ops.ones((), self.dtype), self.alpha,
+              message="mean not defined for components of self.alpha <= 1"),
+      ], mean)
+
+  def _variance(self):
+    var = (math_ops.square(self.beta) /
+           (math_ops.square(self.alpha - 1.) * (self.alpha - 2.)))
+    if self.allow_nan_stats:
+      nan = np.array(np.nan, dtype=self.dtype.as_numpy_dtype())
+      return math_ops.select(
+          self.alpha > 2., var,
+          array_ops.fill(self.batch_shape(), nan, name="nan"))
+    else:
+      return control_flow_ops.with_dependencies([
+          check_ops.assert_less(
+              constant_op.constant(2., dtype=self.dtype), self.alpha,
+              message="variance not defined for components of alpha <= 2"),
+      ], var)
+
+  def _mode(self):
+    return self.beta / (self.alpha + 1.)
+
+
+distribution_util.append_class_fun_doc(InverseGamma.sample_n, doc_str="""
+
+    See the documentation for tf.random_gamma for more details.
+""")
+
+distribution_util.append_class_fun_doc(InverseGamma.entropy, doc_str="""
 
     This is defined to be
 
@@ -346,51 +202,23 @@ class InverseGamma(distribution.Distribution):
     ```
 
     where digamma(alpha) is the digamma function.
+""")
 
-    Args:
-      name: The name to give this op.
-
-    Returns:
-      entropy: tensor of dtype `dtype`, the entropy.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._alpha, self._beta]):
-        alpha = self._alpha
-        beta = self._beta
-        return (alpha + math_ops.log(beta) + math_ops.lgamma(alpha) -
-                (1 + alpha) * math_ops.digamma(alpha))
+distribution_util.append_class_fun_doc(InverseGamma.mean, doc_str="""
 
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Draws `n` samples from these InverseGamma distribution(s).
-
-    See the doc for tf.random_gamma for further details on sampling strategy.
+    The mean of an inverse gamma distribution is `beta / (alpha - 1)`,
+    when `alpha > 1`, and `NaN` otherwise.  If `self.allow_nan_stats` is
+    `False`, an exception will be raised rather than returning `NaN`
+""")
 
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed for this operation.
-      name: Optional name for the operation.
-
-    Returns:
-      samples: a `Tensor` of shape `(n,) + self.batch_shape + self.event_shape`
-          with values of type `self.dtype`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[n, self._alpha, self._beta]):
-        one = constant_op.constant(1.0, dtype=self.dtype)
-        return one / random_ops.random_gamma([n],
-                                             self._alpha,
-                                             beta=self._beta,
-                                             dtype=self.dtype,
-                                             seed=seed)
+distribution_util.append_class_fun_doc(InverseGamma.variance, doc_str="""
 
-  @property
-  def is_reparameterized(self):
-    return False
+    Variance for inverse gamma is defined only for `alpha > 2`. If
+    `self.allow_nan_stats` is `False`, an exception will be raised rather
+    than returning `NaN`.
+""")
 
-  def _ones(self):
-    return array_ops.ones_like(self._alpha + self._beta, dtype=self.dtype)
+distribution_util.append_class_fun_doc(InverseGamma.mode, doc_str="""
 
-  @property
-  def is_continuous(self):
-    return True
+    The mode of an inverse gamma distribution is `beta / (alpha + 1)`.
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/laplace.py b/tensorflow/contrib/distributions/python/ops/laplace.py
index bc8e4fc38c1..59e96739df4 100644
--- a/tensorflow/contrib/distributions/python/ops/laplace.py
+++ b/tensorflow/contrib/distributions/python/ops/laplace.py
@@ -29,7 +29,6 @@ from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import math_ops
@@ -77,88 +76,19 @@ class Laplace(distribution.Distribution):
     Raises:
       TypeError: if `loc` and `scale` are of different dtype.
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
     with ops.name_scope(name, values=[loc, scale]):
-      loc = ops.convert_to_tensor(loc)
-      scale = ops.convert_to_tensor(scale)
       with ops.control_dependencies([check_ops.assert_positive(scale)] if
                                     validate_args else []):
-        self._name = name
         self._loc = array_ops.identity(loc, name="loc")
         self._scale = array_ops.identity(scale, name="scale")
-        self._batch_shape = common_shapes.broadcast_shape(
-            self._loc.get_shape(), self._scale.get_shape())
-        self._event_shape = tensor_shape.TensorShape([])
-
-    contrib_tensor_util.assert_same_float_dtype((loc, scale))
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def dtype(self):
-    return self._loc.dtype
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op.
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return array_ops.shape(self._loc + self._scale)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      batch shape
-    """
-    return self._batch_shape
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
-
-    Args:
-      name: name to give to the op.
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], dtype=dtypes.int32)
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `event_shape`. May be only partially defined.
-
-    Returns:
-      event shape
-    """
-    return self._event_shape
+        contrib_tensor_util.assert_same_float_dtype((self._loc, self._scale))
+        super(Laplace, self).__init__(
+            dtype=self._loc.dtype,
+            parameters={"loc": self._loc, "scale": self._scale},
+            is_reparameterized=True,
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def loc(self):
@@ -170,155 +100,64 @@ class Laplace(distribution.Distribution):
     """Distribution parameter for scale."""
     return self._scale
 
-  def mean(self, name="mean"):
-    """Mean of this distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._scale, self._loc]):
-        return self._loc + array_ops.zeros_like(self._scale)
-
-  def median(self, name="median"):
-    """Median of this distribution."""
-    return self.mean(name="median")
-
-  def mode(self, name="mode"):
-    """Mode of this distribution."""
-    return self.mean(name="mode")
-
-  def std(self, name="std"):
-    """Standard deviation of this distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._scale, self._loc]):
-        sqrt_2 = constant_op.constant(math.sqrt(2.), dtype=self.dtype)
-        return sqrt_2 * self._scale + array_ops.zeros_like(self._loc)
+  def _batch_shape(self):
+    return array_ops.shape(self.loc + self.scale)
 
-  def variance(self, name="variance"):
-    """Variance of this distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return math_ops.square(self.std())
+  def _get_batch_shape(self):
+    return common_shapes.broadcast_shape(self.loc.get_shape(),
+                                         self.scale.get_shape())
 
-  def prob(self, x, name="pdf"):
-    """The prob of observations in `x` under the Laplace distribution(s).
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
 
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `loc` and `scale`.
-      name: The name to give this op.
-
-    Returns:
-      pdf: tensor of dtype `dtype`, the pdf values of `x`.
-    """
-    return 0.5 / self._scale * math_ops.exp(
-        -math_ops.abs(x - self._loc) / self._scale)
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
 
-  def log_prob(self, x, name="log_prob"):
-    """Log prob of observations in `x` under these Laplace distribution(s).
+  def _sample_n(self, n, seed=None):
+    shape = array_ops.concat(0, ([n], self.batch_shape()))
+    # Sample uniformly-at-random from the open-interval (-1, 1).
+    uniform_samples = random_ops.random_uniform(
+        shape=shape,
+        minval=np.nextafter(self.dtype.as_numpy_dtype(-1.),
+                            self.dtype.as_numpy_dtype(0.)),
+        maxval=1.,
+        dtype=self.dtype,
+        seed=seed)
+    return (self.loc - self.scale * math_ops.sign(uniform_samples) *
+            math_ops.log(1. - math_ops.abs(uniform_samples)))
 
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `loc` and `scale`.
-      name: The name to give this op.
+  def _log_prob(self, x):
+    return (-math.log(2.) - math_ops.log(self.scale) -
+            math_ops.abs(x - self.loc) / self.scale)
 
-    Returns:
-      log_prob: tensor of dtype `dtype`, the log-probability of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._loc, self._scale, x]):
-        x = ops.convert_to_tensor(x)
-        if x.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s"
-                          % (x.dtype, self.dtype))
-        log_2 = constant_op.constant(math.log(2.), dtype=self.dtype)
-        return (-log_2 - math_ops.log(self._scale) -
-                math_ops.abs(x - self._loc) / self._scale)
+  def _prob(self, x):
+    return 0.5 / self.scale * math_ops.exp(
+        -math_ops.abs(x - self.loc) / self.scale)
 
-  def cdf(self, x, name="cdf"):
-    """CDF of observations in `x` under the Laplace distribution(s).
+  def _log_cdf(self, x):
+    return math_ops.log(self.cdf(x))
 
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `loc` and `scale`.
-      name: The name to give this op.
+  def _cdf(self, x):
+    y = x - self.loc
+    return (0.5 + 0.5 * math_ops.sign(y) *
+            (1. - math_ops.exp(-math_ops.abs(y) / self.scale)))
 
-    Returns:
-      cdf: tensor of dtype `dtype`, the CDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._loc, self._scale, x]):
-        x = ops.convert_to_tensor(x)
-        if x.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s"
-                          % (x.dtype, self.dtype))
-        y = x - self._loc
-        return 0.5 + 0.5 * math_ops.sign(y) * (
-            1. - math_ops.exp(-math_ops.abs(y) / self._scale))
+  def _entropy(self):
+    # Use broadcasting rules to calculate the full broadcast scale.
+    scale = self.scale + array_ops.zeros_like(self.loc)
+    return math.log(2.) + 1. + math_ops.log(scale)
 
-  def log_cdf(self, x, name="log_cdf"):
-    """Log CDF of observations `x` under the Laplace distribution(s).
+  def _mean(self):
+    return self.loc + array_ops.zeros_like(self.scale)
 
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `loc` and `scale`.
-      name: The name to give this op.
+  def _variance(self):
+    return math_ops.square(self._std())
 
-    Returns:
-      log_cdf: tensor of dtype `dtype`, the log-CDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._loc, self._scale, x]):
-        return math_ops.log(self.cdf(x))
+  def _std(self):
+    return math.sqrt(2.) * self.scale + array_ops.zeros_like(self.loc)
 
-  def entropy(self, name="entropy"):
-    """The entropy of Laplace distribution(s).
+  def _median(self):
+    return self._mean()
 
-    Args:
-      name: The name to give this op.
-
-    Returns:
-      entropy: tensor of dtype `dtype`, the entropy.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._loc, self._scale]):
-        log_2_e = constant_op.constant(math.log(2.) + 1., dtype=self.dtype)
-        # Use broadcasting rules to calculate the full broadcast scale.
-        scale = self._scale + array_ops.zeros_like(self._loc)
-        return log_2_e + math_ops.log(scale)
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the Laplace Distributions.
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
-
-    Returns:
-      samples: `[n, ...]`, a `Tensor` of `n` samples for each
-        of the distributions determined by broadcasting the parameters.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._loc, self._scale, n]):
-        n = ops.convert_to_tensor(n, name="n")
-        n_val = tensor_util.constant_value(n)
-        shape = array_ops.concat(0, ([n], self.batch_shape()))
-        # Sample uniformly-at-random from the open-interval (-1, 1).
-        uniform_samples = random_ops.random_uniform(
-            shape=shape,
-            minval=np.nextafter(self.dtype.as_numpy_dtype(-1.),
-                                self.dtype.as_numpy_dtype(0.)),
-            maxval=self.dtype.as_numpy_dtype(1.),
-            dtype=self.dtype,
-            seed=seed)
-
-        # Provide some hints to shape inference
-        inferred_shape = tensor_shape.vector(n_val).concatenate(
-            self.get_batch_shape())
-        uniform_samples.set_shape(inferred_shape)
-
-        return (self._loc - self._scale * math_ops.sign(uniform_samples) *
-                math_ops.log(1. - math_ops.abs(uniform_samples)))
-
-  @property
-  def is_reparameterized(self):
-    return True
-
-  @property
-  def is_continuous(self):
-    return True
+  def _mode(self):
+    return self._mean()
diff --git a/tensorflow/contrib/distributions/python/ops/multinomial.py b/tensorflow/contrib/distributions/python/ops/multinomial.py
index 931daecc864..bc79510e129 100644
--- a/tensorflow/contrib/distributions/python/ops/multinomial.py
+++ b/tensorflow/contrib/distributions/python/ops/multinomial.py
@@ -144,20 +144,20 @@ class Multinomial(distribution.Distribution):
               n, message="n has non-integer components.")
       ] if validate_args else []):
         self._n = array_ops.identity(n, name="convert_n")
-        self._name = name
-
-        self._validate_args = validate_args
-        self._allow_nan_stats = allow_nan_stats
-
-        self._mean = array_ops.expand_dims(n, -1) * self._p
-        # Only used for inferring shape.
-        self._broadcast_shape = math_ops.reduce_sum(self._mean,
-                                                    reduction_indices=[-1],
-                                                    keep_dims=False)
-
-        self._get_batch_shape = self._broadcast_shape.get_shape()
-        self._get_event_shape = (
-            self._mean.get_shape().with_rank_at_least(1)[-1:])
+        self._mean_val = array_ops.expand_dims(n, -1) * self._p
+        self._broadcast_shape = math_ops.reduce_sum(
+            self._mean_val, reduction_indices=[-1], keep_dims=False)
+        super(Multinomial, self).__init__(
+            dtype=self._p.dtype,
+            parameters={"p": self._p,
+                        "n": self._n,
+                        "mean": self._mean,
+                        "logits": self._logits,
+                        "broadcast_shape": self._broadcast_shape},
+            is_continuous=False,
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def n(self):
@@ -174,127 +174,55 @@ class Multinomial(distribution.Distribution):
     """Log-odds."""
     return self._logits
 
-  @property
-  def name(self):
-    """Name to prepend to all ops."""
-    return self._name
-
-  @property
-  def dtype(self):
-    """dtype of samples from this distribution."""
-    return self._p.dtype
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._broadcast_shape]):
-        return array_ops.shape(self._broadcast_shape)
+  def _batch_shape(self):
+    return array_ops.shape(self._broadcast_shape)
 
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
+  def _get_batch_shape(self):
+    return self._broadcast_shape.get_shape()
 
-    Same meaning as `batch_shape`. May be only partially defined.
+  def _event_shape(self):
+    return array_ops.gather(array_ops.shape(self._mean_val),
+                            [array_ops.rank(self._mean_val) - 1])
 
-    Returns:
-      batch shape
-    """
-    return self._get_batch_shape
+  def _get_event_shape(self):
+    return self._mean_val.get_shape().with_rank_at_least(1)[-1:]
 
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
+  def _log_prob(self, counts):
+    counts = self._assert_valid_sample(counts)
+    log_unnormalized_prob = math_ops.reduce_sum(
+        counts * math_ops.log(self.p),
+        reduction_indices=[-1])
+    log_normalizer = -distribution_util.log_combinations(self.n, counts)
+    return log_unnormalized_prob - log_normalizer
 
-    Args:
-      name: name to give to the op
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mean]):
-        return array_ops.gather(array_ops.shape(self._mean),
-                                [array_ops.rank(self._mean) - 1])
+  def _prob(self, counts):
+    return math_ops.exp(self._log_prob(counts))
 
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
+  def _mean(self):
+    return array_ops.identity(self._mean_val)
 
-    Same meaning as `event_shape`. May be only partially defined.
+  def _variance(self):
+    p = self.p * array_ops.expand_dims(array_ops.ones_like(self.n), -1)
+    outer_prod = math_ops.batch_matmul(
+        array_ops.expand_dims(self._mean_val, -1),
+        array_ops.expand_dims(p, -2))
+    return array_ops.batch_matrix_set_diag(
+        -outer_prod, self._mean_val - self._mean_val * p)
 
-    Returns:
-      event shape
-    """
-    return self._get_event_shape
-
-  def mean(self, name="mean"):
-    """Mean of the distribution."""
-    with ops.name_scope(self.name):
-      return array_ops.identity(self._mean, name=name)
-
-  def variance(self, name="variance"):
-    """Variance of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._n, self._p, self._mean]):
-        p = array_ops.expand_dims(
-            self._p * array_ops.expand_dims(
-                array_ops.ones_like(self._n), -1), -1)
-        variance = -math_ops.batch_matmul(
-            array_ops.expand_dims(self._mean, -1), p, adj_y=True)
-        variance += array_ops.batch_matrix_diag(self._mean)
-        return variance
-
-  def log_prob(self, counts, name="log_prob"):
-    """`Log(P[counts])`, computed for every batch member.
-
-    For each batch of counts `[n_1,...,n_k]`, `P[counts]` is the probability
-    that after sampling `n` draws from this Multinomial distribution, the
-    number of draws falling in class `j` is `n_j`.  Note that different
-    sequences of draws can result in the same counts, thus the probability
-    includes a combinatorial coefficient.
-
-    Args:
-      counts:  Non-negative tensor with dtype `dtype` and whose shape can
-        be broadcast with `self.p` and `self.n`.  For fixed leading dimensions,
-        the last dimension represents counts for the corresponding Multinomial
-        distribution in `self.p`. `counts` is only legal if it sums up to `n`
-        and its components are equal to integer values.
-      name:  Name to give this Op, defaults to "log_prob".
-
-    Returns:
-      Log probabilities for each record, shape `[N1,...,Nm]`.
-    """
-    n = self._n
-    p = self._p
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[n, p, counts]):
-        counts = self._check_counts(counts)
-
-        prob_prob = math_ops.reduce_sum(counts * math_ops.log(self._p),
-                                        reduction_indices=[-1])
-        log_prob = prob_prob + distribution_util.log_combinations(
-            n, counts)
-        return log_prob
+  def _assert_valid_sample(self, counts):
+    """Check counts for proper shape, values, then return tensor version."""
+    if not self.validate_args: return counts
+    return control_flow_ops.with_dependencies([
+        check_ops.assert_non_negative(
+            counts, message="counts has negative components."),
+        check_ops.assert_equal(
+            self.n, math_ops.reduce_sum(counts, reduction_indices=[-1]),
+            message="counts do not sum to n."),
+        distribution_util.assert_integer_form(
+            counts, message="counts have non-integer components.")
+    ], counts)
 
-  def prob(self, counts, name="prob"):
-    """`P[counts]`, computed for every batch member.
+_prob_note = """
 
     For each batch of counts `[n_1,...,n_k]`, `P[counts]` is the probability
     that after sampling `n` draws from this Multinomial distribution, the
@@ -302,38 +230,11 @@ class Multinomial(distribution.Distribution):
     sequences of draws can result in the same counts, thus the probability
     includes a combinatorial coefficient.
 
-    Args:
-      counts:  Non-negative tensor with dtype `dtype` and whose shape can
-        be broadcast with `self.p` and `self.n`.  For fixed leading dimensions,
-        the last dimension represents counts for the corresponding Multinomial
-        distribution in `self.p`. `counts` is only legal if it sums up to `n`
-        and its components are equal to integer values.
-      name:  Name to give this Op, defaults to "prob".
-
-    Returns:
-      Probabilities for each record, shape `[N1,...,Nm]`.
-    """
-    return super(Multinomial, self).prob(counts, name=name)
-
-  @property
-  def is_continuous(self):
-    return False
-
-  @property
-  def is_reparameterized(self):
-    return False
-
-  def _check_counts(self, counts):
-    """Check counts for proper shape, values, then return tensor version."""
-    counts = ops.convert_to_tensor(counts, name="counts_before_deps")
-    candidate_n = math_ops.reduce_sum(counts, reduction_indices=[-1])
-    if not self.validate_args:
-      return counts
-
-    return control_flow_ops.with_dependencies([
-        check_ops.assert_non_negative(
-            counts, message="counts has negative components."),
-        check_ops.assert_equal(
-            self._n, candidate_n, message="counts do not sum to n."),
-        distribution_util.assert_integer_form(
-            counts, message="counts have non-integer components.")], counts)
+    Note that input "counts" must be a non-negative tensor with dtype `dtype`
+    and whose shape can be broadcast with `self.p` and `self.n`.  For fixed
+    leading dimensions, the last dimension represents counts for the
+    corresponding Multinomial distribution in `self.p`. `counts` is only legal
+    if it sums up to `n` and its components are equal to integer values.
+"""
+distribution_util.append_class_fun_doc(Multinomial.log_prob, doc_str=_prob_note)
+distribution_util.append_class_fun_doc(Multinomial.prob, doc_str=_prob_note)
diff --git a/tensorflow/contrib/distributions/python/ops/mvn.py b/tensorflow/contrib/distributions/python/ops/mvn.py
index 0546733cb9b..0f003c22e08 100644
--- a/tensorflow/contrib/distributions/python/ops/mvn.py
+++ b/tensorflow/contrib/distributions/python/ops/mvn.py
@@ -21,6 +21,7 @@ from __future__ import print_function
 import math
 
 from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.contrib.distributions.python.ops import distribution_util
 from tensorflow.contrib.distributions.python.ops import kullback_leibler
 from tensorflow.contrib.distributions.python.ops import operator_pd_cholesky
 from tensorflow.contrib.distributions.python.ops import operator_pd_diag
@@ -29,8 +30,6 @@ from tensorflow.contrib.distributions.python.ops import operator_pd_vdvt_update
 from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import ops
-from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import control_flow_ops
@@ -120,24 +119,27 @@ class _MultivariateNormalOperatorPD(distribution.Distribution):
     Raises:
       TypeError: If `mu` and `cov` are different dtypes.
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
     with ops.name_scope(name):
       with ops.name_scope("init", values=[mu] + cov.inputs):
+        self._mu = array_ops.identity(mu, name="mu")
         self._cov = cov
-        self._mu = self._check_mu(mu)
-        self._name = name
-
-  def _check_mu(self, mu):
+        self._validate_args = validate_args  # Needed by _assert_valid_mu.
+        self._mu = self._assert_valid_mu(self._mu)
+        super(_MultivariateNormalOperatorPD, self).__init__(
+            dtype=self._mu.dtype,
+            parameters={"mu": self._mu, "cov": self._cov},
+            is_reparameterized=True,
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
+
+  def _assert_valid_mu(self, mu):
     """Return `mu` after validity checks and possibly with assertations."""
-    mu = ops.convert_to_tensor(mu)
     cov = self._cov
-
     if mu.dtype != cov.dtype:
       raise TypeError(
           "mu and cov must have the same dtype.  Found mu.dtype = %s, "
-          "cov.dtype = %s"
-          % (mu.dtype, cov.dtype))
+          "cov.dtype = %s" % (mu.dtype, cov.dtype))
 
     # Try to validate with static checks.
     mu_shape = mu.get_shape()
@@ -170,44 +172,6 @@ class _MultivariateNormalOperatorPD(distribution.Distribution):
         )
         return control_flow_ops.with_dependencies([assert_same_shape], mu)
 
-  @property
-  def validate_args(self):
-    """`Boolean` describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def allow_nan_stats(self):
-    """`Boolean` describing behavior when stats are undefined."""
-    return self._allow_nan_stats
-
-  @property
-  def dtype(self):
-    return self._mu.dtype
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time."""
-    # Recall _check_mu ensures mu and self._cov have same batch shape.
-    return self._cov.get_shape()[-1:]
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`."""
-    # Recall _check_mu ensures mu and self._cov have same batch shape.
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=self._cov.inputs):
-        return array_ops.pack([self._cov.vector_space_dimension()])
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`."""
-    # Recall _check_mu ensures mu and self._cov have same batch shape.
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=self._cov.inputs):
-        return self._cov.batch_shape()
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time."""
-    # Recall _check_mu ensures mu and self._cov have same batch shape.
-    return self._cov.get_batch_shape()
-
   @property
   def mu(self):
     return self._mu
@@ -218,23 +182,6 @@ class _MultivariateNormalOperatorPD(distribution.Distribution):
     with ops.name_scope(self.name):
       return self._cov.to_dense()
 
-  def mean(self, name="mean"):
-    """Mean of each batch member."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu]):
-        return array_ops.identity(self._mu)
-
-  def mode(self, name="mode"):
-    """Mode of each batch member."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu]):
-        return array_ops.identity(self._mu)
-
-  def variance(self, name="variance"):
-    """Variance of each batch member."""
-    with ops.name_scope(self.name):
-      return self.sigma
-
   def log_sigma_det(self, name="log_sigma_det"):
     """Log of determinant of covariance matrix."""
     with ops.name_scope(self.name):
@@ -247,161 +194,111 @@ class _MultivariateNormalOperatorPD(distribution.Distribution):
       with ops.name_scope(name, values=self._cov.inputs):
         return math_ops.exp(self._cov.log_det())
 
-  def log_prob(self, x, name="log_prob"):
-    """Log prob of observations `x` given these Multivariate Normals.
+  def _batch_shape(self):
+    return self._cov.batch_shape()
 
-    `x` is a batch vector with compatible shape if `x` is a `Tensor` whose
-    shape can be broadcast up to either:
+  def _get_batch_shape(self):
+    return self._cov.get_batch_shape()
 
-    ````
-    self.batch_shape + self.event_shape
-    OR
-    [M1,...,Mm] + self.batch_shape + self.event_shape
-    ```
+  def _event_shape(self):
+    return array_ops.pack([self._cov.vector_space_dimension()])
 
-    Args:
-      x: Compatible batch vector with same `dtype` as this distribution.
-      name: The name to give this op.
+  def _get_event_shape(self):
+    return self._cov.get_shape()[-1:]
 
-    Returns:
-      log_prob: tensor of dtype `dtype`, the log-PDFs of `x`.
-    """
+  def _sample_n(self, n, seed=None):
+    # Recall _assert_valid_mu ensures mu and self._cov have same batch shape.
+    shape = array_ops.concat(0, [self._cov.vector_shape(), [n]])
+    white_samples = random_ops.random_normal(shape=shape,
+                                             mean=0,
+                                             stddev=1,
+                                             dtype=self.dtype,
+                                             seed=seed)
+
+    correlated_samples = self._cov.sqrt_matmul(white_samples)
+
+    # Move the last dimension to the front
+    perm = array_ops.concat(0, (
+        array_ops.pack([array_ops.rank(correlated_samples) - 1]),
+        math_ops.range(0, array_ops.rank(correlated_samples) - 1)))
+
+    # TODO(ebrevdo): Once we get a proper tensor contraction op,
+    # perform the inner product using that instead of batch_matmul
+    # and this slow transpose can go away!
+    correlated_samples = array_ops.transpose(correlated_samples, perm)
+    samples = correlated_samples + self.mu
+    return samples
+
+  def _log_prob(self, x):
     # Q:  Why are shape requirements as stated above?
     # A:  The compatible shapes are precisely the ones that will broadcast to
     #     a shape compatible with self._cov.
     # See Operator base class for notes about shapes compatible with self._cov.
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, x] + self._cov.inputs):
-        x = ops.convert_to_tensor(x)
-        contrib_tensor_util.assert_same_float_dtype((self._mu, x))
-
-        # _check_mu asserts that self.mu has same batch shape as self.cov.
-        # so batch shape of self.mu = that of self._cov and self, and the
-        # batch shape of x_centered is a broadcast version of these.  If this
-        # broadcast results in a shape like
-        # [M1,...,Mm] + self.batch_shape + self.event_shape
-        # OR
-        # self.batch_shape + self.event_shape
-        # then subsequent operator calls are guaranteed to work.
-        x_centered = x - self.mu
-
-        # Compute the term x^{-1} sigma^{-1} x which appears in the exponent of
-        # the pdf.
-        x_whitened_norm = self._cov.inv_quadratic_form_on_vectors(x_centered)
-
-        log_sigma_det = self.log_sigma_det()
-
-        log_two_pi = constant_op.constant(
-            math.log(2 * math.pi), dtype=self.dtype)
-        k = math_ops.cast(self._cov.vector_space_dimension(), self.dtype)
-        log_prob_value = -(log_sigma_det + k * log_two_pi + x_whitened_norm) / 2
-
-        output_static_shape = x_centered.get_shape()[:-1]
-        log_prob_value.set_shape(output_static_shape)
-        return log_prob_value
-
-  def prob(self, x, name="prob"):
-    """The PDF of observations `x` under these Multivariate Normals.
-
-    `x` is a batch vector with compatible shape if `x` is a `Tensor` whose
-    shape can be broadcast up to either:
-
-    ````
-    self.batch_shape + self.event_shape
-    OR
-    [M1,...,Mm] + self.batch_shape + self.event_shape
-    ```
-
-    Args:
-      x: Compatible batch vector with same `dtype` as this distribution.
-      name: The name to give this op.
-
-    Returns:
-      prob: tensor of dtype `dtype`, the prob values of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, x] + self._cov.inputs):
-        return math_ops.exp(self.log_prob(x))
-
-  def entropy(self, name="entropy"):
-    """The entropies of these Multivariate Normals.
+    x = ops.convert_to_tensor(x)
+    contrib_tensor_util.assert_same_float_dtype((self._mu, x))
 
-    Args:
-      name: The name to give this op.
+    # _assert_valid_mu asserts that self.mu has same batch shape as self.cov.
+    # so batch shape of self.mu = that of self._cov and self, and the
+    # batch shape of x_centered is a broadcast version of these.  If this
+    # broadcast results in a shape like
+    # [M1,...,Mm] + self.batch_shape + self.event_shape
+    # OR
+    # self.batch_shape + self.event_shape
+    # then subsequent operator calls are guaranteed to work.
+    x_centered = x - self.mu
 
-    Returns:
-      entropy: tensor of dtype `dtype`, the entropies.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu] + self._cov.inputs):
-        log_sigma_det = self.log_sigma_det()
-        one_plus_log_two_pi = constant_op.constant(1 + math.log(2 * math.pi),
-                                                   dtype=self.dtype)
+    # Compute the term x^{-1} sigma^{-1} x which appears in the exponent of
+    # the pdf.
+    x_whitened_norm = self._cov.inv_quadratic_form_on_vectors(x_centered)
 
-        # Use broadcasting rules to calculate the full broadcast sigma.
-        k = math_ops.cast(self._cov.vector_space_dimension(), dtype=self.dtype)
-        entropy_value = (k * one_plus_log_two_pi + log_sigma_det) / 2
-        entropy_value.set_shape(log_sigma_det.get_shape())
-        return entropy_value
+    k = math_ops.cast(self._cov.vector_space_dimension(), self.dtype)
+    log_prob_value = -0.5 * (self.log_sigma_det() +
+                             k * math.log(2. * math.pi) +
+                             x_whitened_norm)
 
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the Multivariate Normal Distributions.
+    output_static_shape = x_centered.get_shape()[:-1]
+    log_prob_value.set_shape(output_static_shape)
+    return log_prob_value
 
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
-
-    Returns:
-      samples: `[n, ...]`, a `Tensor` of `n` samples for each
-        of the distributions determined by broadcasting the hyperparameters.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, n] + self._cov.inputs):
-        # Recall _check_mu ensures mu and self._cov have same batch shape.
-        broadcast_shape = self.mu.get_shape()
-        n = ops.convert_to_tensor(n, name="n")
-
-        shape = array_ops.concat(0, [self._cov.vector_shape(), [n]])
-        white_samples = random_ops.random_normal(shape=shape,
-                                                 mean=0,
-                                                 stddev=1,
-                                                 dtype=self.dtype,
-                                                 seed=seed)
+  def _prob(self, x):
+    return math_ops.exp(self.log_prob(x))
 
-        correlated_samples = self._cov.sqrt_matmul(white_samples)
+  def _entropy(self):
+    log_sigma_det = self.log_sigma_det()
+    one_plus_log_two_pi = constant_op.constant(1 + math.log(2 * math.pi),
+                                               dtype=self.dtype)
 
-        # Move the last dimension to the front
-        perm = array_ops.concat(0, (
-            array_ops.pack([array_ops.rank(correlated_samples) - 1]),
-            math_ops.range(0, array_ops.rank(correlated_samples) - 1)))
+    # Use broadcasting rules to calculate the full broadcast sigma.
+    k = math_ops.cast(self._cov.vector_space_dimension(), dtype=self.dtype)
+    entropy_value = (k * one_plus_log_two_pi + log_sigma_det) / 2
+    entropy_value.set_shape(log_sigma_det.get_shape())
+    return entropy_value
 
-        # TODO(ebrevdo): Once we get a proper tensor contraction op,
-        # perform the inner product using that instead of batch_matmul
-        # and this slow transpose can go away!
-        correlated_samples = array_ops.transpose(correlated_samples, perm)
+  def _mean(self):
+    return array_ops.identity(self._mu)
 
-        samples = correlated_samples + self.mu
+  def _variance(self):
+    return self.sigma
 
-        # Provide some hints to shape inference
-        n_val = tensor_util.constant_value(n)
-        final_shape = tensor_shape.vector(n_val).concatenate(broadcast_shape)
-        samples.set_shape(final_shape)
+  def _mode(self):
+    return array_ops.identity(self._mu)
 
-        return samples
 
-  @property
-  def is_reparameterized(self):
-    return True
+_prob_note = """
+    `x` is a batch vector with compatible shape if `x` is a `Tensor` whose
+    shape can be broadcast up to either:
 
-  @property
-  def name(self):
-    return self._name
+    ````
+    self.batch_shape + self.event_shape
+    OR
+    [M1,...,Mm] + self.batch_shape + self.event_shape
+    ```
 
-  @property
-  def is_continuous(self):
-    return True
+"""
+distribution_util.append_class_fun_doc(_MultivariateNormalOperatorPD.log_prob,
+                                       doc_str=_prob_note)
+distribution_util.append_class_fun_doc(_MultivariateNormalOperatorPD.prob,
+                                       doc_str=_prob_note)
 
 
 class MultivariateNormalDiag(_MultivariateNormalOperatorPD):
diff --git a/tensorflow/contrib/distributions/python/ops/normal.py b/tensorflow/contrib/distributions/python/ops/normal.py
index d5444991b1d..2660f2970bc 100644
--- a/tensorflow/contrib/distributions/python/ops/normal.py
+++ b/tensorflow/contrib/distributions/python/ops/normal.py
@@ -20,15 +20,14 @@ from __future__ import print_function
 
 import math
 
-from tensorflow.contrib.distributions.python.ops import distribution  # pylint: disable=line-too-long
-from tensorflow.contrib.distributions.python.ops import kullback_leibler  # pylint: disable=line-too-long
-from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util  # pylint: disable=line-too-long
+from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.contrib.distributions.python.ops import kullback_leibler
+from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util
 from tensorflow.python.framework import common_shapes
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import math_ops
@@ -108,21 +107,19 @@ class Normal(distribution.Distribution):
     Raises:
       TypeError: if mu and sigma are different dtypes.
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
     with ops.name_scope(name, values=[mu, sigma]):
-      mu = ops.convert_to_tensor(mu)
-      sigma = ops.convert_to_tensor(sigma)
       with ops.control_dependencies([check_ops.assert_positive(sigma)] if
                                     validate_args else []):
-        self._name = name
         self._mu = array_ops.identity(mu, name="mu")
         self._sigma = array_ops.identity(sigma, name="sigma")
-        self._batch_shape = common_shapes.broadcast_shape(
-            self._mu.get_shape(), self._sigma.get_shape())
-        self._event_shape = tensor_shape.TensorShape([])
-
-    contrib_tensor_util.assert_same_float_dtype((mu, sigma))
+        contrib_tensor_util.assert_same_float_dtype((self._mu, self._sigma))
+        super(Normal, self).__init__(
+            dtype=self._sigma.dtype,
+            parameters={"mu": self._mu, "sigma": self._sigma},
+            is_reparameterized=True,
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @staticmethod
   def _param_shapes(sample_shape):
@@ -141,73 +138,6 @@ class Normal(distribution.Distribution):
     """
     return {"sigma": nn.softplus}
 
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def dtype(self):
-    return self._mu.dtype
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op.
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, self._sigma]):
-        return array_ops.shape(self._mu + self._sigma)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      batch shape
-    """
-    return self._batch_shape
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
-
-    Args:
-      name: name to give to the op.
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], dtype=dtypes.int32)
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `event_shape`. May be only partially defined.
-
-    Returns:
-      event shape
-    """
-    return self._event_shape
-
   @property
   def mu(self):
     """Distribution parameter for the mean."""
@@ -218,149 +148,58 @@ class Normal(distribution.Distribution):
     """Distribution parameter for standard deviation."""
     return self._sigma
 
-  def mean(self, name="mean"):
-    """Mean of this distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._sigma, self._mu]):
-        return self._mu * array_ops.ones_like(self._sigma)
-
-  def mode(self, name="mode"):
-    """Mode of this distribution."""
-    return self.mean(name="mode")
-
-  def std(self, name="std"):
-    """Standard deviation of this distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._sigma, self._mu]):
-        return self._sigma * array_ops.ones_like(self._mu)
-
-  def variance(self, name="variance"):
-    """Variance of this distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return math_ops.square(self.std())
-
-  def log_prob(self, x, name="log_prob"):
-    """Log prob of observations in `x` under these Normal distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `mu` and `sigma`.
-      name: The name to give this op.
-
-    Returns:
-      log_prob: tensor of dtype `dtype`, the log-PDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, self._sigma, x]):
-        x = ops.convert_to_tensor(x)
-        if x.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s"
-                          % (x.dtype, self.dtype))
-        log_2_pi = constant_op.constant(math.log(2 * math.pi), dtype=self.dtype)
-        return (-0.5*log_2_pi - math_ops.log(self._sigma)
-                -0.5*math_ops.square((x - self._mu) / self._sigma))
-
-  def cdf(self, x, name="cdf"):
-    """CDF of observations in `x` under these Normal distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `mu` and `sigma`.
-      name: The name to give this op.
-
-    Returns:
-      cdf: tensor of dtype `dtype`, the CDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, self._sigma, x]):
-        x = ops.convert_to_tensor(x)
-        if x.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s"
-                          % (x.dtype, self.dtype))
-        # TODO(ebrevdo): wrap this in a Defun with a custom Defun
-        # gradient because the analytic gradient may be faster than
-        # automatic differentiation.
-        return (0.5 + 0.5*math_ops.erf(
-            1.0/(math.sqrt(2.0) * self._sigma)*(x - self._mu)))
-
-  def log_cdf(self, x, name="log_cdf"):
-    """Log CDF of observations `x` under these Normal distribution(s).
+  def _batch_shape(self):
+    return array_ops.shape(self.mu + self.sigma)
 
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `mu` and `sigma`.
-      name: The name to give this op.
+  def _get_batch_shape(self):
+    return common_shapes.broadcast_shape(
+        self._mu.get_shape(), self.sigma.get_shape())
 
-    Returns:
-      log_cdf: tensor of dtype `dtype`, the log-CDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, self._sigma, x]):
-        return math_ops.log(self.cdf(x))
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
 
-  def prob(self, x, name="prob"):
-    """The PDF of observations in `x` under these Normal distribution(s).
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
 
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `mu` and `sigma`.
-      name: The name to give this op.
+  def _sample_n(self, n, seed=None):
+    shape = array_ops.concat(0, ([n], array_ops.shape(self.mean())))
+    sampled = random_ops.random_normal(
+        shape=shape, mean=0, stddev=1, dtype=self.mu.dtype, seed=seed)
+    return sampled * self.sigma + self.mu
 
-    Returns:
-      prob: tensor of dtype `dtype`, the prob values of `x`.
-    """
-    return super(Normal, self).prob(x, name=name)
+  def _log_prob(self, x):
+    return (-0.5 * math.log(2. * math.pi) - math_ops.log(self.sigma)
+            -0.5 * math_ops.square((x - self.mu) / self.sigma))
 
-  def entropy(self, name="entropy"):
-    """The entropy of Normal distribution(s).
+  def _prob(self, x):
+    return math_ops.exp(self._log_prob(x))
 
-    Args:
-      name: The name to give this op.
+  def _log_cdf(self, x):
+    return math_ops.log(self._cdf(x))
 
-    Returns:
-      entropy: tensor of dtype `dtype`, the entropy.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, self._sigma]):
-        two_pi_e1 = constant_op.constant(
-            2 * math.pi * math.exp(1), dtype=self.dtype)
-        # Use broadcasting rules to calculate the full broadcast sigma.
-        sigma = self._sigma * array_ops.ones_like(self._mu)
-        return 0.5 * math_ops.log(two_pi_e1 * math_ops.square(sigma))
+  def _cdf(self, x):
+    # TODO(ebrevdo): wrap this in a Defun with a custom Defun
+    # gradient because the analytic gradient may be faster than
+    # automatic differentiation.
+    return (0.5 + 0.5*math_ops.erf(
+        1. / (math.sqrt(2.) * self.sigma) * (x - self.mu)))
 
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the Normal Distributions.
+  def _entropy(self):
+    # Use broadcasting rules to calculate the full broadcast sigma.
+    sigma = self.sigma * array_ops.ones_like(self.mu)
+    return 0.5 * math.log(2. * math.pi * math.e) + math_ops.log(sigma)
 
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
+  def _mean(self):
+    return self.mu * array_ops.ones_like(self.sigma)
 
-    Returns:
-      samples: `[n, ...]`, a `Tensor` of `n` samples for each
-        of the distributions determined by broadcasting the hyperparameters.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu, self._sigma, n]):
-        broadcast_shape = common_shapes.broadcast_shape(
-            self._mu.get_shape(), self._sigma.get_shape())
-        n = ops.convert_to_tensor(n, name="n")
-        shape = array_ops.concat(0, ([n], array_ops.shape(self.mean())))
-        sampled = random_ops.random_normal(
-            shape=shape, mean=0, stddev=1, dtype=self._mu.dtype, seed=seed)
-
-        # Provide some hints to shape inference
-        n_val = tensor_util.constant_value(n)
-        final_shape = tensor_shape.vector(n_val).concatenate(broadcast_shape)
-        sampled.set_shape(final_shape)
-
-        return sampled * self._sigma + self._mu
+  def _variance(self):
+    return math_ops.square(self.std())
 
-  @property
-  def is_reparameterized(self):
-    return True
+  def _std(self):
+    return self.sigma * array_ops.ones_like(self.mu)
 
-  @property
-  def is_continuous(self):
-    return True
+  def _mode(self):
+    return self._mean()
 
 
 @kullback_leibler.RegisterKL(Normal, Normal)
@@ -383,5 +222,5 @@ def _kl_normal_normal(n_a, n_b, name=None):
     s_a_squared = math_ops.square(n_a.sigma)
     s_b_squared = math_ops.square(n_b.sigma)
     ratio = s_a_squared / s_b_squared
-    return (math_ops.square(n_a.mu - n_b.mu) / (two * s_b_squared)
-            + half * (ratio - one - math_ops.log(ratio)))
+    return (math_ops.square(n_a.mu - n_b.mu) / (two * s_b_squared) +
+            half * (ratio - one - math_ops.log(ratio)))
diff --git a/tensorflow/contrib/distributions/python/ops/poisson.py b/tensorflow/contrib/distributions/python/ops/poisson.py
index 524dd13442f..778b6af33da 100644
--- a/tensorflow/contrib/distributions/python/ops/poisson.py
+++ b/tensorflow/contrib/distributions/python/ops/poisson.py
@@ -29,10 +29,6 @@ from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import control_flow_ops
 from tensorflow.python.ops import math_ops
 
-__all__ = [
-  'Poisson',
-]
-
 
 class Poisson(distribution.Distribution):
   """Poisson distribution.
@@ -68,186 +64,83 @@ class Poisson(distribution.Distribution):
         undefined statistics will return NaN for this statistic.
       name: A name for this distribution.
     """
-    with ops.name_scope(name, values=[lam]) as scope:
-      self._name = scope
-      with ops.control_dependencies(
-          [check_ops.assert_positive(lam)] if validate_args else []):
+    with ops.name_scope(name, values=[lam]):
+      with ops.control_dependencies([check_ops.assert_positive(lam)] if
+                                    validate_args else []):
         self._lam = array_ops.identity(lam, name="lam")
-        self._validate_args = validate_args
-        self._allow_nan_stats = allow_nan_stats
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def dtype(self):
-    return self._lam.dtype
+        super(Poisson, self).__init__(
+            dtype=self._lam.dtype,
+            parameters={"lam": self._lam},
+            is_continuous=False,
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def lam(self):
     """Rate parameter."""
     return self._lam
 
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  def batch_shape(self, name="batch_shape"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.lam]):
-        return array_ops.shape(self.lam)
+  def _batch_shape(self):
+    return array_ops.shape(self.lam)
 
-  def get_batch_shape(self):
+  def _get_batch_shape(self):
     return self.lam.get_shape()
 
-  def event_shape(self, name="event_shape"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], dtype=dtypes.int32)
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
 
-  def get_event_shape(self):
+  def _get_event_shape(self):
     return tensor_shape.scalar()
 
-  def log_cdf(self, x, name="log_cdf"):
-    """Log cumulative density function.
+  def _log_prob(self, x):
+    x = self._assert_valid_sample(x, check_integer=True)
+    return x * math_ops.log(self.lam) - self.lam - math_ops.lgamma(x + 1)
 
-    Args:
-      x: Non-negative floating point tensor with dtype `dtype` and whose shape
-        can be broadcast with `self.lam`.
-      name: A name for this operation.
+  def _prob(self, x):
+    return math_ops.exp(self._log_prob(x))
 
-    Returns:
-      The Log CDF of the events.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[x]):
-        return math_ops.log(self.cdf(x))
+  def _log_cdf(self, x):
+    return math_ops.log(self.cdf(x))
 
-  def cdf(self, x, name="cdf"):
-    """Cumulative density function.
+  def _cdf(self, x):
+    x = self._assert_valid_sample(x, check_integer=False)
+    return math_ops.igammac(math_ops.floor(x + 1), self.lam)
 
-    Args:
-      x: Non-negative floating point tensor with dtype `dtype` and whose shape
-        can be broadcast with `self.lam`.
-      name: A name for this operation.
-
-    Returns:
-      The CDF of the events.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.lam, x]):
-        x = self._check_x(x, check_integer=False)
-        return math_ops.igammac(math_ops.floor(x + 1), self.lam)
+  def _mean(self):
+    return array_ops.identity(self.lam)
 
-  def prob(self, x, name="prob"):
-    """Probability mass function.
+  def _variance(self):
+    return array_ops.identity(self.lam)
 
-    Args:
-      x: Non-negative floating point tensor with dtype `dtype` and whose shape
-        can be broadcast with `self.lam`. `x` is only legal if it is
-        non-negative and its components are equal to integer values.
-      name: A name for this operation.
-
-    Returns:
-      The probabilities of the events.
-    """
-    return super(Poisson, self).prob(x, name)
-
-  def log_prob(self, x, name="log_prob"):
-    """Log probability mass function.
-
-    Args:
-      x: Non-negative floating point tensor with dtype `dtype` and whose shape
-        can be broadcast with `self.lam`. `x` is only legal if it is
-        non-negative and its components are equal to integer values.
-      name: A name for this operation (optional).
-
-    Returns:
-      The log-probabilities of the events.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.lam, x]):
-        x = self._check_x(x, check_integer=True)
-        return x * math_ops.log(self.lam) - self.lam - math_ops.lgamma(x + 1)
-
-  def mean(self, name="mean"):
-    """Mean of the distribution.
-
-    Args:
-      name: Name for the op.
-
-    Returns:
-      mean: `Tensor` of the same type and shape as `lam`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.lam]):
-        return array_ops.identity(self.lam)
+  def _std(self):
+    return math_ops.sqrt(self.variance())
 
-  def variance(self, name="variance"):
-    """Variance of the distribution.
+  def _mode(self):
+    return math_ops.floor(self.lam)
 
-    Args:
-      name: Name for the op.
-
-    Returns:
-      variance: `Tensor` of the same type and shape as `lam`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.lam]):
-        return array_ops.identity(self.lam)
+  def _assert_valid_sample(self, x, check_integer=True):
+    if not self.validate_args: return x
+    with ops.name_scope('check_x', values=[x]):
+      dependencies = [check_ops.assert_non_negative(x)]
+      if check_integer:
+        dependencies += [distribution_util.assert_integer_form(
+            x, message="x has non-integer components.")]
+      return control_flow_ops.with_dependencies(dependencies, x)
 
-  def std(self, name="std"):
-    """Standard deviation of the distribution.
 
-    Args:
-      name: Name for the op.
+_prob_note = """
 
-    Returns:
-      std: `Tensor` of the same type and shape as `lam`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.lam]):
-        return math_ops.sqrt(self.variance())
+    Note thet the input value must be a non-negative floating point tensor with
+    dtype `dtype` and whose shape can be broadcast with `self.lam`. `x` is only
+    legal if it is non-negative and its components are equal to integer values.
+"""
+distribution_util.append_class_fun_doc(Poisson.log_prob, doc_str=_prob_note)
+distribution_util.append_class_fun_doc(Poisson.prob, doc_str=_prob_note)
 
-  def mode(self, name="mode"):
-    """Mode of the distribution.
+distribution_util.append_class_fun_doc(Poisson.mode, doc_str="""
 
     Note that when `lam` is an integer, there are actually two modes.
     Namely, `lam` and `lam - 1` are both modes. Here we return
     only the larger of the two modes.
-
-    Args:
-      name: Name for the op.
-
-    Returns:
-      mode: `Tensor` of the same type and shape as `lam`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.lam]):
-        return math_ops.floor(self.lam)
-
-  @property
-  def is_continuous(self):
-    return False
-
-  @property
-  def is_reparameterized(self):
-    return False
-
-  def _check_x(self, x, check_integer=True):
-    with ops.name_scope('check_x', values=[x]):
-      x = ops.convert_to_tensor(x, name="x")
-      if not self.validate_args:
-        return x
-      dependencies = [check_ops.assert_non_negative(x)]
-      if check_integer:
-        dependencies += [distribution_util.assert_integer_form(
-            x, message="x has non-integer components.")]
-      return control_flow_ops.with_dependencies(dependencies, x)
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/student_t.py b/tensorflow/contrib/distributions/python/ops/student_t.py
index dfcc2f2c3fe..7ed661e0cb5 100644
--- a/tensorflow/contrib/distributions/python/ops/student_t.py
+++ b/tensorflow/contrib/distributions/python/ops/student_t.py
@@ -19,16 +19,15 @@ from __future__ import division
 from __future__ import print_function
 
 import math
-
 import numpy as np
 
-from tensorflow.contrib.distributions.python.ops import distribution  # pylint: disable=line-too-long
-from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util  # pylint: disable=line-too-long
+from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.contrib.distributions.python.ops import distribution_util
+from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util
 from tensorflow.python.framework import common_shapes
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import control_flow_ops
@@ -119,39 +118,23 @@ class StudentT(distribution.Distribution):
     Raises:
       TypeError: if mu and sigma are different dtypes.
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
-    with ops.name_scope(name, values=[df, mu, sigma]) as scope:
-      with ops.control_dependencies([check_ops.assert_positive(
-          df), check_ops.assert_positive(sigma)] if validate_args else []):
-        self._df = ops.convert_to_tensor(df, name="df")
-        self._mu = ops.convert_to_tensor(mu, name="mu")
-        self._sigma = ops.convert_to_tensor(sigma, name="sigma")
+    with ops.name_scope(name, values=[df, mu, sigma]):
+      with ops.control_dependencies([
+          check_ops.assert_positive(df),
+          check_ops.assert_positive(sigma),
+      ] if validate_args else []):
+        self._df = array_ops.identity(df, name="df")
+        self._mu = array_ops.identity(mu, name="mu")
+        self._sigma = array_ops.identity(sigma, name="sigma")
         contrib_tensor_util.assert_same_float_dtype(
             (self._df, self._mu, self._sigma))
-      self._name = scope
-      self._get_batch_shape = common_shapes.broadcast_shape(
-          self._sigma.get_shape(), common_shapes.broadcast_shape(
-              self._df.get_shape(), self._mu.get_shape()))
-      self._get_event_shape = tensor_shape.TensorShape([])
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def dtype(self):
-    return self._df.dtype
+        super(StudentT, self).__init__(
+            dtype=self._sigma.dtype,
+            parameters={"df": self._df, "mu": self._mu, "sigma": self._sigma},
+            is_reparameterized=True,
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def df(self):
@@ -168,41 +151,125 @@ class StudentT(distribution.Distribution):
     """Scaling factors of these Student's t distribution(s)."""
     return self._sigma
 
-  def mean(self, name="mean"):
-    """Mean of the distribution.
+  def _batch_shape(self):
+    return array_ops.shape(self.df + self.mu + self.sigma)
+
+  def _get_batch_shape(self):
+    return common_shapes.broadcast_shape(
+        self.sigma.get_shape(),
+        common_shapes.broadcast_shape(
+            self.df.get_shape(),
+            self.mu.get_shape()))
+
+  def _event_shape(self):
+    return constant_op.constant([], dtype=math_ops.int32)
+
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
+
+  def _sample_n(self, n, seed=None):
+    # We use 2 uniform random floats to generate polar random variates.
+    # http://dl.acm.org/citation.cfm?id=179631
+    # Theorem 2. Let G, H be iid variates, uniformly distributed on [0,1].
+    # Let theta = 2*pi*H, let R = sqrt(df*(G^(-2/df) - 1)) for df > 0.
+    # Let X = R*cos(theta), and let Y = R*sin(theta).
+    # Then X ~ t_df and Y ~ t_df.
+    # The variates X and Y are not independent.
+    shape = array_ops.concat(0, ([2, n], self.batch_shape()))
+    uniform = random_ops.random_uniform(shape=shape,
+                                        dtype=self.dtype,
+                                        seed=seed)
+    samples_g, samples_h = array_ops.unpack(uniform, num=2)
+    theta = (2. * math.pi) * samples_h
+    r = math_ops.sqrt(self.df *
+                      (math_ops.pow(samples_g, -2 / self.df) - 1))
+    samples = r * math_ops.cos(theta)
+    return samples * self.sigma + self.mu
+
+  def _log_prob(self, x):
+    y = (x - self.mu) / self.sigma
+    half_df = 0.5 * self.df
+    return (math_ops.lgamma(0.5 + half_df) -
+            math_ops.lgamma(half_df) -
+            0.5 * math_ops.log(self.df) -
+            0.5 * math.log(math.pi) -
+            math_ops.log(self.sigma) -
+            (0.5 + half_df) * math_ops.log(1. + math_ops.square(y) / self.df))
+
+  def _prob(self, x):
+    y = (x - self.mu) / self.sigma
+    half_df = 0.5 * self.df
+    return (math_ops.exp(math_ops.lgamma(0.5 + half_df) -
+                         math_ops.lgamma(half_df)) /
+            (math_ops.sqrt(self.df) * math.sqrt(math.pi) * self.sigma) *
+            math_ops.pow(1. + math_ops.square(y) / self.df, -(0.5 + half_df)))
+
+  def _entropy(self):
+    u = array_ops.expand_dims(self.df * self._ones(), -1)
+    v = array_ops.expand_dims(self._ones(), -1)
+    beta_arg = array_ops.concat(len(u.get_shape()) - 1, [u, v]) / 2
+    half_df = 0.5 * self.df
+    return ((0.5 + half_df) * (math_ops.digamma(0.5 + half_df) -
+                               math_ops.digamma(half_df)) +
+            0.5 * math_ops.log(self.df) +
+            special_math_ops.lbeta(beta_arg) +
+            math_ops.log(self.sigma))
+
+  def _mean(self):
+    mean = self.mu * self._ones()
+    if self.allow_nan_stats:
+      nan = np.array(np.nan, dtype=self.dtype.as_numpy_dtype())
+      return math_ops.select(
+          math_ops.greater(self.df, self._ones()), mean,
+          array_ops.fill(self.batch_shape(), nan, name="nan"))
+    else:
+      return control_flow_ops.with_dependencies([
+          check_ops.assert_less(
+              array_ops.ones((), dtype=self.dtype), self.df,
+              message="mean not defined for components of df <= 1"),
+      ], mean)
+
+  def _variance(self):
+    var = (self._ones() *
+           math_ops.square(self.sigma) * self.df / (self.df - 2))
+    # When 1 < df <= 2, variance is infinite.
+    inf = np.array(np.inf, dtype=self.dtype.as_numpy_dtype())
+    result_where_defined = math_ops.select(
+        math_ops.greater(self.df, array_ops.fill(self.batch_shape(), 2.)),
+        var,
+        array_ops.fill(self.batch_shape(), inf, name="inf"))
+
+    if self.allow_nan_stats:
+      nan = np.array(np.nan, dtype=self.dtype.as_numpy_dtype())
+      return math_ops.select(
+          math_ops.greater(self.df, self._ones()),
+          result_where_defined,
+          array_ops.fill(self.batch_shape(), nan, name="nan"))
+    else:
+      return control_flow_ops.with_dependencies([
+          check_ops.assert_less(
+              array_ops.ones((), dtype=self.dtype), self.df,
+              message="variance not defined for components of df <= 1"),
+      ], result_where_defined)
+
+  def _std(self):
+    return math_ops.sqrt(self.variance())
+
+  def _mode(self):
+    return array_ops.identity(self.mu)
+
+  def _ones(self):
+    return array_ops.ones(self.batch_shape(), dtype=self.dtype)
+
+
+distribution_util.append_class_fun_doc(StudentT.mean, doc_str="""
 
     The mean of Student's T equals `mu` if `df > 1`, otherwise it is `NaN`.  If
     `self.allow_nan_stats=False`, then an exception will be raised rather than
     returning `NaN`.
+""")
 
-    Args:
-      name:  A name to give this op.
-
-    Returns:
-      The mean for every batch member, a `Tensor` with same `dtype` as self.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu]):
-        result_if_defined = self._mu * self._ones()
-        if self.allow_nan_stats:
-          df_gt_1 = self._df > self._ones()
-          nan = np.nan + self._zeros()
-          return math_ops.select(df_gt_1, result_if_defined, nan)
-        else:
-          one = constant_op.constant(1.0, dtype=self.dtype)
-          return control_flow_ops.with_dependencies(
-              [check_ops.assert_less(
-                  one, self._df,
-                  message="mean not defined for components of df <= 1"
-              )], result_if_defined)
-
-  def mode(self, name="mode"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._mu]):
-        return array_ops.identity(self._mu)
-
-  def variance(self, name="variance"):
-    """Variance of the distribution.
+distribution_util.append_class_fun_doc(StudentT.variance, doc_str="""
 
     Variance for Student's T equals
 
@@ -212,180 +279,4 @@ class StudentT(distribution.Distribution):
     NaN, when df <= 1
     ```
 
-    The NaN state occurs because mean is undefined for `df <= 1`, and if
-    `self.allow_nan_stats` is `False`, an exception will be raised if any batch
-    members fall into this state.
-
-    Args:
-      name:  A name for this op.
-
-    Returns:
-      The variance for every batch member, a `Tensor` with same `dtype` as self.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._df, self._sigma]):
-        result_where_finite = (
-            self._zeros()
-            + math_ops.square(self._sigma) * self._df / (self._df - 2))
-        # When 1 < df <= 2, variance is infinite.
-        result_where_defined = math_ops.select(
-            self._zeros() + self._df > 2,
-            result_where_finite,
-            self._zeros() + np.inf)
-
-        if self.allow_nan_stats:
-          return math_ops.select(
-              (self._zeros() + self._df > 1),
-              result_where_defined,
-              self._zeros() + np.nan)
-        else:
-          one = constant_op.constant(1.0, dtype=self.dtype)
-          return control_flow_ops.with_dependencies(
-              [check_ops.assert_less(
-                  one, self._df,
-                  message="variance not defined for components of df <= 1"
-              )], result_where_defined)
-
-  def std(self, name="std"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return math_ops.sqrt(self.variance())
-
-  def batch_shape(self, name="batch_shape"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return array_ops.shape(self._ones())
-
-  def get_batch_shape(self):
-    return self._get_batch_shape
-
-  def event_shape(self, name="event_shape"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], dtype=math_ops.int32)
-
-  def get_event_shape(self):
-    return self._event_shape
-
-  def log_prob(self, x, name="log_prob"):
-    """Log prob of observations in `x` under these Student's t-distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `mu` and `df`.
-      name: The name to give this op.
-
-    Returns:
-      log_prob: tensor of dtype `dtype`, the log-PDFs of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._df, self._mu, self._sigma, x]):
-        x = ops.convert_to_tensor(x)
-        if x.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s" %
-                          (x.dtype, self.dtype))
-        df_2 = self._df / 2
-        log_beta = (math_ops.lgamma(0.5) + math_ops.lgamma(df_2) -
-                    math_ops.lgamma(0.5 + df_2))
-        return (-math_ops.log(self._df) / 2 - log_beta - (self._df + 1) / 2 *
-                math_ops.log(1 + math_ops.square((x - self._mu) / self._sigma) /
-                             self._df) - math_ops.log(self._sigma))
-
-  def prob(self, x, name="prob"):
-    """The PDF of observations in `x` under these Student's t distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `df`, `mu`, and
-        `sigma`.
-      name: The name to give this op.
-
-    Returns:
-      prob: tensor of dtype `dtype`, the prob values of `x`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._df, self._mu, self._sigma, x]):
-        x = ops.convert_to_tensor(x)
-        if x.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s" %
-                          (x.dtype, self.dtype))
-        reloc_scaled = (x - self._mu) / self._sigma
-        return (math_ops.exp(math_ops.lgamma((self._df + 1) / 2) -
-                             math_ops.lgamma(self._df / 2)) /
-                math_ops.sqrt(self._df) / math.sqrt(np.pi) *
-                math_ops.pow(1 + math_ops.square(reloc_scaled) / self._df,
-                             -(self._df + 1) / 2) / self.sigma)
-
-  def entropy(self, name="entropy"):
-    """The entropy of Student t distribution(s).
-
-    Args:
-      name: The name to give this op.
-
-    Returns:
-      entropy: tensor of dtype `dtype`, the entropy.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._df, self._sigma]):
-        u = array_ops.expand_dims(self._df + self._zeros(), -1)
-        v = array_ops.expand_dims(self._ones(), -1)
-        beta_arg = array_ops.concat(len(u.get_shape()) - 1, [u, v]) / 2
-        return ((self._df + 1) / 2 * (math_ops.digamma((self._df + 1) / 2) -
-                                      math_ops.digamma(self._df / 2)) +
-                math_ops.log(self._df) / 2 +
-                special_math_ops.lbeta(beta_arg) +
-                math_ops.log(self._sigma))
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the Student t Distributions.
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
-
-    Returns:
-      samples: a `Tensor` of shape `(n,) + self.batch_shape + self.event_shape`
-          with values of type `self.dtype`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._df, self._mu, self._sigma, n]):
-        n = ops.convert_to_tensor(n, name="n")
-        n_val = tensor_util.constant_value(n)
-
-        # We use 2 uniform random floats to generate polar random variates.
-        # http://dl.acm.org/citation.cfm?id=179631
-        # Theorem 2. Let G, H be iid variates, uniformly distributed on [0,1].
-        # Let theta = 2*pi*H, let R = sqrt(df*(G^(-2/df) - 1)) for df > 0.
-        # Let X = R*cos(theta), and let Y = R*sin(theta).
-        # Then X ~ t_df and Y ~ t_df.
-        # The variates X and Y are not independent.
-        shape = array_ops.concat(0, ([2, n], self.batch_shape()))
-        uniform = random_ops.random_uniform(shape=shape,
-                                            dtype=self.dtype,
-                                            seed=seed)
-        samples_g, samples_h = array_ops.unpack(uniform, num=2)
-        theta = (2 * np.pi) * samples_h
-        r = math_ops.sqrt(self._df *
-                          (math_ops.pow(samples_g, -2 / self._df) - 1))
-        samples = r * math_ops.cos(theta)
-
-        # Provide some hints to shape inference
-        inferred_shape = tensor_shape.vector(n_val).concatenate(
-            self.get_batch_shape())
-        samples.set_shape(inferred_shape)
-
-        return samples * self._sigma + self._mu
-
-  @property
-  def is_reparameterized(self):
-    return True
-
-  def _ones(self):
-    return array_ops.ones_like(self._df + self._mu + self._sigma)
-
-  def _zeros(self):
-    return array_ops.zeros_like(self._df + self._mu + self._sigma)
-
-  @property
-  def is_continuous(self):
-    return True
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/transformed_distribution.py b/tensorflow/contrib/distributions/python/ops/transformed_distribution.py
index 657700e9ed9..68a19cd9ebc 100644
--- a/tensorflow/contrib/distributions/python/ops/transformed_distribution.py
+++ b/tensorflow/contrib/distributions/python/ops/transformed_distribution.py
@@ -17,8 +17,10 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from tensorflow.contrib.distributions.python.ops import distribution  # pylint: disable=line-too-long
+from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.contrib.distributions.python.ops import distribution_util
 from tensorflow.python.framework import ops
+from tensorflow.python.ops import math_ops
 
 
 class TransformedDistribution(distribution.Distribution):
@@ -85,70 +87,18 @@ class TransformedDistribution(distribution.Distribution):
       TypeError: if `base_dist_cls` is not a subclass of
           `Distribution`.
     """
-    if not issubclass(base_dist_cls, distribution.Distribution):
-      raise TypeError("base_dist_cls must be a subclass of Distribution.")
-    with ops.name_scope(name, values=base_dist_args.values()) as scope:
-      self._name = scope
+    with ops.name_scope(name, values=base_dist_args.values()):
       self._base_dist = base_dist_cls(**base_dist_args)
-    self._transform = transform
-    self._inverse = inverse
-    self._log_det_jacobian = log_det_jacobian
-    self._inverse_cache = {}
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def dtype(self):
-    return self._base_dist.dtype
-
-  def batch_shape(self, name="batch_shape"):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`.
-
-    The product of the dimensions of the `batch_shape` is the number of
-    independent distributions of this kind the instance represents.
-
-    Args:
-      name: name to give to the op.
-
-    Returns:
-      `Tensor` `batch_shape`
-    """
-    with ops.name_scope(self.name):
-      return self._base_dist.batch_shape(name)
-
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `batch_shape`. May be only partially defined.
-
-    Returns:
-      batch shape
-    """
-    return self._base_dist.get_batch_shape()
-
-  def event_shape(self, name="event_shape"):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`.
-
-    Args:
-      name: name to give to the op.
-
-    Returns:
-      `Tensor` `event_shape`
-    """
-    with ops.name_scope(self.name):
-      return self._base_dist.event_shape(name)
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time.
-
-    Same meaning as `event_shape`. May be only partially defined.
-
-    Returns:
-      event shape
-    """
-    return self._base_dist.get_event_shape()
+      self._transform = transform
+      self._inverse = inverse
+      self._log_det_jacobian = log_det_jacobian
+      self._inverse_cache = {}
+      super(TransformedDistribution, self).__init__(
+          dtype=self._base_dist.dtype,
+          is_reparameterized=self._base_dist.is_reparameterized,
+          validate_args=self._base_dist.validate_args,
+          allow_nan_stats=self._base_dist.allow_nan_stats,
+          name=name)
 
   @property
   def base_distribution(self):
@@ -170,88 +120,74 @@ class TransformedDistribution(distribution.Distribution):
     """Function computing the log determinant of the Jacobian of transform."""
     return self._log_det_jacobian
 
-  def log_prob(self, y, name="log_prob"):
-    """Log prob of observations in `y`.
+  def _batch_shape(self):
+    return self.base_distribution.batch_shape()
 
-    `log ( p(g(y)) / det|J(g(y))| )`, where `g` is the inverse of `transform`.
+  def _get_batch_shape(self):
+    return self.base_distribution.get_batch_shape()
 
-    Args:
-      y: tensor of dtype `dtype`.
-      name: The name to give this op.
+  def _event_shape(self):
+    return self.base_distribution.event_shape()
 
-    Returns:
-      log_pdf: tensor of dtype `dtype`, the log-PDFs of `y`.
+  def _get_event_shape(self):
+    return self.base_distribution.get_event_shape()
 
-    Raises:
-      ValueError: if `inverse` was not provided to the distribution and `y` was
-          not returned from `sample`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[y]):
-        y = ops.convert_to_tensor(y)
-        if y.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s" %
-                          (y.dtype, self.dtype))
-        with ops.name_scope("inverse"):
-          if y in self._inverse_cache:
-            x = self._inverse_cache[y]
-          elif self._inverse:
-            x = self._inverse(y)
-          else:
-            raise ValueError("No inverse function exists and input `y` was not "
-                             "returned from `sample`.")
-        with ops.name_scope("log_det_jacobian"):
-          log_det_jacobian = self._log_det_jacobian(x)
-        return self._base_dist.log_prob(x) - log_det_jacobian
-
-  def prob(self, y, name="prob"):
-    """The prob of observations in `y`.
-
-    `p(g(y)) / det|J(g(y))|`, where `g` is the inverse of `transform`.
+  def _sample_n(self, n, seed=None):
+    samples = self.base_distribution.sample_n(n=n, seed=seed)
+    with ops.name_scope("transform"):
+      transformed = self.transform(samples)
+      self._inverse_cache[transformed] = samples
+      return transformed
 
-    Args:
-      y: `Tensor` of dtype `dtype`.
-      name: The name to give this op.
+  def _log_prob(self, y):
+    y = ops.convert_to_tensor(y, name="y")
+    with ops.name_scope("inverse"):
+      if y in self._inverse_cache:
+        x = self._inverse_cache[y]
+      elif self.inverse:
+        x = self.inverse(y)
+      else:
+        raise ValueError("No inverse function exists and input `y` was not "
+                         "returned from `sample`.")
+    with ops.name_scope("log_det_jacobian"):
+      log_det_jacobian = self.log_det_jacobian(x)
+    return self.base_distribution.log_prob(x) - log_det_jacobian
 
-    Returns:
-      pdf: `Tensor` of dtype `dtype`, the pdf values of `y`.
-    """
-    return super(TransformedDistribution, self).prob(y, name=name)
+  def _prob(self, y):
+    return math_ops.exp(self._log_prob(y))
 
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations.
+
+distribution_util.append_class_fun_doc(TransformedDistribution.batch_shape,
+                                       doc_str="""
+
+    The product of the dimensions of the `batch_shape` is the number of
+    independent distributions of this kind the instance represents.
+
+""")
+
+distribution_util.append_class_fun_doc(TransformedDistribution.sample_n,
+                                       doc_str="""
 
     Samples from the base distribution and then passes through the transform.
+""")
 
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
+distribution_util.append_class_fun_doc(TransformedDistribution.log_prob,
+                                       doc_str="""
 
-    Returns:
-      samples: `[n, ...]`, a `Tensor` of `n` samples.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        samples = self._base_dist.sample_n(n=n, seed=seed)
-        with ops.name_scope("transform"):
-          transformed = self._transform(samples)
-          self._inverse_cache[transformed] = samples
-          return transformed
+  `(log o p o g)(y) - (log o det o J o g)(y)`,
+  where `g` is the inverse of `transform`.
 
-  @property
-  def is_reparameterized(self):
-    return self._base_dist.is_reparameterized
+  Raises:
+    ValueError: if `inverse` was not provided to the distribution and `y` was
+        not returned from `sample`.
+""")
 
-  @property
-  def allow_nan_stats(self):
-    return self._base_dist.allow_nan_stats
+distribution_util.append_class_fun_doc(TransformedDistribution.prob,
+                                       doc_str="""
 
-  @property
-  def validate_args(self):
-    return self._base_dist.validate_args
+  `p(g(y)) / det|J(g(y))|`, where `g` is the inverse of `transform`.
 
-  @property
-  def is_continuous(self):
-    return True
+  Raises:
+    ValueError: if `inverse` was not provided to the distribution and `y` was
+        not returned from `sample`.
+""")
diff --git a/tensorflow/contrib/distributions/python/ops/uniform.py b/tensorflow/contrib/distributions/python/ops/uniform.py
index f5d4e3e48f9..0df823315f0 100644
--- a/tensorflow/contrib/distributions/python/ops/uniform.py
+++ b/tensorflow/contrib/distributions/python/ops/uniform.py
@@ -18,14 +18,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from tensorflow.contrib.distributions.python.ops import distribution  # pylint: disable=line-too-long
-from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util  # pylint: disable=line-too-long
+import math
+
+from tensorflow.contrib.distributions.python.ops import distribution
+from tensorflow.contrib.framework.python.framework import tensor_util as contrib_tensor_util
 from tensorflow.python.framework import common_shapes
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
 from tensorflow.python.framework import tensor_shape
-from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import check_ops
 from tensorflow.python.ops import math_ops
@@ -39,8 +40,8 @@ class Uniform(distribution.Distribution):
   """
 
   def __init__(self,
-               a=0.0,
-               b=1.0,
+               a=0.,
+               b=1.,
                validate_args=True,
                allow_nan_stats=False,
                name="Uniform"):
@@ -81,57 +82,19 @@ class Uniform(distribution.Distribution):
     Raises:
       InvalidArgumentError: if `a >= b` and `validate_args=True`.
     """
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
-    with ops.name_scope(name, values=[a, b]):
-      with ops.control_dependencies([check_ops.assert_less(
-          a, b, message="uniform not defined when a > b.")] if validate_args
-                                    else []):
-        a = array_ops.identity(a, name="a")
-        b = array_ops.identity(b, name="b")
-
-    self._a = a
-    self._b = b
-    self._name = name
-    self._batch_shape = common_shapes.broadcast_shape(
-        self._a.get_shape(), self._b.get_shape())
-    self._event_shape = tensor_shape.TensorShape([])
-
-    contrib_tensor_util.assert_same_float_dtype((a, b))
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    return self._name
-
-  @property
-  def dtype(self):
-    return self.a.dtype
-
-  def batch_shape(self, name="batch_shape"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._a, self._b]):
-        return array_ops.shape(self._a + self._b)
-
-  def get_batch_shape(self):
-    return self._batch_shape
-
-  def event_shape(self, name="event_shape"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name):
-        return constant_op.constant([], dtype=dtypes.int32)
-
-  def get_event_shape(self):
-    return self._event_shape
+    with ops.control_dependencies([
+        check_ops.assert_less(a, b, message="uniform not defined when a > b.")
+    ] if validate_args else []):
+      self._a = array_ops.identity(a, name="a")
+      self._b = array_ops.identity(b, name="b")
+      contrib_tensor_util.assert_same_float_dtype((self._a, self._b))
+      super(Uniform, self).__init__(
+          dtype=self._a.dtype,
+          parameters={"a": self._a, "b": self._b},
+          is_reparameterized=True,
+          validate_args=validate_args,
+          allow_nan_stats=allow_nan_stats,
+          name=name)
 
   @property
   def a(self):
@@ -141,138 +104,65 @@ class Uniform(distribution.Distribution):
   def b(self):
     return self._b
 
-  def prob(self, x, name="prob"):
-    """The PDF of observations in `x` under these Uniform distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `a` and `b`.
-      name: The name to give this op.
-
-    Returns:
-      prob: tensor of dtype `dtype`, the prob values of `x`. If `x` is `nan`,
-          will return `nan`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.a, self.b, x]):
-        x = ops.convert_to_tensor(x, name="x")
-        if x.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s" %
-                          (x.dtype, self.dtype))
-
-        broadcasted_x = x * self._ones()
-        return math_ops.select(
-            math_ops.is_nan(broadcasted_x), broadcasted_x, math_ops.select(
-                math_ops.logical_or(broadcasted_x < self.a,
-                                    broadcasted_x > self.b),
-                array_ops.zeros_like(broadcasted_x),
-                (1.0 / self.range()) * array_ops.ones_like(broadcasted_x)))
-
-  def log_prob(self, x, name="log_prob"):
-    return super(Uniform, self).log_prob(x, name)
-
-  def cdf(self, x, name="cdf"):
-    """CDF of observations in `x` under these Uniform distribution(s).
-
-    Args:
-      x: tensor of dtype `dtype`, must be broadcastable with `a` and `b`.
-      name: The name to give this op.
-
-    Returns:
-      cdf: tensor of dtype `dtype`, the CDFs of `x`. If `x` is `nan`, will
-          return `nan`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.a, self.b, x]):
-        x = ops.convert_to_tensor(x, name="x")
-        if x.dtype != self.dtype:
-          raise TypeError("Input x dtype does not match dtype: %s vs. %s" %
-                          (x.dtype, self.dtype))
-
-        broadcasted_x = x * self._ones()
-        zeros = array_ops.zeros_like(x + self.a + self.b, dtype=self.dtype)
-        ones = array_ops.ones_like(x + self.a + self.b, dtype=self.dtype)
-        result_if_not_big = math_ops.select(
-            x < self.a, zeros, (broadcasted_x - self.a) / self.range())
-        return math_ops.select(x >= self.b, ones, result_if_not_big)
-
-  def log_cdf(self, x, name="log_cdf"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.a, self.b, x]):
-        x = ops.convert_to_tensor(x, name="x")
-        return math_ops.log(self.cdf(x))
-
-  def entropy(self, name="entropy"):
-    """The entropy of Uniform distribution(s).
-
-    Args:
-      name: The name to give this op.
-
-    Returns:
-      entropy: tensor of dtype `dtype`, the entropy.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.a, self.b, self.range()]):
-        return math_ops.log(self.range())
-
-  def sample_n(self, n, seed=None, name="sample_n"):
-    """Sample `n` observations from the Uniform Distributions.
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer, the random seed.
-      name: The name to give this op.
-
-    Returns:
-      samples: a `Tensor` of shape `(n,) + self.batch_shape + self.event_shape`
-          with values of type `self.dtype`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.a, self.b, n]):
-        n = ops.convert_to_tensor(n, name="n")
-        n_val = tensor_util.constant_value(n)
-
-        shape = array_ops.concat(0, ([n], self.batch_shape()))
-        samples = random_ops.random_uniform(shape=shape,
-                                            dtype=self.dtype,
-                                            seed=seed)
-
-        # Provide some hints to shape inference
-        inferred_shape = tensor_shape.vector(n_val).concatenate(
-            self.get_batch_shape())
-        samples.set_shape(inferred_shape)
-
-        return (array_ops.expand_dims(self.a, 0) + array_ops.expand_dims(
-            self.range(), 0) * samples)
-
-  def mean(self, name="mean"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self._a, self._b]):
-        return (self.a + self.b) / 2
-
-  def variance(self, name="variance"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.range()]):
-        return math_ops.square(self.range()) / 12.
-
-  def std(self, name="std"):
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.range()]):
-        return self.range() / math_ops.sqrt(12.)
-
   def range(self, name="range"):
     """`b - a`."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[self.a, self.b]):
-        return self.b - self.a
+    with self._name_scope(name):
+      return self.b - self.a
 
-  @property
-  def is_reparameterized(self):
-    return True
+  def _batch_shape(self):
+    return array_ops.shape(self._a + self._b)
 
-  def _ones(self):
-    return array_ops.ones_like(self.a + self.b)
+  def _get_batch_shape(self):
+    return common_shapes.broadcast_shape(
+        self._a.get_shape(), self._b.get_shape())
 
-  @property
-  def is_continuous(self):
-    return True
+  def _event_shape(self):
+    return constant_op.constant([], dtype=dtypes.int32)
+
+  def _get_event_shape(self):
+    return tensor_shape.scalar()
+
+  def _sample_n(self, n, seed=None):
+    shape = array_ops.concat(0, ([n], self.batch_shape()))
+    samples = random_ops.random_uniform(shape=shape,
+                                        dtype=self.dtype,
+                                        seed=seed)
+    return (array_ops.expand_dims(self.a, 0) +
+            array_ops.expand_dims(self.range(), 0) * samples)
+
+  def _log_prob(self, x):
+    return math_ops.log(self._prob(x))
+
+  def _prob(self, x):
+    broadcasted_x = x * array_ops.ones(self.batch_shape())
+    return math_ops.select(
+        math_ops.is_nan(broadcasted_x),
+        broadcasted_x,
+        math_ops.select(
+            math_ops.logical_or(broadcasted_x < self.a,
+                                broadcasted_x > self.b),
+            array_ops.zeros_like(broadcasted_x),
+            (1. / self.range()) * array_ops.ones_like(broadcasted_x)))
+
+  def _log_cdf(self, x):
+    return math_ops.log(self.cdf(x))
+
+  def _cdf(self, x):
+    broadcasted_x = x * array_ops.ones(self.batch_shape())
+    zeros = array_ops.zeros_like(x + self.a + self.b, dtype=self.dtype)
+    ones = array_ops.ones_like(x + self.a + self.b, dtype=self.dtype)
+    result_if_not_big = math_ops.select(
+        x < self.a, zeros, (broadcasted_x - self.a) / self.range())
+    return math_ops.select(x >= self.b, ones, result_if_not_big)
+
+  def _entropy(self):
+    return math_ops.log(self.range())
+
+  def _mean(self):
+    return (self.a + self.b) / 2.
+
+  def _variance(self):
+    return math_ops.square(self.range()) / 12.
+
+  def _std(self):
+    return self.range() / math.sqrt(12.)
diff --git a/tensorflow/contrib/distributions/python/ops/wishart.py b/tensorflow/contrib/distributions/python/ops/wishart.py
index 0c827202837..3ffcf531e0b 100644
--- a/tensorflow/contrib/distributions/python/ops/wishart.py
+++ b/tensorflow/contrib/distributions/python/ops/wishart.py
@@ -28,7 +28,6 @@ from tensorflow.contrib.framework.python.framework import tensor_util as contrib
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import ops
-from tensorflow.python.framework import tensor_shape
 from tensorflow.python.framework import tensor_util
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import check_ops
@@ -72,9 +71,9 @@ class _WishartOperatorPD(distribution.Distribution):
                df,
                scale_operator_pd,
                cholesky_input_output_matrices=False,
-               allow_nan_stats=False,
                validate_args=True,
-               name='Wishart'):
+               allow_nan_stats=False,
+               name=None):
     """Construct Wishart distributions.
 
     Args:
@@ -86,13 +85,13 @@ class _WishartOperatorPD(distribution.Distribution):
         Cholesky factored matrix. Example`log_pdf` input takes a Cholesky and
         `sample_n` returns a Cholesky when
         `cholesky_input_output_matrices=True`.
+      validate_args: Whether to validate input with asserts. If `validate_args`
+        is `False`, and the inputs are invalid, correct behavior is not
+        guaranteed.
       allow_nan_stats:  `Boolean`, default `False`. If `False`, raise an
         exception if a statistic (e.g., mean, mode) is undefined for any batch
         member. If True, batch members with valid parameters leading to
         undefined statistics will return `NaN` for this statistic.
-      validate_args: Whether to validate input with asserts. If `validate_args`
-        is `False`, and the inputs are invalid, correct behavior is not
-        guaranteed.
       name: The name to give Ops created by the initializer.
 
     Raises:
@@ -100,31 +99,29 @@ class _WishartOperatorPD(distribution.Distribution):
       TypeError: if scale.dtype != df.dtype
       ValueError: if df < k, where scale operator event shape is `(k, k)`
     """
-    self._scale_operator_pd = scale_operator_pd
     self._cholesky_input_output_matrices = cholesky_input_output_matrices
-    self._allow_nan_stats = allow_nan_stats
-    self._validate_args = validate_args
-    self._name = name
     with ops.name_scope(name):
       with ops.name_scope('init', values=[df, scale_operator_pd]):
-        if not self.dtype.is_floating:
+        if not scale_operator_pd.dtype.is_floating:
           raise TypeError(
               'scale_operator_pd.dtype=%s is not a floating-point type' %
-              self.dtype)
-        self._df = ops.convert_to_tensor(df, dtype=self.dtype, name='df')
+              scale_operator_pd.dtype)
+        self._scale_operator_pd = scale_operator_pd
+        self._df = ops.convert_to_tensor(
+            df, dtype=scale_operator_pd.dtype, name='df')
         contrib_tensor_util.assert_same_float_dtype(
-            (self._df, self.scale_operator_pd))
-        if (self.scale_operator_pd.get_shape().ndims is None or
-            self.scale_operator_pd.get_shape()[-1].value is None):
+            (self._df, self._scale_operator_pd))
+        if (self._scale_operator_pd.get_shape().ndims is None or
+            self._scale_operator_pd.get_shape()[-1].value is None):
           self._dimension = math_ops.cast(
-              self.scale_operator_pd.vector_space_dimension(),
-              dtype=self.dtype, name='dimension')
+              self._scale_operator_pd.vector_space_dimension(),
+              dtype=self._scale_operator_pd.dtype, name='dimension')
         else:
           self._dimension = ops.convert_to_tensor(
-              self.scale_operator_pd.get_shape()[-1].value,
-              dtype=self.dtype, name='dimension')
-        df_val = tensor_util.constant_value(self.df)
-        dim_val = tensor_util.constant_value(self.dimension)
+              self._scale_operator_pd.get_shape()[-1].value,
+              dtype=self._scale_operator_pd.dtype, name='dimension')
+        df_val = tensor_util.constant_value(self._df)
+        dim_val = tensor_util.constant_value(self._dimension)
         if df_val is not None and dim_val is not None:
           df_val = np.asarray(df_val)
           if not df_val.shape: df_val = (df_val,)
@@ -133,38 +130,21 @@ class _WishartOperatorPD(distribution.Distribution):
                 'Degrees of freedom (df = %s) cannot be less than dimension of '
                 'scale matrix (scale.dimension = %s)'
                 % (df_val, dim_val))
-        elif self.validate_args:
+        elif validate_args:
           assertions = check_ops.assert_less_equal(
-              self.dimension, self.df,
+              self._dimension, self._df,
               message=('Degrees of freedom (df = %s) cannot be less than '
                        'dimension of scale matrix (scale.dimension = %s)' %
-                       (self.dimension, self.df)))
+                       (self._dimension, self._df)))
           self._df = control_flow_ops.with_dependencies([assertions], self._df)
-
-  @property
-  def inputs(self):
-    """Dictionary of inputs provided at initialization."""
-    return {'scale_operator_pd': self.scale_operator_pd, 'df': self._df}
-
-  @property
-  def allow_nan_stats(self):
-    """Boolean describing behavior when a stat is undefined for batch member."""
-    return self._allow_nan_stats
-
-  @property
-  def validate_args(self):
-    """Boolean describing behavior on invalid input."""
-    return self._validate_args
-
-  @property
-  def name(self):
-    """Name prepended to all ops."""
-    return self._name
-
-  @property
-  def dtype(self):
-    """dtype of samples from this distribution."""
-    return self.scale_operator_pd.dtype
+        super(_WishartOperatorPD, self).__init__(
+            dtype=self._scale_operator_pd.dtype,
+            parameters={'df': self._df,
+                        'scale_operator_pd': self._scale_operator_pd,
+                        'dimension': self._dimension},
+            validate_args=validate_args,
+            allow_nan_stats=allow_nan_stats,
+            name=name)
 
   @property
   def df(self):
@@ -193,240 +173,165 @@ class _WishartOperatorPD(distribution.Distribution):
     """Dimension of underlying vector space. The `p` in `R^(p*p)`."""
     return self._dimension
 
-  def is_continuous(self):
-    return True
-
-  def is_reparameterized(self):
-    return True
+  def _event_shape(self):
+    s = self.scale_operator_pd.shape()
+    return array_ops.slice(s, array_ops.shape(s) - 2, [2])
 
-  def event_shape(self, name='event_shape'):
-    """Shape of a sample from a single distribution as a 1-D int32 `Tensor`."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=list(self.inputs.values())):
-        s = self.scale_operator_pd.shape()
-        return array_ops.slice(s, array_ops.shape(s) - 2, [2])
-
-  def get_event_shape(self):
-    """`TensorShape` available at graph construction time."""
+  def _get_event_shape(self):
     return self.scale_operator_pd.get_shape()[-2:]
 
-  def batch_shape(self, name='batch_shape'):
-    """Batch dimensions of this instance as a 1-D int32 `Tensor`."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=list(self.inputs.values())):
-        return self.scale_operator_pd.batch_shape()
+  def _batch_shape(self):
+    return self.scale_operator_pd.batch_shape()
 
-  def get_batch_shape(self):
-    """`TensorShape` available at graph construction time."""
+  def _get_batch_shape(self):
     return self.scale_operator_pd.get_batch_shape()
 
-  def prob(self, value, name='prob'):
-    """Probability density/mass function."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[value]):
-        return math_ops.exp(self.log_prob(value))
-
-  def log_prob(self, x, name='log_prob'):
-    """Log of the probability density/mass function.
-
-    Args:
-      x: `float` or `double` `Tensor`.
-      name: The name to give this op.
-
-    Returns:
-      log_prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with
-        values of type `self.dtype`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[x] + list(self.inputs.values())):
-        x = ops.convert_to_tensor(x, name='x')
-        contrib_tensor_util.assert_same_float_dtype(
-            (self.scale_operator_pd, x))
-        if self.cholesky_input_output_matrices:
-          x_sqrt = x
-        else:
-          # Complexity: O(nbk^3)
-          x_sqrt = linalg_ops.batch_cholesky(x)
-
-        batch_shape = self.batch_shape()
-        event_shape = self.event_shape()
-        ndims = array_ops.rank(x_sqrt)
-        # sample_ndims = ndims - batch_ndims - event_ndims
-        sample_ndims = ndims - array_ops.shape(batch_shape)[0] - 2
-        sample_shape = array_ops.slice(
-            array_ops.shape(x_sqrt), [0], [sample_ndims])
-
-        # We need to be able to pre-multiply each matrix by its corresponding
-        # batch scale matrix.  Since a Distribution Tensor supports multiple
-        # samples per batch, this means we need to reshape the input matrix `x`
-        # so that the first b dimensions are batch dimensions and the last two
-        # are of shape [dimension, dimensions*number_of_samples]. Doing these
-        # gymnastics allows us to do a batch_solve.
-        #
-        # After we're done with sqrt_solve (the batch operation) we need to undo
-        # this reshaping so what we're left with is a Tensor partitionable by
-        # sample, batch, event dimensions.
-
-        # Complexity: O(nbk^2) since transpose must access every element.
-        scale_sqrt_inv_x_sqrt = x_sqrt
-        perm = array_ops.concat(0, (math_ops.range(sample_ndims, ndims),
-                                    math_ops.range(0, sample_ndims)))
-        scale_sqrt_inv_x_sqrt = array_ops.transpose(scale_sqrt_inv_x_sqrt, perm)
-        shape = array_ops.concat(
-            0, (batch_shape,
-                (math_ops.cast(self.dimension, dtype=dtypes.int32), -1)))
-        scale_sqrt_inv_x_sqrt = array_ops.reshape(scale_sqrt_inv_x_sqrt, shape)
-
-        # Complexity: O(nbM*k) where M is the complexity of the operator solving
-        # a vector system.  E.g., for OperatorPDDiag, each solve is O(k), so
-        # this complexity is O(nbk^2). For OperatorPDCholesky, each solve is
-        # O(k^2) so this step has complexity O(nbk^3).
-        scale_sqrt_inv_x_sqrt = self.scale_operator_pd.sqrt_solve(
-            scale_sqrt_inv_x_sqrt)
-
-        # Undo make batch-op ready.
-        # Complexity: O(nbk^2)
-        shape = array_ops.concat(0, (batch_shape, event_shape, sample_shape))
-        scale_sqrt_inv_x_sqrt = array_ops.reshape(scale_sqrt_inv_x_sqrt, shape)
-        perm = array_ops.concat(0, (math_ops.range(ndims - sample_ndims, ndims),
-                                    math_ops.range(0, ndims - sample_ndims)))
-        scale_sqrt_inv_x_sqrt = array_ops.transpose(scale_sqrt_inv_x_sqrt, perm)
-
-        # Write V = SS', X = LL'. Then:
-        # tr[inv(V) X] = tr[inv(S)' inv(S) L L']
-        #              = tr[inv(S) L L' inv(S)']
-        #              = tr[(inv(S) L) (inv(S) L)']
-        #              = sum_{ik} (inv(S) L)_{ik}^2
-        # The second equality follows from the cyclic permutation property.
-        # Complexity: O(nbk^2)
-        trace_scale_inv_x = math_ops.reduce_sum(
-            math_ops.square(scale_sqrt_inv_x_sqrt),
-            reduction_indices=[-2, -1])
-
-        # Complexity: O(nbk)
-        half_log_det_x = math_ops.reduce_sum(
-            math_ops.log(array_ops.batch_matrix_diag_part(x_sqrt)),
-            reduction_indices=[-1])
-
-        # Complexity: O(nbk^2)
-        log_prob = ((self.df - self.dimension - 1.) * half_log_det_x -
-                    0.5 * trace_scale_inv_x -
-                    self.log_normalizing_constant())
-
-        # Set shape hints.
-        # Try to merge what we know from the input then what we know from the
-        # parameters of this distribution.
-        if x.get_shape().ndims is not None:
-          log_prob.set_shape(x.get_shape()[:-2])
-        if (log_prob.get_shape().ndims is not None and
-            self.get_batch_shape().ndims is not None and
-            self.get_batch_shape().ndims > 0):
-          log_prob.get_shape()[-self.get_batch_shape().ndims:].merge_with(
-              self.get_batch_shape())
-
-        return log_prob
-
-  def sample_n(self, n, seed=None, name='sample'):
-    # pylint: disable=line-too-long
-    """Generate `n` samples.
-
-    Complexity: O(nbk^3)
-
-    The sampling procedure is based on the [Bartlett decomposition](
-    https://en.wikipedia.org/wiki/Wishart_distribution#Bartlett_decomposition)
-    and [using a Gamma distribution to generate Chi2 random variates](
-    https://en.wikipedia.org/wiki/Chi-squared_distribution#Gamma.2C_exponential.2C_and_related_distributions).
-
-    Args:
-      n: `Scalar` `Tensor` of type `int32` or `int64`, the number of
-        observations to sample.
-      seed: Python integer; random number generator seed.
-      name: The name of this op.
-
-    Returns:
-      samples: a `Tensor` of shape `(n,) + self.batch_shape + self.event_shape`
-          with values of type `self.dtype`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[n] + list(self.inputs.values())):
-        n = ops.convert_to_tensor(n, name='n')
-        if n.dtype != dtypes.int32:
-          raise TypeError('n.dtype=%s which is not int32' % n.dtype)
-        batch_shape = self.batch_shape()
-        event_shape = self.event_shape()
-        batch_ndims = array_ops.shape(batch_shape)[0]
-
-        ndims = batch_ndims + 3  # sample_ndims=1, event_ndims=2
-        shape = array_ops.concat(0, ((n,), batch_shape, event_shape))
-
-        # Complexity: O(nbk^2)
-        x = random_ops.random_normal(shape=shape,
-                                     mean=0.,
-                                     stddev=1.,
-                                     dtype=self.dtype,
-                                     seed=seed)
-
-        # Complexity: O(nbk)
-        # This parametrization is equivalent to Chi2, i.e.,
-        # ChiSquared(k) == Gamma(alpha=k/2, beta=1/2)
-        g = random_ops.random_gamma(shape=(n,),
-                                    alpha=self._multi_gamma_sequence(
-                                        0.5 * self.df, self.dimension),
-                                    beta=0.5,
-                                    dtype=self.dtype,
-                                    seed=seed)
-
-        # Complexity: O(nbk^2)
-        x = array_ops.batch_matrix_band_part(x, -1, 0)  # Tri-lower.
-
-        # Complexity: O(nbk)
-        x = array_ops.batch_matrix_set_diag(x, math_ops.sqrt(g))
-
-        # Make batch-op ready.
-        # Complexity: O(nbk^2)
-        perm = array_ops.concat(0, (math_ops.range(1, ndims), (0,)))
-        x = array_ops.transpose(x, perm)
-        shape = array_ops.concat(0, (batch_shape, (event_shape[0], -1)))
-        x = array_ops.reshape(x, shape)
-
-        # Complexity: O(nbM) where M is the complexity of the operator solving a
-        # vector system.  E.g., for OperatorPDDiag, each matmul is O(k^2), so
-        # this complexity is O(nbk^2). For OperatorPDCholesky, each matmul is
-        # O(k^3) so this step has complexity O(nbk^3).
-        x = self.scale_operator_pd.sqrt_matmul(x)
-
-        # Undo make batch-op ready.
-        # Complexity: O(nbk^2)
-        shape = array_ops.concat(0, (batch_shape, event_shape, (n,)))
-        x = array_ops.reshape(x, shape)
-        perm = array_ops.concat(0, ((ndims-1,), math_ops.range(0, ndims-1)))
-        x = array_ops.transpose(x, perm)
-
-        if not self.cholesky_input_output_matrices:
-          # Complexity: O(nbk^3)
-          x = math_ops.batch_matmul(x, x, adj_y=True)
-
-        # Set shape hints.
-        if self.scale_operator_pd.get_shape().ndims is not None:
-          x.set_shape(tensor_shape.TensorShape(
-              [tensor_util.constant_value(n)] +
-              self.scale_operator_pd.get_shape().as_list()))
-        elif x.get_shape().ndims is not None:
-          x.get_shape()[0].merge_with(
-              tensor_shape.TensorDimension(tensor_util.constant_value(n)))
-
-        return x
-
-  def cdf(self, value, name='cdf'):
-    """Cumulative distribution function."""
-    raise NotImplementedError('cdf is not implemented')
-
-  def log_cdf(self, value, name='log_cdf'):
-    """Log CDF."""
-    raise NotImplementedError('log_cdf is not implemented')
-
-  def entropy(self, name='entropy'):
-    """Entropy of the distribution in nats."""
+  def _sample_n(self, n, seed):
+    batch_shape = self.batch_shape()
+    event_shape = self.event_shape()
+    batch_ndims = array_ops.shape(batch_shape)[0]
+
+    ndims = batch_ndims + 3  # sample_ndims=1, event_ndims=2
+    shape = array_ops.concat(0, ((n,), batch_shape, event_shape))
+
+    # Complexity: O(nbk^2)
+    x = random_ops.random_normal(shape=shape,
+                                 mean=0.,
+                                 stddev=1.,
+                                 dtype=self.dtype,
+                                 seed=seed)
+
+    # Complexity: O(nbk)
+    # This parametrization is equivalent to Chi2, i.e.,
+    # ChiSquared(k) == Gamma(alpha=k/2, beta=1/2)
+    g = random_ops.random_gamma(shape=(n,),
+                                alpha=self._multi_gamma_sequence(
+                                    0.5 * self.df, self.dimension),
+                                beta=0.5,
+                                dtype=self.dtype,
+                                seed=seed)
+
+    # Complexity: O(nbk^2)
+    x = array_ops.batch_matrix_band_part(x, -1, 0)  # Tri-lower.
+
+    # Complexity: O(nbk)
+    x = array_ops.batch_matrix_set_diag(x, math_ops.sqrt(g))
+
+    # Make batch-op ready.
+    # Complexity: O(nbk^2)
+    perm = array_ops.concat(0, (math_ops.range(1, ndims), (0,)))
+    x = array_ops.transpose(x, perm)
+    shape = array_ops.concat(0, (batch_shape, (event_shape[0], -1)))
+    x = array_ops.reshape(x, shape)
+
+    # Complexity: O(nbM) where M is the complexity of the operator solving a
+    # vector system.  E.g., for OperatorPDDiag, each matmul is O(k^2), so
+    # this complexity is O(nbk^2). For OperatorPDCholesky, each matmul is
+    # O(k^3) so this step has complexity O(nbk^3).
+    x = self.scale_operator_pd.sqrt_matmul(x)
+
+    # Undo make batch-op ready.
+    # Complexity: O(nbk^2)
+    shape = array_ops.concat(0, (batch_shape, event_shape, (n,)))
+    x = array_ops.reshape(x, shape)
+    perm = array_ops.concat(0, ((ndims-1,), math_ops.range(0, ndims-1)))
+    x = array_ops.transpose(x, perm)
+
+    if not self.cholesky_input_output_matrices:
+      # Complexity: O(nbk^3)
+      x = math_ops.batch_matmul(x, x, adj_y=True)
+
+    return x
+
+  def _log_prob(self, x):
+    if self.cholesky_input_output_matrices:
+      x_sqrt = x
+    else:
+      # Complexity: O(nbk^3)
+      x_sqrt = linalg_ops.batch_cholesky(x)
+
+    batch_shape = self.batch_shape()
+    event_shape = self.event_shape()
+    ndims = array_ops.rank(x_sqrt)
+    # sample_ndims = ndims - batch_ndims - event_ndims
+    sample_ndims = ndims - array_ops.shape(batch_shape)[0] - 2
+    sample_shape = array_ops.slice(
+        array_ops.shape(x_sqrt), [0], [sample_ndims])
+
+    # We need to be able to pre-multiply each matrix by its corresponding
+    # batch scale matrix.  Since a Distribution Tensor supports multiple
+    # samples per batch, this means we need to reshape the input matrix `x`
+    # so that the first b dimensions are batch dimensions and the last two
+    # are of shape [dimension, dimensions*number_of_samples]. Doing these
+    # gymnastics allows us to do a batch_solve.
+    #
+    # After we're done with sqrt_solve (the batch operation) we need to undo
+    # this reshaping so what we're left with is a Tensor partitionable by
+    # sample, batch, event dimensions.
+
+    # Complexity: O(nbk^2) since transpose must access every element.
+    scale_sqrt_inv_x_sqrt = x_sqrt
+    perm = array_ops.concat(0, (math_ops.range(sample_ndims, ndims),
+                                math_ops.range(0, sample_ndims)))
+    scale_sqrt_inv_x_sqrt = array_ops.transpose(scale_sqrt_inv_x_sqrt, perm)
+    shape = array_ops.concat(
+        0, (batch_shape,
+            (math_ops.cast(self.dimension, dtype=dtypes.int32), -1)))
+    scale_sqrt_inv_x_sqrt = array_ops.reshape(scale_sqrt_inv_x_sqrt, shape)
+
+    # Complexity: O(nbM*k) where M is the complexity of the operator solving
+    # a vector system.  E.g., for OperatorPDDiag, each solve is O(k), so
+    # this complexity is O(nbk^2). For OperatorPDCholesky, each solve is
+    # O(k^2) so this step has complexity O(nbk^3).
+    scale_sqrt_inv_x_sqrt = self.scale_operator_pd.sqrt_solve(
+        scale_sqrt_inv_x_sqrt)
+
+    # Undo make batch-op ready.
+    # Complexity: O(nbk^2)
+    shape = array_ops.concat(0, (batch_shape, event_shape, sample_shape))
+    scale_sqrt_inv_x_sqrt = array_ops.reshape(scale_sqrt_inv_x_sqrt, shape)
+    perm = array_ops.concat(0, (math_ops.range(ndims - sample_ndims, ndims),
+                                math_ops.range(0, ndims - sample_ndims)))
+    scale_sqrt_inv_x_sqrt = array_ops.transpose(scale_sqrt_inv_x_sqrt, perm)
+
+    # Write V = SS', X = LL'. Then:
+    # tr[inv(V) X] = tr[inv(S)' inv(S) L L']
+    #              = tr[inv(S) L L' inv(S)']
+    #              = tr[(inv(S) L) (inv(S) L)']
+    #              = sum_{ik} (inv(S) L)_{ik}^2
+    # The second equality follows from the cyclic permutation property.
+    # Complexity: O(nbk^2)
+    trace_scale_inv_x = math_ops.reduce_sum(
+        math_ops.square(scale_sqrt_inv_x_sqrt),
+        reduction_indices=[-2, -1])
+
+    # Complexity: O(nbk)
+    half_log_det_x = math_ops.reduce_sum(
+        math_ops.log(array_ops.batch_matrix_diag_part(x_sqrt)),
+        reduction_indices=[-1])
+
+    # Complexity: O(nbk^2)
+    log_prob = ((self.df - self.dimension - 1.) * half_log_det_x -
+                0.5 * trace_scale_inv_x -
+                self.log_normalizing_constant())
+
+    # Set shape hints.
+    # Try to merge what we know from the input then what we know from the
+    # parameters of this distribution.
+    if x.get_shape().ndims is not None:
+      log_prob.set_shape(x.get_shape()[:-2])
+    if (log_prob.get_shape().ndims is not None and
+        self.get_batch_shape().ndims is not None and
+        self.get_batch_shape().ndims > 0):
+      log_prob.get_shape()[-self.get_batch_shape().ndims:].merge_with(
+          self.get_batch_shape())
+
+    return log_prob
+
+  def _prob(self, x):
+    return math_ops.exp(self._log_prob(x))
+
+  def _entropy(self):
     half_dp1 = 0.5 * self.dimension + 0.5
     half_df = 0.5 * self.df
     return (self.dimension * (half_df + half_dp1 * math.log(2.)) +
@@ -434,110 +339,74 @@ class _WishartOperatorPD(distribution.Distribution):
             self._multi_lgamma(half_df, self.dimension) +
             (half_dp1 - half_df) * self._multi_digamma(half_df, self.dimension))
 
-  def mean(self, name='mean'):
-    """Mean of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=list(self.inputs.values())):
-        if self.cholesky_input_output_matrices:
-          return math_ops.sqrt(self.df) * self.scale_operator_pd.sqrt_to_dense()
-        else:
-          return self.df * self.scale_operator_pd.to_dense()
-
-  def mode(self, name='mode'):
-    """Mode of the distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=list(self.inputs.values())):
-        s = self.df - self.dimension - 1.
-        s = math_ops.select(
-            math_ops.less(s, 0.),
-            constant_op.constant(float('NaN'), dtype=self.dtype, name='nan'),
-            s)
-        if self.cholesky_input_output_matrices:
-          return math_ops.sqrt(s) * self.scale_operator_pd.sqrt_to_dense()
-        else:
-          return s * self.scale_operator_pd.to_dense()
-
-  def std(self, name='std'):
-    """Standard deviation of the Wishart distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=list(self.inputs.values())):
-        if self.cholesky_input_output_matrices:
-          raise ValueError(
-              'Computing std. dev. when is cholesky_input_output_matrices=True '
-              'does not make sense.')
-        return linalg_ops.batch_cholesky(self.variance())
-
-  def variance(self, name='variance'):
-    """Variance of the Wishart distribution.
-
-    This function should not be confused with the covariance of the Wishart. The
-    covariance matrix would have shape `q x q` where,
-    `q = dimension * (dimension+1) / 2`
-    and having elements corresponding to some mapping from a lower-triangular
-    matrix to a vector-space.
-
-    This function returns the diagonal of the Covariance matrix but shaped
-    as a `dimension x dimension` matrix.
-
-    Args:
-      name: The name of this op.
-
-    Returns:
-      variance: `Tensor` of dtype `self.dtype`.
-    """
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=list(self.inputs.values())):
-        x = math_ops.sqrt(self.df) * self.scale_operator_pd.to_dense()
-        d = array_ops.expand_dims(array_ops.batch_matrix_diag_part(x), -1)
-        v = math_ops.square(x) + math_ops.batch_matmul(d, d, adj_y=True)
-        if self.cholesky_input_output_matrices:
-          return linalg_ops.batch_cholesky(v)
-        else:
-          return v
+  def _mean(self):
+    if self.cholesky_input_output_matrices:
+      return math_ops.sqrt(self.df) * self.scale_operator_pd.sqrt_to_dense()
+    return self.df * self.scale_operator_pd.to_dense()
+
+  def _variance(self):
+    x = math_ops.sqrt(self.df) * self.scale_operator_pd.to_dense()
+    d = array_ops.expand_dims(array_ops.batch_matrix_diag_part(x), -1)
+    v = math_ops.square(x) + math_ops.batch_matmul(d, d, adj_y=True)
+    if self.cholesky_input_output_matrices:
+      return linalg_ops.batch_cholesky(v)
+    return v
+
+  def _std(self):
+    if self.cholesky_input_output_matrices:
+      raise ValueError(
+          'Computing std. dev. when is cholesky_input_output_matrices=True '
+          'does not make sense.')
+    return linalg_ops.batch_cholesky(self.variance())
+
+  def _mode(self):
+    s = self.df - self.dimension - 1.
+    s = math_ops.select(
+        math_ops.less(s, 0.),
+        constant_op.constant(float('NaN'), dtype=self.dtype, name='nan'),
+        s)
+    if self.cholesky_input_output_matrices:
+      return math_ops.sqrt(s) * self.scale_operator_pd.sqrt_to_dense()
+    return s * self.scale_operator_pd.to_dense()
 
   def mean_log_det(self, name='mean_log_det'):
     """Computes E[log(det(X))] under this Wishart distribution."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=list(self.inputs.values())):
-        return (self._multi_digamma(0.5 * self.df, self.dimension) +
-                self.dimension * math.log(2.) +
-                self.scale_operator_pd.log_det())
+    with self._name_scope(name):
+      return (self._multi_digamma(0.5 * self.df, self.dimension) +
+              self.dimension * math.log(2.) +
+              self.scale_operator_pd.log_det())
 
   def log_normalizing_constant(self, name='log_normalizing_constant'):
     """Computes the log normalizing constant, log(Z)."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=list(self.inputs.values())):
-        return (self.df * self.scale_operator_pd.sqrt_log_det() +
-                0.5 * self.df * self.dimension * math.log(2.) +
-                self._multi_lgamma(0.5 * self.df, self.dimension))
+    with self._name_scope(name):
+      return (self.df * self.scale_operator_pd.sqrt_log_det() +
+              0.5 * self.df * self.dimension * math.log(2.) +
+              self._multi_lgamma(0.5 * self.df, self.dimension))
 
   def _multi_gamma_sequence(self, a, p, name='multi_gamma_sequence'):
     """Creates sequence used in multivariate (di)gamma; shape = shape(a)+[p]."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[a, p]):
-        # Linspace only takes scalars, so we'll add in the offset afterwards.
-        seq = math_ops.linspace(
-            constant_op.constant(0., dtype=self.dtype),
-            0.5 - 0.5 * p,
-            math_ops.cast(p, dtypes.int32))
-        return seq + array_ops.expand_dims(a, [-1])
+    with self._name_scope(name, values=[a, p]):
+      # Linspace only takes scalars, so we'll add in the offset afterwards.
+      seq = math_ops.linspace(
+          constant_op.constant(0., dtype=self.dtype),
+          0.5 - 0.5 * p,
+          math_ops.cast(p, dtypes.int32))
+      return seq + array_ops.expand_dims(a, [-1])
 
   def _multi_lgamma(self, a, p, name='multi_lgamma'):
     """Computes the log multivariate gamma function; log(Gamma_p(a))."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[a, p]):
-        seq = self._multi_gamma_sequence(a, p)
-        return (0.25 * p * (p - 1.) * math.log(math.pi) +
-                math_ops.reduce_sum(math_ops.lgamma(seq),
-                                    reduction_indices=(-1,)))
+    with self._name_scope(name, values=[a, p]):
+      seq = self._multi_gamma_sequence(a, p)
+      return (0.25 * p * (p - 1.) * math.log(math.pi) +
+              math_ops.reduce_sum(math_ops.lgamma(seq),
+                                  reduction_indices=(-1,)))
 
   def _multi_digamma(self, a, p, name='multi_digamma'):
     """Computes the multivariate digamma function; Psi_p(a)."""
-    with ops.name_scope(self.name):
-      with ops.name_scope(name, values=[a, p]):
-        seq = self._multi_gamma_sequence(a, p)
-        return math_ops.reduce_sum(math_ops.digamma(seq),
-                                   reduction_indices=(-1,))
+    with self._name_scope(name, values=[a, p]):
+      seq = self._multi_gamma_sequence(a, p)
+      return math_ops.reduce_sum(math_ops.digamma(seq),
+                                 reduction_indices=(-1,))
 
 
 class WishartCholesky(_WishartOperatorPD):
@@ -607,9 +476,9 @@ class WishartCholesky(_WishartOperatorPD):
                df,
                scale,
                cholesky_input_output_matrices=False,
-               allow_nan_stats=False,
                validate_args=True,
-               name='Wishart'):
+               allow_nan_stats=False,
+               name='WishartCholesky'):
     """Construct Wishart distributions.
 
     Args:
@@ -622,13 +491,13 @@ class WishartCholesky(_WishartOperatorPD):
         Cholesky factored matrix. Example`log_pdf` input takes a Cholesky and
         `sample_n` returns a Cholesky when
         `cholesky_input_output_matrices=True`.
+      validate_args: Whether to validate input with asserts. If `validate_args`
+        is `False`, and the inputs are invalid, correct behavior is not
+        guaranteed.
       allow_nan_stats:  `Boolean`, default `False`. If `False`, raise an
         exception if a statistic (e.g., mean, mode) is undefined for any batch
         member. If True, batch members with valid parameters leading to
         undefined statistics will return `NaN` for this statistic.
-      validate_args: Whether to validate input with asserts. If `validate_args`
-        is `False`, and the inputs are invalid, correct behavior is not
-        guaranteed.
       name: The name scope to give class member ops.
     """
     super(WishartCholesky, self).__init__(
@@ -636,8 +505,8 @@ class WishartCholesky(_WishartOperatorPD):
         scale_operator_pd=operator_pd_cholesky.OperatorPDCholesky(
             scale, verify_pd=validate_args),
         cholesky_input_output_matrices=cholesky_input_output_matrices,
-        allow_nan_stats=allow_nan_stats,
         validate_args=validate_args,
+        allow_nan_stats=allow_nan_stats,
         name=name)
 
 
@@ -704,9 +573,9 @@ class WishartFull(_WishartOperatorPD):
                df,
                scale,
                cholesky_input_output_matrices=False,
-               allow_nan_stats=False,
                validate_args=True,
-               name='Wishart'):
+               allow_nan_stats=False,
+               name='WishartFull'):
     """Construct Wishart distributions.
 
     Args:
@@ -719,13 +588,13 @@ class WishartFull(_WishartOperatorPD):
         Cholesky factored matrix. Example`log_pdf` input takes a Cholesky and
         `sample_n` returns a Cholesky when
         `cholesky_input_output_matrices=True`.
+      validate_args: Whether to validate input with asserts. If `validate_args`
+        is `False`, and the inputs are invalid, correct behavior is not
+        guaranteed.
       allow_nan_stats:  `Boolean`, default `False`. If `False`, raise an
         exception if a statistic (e.g., mean, mode) is undefined for any batch
         member. If True, batch members with valid parameters leading to
         undefined statistics will return `NaN` for this statistic.
-      validate_args: Whether to validate input with asserts. If `validate_args`
-        is `False`, and the inputs are invalid, correct behavior is not
-        guaranteed.
       name: The name scope to give class member ops.
     """
     super(WishartFull, self).__init__(
@@ -733,6 +602,6 @@ class WishartFull(_WishartOperatorPD):
         scale_operator_pd=operator_pd_full.OperatorPDFull(
             scale, verify_pd=validate_args),
         cholesky_input_output_matrices=cholesky_input_output_matrices,
-        allow_nan_stats=allow_nan_stats,
         validate_args=validate_args,
+        allow_nan_stats=allow_nan_stats,
         name=name)

commit 8e37ef50c73d6b3f3ec530a3393fe2cba5ad3a30
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Tue May 10 10:55:24 2016 -0800

    tensorflow: finer-grained Shard parallelization
    
    Provide finer-grained Shard parallelization for the new non-blocking thread pool.
    This significantly resembles the parallel for algorithm in eigen executors:
    we choose a good block size based on amount of work and parallel efficiency,
    and then use recursive division in halves.
    
    Benchmark               Time(ns): old       new     diff  CPU(ns): old        new     diff
    ==========================================================================================
    cpu_RandomUniform/1M           647541    301220  -53.48%       9576553   10553619  +10.20%
    cpu_RandomUniform/2M          1116118    495724  -55.58%      18285896   19635580   +7.38%
    cpu_RandomUniform/8M          2691384   1671594  -37.89%      67830397   72105713   +6.30%
    cpu_RandomNormal/1M           2126780   1269039  -40.33%      46887528   53197040  +13.46%
    cpu_RandomNormal/2M           3529118   2350399  -33.40%      94337705  104481933  +10.75%
    cpu_RandomNormal/8M          12429704   8984079  -27.72%     383278086  410900286   +7.21%
    cpu_TruncatedNormal/1M        2513508   1504161  -40.16%      59181937   66096798  +11.68%
    cpu_TruncatedNormal/2M        4012258   2890855  -27.95%     122164300  129760843   +6.22%
    cpu_TruncatedNormal/8M       17628696  11159204  -36.70%     465946492  513345503  +10.17%
    
    TESTED:
      - passed opensource_build
        http://ci.tensorflow.org/view/Internal/job/tensorflow-cl-presubmit-multijob/281/
    Change: 121971279

diff --git a/tensorflow/core/lib/core/threadpool.cc b/tensorflow/core/lib/core/threadpool.cc
index f88579418e4..52550d8ae00 100644
--- a/tensorflow/core/lib/core/threadpool.cc
+++ b/tensorflow/core/lib/core/threadpool.cc
@@ -83,7 +83,24 @@ struct ThreadPool::Impl : Eigen::ThreadPoolTempl<EigenEnvironment> {
   Impl(Env* env, const ThreadOptions& thread_options, const string& name,
        int num_threads)
       : Eigen::ThreadPoolTempl<EigenEnvironment>(
-            num_threads, EigenEnvironment(env, thread_options, name)) {}
+            num_threads, EigenEnvironment(env, thread_options, name)),
+        num_threads_(num_threads) {}
+
+  void ParallelFor(int64 total, int64 cost_per_unit,
+                   std::function<void(int64, int64)> fn) {
+#ifdef EIGEN_USE_NONBLOCKING_THREAD_POOL
+    CHECK_GE(total, 0);
+    CHECK_EQ(total, (int64)(Eigen::Index)total);
+    Eigen::ThreadPoolDevice device(this, num_threads_);
+    device.parallelFor(
+        total, Eigen::TensorOpCost(0, 0, cost_per_unit),
+        [&fn](Eigen::Index first, Eigen::Index last) { fn(first, last); });
+#else
+    CHECK(0);  // should not be used with the old thread pool
+#endif
+  }
+
+  const int num_threads_;
 };
 
 #else
@@ -93,6 +110,10 @@ struct ThreadPool::Impl {
        int num_threads);
   ~Impl();
   void Schedule(std::function<void()> fn);
+  void ParallelFor(int64 total, int64 cost_per_unit,
+                   std::function<void(int64, int64)> fn) {
+    CHECK(0);  // should not be used with the old thread pool
+  }
 
  private:
   struct Waiter {
@@ -216,5 +237,10 @@ void ThreadPool::Schedule(std::function<void()> fn) {
   impl_->Schedule(std::move(fn));
 }
 
+void ThreadPool::ParallelFor(int64 total, int64 cost_per_unit,
+                             std::function<void(int64, int64)> fn) {
+  impl_->ParallelFor(total, cost_per_unit, std::move(fn));
+}
+
 }  // namespace thread
 }  // namespace tensorflow
diff --git a/tensorflow/core/lib/core/threadpool.h b/tensorflow/core/lib/core/threadpool.h
index ae709e08249..ae902f39cb6 100644
--- a/tensorflow/core/lib/core/threadpool.h
+++ b/tensorflow/core/lib/core/threadpool.h
@@ -47,6 +47,13 @@ class ThreadPool {
   // Schedule fn() for execution in the pool of threads.
   void Schedule(std::function<void()> fn);
 
+  // ParallelFor shards the "total" unit of work assuming each unit of work
+  // having roughly "cost_per_unit" cost, in cycles. Each unit of work is
+  // indexed 0, 1, ..., total - 1. Each shard contains 1 or more units of work
+  // and the total cost of each shard is roughly the same.
+  void ParallelFor(int64 total, int64 cost_per_unit,
+                   std::function<void(int64, int64)> fn);
+
   struct Impl;
 
  private:
diff --git a/tensorflow/core/util/work_sharder.cc b/tensorflow/core/util/work_sharder.cc
index 046d69a939f..38346d17164 100644
--- a/tensorflow/core/util/work_sharder.cc
+++ b/tensorflow/core/util/work_sharder.cc
@@ -22,6 +22,9 @@ namespace tensorflow {
 
 void Shard(int num_workers, thread::ThreadPool* workers, int64 total,
            int64 cost_per_unit, std::function<void(int64, int64)> work) {
+#ifdef EIGEN_USE_NONBLOCKING_THREAD_POOL
+  workers->ParallelFor(total, cost_per_unit, work);
+#else
   CHECK_GE(total, 0);
   if (total == 0) {
     return;
@@ -68,6 +71,7 @@ void Shard(int num_workers, thread::ThreadPool* workers, int64 total,
   // Inline execute the 1st shard.
   work(0, std::min(block_size, total));
   counter.Wait();
+#endif
 }
 
 }  // end namespace tensorflow
diff --git a/tensorflow/core/util/work_sharder.h b/tensorflow/core/util/work_sharder.h
index ad21100b005..59c4ac22a08 100644
--- a/tensorflow/core/util/work_sharder.h
+++ b/tensorflow/core/util/work_sharder.h
@@ -26,8 +26,7 @@ namespace tensorflow {
 // Shards the "total" unit of work assuming each unit of work having
 // roughly "cost_per_unit". Each unit of work is indexed 0, 1, ...,
 // total - 1. Each shard contains 1 or more units of work and the
-// total cost of each shard is roughly the same. The total number of
-// shards is no more than num_workers. The calling thread and the
+// total cost of each shard is roughly the same. The calling thread and the
 // "workers" are used to compute each shard (calling work(start,
 // limit). A common configuration is that "workers" is a thread pool
 // with "num_workers" threads.
diff --git a/tensorflow/core/util/work_sharder_test.cc b/tensorflow/core/util/work_sharder_test.cc
index c0d7267da9d..c11db2904fe 100644
--- a/tensorflow/core/util/work_sharder_test.cc
+++ b/tensorflow/core/util/work_sharder_test.cc
@@ -15,6 +15,7 @@ limitations under the License.
 
 #include "tensorflow/core/util/work_sharder.h"
 
+#include <atomic>
 #include <vector>
 #include "tensorflow/core/lib/core/threadpool.h"
 #include "tensorflow/core/platform/logging.h"
@@ -33,8 +34,10 @@ void RunSharding(int64 num_workers, int64 total, int64 cost_per_unit) {
   int64 num_done_work = 0;
   std::vector<bool> work(total, false);
   Shard(num_workers, &threads, total, cost_per_unit,
-        [&mu, &num_shards, &num_done_work, &work](int start, int limit) {
+        [=, &mu, &num_shards, &num_done_work, &work](int64 start, int64 limit) {
           VLOG(1) << "Shard [" << start << "," << limit << ")";
+          EXPECT_GE(start, 0);
+          EXPECT_LE(limit, total);
           mutex_lock l(mu);
           ++num_shards;
           for (; start < limit; ++start) {
@@ -43,7 +46,6 @@ void RunSharding(int64 num_workers, int64 total, int64 cost_per_unit) {
             work[start] = true;
           }
         });
-  EXPECT_LE(num_shards, num_workers + 1);
   EXPECT_EQ(num_done_work, total);
   LOG(INFO) << num_workers << " " << total << " " << cost_per_unit << " "
             << num_shards;
@@ -61,20 +63,15 @@ TEST(Shard, Basic) {
 
 TEST(Shard, OverflowTest) {
   thread::ThreadPool threads(Env::Default(), "test", 3);
-  mutex mu;
   for (auto workers : {1, 2, 3}) {
     const int64 total_elements = 1LL << 32;
-    const int64 cost_per_unit = 10000;
-    int num_shards = 0;
-    int64 num_elements = 0;
+    const int64 cost_per_unit = 10;
+    std::atomic<int64> num_elements(0);
     Shard(workers, &threads, total_elements, cost_per_unit,
-          [&mu, &num_shards, &num_elements](int64 start, int64 limit) {
-            mutex_lock l(mu);
-            ++num_shards;
+          [&num_elements](int64 start, int64 limit) {
             num_elements += limit - start;
           });
-    EXPECT_EQ(num_shards, workers);
-    EXPECT_EQ(num_elements, total_elements);
+    EXPECT_EQ(num_elements.load(), total_elements);
   }
 }
 

commit b09764d0c95ec3c8f56bc930ab2cc25157630cfb
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Mon Apr 25 08:07:56 2016 -0800

    Rewrite Tensor::SummarizeValue to avoid a segfault when passed an
    uninitialized Tensor, and also for efficiency.
    Change: 120712310

diff --git a/tensorflow/core/framework/tensor.cc b/tensorflow/core/framework/tensor.cc
index 6d989fd1d6e..1291fae3aed 100644
--- a/tensorflow/core/framework/tensor.cc
+++ b/tensorflow/core/framework/tensor.cc
@@ -643,44 +643,79 @@ bool Tensor::CanUseDMA() const {
 #undef CASES
 #undef CASE
 
-string Tensor::SummarizeValue(int64 max_entries) const {
+namespace {
+template <typename T>
+string SummarizeArray(int64 limit, int64 num_elts, const char* data) {
   string ret;
-  // TODO(irving): Don't call NumElements and flat every time around this
-  // loop.
-  for (int64 i = 0; i < std::min(max_entries, NumElements()); ++i) {
+  const T* array = reinterpret_cast<const T*>(data);
+  for (int64 i = 0; i < limit; ++i) {
     if (i > 0) strings::StrAppend(&ret, " ");
-    switch (dtype()) {
-      case DT_STRING:
-        strings::StrAppend(&ret, str_util::CEscape(flat<string>()(i)));
-        break;
-      case DT_BOOL:
-        strings::StrAppend(&ret, flat<bool>()(i) ? "True" : "False");
-        break;
-
-#define CASE(DT_ENUM)                                                   \
-  case DT_ENUM:                                                         \
-    strings::StrAppend(&ret, flat<EnumToDataType<DT_ENUM>::Type>()(i)); \
-    break
-
-        CASE(DT_FLOAT);
-        CASE(DT_DOUBLE);
-        CASE(DT_INT32);
-        CASE(DT_UINT8);
-        CASE(DT_UINT16);
-        CASE(DT_INT16);
-        CASE(DT_INT8);
-        CASE(DT_INT64);
+    strings::StrAppend(&ret, array[i]);
+  }
+  if (num_elts > limit) strings::StrAppend(&ret, "...");
+  return ret;
+}
+}  // namespace
 
-#undef CASE
-      default:
-        // TODO(zhifengc, josh11b): Pretty-print other types (bool,
-        // complex64, quantized, bfloat16).
-        strings::StrAppend(&ret, " ?");
+string Tensor::SummarizeValue(int64 max_entries) const {
+  const int64 num_elts = NumElements();
+  size_t limit = std::min(max_entries, num_elts);
+  if ((limit > 0) && (buf_ == nullptr)) {
+    return strings::StrCat("uninitialized Tensor of ", num_elts,
+                           " elements of type ", dtype());
+  }
+  const char* data = limit > 0 ? tensor_data().data() : nullptr;
+  switch (dtype()) {
+    case DT_FLOAT:
+      return SummarizeArray<float>(limit, num_elts, data);
+      break;
+    case DT_DOUBLE:
+      return SummarizeArray<double>(limit, num_elts, data);
+      break;
+    case DT_INT32:
+      return SummarizeArray<int32>(limit, num_elts, data);
+      break;
+    case DT_UINT8:
+      return SummarizeArray<uint8>(limit, num_elts, data);
+      break;
+    case DT_UINT16:
+      return SummarizeArray<uint16>(limit, num_elts, data);
+      break;
+    case DT_INT16:
+      return SummarizeArray<int16>(limit, num_elts, data);
+      break;
+    case DT_INT8:
+      return SummarizeArray<int8>(limit, num_elts, data);
+      break;
+    case DT_INT64:
+      return SummarizeArray<int64>(limit, num_elts, data);
+      break;
+    case DT_BOOL:
+      // TODO(tucker): Is it better to emit "True False..."?  This
+      // will emit "1 0..." which is more compact.
+      return SummarizeArray<bool>(limit, num_elts, data);
+      break;
+    default: {
+      // All irregular cases
+      string ret;
+      // TODO(irving): Don't call flat every time around this
+      // loop.
+      for (int64 i = 0; i < limit; ++i) {
+        if (i > 0) strings::StrAppend(&ret, " ");
+        switch (dtype()) {
+          case DT_STRING:
+            strings::StrAppend(&ret, str_util::CEscape(flat<string>()(i)));
+            break;
+          default:
+            // TODO(zhifengc, josh11b): Pretty-print other types (bool,
+            // complex64, quantized, bfloat16).
+            strings::StrAppend(&ret, "?");
+        }
+      }
+      if (max_entries < num_elts) strings::StrAppend(&ret, "...");
+      return ret;
     }
   }
-  if (max_entries < NumElements()) strings::StrAppend(&ret, "...");
-
-  return ret;
 }
 
 StringPiece Tensor::tensor_data() const {
diff --git a/tensorflow/core/framework/tensor.h b/tensorflow/core/framework/tensor.h
index 32d550bbce1..a8578e789db 100644
--- a/tensorflow/core/framework/tensor.h
+++ b/tensorflow/core/framework/tensor.h
@@ -375,6 +375,7 @@ class Tensor {
   friend class TensorReference;       // For access to buf_
   friend class VariableOp;            // For access to set_shape
   friend class AutoReloadVariableOp;  // For access to set_shape
+  friend class TensorTestHelper;      // For access to set_shape
 
   // Creates a tensor with the input datatype, shape and buf.
   //
diff --git a/tensorflow/core/framework/tensor_test.cc b/tensorflow/core/framework/tensor_test.cc
index a26a392dfca..f13c85c188d 100644
--- a/tensorflow/core/framework/tensor_test.cc
+++ b/tensorflow/core/framework/tensor_test.cc
@@ -23,6 +23,11 @@ limitations under the License.
 #include "tensorflow/core/platform/test_benchmark.h"
 
 namespace tensorflow {
+class TensorTestHelper {
+ public:
+  // This is an operation that can be done by VariableOp.
+  static void set_shape(Tensor* t, const TensorShape& s) { t->set_shape(s); }
+};
 
 TEST(TensorTest, Default) {
   Tensor t;
@@ -572,12 +577,12 @@ TEST(Tensor_Complex, SimpleWithHelper64) {
 TEST(Tensor_Complex, SimpleWithHelper128) {
   {
     Tensor t1 = test::AsTensor<complex128>({0,
-                                           {1, 1},
-                                           complex128(2),
-                                           complex128(3, 3),
-                                           complex128(0, 4),
-                                           complex128(2, 5)},
-                                          {2, 3});
+                                            {1, 1},
+                                            complex128(2),
+                                            complex128(3, 3),
+                                            complex128(0, 4),
+                                            complex128(2, 5)},
+                                           {2, 3});
     Tensor t2(t1.dtype(), t1.shape());
     t2.flat<complex128>() = t1.flat<complex128>() * complex128(0, 2);
     Tensor t3 = test::AsTensor<complex128>(
@@ -700,6 +705,63 @@ TEST(Tensor, Slice_Basic) {
   }
 }
 
+namespace {
+template <typename T>
+Tensor MkTensor(DataType dt, TensorShape shape, std::vector<T> init_values) {
+  Tensor x(dt, shape);
+  const int limit = x.NumElements();
+  int vi = 0;
+  for (int i = 0; i < limit; ++i) {
+    x.flat<T>()(i) = init_values[vi++];
+    if (vi >= init_values.size()) vi = 0;
+  }
+  return x;
+}
+}  // namespace
+
+TEST(SummarizeValue, Uninitialized) {
+  Tensor x(DT_INT32);
+  TensorTestHelper::set_shape(&x, TensorShape({4, 4}));
+  EXPECT_EQ(
+      strings::StrCat("uninitialized Tensor of 16 elements of type ", DT_INT32),
+      x.SummarizeValue(16));
+}
+
+TEST(SummarizeValue, INT32) {
+  Tensor x = MkTensor<int>(DT_INT32, TensorShape({5}), {1, 2, 3, 4, 0});
+  EXPECT_EQ("1 2 3 4 0", x.SummarizeValue(16));
+  x = MkTensor<int>(DT_INT32, TensorShape({2, 2}), {1, 2, 3, 4, 0});
+  EXPECT_EQ("1 2 3 4", x.SummarizeValue(16));
+  x = MkTensor<int>(DT_INT32, TensorShape({2, 2, 1, 1}), {1, 2, 3, 4, 0});
+  EXPECT_EQ("1 2 3 4", x.SummarizeValue(16));
+  EXPECT_EQ("1 2 3...", x.SummarizeValue(3));
+}
+
+TEST(SummarizeValue, FLOAT) {
+  Tensor x = MkTensor<float>(DT_FLOAT, TensorShape({5}), {1, 2, 3, 4, 0});
+  EXPECT_EQ("1 2 3 4 0", x.SummarizeValue(16));
+  x = MkTensor<float>(DT_FLOAT, TensorShape({2, 2}), {1, 2, 3, 4, 0});
+  EXPECT_EQ("1 2 3 4", x.SummarizeValue(16));
+  x = MkTensor<float>(DT_FLOAT, TensorShape({2, 2, 1, 1}), {1, 2, 3, 4, 0});
+  EXPECT_EQ("1 2 3 4", x.SummarizeValue(16));
+  EXPECT_EQ("1 2 3...", x.SummarizeValue(3));
+}
+
+TEST(SummarizeValue, BOOL) {
+  Tensor x = MkTensor<bool>(DT_BOOL, TensorShape({5}), {false, true, true});
+  EXPECT_EQ("0 1 1 0 1", x.SummarizeValue(16));
+  EXPECT_EQ("0 1 1...", x.SummarizeValue(3));
+}
+
+TEST(SummarizeValue, STRING) {
+  Tensor x = MkTensor<string>(DT_STRING, TensorShape({5}),
+                              {"one", "two", "three", "four", "five"});
+  EXPECT_EQ("one two three four five", x.SummarizeValue(16));
+  x = MkTensor<string>(DT_STRING, TensorShape({5, 1, 5}),
+                       {"one", "two", "three", "four", "five"});
+  EXPECT_EQ("one two three four five one...", x.SummarizeValue(6));
+}
+
 static void BM_CreateAndDestroy(int iters) {
   TensorShape shape({10, 20});
   while (--iters) {

commit 7cc6ba76176b84c828dd6b40ec5d2bc0d481f46a
Author: Xiaoqiang Zheng <zhengxq@google.com>
Date:   Thu Mar 10 13:23:33 2016 -0800

    Improve the BiasGrad for NCHW using less shared memory and better memory
    efficiency.
    
    With GoogleNet V1, time spent in BiasGrad in ms:
    
                  Before    After     Improvement
    GoogleNet V1  19.70     13.14     49.93%
    Change: 116901889

diff --git a/tensorflow/core/kernels/bias_op_gpu.cu.cc b/tensorflow/core/kernels/bias_op_gpu.cu.cc
index 5c90c3715f2..79344e79751 100644
--- a/tensorflow/core/kernels/bias_op_gpu.cu.cc
+++ b/tensorflow/core/kernels/bias_op_gpu.cu.cc
@@ -118,31 +118,51 @@ __global__ void BiasGradNHWC_SharedAtomics(int32 nthreads,
 }
 
 template <typename T>
-__global__ void BiasGradNCHW_SharedAtomics(int32 nthreads,
-                                           const T* output_backprop,
-                                           T* bias_backprop, int32 bias_size,
-                                           int32 image_size,
-                                           int32 shared_replicas) {
-  T* s_data = reinterpret_cast<T*>(s_buf);
-  int32 s_data_size = bias_size * shared_replicas;
+__global__ void BiasGradNCHW_SharedAtomics(const T* output_backprop,
+                                           T* bias_backprop, int32 batch,
+                                           int32 bias_size, int32 image_size,
+                                           int group_size) {
+  // Initialize the shared memory.
+  __shared__ T s_data[32];
+  int32 s_data_size = sizeof(s_data) / sizeof(T);
   for (int32 index = threadIdx.x; index < s_data_size; index += blockDim.x) {
     s_data[index] = 0;
   }
   __syncthreads();
 
-  for (int32 index = blockIdx.x * blockDim.x + threadIdx.x; index < nthreads;
-       index += blockDim.x * gridDim.x) {
-    int32 index2 = index / image_size;
-    int32 bias_slot_index = index2 % bias_size;
-    int32 bias_slot_offset = index % shared_replicas;
-    int32 bias_offset = bias_slot_index * shared_replicas + bias_slot_offset;
-    CudaAtomicAdd(s_data + bias_offset, ldg(output_backprop + index));
+  // Accumulate all the values within this thread. They all have the same bias
+  // index.
+  int32 bias_index = blockIdx.x % bias_size;
+  int32 group_index = blockIdx.x / bias_size;
+  int32 total_count = batch * image_size;
+  T sum = 0;
+  for (int32 index = group_index * blockDim.x + threadIdx.x;
+       index < total_count; index += blockDim.x * group_size) {
+    int32 image_offset = index % image_size;
+    int32 batch = index / image_size;
+    T val = ldg(output_backprop +
+                (batch * bias_size + bias_index) * image_size + image_offset);
+    sum += val;
   }
+
+  // Write the accumulated sum in this thread to the shared memory. Each thread
+  // shifts their write location to avoid bank conflict.
+  int bias_offset = threadIdx.x % 32;
+  CudaAtomicAdd(s_data + bias_offset, sum);
   __syncthreads();
 
-  for (int32 index = threadIdx.x; index < s_data_size; index += blockDim.x) {
-    int bias_slot_index = index / shared_replicas;
-    CudaAtomicAdd(bias_backprop + bias_slot_index, s_data[index]);
+  // Accumulate the results in the shared memory into the first element.
+  // No syncthreads is needed since this is only in the same warp.
+  int32 thread_index = threadIdx.x;
+  if (thread_index < 16) s_data[thread_index] += s_data[thread_index + 16];
+  if (thread_index < 8) s_data[thread_index] += s_data[thread_index + 8];
+  if (thread_index < 4) s_data[thread_index] += s_data[thread_index + 4];
+  if (thread_index < 2) s_data[thread_index] += s_data[thread_index + 2];
+  if (thread_index < 1) s_data[thread_index] += s_data[thread_index + 1];
+
+  // The first thread writes out the accumulated result to the global location.
+  if (thread_index == 0) {
+    CudaAtomicAdd(bias_backprop + bias_index, s_data[0]);
   }
 }
 
@@ -154,24 +174,13 @@ void BiasGradGPU<T>::compute(const GPUDevice& d, const T* output_backprop,
   const int32 bias_size = channel;
   const int32 image_size = height * width;
   const int32 total_count = batch * bias_size * image_size;
+  static constexpr int32 kWarpSize = 32;
   CudaLaunchConfig config = GetCudaLaunchConfig(total_count, d);
 
   const int max_shared_memory_size = d.sharedMemPerBlock() / 2;
-  int32 shared_memory_size = bias_size * sizeof(T);
-  int shared_replicas = 1;
-  if (data_format == FORMAT_NCHW) {
-    // For NCHW, the reduction in the HW dimensions all go to the same locaiton,
-    // which causes a lot of bank conflicts. So having a number of them can
-    // improve the performance. But we also want to limit their usage so the
-    // warp occupancy does not decrease.
-    if (shared_memory_size <= max_shared_memory_size) {
-      // We need enough shared memory to avoid bank conflict. But not too much
-      // so that it would reduce occupancy.
-      static constexpr int kMaxSharedReplicas = 8;
-      shared_replicas = std::min(kMaxSharedReplicas,
-                                 max_shared_memory_size / shared_memory_size);
-      shared_memory_size *= shared_replicas;
-    }
+  int32 shared_memory_size = 0;
+  if (data_format == FORMAT_NHWC) {
+    shared_memory_size = bias_size * sizeof(T);
   }
   // Check if we have enough shared memory.
   if (shared_memory_size <= max_shared_memory_size) {
@@ -181,10 +190,16 @@ void BiasGradGPU<T>::compute(const GPUDevice& d, const T* output_backprop,
                d.stream()>>>(total_count, output_backprop, bias_backprop,
                              bias_size);
     } else {
+      // Round up the block count to multiple of bias_size.
+      int group_size = (config.block_count + bias_size - 1) / bias_size;
+      config.block_count = group_size * bias_size;
+      if (config.thread_per_block < kWarpSize) {
+        config.thread_per_block = kWarpSize;
+      }
       BiasGradNCHW_SharedAtomics<
-          T><<<config.block_count, config.thread_per_block, shared_memory_size,
-               d.stream()>>>(total_count, output_backprop, bias_backprop,
-                             bias_size, image_size, shared_replicas);
+          T><<<config.block_count, config.thread_per_block, 0, d.stream()>>>(
+          output_backprop, bias_backprop, batch, bias_size, image_size,
+          group_size);
     }
   } else {
     // Note that even if we don't have enough shared memory to fit the entire

commit 7eae838548b2cc940b68c89d27e06fff46c78eb2
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Mon Feb 8 12:19:48 2016 -0800

    Changed the representation for pending and dead counts.  Previously,
    each of these was a std::vector<int>*, sized to the number of nodes in
    the graph.  This meant touching two cache lines every time we wanted
    to manipulate these values, and also required 8 bytes per node in
    total.  Instead, we now use a PendingCounts helper class added in this
    change that uses just a single byte to represent both these counts in
    the common case of fewer than 7 outgoing edges.  This helps with cache
    efficiency considerably because these values are heavily modified
    throughout the graph execution process by different threads running on
    different cores, and also because getting this to fit in, say, L1
    cache instead of L2 cache, also helps efficiency on just a single
    core.  As part of this, added a real interface that describes the
    various operations we want to do to these counts ("decrement_pending",
    "mark_live", "increment_dead_count", etc.), which is more
    understandable than having direct manipulation of counter values
    scattered throughout the ExecutorState code.
    Change: 114137346

diff --git a/tensorflow/core/common_runtime/executor.cc b/tensorflow/core/common_runtime/executor.cc
index f5afbff6ee5..9ea61227524 100644
--- a/tensorflow/core/common_runtime/executor.cc
+++ b/tensorflow/core/common_runtime/executor.cc
@@ -22,6 +22,7 @@ limitations under the License.
 #include <unordered_map>
 #include <vector>
 
+#include "tensorflow/core/common_runtime/pending_counts.h"
 #include "tensorflow/core/common_runtime/step_stats_collector.h"
 #include "tensorflow/core/framework/allocation_description.pb.h"
 #include "tensorflow/core/framework/allocator.h"
@@ -417,18 +418,12 @@ class ExecutorState {
   DeviceContextMap device_context_map_;
 
   struct IterationState {
-    // The state of an iteration.
-
-    // The pending count for each graph node. One copy per iteration.
-    // Iteration i can be garbage collected when it is done.
-    // TODO(yuanbyu): This vector currently has size of the number of nodes
-    // in this partition. This is not efficient if the subgraph for the frame
-    // is only a small subset of the partition. We should make the vector
-    // size to be only the size of the frame subgraph.
-    std::vector<int>* pending_count;
+    explicit IterationState(const Graph* g)
+        : outstanding_ops(0),
+          outstanding_frame_count(0),
+          counts_(g->num_node_ids()) {}
 
-    // The dead input count for each graph node. One copy per iteration.
-    std::vector<int>* dead_count;
+    // The state of an iteration.
 
     // One copy per iteration. For iteration k, i-th node's j-th input is in
     // input_tensors[k][impl_->nodes[i].input_start + j]. An entry is either
@@ -445,12 +440,22 @@ class ExecutorState {
 
     // The number of outstanding frames for each iteration.
     int outstanding_frame_count;
-
-    ~IterationState() {
-      delete pending_count;
-      delete dead_count;
-      delete input_tensors;
+    int pending(int id) { return counts_.pending(id); }
+    int decrement_pending(int id, int v) {
+      return counts_.decrement_pending(id, v);
     }
+    // Mark a merge node as live
+    // REQUIRES: Node corresponding to "id" is a merge node
+    void mark_live(int id) { counts_.mark_live(id); }
+
+    int dead_count(int id) { return counts_.dead_count(id); }
+    void increment_dead_count(int id) { counts_.increment_dead_count(id); }
+
+    ~IterationState() { delete input_tensors; }
+    void InitializePending(const Graph* g);
+
+   private:
+    PendingCounts counts_;
   };
 
   struct FrameState {
@@ -607,9 +612,6 @@ class ExecutorState {
     return strings::StrCat(frame->frame_name, ";", iter_id, ";", name);
   }
 
-  // Initialize the pending count for a graph.
-  static void InitializePending(const Graph* graph, std::vector<int>* pending);
-
   // Find an existing or create a new child frame in the frame 'frame' at
   // iteration 'iter'.
   void FindOrCreateChildFrame(FrameState* frame, int64 iter, const Node* node,
@@ -725,12 +727,8 @@ ExecutorState::ExecutorState(const Executor::Args& args, ExecutorImpl* impl)
   VLOG(2) << "Create frame: " << root_frame_->frame_name;
 
   // Initialize the iteration.
-  IterationState* iter_state = new IterationState;
+  IterationState* iter_state = new IterationState(impl->graph_);
   root_frame_->iterations[0] = iter_state;
-  iter_state->outstanding_ops = 0;
-  iter_state->outstanding_frame_count = 0;
-  iter_state->pending_count = new std::vector<int>;
-  iter_state->dead_count = new std::vector<int>(impl->graph_->num_node_ids());
   iter_state->input_tensors =
       new std::vector<Entry>(impl_->total_input_tensors_);
 
@@ -750,12 +748,11 @@ ExecutorState::~ExecutorState() {
   delete slice_reader_cache_;
 }
 
-void ExecutorState::InitializePending(const Graph* graph,
-                                      std::vector<int>* pending) {
-  pending->resize(graph->num_node_ids());
+void ExecutorState::IterationState::InitializePending(const Graph* graph) {
   for (const Node* n : graph->nodes()) {
     const int id = n->id();
     const int num_in_edges = n->in_edges().size();
+    int initial_count;
     if (IsMerge(n)) {
       // merge waits all control inputs so we initialize the pending
       // count to be the number of control edges.
@@ -766,10 +763,11 @@ void ExecutorState::InitializePending(const Graph* graph,
         }
       }
       // Use bit 0 to indicate if there is a ready live data input.
-      (*pending)[id] = num_control_edges << 1;
+      initial_count = num_control_edges << 1;
     } else {
-      (*pending)[id] = num_in_edges;
+      initial_count = num_in_edges;
     }
+    counts_.set_initial_count(id, initial_count, num_in_edges);
   }
 }
 
@@ -781,8 +779,7 @@ void ExecutorState::RunAsync(Executor::DoneCallback done) {
     // Initialize the executor state. We grab the mutex here just to
     // keep the thread safety analysis happy.
     mutex_lock l(mu_);
-    std::vector<int>* pending = root_frame_->iterations[0]->pending_count;
-    InitializePending(graph, pending);
+    root_frame_->iterations[0]->InitializePending(graph);
   }
 
   // Ask the device to fill in the device context map.
@@ -1196,8 +1193,6 @@ void ExecutorState::ActivateNode(const Node* node, const bool is_dead,
                                  TaggedNodeSeq* ready) {
   const std::vector<NodeItem>& nodes = impl_->nodes_;
   IterationState* output_iter_state = output_frame->GetIteration(output_iter);
-  std::vector<int>* pending = output_iter_state->pending_count;
-  std::vector<int>* dead_count = output_iter_state->dead_count;
   for (const Edge* e : node->out_edges()) {
     const Node* dst_node = e->dst();
     const int dst_id = dst_node->id();
@@ -1214,22 +1209,24 @@ void ExecutorState::ActivateNode(const Node* node, const bool is_dead,
       // a) a live data input becomes available or b) all data inputs are dead.
       // For Merge, pending's LSB is set iff a live data input has arrived.
       if (e->IsControlEdge()) {
-        (*pending)[dst_id] -= 2;
-        int count = (*pending)[dst_id];
-        dst_dead = ((*dead_count)[dst_id] == dst_node->num_inputs());
+        output_iter_state->decrement_pending(dst_id, 2);
+        int count = output_iter_state->pending(dst_id);
+        dst_dead =
+            (output_iter_state->dead_count(dst_id) == dst_node->num_inputs());
         dst_ready = (count == 1) || ((count == 0) && dst_dead);
       } else {
         if (outputs[src_slot].has_value) {
           // This is a live data input.
-          int count = (*pending)[dst_id];
-          (*pending)[dst_id] |= 0x1;
+          int count = output_iter_state->pending(dst_id);
+          output_iter_state->mark_live(dst_id);
           dst_ready = (count == 0);
           dst_need_input = (count & 0x1) == 0;
         } else {
           // This is a dead data input.
-          ++(*dead_count)[dst_id];
-          dst_dead = ((*dead_count)[dst_id] == dst_node->num_inputs());
-          dst_ready = ((*pending)[dst_id] == 0) && dst_dead;
+          output_iter_state->increment_dead_count(dst_id);
+          dst_dead =
+              (output_iter_state->dead_count(dst_id) == dst_node->num_inputs());
+          dst_ready = (output_iter_state->pending(dst_id) == 0) && dst_dead;
           dst_need_input = false;
         }
       }
@@ -1238,10 +1235,10 @@ void ExecutorState::ActivateNode(const Node* node, const bool is_dead,
       // for all inputs to come in even if we know the node is dead. This
       // ensures that all input tensors get cleaned up.
       if (is_dead || (!e->IsControlEdge() && !outputs[src_slot].has_value)) {
-        ++(*dead_count)[dst_id];
+        output_iter_state->increment_dead_count(dst_id);
       }
-      dst_dead = (*dead_count)[dst_id] > 0;
-      dst_ready = (--(*pending)[dst_id] == 0);
+      dst_dead = output_iter_state->dead_count(dst_id) > 0;
+      dst_ready = (output_iter_state->decrement_pending(dst_id, 1) == 0);
     }
 
     if (dst_need_input) {
@@ -1456,15 +1453,10 @@ void ExecutorState::FindOrCreateChildFrame(FrameState* frame, int64 iter,
     CHECK(s.ok()) << s;
     // 'iterations' is a fixed-length circular buffer.
     temp->iterations.resize(temp->max_parallel_iterations + 1);
-    IterationState* iter_state = new IterationState;
+    IterationState* iter_state = new IterationState(impl_->graph_);
     temp->iterations[0] = iter_state;
 
-    iter_state->outstanding_ops = 0;
-    iter_state->outstanding_frame_count = 0;
-    iter_state->pending_count = new std::vector<int>;
-    InitializePending(impl_->graph_, iter_state->pending_count);
-    iter_state->dead_count =
-        new std::vector<int>(impl_->graph_->num_node_ids());
+    iter_state->InitializePending(impl_->graph_);
     iter_state->input_tensors =
         new std::vector<Entry>(impl_->total_input_tensors_);
 
@@ -1487,16 +1479,12 @@ void ExecutorState::IncrementIteration(FrameState* frame,
   VLOG(2) << "Create iteration: [" << frame->frame_name << ", " << next_iter
           << "]";
 
-  IterationState* iter_state = new IterationState;
+  IterationState* iter_state = new IterationState(impl_->graph_);
   frame->SetIteration(next_iter, iter_state);
   frame->num_outstanding_iterations++;
   frame->dead_exits.clear();
 
-  iter_state->outstanding_ops = 0;
-  iter_state->outstanding_frame_count = 0;
-  iter_state->pending_count = new std::vector<int>;
-  InitializePending(impl_->graph_, iter_state->pending_count);
-  iter_state->dead_count = new std::vector<int>(impl_->graph_->num_node_ids());
+  iter_state->InitializePending(impl_->graph_);
   iter_state->input_tensors =
       new std::vector<Entry>(impl_->total_input_tensors_);
 
@@ -1590,8 +1578,6 @@ void ExecutorState::CleanupFramesIterations(FrameState* frame, int64 iter,
     // Propagate all the dead exits to the parent frame.
     for (const Node* node : frame->dead_exits) {
       auto parent_iter_state = parent_frame->GetIteration(parent_iter);
-      std::vector<int>* pending = parent_iter_state->pending_count;
-      std::vector<int>* dead_count = parent_iter_state->dead_count;
       for (const Edge* e : node->out_edges()) {
         const Node* dst_node = e->dst();
         const int dst_id = dst_node->id();
@@ -1601,18 +1587,20 @@ void ExecutorState::CleanupFramesIterations(FrameState* frame, int64 iter,
         // We know this is a dead input to dst
         if (IsMerge(dst_node)) {
           if (e->IsControlEdge()) {
-            (*pending)[dst_id] -= 2;
-            int count = (*pending)[dst_id];
-            dst_dead = ((*dead_count)[dst_id] == dst_node->num_inputs());
+            parent_iter_state->decrement_pending(dst_id, 2);
+            int count = parent_iter_state->pending(dst_id);
+            dst_dead = (parent_iter_state->dead_count(dst_id) ==
+                        dst_node->num_inputs());
             dst_ready = (count == 1) || ((count == 0) && dst_dead);
           } else {
-            ++(*dead_count)[dst_id];
-            dst_dead = ((*dead_count)[dst_id] == dst_node->num_inputs());
-            dst_ready = ((*pending)[dst_id] == 0) && dst_dead;
+            parent_iter_state->increment_dead_count(dst_id);
+            dst_dead = (parent_iter_state->dead_count(dst_id) ==
+                        dst_node->num_inputs());
+            dst_ready = (parent_iter_state->pending(dst_id) == 0) && dst_dead;
           }
         } else {
-          ++(*dead_count)[dst_id];
-          dst_ready = (--(*pending)[dst_id] == 0);
+          parent_iter_state->increment_dead_count(dst_id);
+          dst_ready = (parent_iter_state->decrement_pending(dst_id, 1) == 0);
         }
         if (dst_ready) {
           ready->push_back(
diff --git a/tensorflow/core/common_runtime/pending_counts.h b/tensorflow/core/common_runtime/pending_counts.h
new file mode 100644
index 00000000000..301b7154bcf
--- /dev/null
+++ b/tensorflow/core/common_runtime/pending_counts.h
@@ -0,0 +1,149 @@
+#ifndef THIRD_PARTY_TENSORFLOW_CORE_COMMON_RUNTIME_PENDING_COUNTS_H_
+#define THIRD_PARTY_TENSORFLOW_CORE_COMMON_RUNTIME_PENDING_COUNTS_H_
+
+/* Copyright 2015 Google Inc. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <unordered_map>
+#include "tensorflow/core/platform/logging.h"
+#include "tensorflow/core/platform/macros.h"
+#include "tensorflow/core/util/port.h"
+
+namespace tensorflow {
+
+// An internal helper class to keep track of pending and dead counts for nodes,
+// for use in the ExecutorState module.
+class PendingCounts {
+ public:
+  explicit PendingCounts(int num_nodes)
+      : num_nodes_(num_nodes), counts_(new PackedCounts[num_nodes]) {}
+
+  ~PendingCounts() { delete[] counts_; }
+
+  void set_initial_count(int id, int pending_count, int max_dead_count) {
+    DCHECK_GE(id, 0);
+    DCHECK_LT(id, num_nodes_);
+    if ((pending_count > kMaxCountForPackedCounts) ||
+        (max_dead_count > kMaxCountForPackedCounts)) {
+      // A value for which we have to use the large representation
+      DCHECK(overflow_.count(id) == 0);
+      LargeCounts c;
+      c.pending = pending_count;
+      c.dead_count = 0;
+      overflow_[id] = c;
+      PackedCounts pc;
+      pc.pending = 0;
+      pc.dead_count = 0;
+      pc.is_large = 1;
+      counts_[id] = pc;
+    } else {
+      PackedCounts pc;
+      pc.pending = pending_count;
+      pc.dead_count = 0;
+      pc.is_large = 0;
+      counts_[id] = pc;
+    }
+  }
+
+  int pending(int id) {
+    if (IsLarge(id)) {
+      return overflow_[id].pending;
+    } else {
+      return counts_[id].pending;
+    }
+  }
+  int decrement_pending(int id, int v) {
+    DCHECK_GE(pending(id), v);
+    if (IsLarge(id)) {
+      int* p = &(overflow_[id].pending);
+      (*p) -= v;
+      return *p;
+    } else {
+      counts_[id].pending -= v;
+      return counts_[id].pending;
+    }
+  }
+  // Mark a merge node as live
+  // REQUIRES: Node corresponding to "id" is a merge node
+  void mark_live(int id) {
+    if (IsLarge(id)) {
+      overflow_[id].pending |= 0x1;
+    } else {
+      counts_[id].pending |= 0x1;
+    }
+  }
+
+  int dead_count(int id) {
+    int r = IsLarge(id) ? overflow_[id].dead_count : counts_[id].dead_count;
+    return r;
+  }
+  void increment_dead_count(int id) {
+    if (IsLarge(id)) {
+      overflow_[id].dead_count++;
+    } else {
+      DCHECK_LT(counts_[id].dead_count, kMaxCountForPackedCounts);
+      counts_[id].dead_count++;
+    }
+  }
+
+ private:
+  // We keep track of the pending count and dead input count for each
+  // graph node.  The representation used here is designed to be cache
+  // efficient for graphs with large numbers of nodes, where most
+  // nodes have relatively small maximum pending counts (e.g. for one
+  // LSTM model, 99% of 5000+ nodes had in-degrees of 3 or less).  We
+  // use one byte to hold both the pending and dead count for a node
+  // where these together can fit in one byte, and we use a hash table
+  // to handle the rare node ids that need larger counts than this.
+
+  // TODO(yuanbyu): We current use O(# of nodes in partition) space
+  // even for nested iterations where only a small fraction of the
+  // nodes are involved.  This is not efficient if the subgraph for
+  // the frame is only a small subset of the partition. We should make
+  // the vector size to be only the size of the frame subgraph.
+
+  const int kMaxCountForPackedCounts = 7;  // We use 3 bits for dead_count
+
+  bool IsLarge(int id) const {
+    DCHECK_GE(id, 0);
+    DCHECK_LT(id, num_nodes_);
+    return counts_[id].is_large;
+  }
+  // Most counts are small, so we pack a pending count and a dead
+  // count into 4 bits and 3 bits, respectively, and then use one bit
+  // as a marker bit.  If "is_large" is true, then the true pending
+  // and dead_count for that "id" are stored as full 32-bit counts in
+  // "overflow_", a hash table indexed by id.
+  struct PackedCounts {
+    uint8 pending : 4;
+    uint8 dead_count : 3;
+    uint8 is_large : 1;
+  };
+
+  struct LargeCounts {
+    int pending = 0;
+    int dead_count = 0;
+  };
+
+  const int num_nodes_;  // Just for bounds checking in debug mode
+  PackedCounts* counts_;
+  std::unordered_map<int, LargeCounts> overflow_;
+
+  TF_DISALLOW_COPY_AND_ASSIGN(PendingCounts);
+};
+
+}  // end namespace tensorflow
+
+#endif  // THIRD_PARTY_TENSORFLOW_CORE_COMMON_RUNTIME_PENDING_COUNTS_H_
diff --git a/tensorflow/core/common_runtime/pending_counts_test.cc b/tensorflow/core/common_runtime/pending_counts_test.cc
new file mode 100644
index 00000000000..464828365b7
--- /dev/null
+++ b/tensorflow/core/common_runtime/pending_counts_test.cc
@@ -0,0 +1,80 @@
+/* Copyright 2015 Google Inc. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <memory>
+#include <unordered_map>
+
+#include "tensorflow/core/common_runtime/pending_counts.h"
+#include "tensorflow/core/platform/test.h"
+
+namespace tensorflow {
+
+TEST(PendingCounts, Simple) {
+  const int C = 300;
+  PendingCounts c(C);
+  for (int id = 0; id < C; id++) {
+    c.set_initial_count(id, id, id);
+  }
+  for (int id = 0; id < C; id++) {
+    EXPECT_EQ(c.pending(id), id);
+    EXPECT_EQ(c.dead_count(id), 0);
+  }
+  EXPECT_EQ(c.decrement_pending(1, 1), 0);
+  EXPECT_EQ(c.decrement_pending(3, 1), 2);
+  EXPECT_EQ(c.decrement_pending(3, 1), 1);
+  c.decrement_pending(5, 1);
+  c.decrement_pending(5, 3);
+  c.decrement_pending(170, 1);
+  c.decrement_pending(170, 13);
+  EXPECT_EQ(c.pending(1), 0);
+  EXPECT_EQ(c.pending(3), 1);
+  EXPECT_EQ(c.pending(5), 1);
+  EXPECT_EQ(c.pending(170), 156);
+
+  for (int id = 0; id < C; id++) {
+    c.increment_dead_count(id);
+    EXPECT_EQ(c.dead_count(id), 1);
+  }
+}
+
+TEST(PendingCounts, SmallPendingLargeDead) {
+  PendingCounts c(1);
+  c.set_initial_count(0, 1, 10);
+  EXPECT_EQ(c.pending(0), 1);
+  EXPECT_EQ(c.dead_count(0), 0);
+  for (int i = 1; i <= 10; i++) {
+    c.increment_dead_count(0);
+    EXPECT_EQ(c.dead_count(0), i);
+  }
+  EXPECT_EQ(c.pending(0), 1);
+  EXPECT_EQ(c.decrement_pending(0, 1), 0);
+  EXPECT_EQ(c.dead_count(0), 10);
+}
+
+TEST(PendingCounts, MarkLiveShowsUpAsCount) {
+  PendingCounts c(3);
+  c.set_initial_count(1, 4, 4);
+  EXPECT_EQ(c.pending(1), 4);
+  c.mark_live(1);
+  EXPECT_EQ(c.pending(1), 5);
+  // mark_live should be idempotent
+  c.mark_live(1);
+  EXPECT_EQ(c.pending(1), 5);
+
+  c.decrement_pending(1, 2);
+  EXPECT_EQ(c.pending(1), 3);
+}
+
+}  // namespace tensorflow

commit bd3af957185f2835c638a045c80d6c11b7f71df1
Author: A. Unique TensorFlower <nobody@tensorflow.org>
Date:   Fri Jan 15 16:55:13 2016 -0800

    Slight efficiency improvements for setting up binary op kernels.
    Change: 112296712

diff --git a/tensorflow/core/kernels/cwise_ops_common.cc b/tensorflow/core/kernels/cwise_ops_common.cc
index 3af02f4ee6d..82e12f0540b 100644
--- a/tensorflow/core/kernels/cwise_ops_common.cc
+++ b/tensorflow/core/kernels/cwise_ops_common.cc
@@ -30,14 +30,16 @@ void BinaryOpShared::SetUnimplementedError(OpKernelContext* ctx) {
 }
 
 static BCast::Vec FromShape(const TensorShape& shape) {
-  BCast::Vec ret;
-  for (int i = 0; i < shape.dims(); ++i) ret.push_back(shape.dim_size(i));
+  const int N = shape.dims();
+  BCast::Vec ret(N);
+  for (int i = 0; i < N; ++i) {
+    ret[i] = shape.dim_size(i);
+  }
   return ret;
 }
 
 static TensorShape ToShape(const BCast::Vec& vec) {
-  TensorShape shape;
-  for (auto elem : vec) shape.AddDim(elem);
+  TensorShape shape(vec);
   return shape;
 }
 
commit f6548bffe577202381fa5893ad7aa452ae4e4931
Author: Xuechen Li <lxuechen@google.com>
Date:   Mon Jul 23 12:46:01 2018 -0700

    Add main script training sampler on 2D energy landscapes.
    
    PiperOrigin-RevId: 205707483

diff --git a/tensorflow/contrib/eager/python/examples/l2hmc/l2hmc.py b/tensorflow/contrib/eager/python/examples/l2hmc/l2hmc.py
index 275aee51308..14b8324e488 100644
--- a/tensorflow/contrib/eager/python/examples/l2hmc/l2hmc.py
+++ b/tensorflow/contrib/eager/python/examples/l2hmc/l2hmc.py
@@ -32,20 +32,28 @@ from tensorflow.contrib.eager.python.examples.l2hmc import neural_nets
 
 
 class Dynamics(tf.keras.Model):
-  """Dynamics engine of naive L2HMC sampler.
-
-  Args:
-    x_dim: dimensionality of observed data
-    loglikelihood_fn: log-likelihood function of conditional probability
-    n_steps: number of leapfrog steps within each transition
-    eps: initial value learnable scale of step size
-  """
-
-  def __init__(self, x_dim, loglikelihood_fn, n_steps=25, eps=.1):
+  """Dynamics engine of naive L2HMC sampler."""
+
+  def __init__(self,
+               x_dim,
+               minus_loglikelihood_fn,
+               n_steps=25,
+               eps=.1,
+               np_seed=1):
+    """Initialization.
+
+    Args:
+      x_dim: dimensionality of observed data
+      minus_loglikelihood_fn: log-likelihood function of conditional probability
+      n_steps: number of leapfrog steps within each transition
+      eps: initial value learnable scale of step size
+      np_seed: Random seed for numpy; used to control sampled masks.
+    """
     super(Dynamics, self).__init__()
 
+    npr.seed(np_seed)
     self.x_dim = x_dim
-    self.potential = loglikelihood_fn
+    self.potential = minus_loglikelihood_fn
     self.n_steps = n_steps
 
     self._construct_time()
@@ -68,8 +76,8 @@ class Dynamics(tf.keras.Model):
         position, forward=False)
 
     # Decide direction uniformly
-    forward_mask = tf.cast(
-        tf.random_uniform(shape=[tf.shape(position)[0]]) > .5, tf.float32)
+    batch_size = tf.shape(position)[0]
+    forward_mask = tf.cast(tf.random_uniform((batch_size,)) > .5, tf.float32)
     backward_mask = 1. - forward_mask
 
     # Obtain proposed states
@@ -108,7 +116,6 @@ class Dynamics(tf.keras.Model):
       position_post, momentum_post, logdet = lf_fn(position_post, momentum_post,
                                                    i)
       sumlogdet += logdet
-
     accept_prob = self._compute_accept_prob(position, momentum, position_post,
                                             momentum_post, sumlogdet)
 
@@ -125,17 +132,17 @@ class Dynamics(tf.keras.Model):
     sumlogdet += logdet
 
     position, logdet = self._update_position_forward(position, momentum, t,
-                                                     mask)
+                                                     mask, mask_inv)
     sumlogdet += logdet
 
     position, logdet = self._update_position_forward(position, momentum, t,
-                                                     mask_inv)
+                                                     mask_inv, mask)
     sumlogdet += logdet
 
     momentum, logdet = self._update_momentum_forward(position, momentum, t)
     sumlogdet += logdet
 
-    return position, momentum, tf.reduce_sum(sumlogdet, axis=1)
+    return position, momentum, sumlogdet
 
   def _backward_lf(self, position, momentum, i):
     """One backward augmented leapfrog step. See Appendix A in paper."""
@@ -149,17 +156,17 @@ class Dynamics(tf.keras.Model):
     sumlogdet += logdet
 
     position, logdet = self._update_position_backward(position, momentum, t,
-                                                      mask)
+                                                      mask_inv, mask)
     sumlogdet += logdet
 
     position, logdet = self._update_position_backward(position, momentum, t,
-                                                      mask_inv)
+                                                      mask, mask_inv)
     sumlogdet += logdet
 
     momentum, logdet = self._update_momentum_backward(position, momentum, t)
     sumlogdet += logdet
 
-    return position, momentum, tf.reduce_sum(sumlogdet, axis=1)
+    return position, momentum, sumlogdet
 
   def _update_momentum_forward(self, position, momentum, t):
     """Update v in the forward leapfrog step."""
@@ -172,12 +179,11 @@ class Dynamics(tf.keras.Model):
         momentum * tf.exp(scale) -
         .5 * self.eps * (tf.exp(transformed) * grad - translation))
 
-    return momentum, scale
+    return momentum, tf.reduce_sum(scale, axis=1)
 
-  def _update_position_forward(self, position, momentum, t, mask):
+  def _update_position_forward(self, position, momentum, t, mask, mask_inv):
     """Update x in the forward leapfrog step."""
 
-    mask_inv = 1. - mask
     scale, translation, transformed = self.position_fn(
         [momentum, mask * position, t])
     scale *= self.eps
@@ -186,8 +192,7 @@ class Dynamics(tf.keras.Model):
         mask * position +
         mask_inv * (position * tf.exp(scale) + self.eps *
                     (tf.exp(transformed) * momentum + translation)))
-
-    return position, mask_inv * scale
+    return position, tf.reduce_sum(mask_inv * scale, axis=1)
 
   def _update_momentum_backward(self, position, momentum, t):
     """Update v in the backward leapfrog step. Inverting the forward update."""
@@ -200,21 +205,20 @@ class Dynamics(tf.keras.Model):
         tf.exp(scale) * (momentum + .5 * self.eps *
                          (tf.exp(transformed) * grad - translation)))
 
-    return momentum, scale
+    return momentum, tf.reduce_sum(scale, axis=1)
 
-  def _update_position_backward(self, position, momentum, t, mask):
+  def _update_position_backward(self, position, momentum, t, mask, mask_inv):
     """Update x in the backward leapfrog step. Inverting the forward update."""
 
-    mask_inv = 1. - mask
     scale, translation, transformed = self.position_fn(
-        [momentum, mask_inv * position, t])
+        [momentum, mask * position, t])
     scale *= -self.eps
     transformed *= self.eps
     position = (
-        mask_inv * position + mask * tf.exp(scale) *
-        (position - self.eps * tf.exp(transformed) * momentum + translation))
+        mask * position + mask_inv * tf.exp(scale) *
+        (position - self.eps * (tf.exp(transformed) * momentum + translation)))
 
-    return position, mask * scale
+    return position, tf.reduce_sum(mask_inv * scale, axis=1)
 
   def _compute_accept_prob(self, position, momentum, position_post,
                            momentum_post, sumlogdet):
@@ -222,8 +226,10 @@ class Dynamics(tf.keras.Model):
 
     old_hamil = self.hamiltonian(position, momentum)
     new_hamil = self.hamiltonian(position_post, momentum_post)
+    prob = tf.exp(tf.minimum(old_hamil - new_hamil + sumlogdet, 0.))
 
-    return tf.exp(tf.minimum(old_hamil - new_hamil + sumlogdet, 0.))
+    # Ensure numerical stability as well as correct gradients
+    return tf.where(tf.is_finite(prob), prob, tf.zeros_like(prob))
 
   def _construct_time(self):
     """Convert leapfrog step index into sinusoidal time."""
@@ -248,6 +254,8 @@ class Dynamics(tf.keras.Model):
 
     self.masks = []
     for _ in range(self.n_steps):
+      # Need to use npr here because tf would generated different random
+      # values across different `sess.run`
       idx = npr.permutation(np.arange(self.x_dim))[:self.x_dim // 2]
       mask = np.zeros((self.x_dim,))
       mask[idx] = 1.
@@ -273,19 +281,15 @@ class Dynamics(tf.keras.Model):
   def grad_potential(self, position, check_numerics=True):
     """Get gradient of potential function at current location."""
 
-    if not tf.executing_eagerly():
-      # TODO(lxuechen): Change this to tfe.gradients_function when it works
-      grad = tf.gradients(self.potential(position), position)[0]
-    else:
+    if tf.executing_eagerly():
       grad = tfe.gradients_function(self.potential)(position)[0]
-
-    if check_numerics:
-      return tf.check_numerics(grad, message="gradient of potential")
+    else:
+      grad = tf.gradients(self.potential(position), position)[0]
 
     return grad
 
 
-# Examples of unnormalized log density/probabilities
+# Examples of unnormalized log densities
 def get_scg_energy_fn():
   """Get energy function for 2d strongly correlated Gaussian."""
 
@@ -295,32 +299,53 @@ def get_scg_energy_fn():
   sigma_inv = tf.matrix_inverse(sigma)
 
   def energy(x):
-    """Unnormalized log density/energy of 2d strongly correlated Gaussian."""
+    """Unnormalized minus log density of 2d strongly correlated Gaussian."""
 
     xmmu = x - mu
     return .5 * tf.diag_part(
         tf.matmul(tf.matmul(xmmu, sigma_inv), tf.transpose(xmmu)))
 
-  return energy
+  return energy, mu, sigma
 
 
-def get_multivariate_gaussian_energy_fn(x_dim=2):
-  """Get energy function for 2d strongly correlated Gaussian."""
-
-  mu = tf.random_normal(shape=[x_dim])
-  # Lower triangularize and positive diagonal
-  l = tf.sigmoid(
-      tf.matrix_band_part(tf.random_normal(shape=[x_dim, x_dim]), -1, 0))
-  # Exploit Cholesky decomposition
-  sigma = tf.matmul(l, tf.transpose(l))
-  sigma *= 100.  # Small covariance causes extreme numerical instability
-  sigma_inv = tf.matrix_inverse(sigma)
+def get_rw_energy_fn():
+  """Get energy function for rough well distribution."""
+  # For small eta, the density underlying the rough-well energy is very close to
+  # a unit Gaussian; however, the gradient is greatly affected by the small
+  # cosine perturbations
+  eta = 1e-2
+  mu = tf.constant([0., 0.])
+  sigma = tf.constant([[1., 0.], [0., 1.]])
 
   def energy(x):
-    """Unnormalized log density/energy of 2d strongly correlated Gaussian."""
+    ip = tf.reduce_sum(x**2., axis=1)
+    return .5 * ip + eta * tf.reduce_sum(tf.cos(x / eta), axis=1)
 
-    xmmu = x - mu
-    return .5 * tf.diag_part(
-        tf.matmul(tf.matmul(xmmu, sigma_inv), tf.transpose(xmmu)))
+  return energy, mu, sigma
+
+
+# Loss function
+def compute_loss(dynamics, x, scale=.1, eps=1e-4):
+  """Compute loss defined in equation (8)."""
+
+  z = tf.random_normal(tf.shape(x))  # Auxiliary variable
+  x_, _, x_accept_prob, x_out = dynamics.apply_transition(x)
+  z_, _, z_accept_prob, _ = dynamics.apply_transition(z)
+
+  # Add eps for numerical stability; following released impl
+  x_loss = tf.reduce_sum((x - x_)**2, axis=1) * x_accept_prob + eps
+  z_loss = tf.reduce_sum((z - z_)**2, axis=1) * z_accept_prob + eps
+
+  loss = tf.reduce_mean(
+      (1. / x_loss + 1. / z_loss) * scale - (x_loss + z_loss) / scale, axis=0)
+
+  return loss, x_out, x_accept_prob
+
+
+def loss_and_grads(dynamics, x, loss_fn=compute_loss):
+  """Obtain loss value and gradients."""
+  with tf.GradientTape() as tape:
+    loss_val, out, accept_prob = loss_fn(dynamics, x)
+  grads = tape.gradient(loss_val, dynamics.trainable_variables)
 
-  return energy
+  return loss_val, grads, out, accept_prob
diff --git a/tensorflow/contrib/eager/python/examples/l2hmc/l2hmc_test.py b/tensorflow/contrib/eager/python/examples/l2hmc/l2hmc_test.py
index e33b4cae4c7..95574798853 100644
--- a/tensorflow/contrib/eager/python/examples/l2hmc/l2hmc_test.py
+++ b/tensorflow/contrib/eager/python/examples/l2hmc/l2hmc_test.py
@@ -37,63 +37,37 @@ def get_default_hparams():
       n_warmup_iters=3)
 
 
-# Relevant functions for benchmarking
-def compute_loss(dynamics, x, scale=.1, eps=1e-4):
-  """Compute loss defined in equation (8)."""
-
-  z = tf.random_normal(tf.shape(x))
-  x_, _, x_accept_prob, x_out = dynamics.apply_transition(x)
-  z_, _, z_accept_prob, _ = dynamics.apply_transition(z)
-
-  # Add eps for numerical stability; following released impl
-  x_loss = tf.reduce_sum((x - x_)**2, axis=1) * x_accept_prob + eps
-  z_loss = tf.reduce_sum((z - z_)**2, axis=1) * z_accept_prob + eps
-
-  loss = tf.reduce_mean(
-      (1. / x_loss + 1. / z_loss) * scale - (x_loss + z_loss) / scale, axis=0)
-
-  return loss, x_out
-
-
-def loss_and_grads(dynamics, x, loss_fn=compute_loss):
-  """Obtain loss value and gradients."""
-
-  with tf.GradientTape() as tape:
-    loss_val, x_out = loss_fn(dynamics, x)
-  grads = tape.gradient(loss_val, dynamics.variables)
-
-  return loss_val, grads, x_out
-
-
-def warmup(dynamics, optimizer, n_iters=1, n_samples=200, loss_fn=compute_loss):
+def warmup(dynamics,
+           optimizer,
+           n_iters=1,
+           n_samples=200,
+           loss_fn=l2hmc.compute_loss):
   """Warmup optimization to reduce overhead."""
 
   samples = tf.random_normal(
       shape=[n_samples, dynamics.x_dim], dtype=tf.float32)
 
   for _ in range(n_iters):
-    _, grads, samples = loss_and_grads(dynamics, samples, loss_fn=loss_fn)
+    _, grads, samples, _ = l2hmc.loss_and_grads(
+        dynamics, samples, loss_fn=loss_fn)
     optimizer.apply_gradients(zip(grads, dynamics.variables))
 
 
 def fit(dynamics,
         samples,
         optimizer,
-        loss_fn=compute_loss,
+        loss_fn=l2hmc.compute_loss,
         n_iters=5000,
         verbose=True,
-        logdir=None,
-        decay_lr=True):
+        logdir=None):
   """Fit L2HMC sampler with given log-likelihood function."""
 
   if logdir:
     summary_writer = tf.contrib.summary.create_file_writer(logdir)
 
   for i in range(n_iters):
-    loss, grads, samples = loss_and_grads(dynamics, samples, loss_fn=loss_fn)
-    # TODO(lxuechen): Proper learning rate decay
-    if decay_lr:
-      grads = [grad * .96**(i // 1000) for grad in grads]
+    loss, grads, samples, _ = l2hmc.loss_and_grads(
+        dynamics, samples, loss_fn=loss_fn)
     optimizer.apply_gradients(zip(grads, dynamics.variables))
     if verbose:
       print("Iteration %d: loss %.4f" % (i, loss))
@@ -112,9 +86,10 @@ class L2hmcTest(tf.test.TestCase):
 
     # Eager mode testing
     hparams = get_default_hparams()
+    energy_fn, _, _ = l2hmc.get_scg_energy_fn()
     dynamics = l2hmc.Dynamics(
         x_dim=hparams.x_dim,
-        loglikelihood_fn=l2hmc.get_scg_energy_fn(),
+        minus_loglikelihood_fn=energy_fn,
         n_steps=hparams.n_steps,
         eps=hparams.eps)
     samples = tf.random_normal(shape=[hparams.n_samples, hparams.x_dim])
@@ -127,9 +102,10 @@ class L2hmcTest(tf.test.TestCase):
 
     # Graph mode testing
     with tf.Graph().as_default():
+      energy_fn, _, _ = l2hmc.get_scg_energy_fn()
       dynamics = l2hmc.Dynamics(
           x_dim=hparams.x_dim,
-          loglikelihood_fn=l2hmc.get_scg_energy_fn(),
+          minus_loglikelihood_fn=energy_fn,
           n_steps=hparams.n_steps,
           eps=hparams.eps)
       x = tf.placeholder(tf.float32, shape=[None, hparams.x_dim])
@@ -150,32 +126,20 @@ class L2hmcTest(tf.test.TestCase):
 class L2hmcBenchmark(tf.test.Benchmark):
   """Eager and graph benchmarks for l2hmc."""
 
-  def _get_energy_fn(self):
-    """Get specific energy function according to FLAGS."""
-
-    if FLAGS.energy_fn == "scg":
-      energy_fn = l2hmc.get_scg_energy_fn()
-    elif FLAGS.energy_fn == "multivariate_gaussian":
-      energy_fn = l2hmc.get_multivariate_gaussian_energy_fn(x_dim=FLAGS.x_dim)
-    else:
-      raise ValueError("No such energy function %s" % FLAGS.energy_fn)
-
-    return energy_fn
-
   def benchmark_graph(self):
     """Benchmark Graph performance."""
 
     hparams = get_default_hparams()
     tf.reset_default_graph()
     with tf.Graph().as_default():
-      energy_fn = self._get_energy_fn()
+      energy_fn, _, _ = l2hmc.get_scg_energy_fn()
       dynamics = l2hmc.Dynamics(
           x_dim=hparams.x_dim,
-          loglikelihood_fn=energy_fn,
+          minus_loglikelihood_fn=energy_fn,
           n_steps=hparams.n_steps,
           eps=hparams.eps)
       x = tf.placeholder(tf.float32, shape=[None, hparams.x_dim])
-      loss, x_out = compute_loss(dynamics, x)
+      loss, x_out, _ = l2hmc.compute_loss(dynamics, x)
 
       global_step = tf.Variable(0., name="global_step", trainable=False)
       learning_rate = tf.train.exponential_decay(
@@ -183,7 +147,11 @@ class L2hmcBenchmark(tf.test.Benchmark):
       optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
       train_op = optimizer.minimize(loss, global_step=global_step)
 
-      with tf.Session() as sess:
+      # Single thread; fairer comparison against eager
+      session_conf = tf.ConfigProto(
+          intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
+
+      with tf.Session(config=session_conf) as sess:
         sess.run(tf.global_variables_initializer())
 
         # Warmup to reduce initialization effect when timing
@@ -218,14 +186,14 @@ class L2hmcBenchmark(tf.test.Benchmark):
     """Benchmark Eager performance."""
 
     hparams = get_default_hparams()
-    energy_fn = self._get_energy_fn()
+    energy_fn, _, _ = l2hmc.get_scg_energy_fn()
     dynamics = l2hmc.Dynamics(
         x_dim=hparams.x_dim,
-        loglikelihood_fn=energy_fn,
+        minus_loglikelihood_fn=energy_fn,
         n_steps=hparams.n_steps,
         eps=hparams.eps)
     optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)
-    loss_fn = tfe.defun(compute_loss) if defun else compute_loss
+    loss_fn = tfe.defun(l2hmc.compute_loss) if defun else l2hmc.compute_loss
 
     # Warmup to reduce initialization effect when timing
     warmup(dynamics, optimizer, n_iters=hparams.n_warmup_iters, loss_fn=loss_fn)
@@ -234,12 +202,7 @@ class L2hmcBenchmark(tf.test.Benchmark):
     samples = tf.random_normal(
         shape=[hparams.n_samples, hparams.x_dim], dtype=tf.float32)
     start_time = time.time()
-    fit(dynamics,
-        samples,
-        optimizer,
-        loss_fn=loss_fn,
-        n_iters=hparams.n_iters,
-        decay_lr=True)
+    fit(dynamics, samples, optimizer, loss_fn=loss_fn, n_iters=hparams.n_iters)
     wall_time = time.time() - start_time
     examples_per_sec = hparams.n_samples / wall_time
 
@@ -251,14 +214,8 @@ class L2hmcBenchmark(tf.test.Benchmark):
         wall_time=wall_time)
 
     del dynamics
-    del loss_fn
 
 
 if __name__ == "__main__":
-  tf.flags.DEFINE_string("energy_fn", "scg",
-                         ("The energy function/unnormalized log-probability. "
-                          "Either be `scg` or `multivariate_gaussian`"))
-  tf.flags.DEFINE_integer("x_dim", 2, "Dimensionality of observation space.")
-  FLAGS = tf.flags.FLAGS
   tf.enable_eager_execution()
   tf.test.main()
diff --git a/tensorflow/contrib/eager/python/examples/l2hmc/main.py b/tensorflow/contrib/eager/python/examples/l2hmc/main.py
new file mode 100644
index 00000000000..45e1f98429f
--- /dev/null
+++ b/tensorflow/contrib/eager/python/examples/l2hmc/main.py
@@ -0,0 +1,235 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""L2HMC on simple Gaussian mixture model with TensorFlow eager."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import sys
+
+from absl import flags
+import numpy as np
+import tensorflow as tf
+from tensorflow.contrib.eager.python.examples.l2hmc import l2hmc
+try:
+  import matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top
+  HAS_MATPLOTLIB = True
+except ImportError:
+  HAS_MATPLOTLIB = False
+tfe = tf.contrib.eager
+
+
+def main(_):
+  tf.enable_eager_execution()
+  global_step = tf.train.get_or_create_global_step()
+  global_step.assign(1)
+
+  energy_fn, mean, covar = {
+      "scg": l2hmc.get_scg_energy_fn(),
+      "rw": l2hmc.get_rw_energy_fn()
+  }[FLAGS.energy_fn]
+
+  x_dim = 2
+  train_iters = 5000
+  eval_iters = 2000
+  eps = 0.1
+  n_steps = 10  # Chain length
+  n_samples = 200
+  record_loss_every = 100
+
+  dynamics = l2hmc.Dynamics(
+      x_dim=x_dim, minus_loglikelihood_fn=energy_fn, n_steps=n_steps, eps=eps)
+  learning_rate = tf.train.exponential_decay(
+      1e-3, global_step, 1000, 0.96, staircase=True)
+  optimizer = tf.train.AdamOptimizer(learning_rate)
+  checkpointer = tf.train.Checkpoint(
+      optimizer=optimizer, dynamics=dynamics, global_step=global_step)
+
+  if FLAGS.train_dir:
+    summary_writer = tf.contrib.summary.create_file_writer(FLAGS.train_dir)
+    if FLAGS.restore:
+      latest_path = tf.train.latest_checkpoint(FLAGS.train_dir)
+      checkpointer.restore(latest_path)
+      print("Restored latest checkpoint at path:\"{}\" ".format(latest_path))
+      sys.stdout.flush()
+
+  if not FLAGS.restore:
+    # Training
+    if FLAGS.use_defun:
+      # Use `tfe.deun` to boost performance when there are lots of small ops
+      loss_fn = tfe.defun(l2hmc.compute_loss)
+    else:
+      loss_fn = l2hmc.compute_loss
+
+    samples = tf.random_normal(shape=[n_samples, x_dim])
+    for i in range(1, train_iters + 1):
+      loss, samples, accept_prob = train_one_iter(
+          dynamics,
+          samples,
+          optimizer,
+          loss_fn=loss_fn,
+          global_step=global_step)
+
+      if i % record_loss_every == 0:
+        print("Iteration {}, loss {:.4f}, x_accept_prob {:.4f}".format(
+            i, loss.numpy(),
+            accept_prob.numpy().mean()))
+        if FLAGS.train_dir:
+          with summary_writer.as_default():
+            with tf.contrib.summary.always_record_summaries():
+              tf.contrib.summary.scalar("Training loss", loss, step=global_step)
+    print("Training complete.")
+    sys.stdout.flush()
+
+    if FLAGS.train_dir:
+      saved_path = checkpointer.save(
+          file_prefix=os.path.join(FLAGS.train_dir, "ckpt"))
+      print("Saved checkpoint at path: \"{}\" ".format(saved_path))
+      sys.stdout.flush()
+
+  # Evaluation
+  if FLAGS.use_defun:
+    # Use tfe.deun to boost performance when there are lots of small ops
+    apply_transition = tfe.defun(dynamics.apply_transition)
+  else:
+    apply_transition = dynamics.apply_transition
+
+  samples = tf.random_normal(shape=[n_samples, x_dim])
+  samples_history = []
+  for i in range(eval_iters):
+    samples_history.append(samples.numpy())
+    _, _, _, samples = apply_transition(samples)
+  samples_history = np.array(samples_history)
+  print("Sampling complete.")
+  sys.stdout.flush()
+
+  # Mean and covariance of target distribution
+  mean = mean.numpy()
+  covar = covar.numpy()
+  ac_spectrum = compute_ac_spectrum(samples_history, mean, covar)
+  print("First 25 entries of the auto-correlation spectrum: {}".format(
+      ac_spectrum[:25]))
+  ess = compute_ess(ac_spectrum)
+  print("Effective sample size per Metropolis-Hastings step: {}".format(ess))
+  sys.stdout.flush()
+
+  if FLAGS.train_dir:
+    # Plot autocorrelation spectrum in tensorboard
+    plot_step = tfe.Variable(1, trainable=False, dtype=tf.int64)
+
+    for ac in ac_spectrum:
+      with summary_writer.as_default():
+        with tf.contrib.summary.always_record_summaries():
+          tf.contrib.summary.scalar("Autocorrelation", ac, step=plot_step)
+      plot_step.assign(plot_step + n_steps)
+
+    if HAS_MATPLOTLIB:
+      # Choose a single chain and plot the trajectory
+      single_chain = samples_history[:, 0, :]
+      xs = single_chain[:100, 0]
+      ys = single_chain[:100, 1]
+      plt.figure()
+      plt.plot(xs, ys, color="orange", marker="o", alpha=0.6)  # Trained chain
+      plt.savefig(os.path.join(FLAGS.train_dir, "single_chain.png"))
+
+
+def train_one_iter(dynamics,
+                   x,
+                   optimizer,
+                   loss_fn=l2hmc.compute_loss,
+                   global_step=None):
+  """Train the sampler for one iteration."""
+  loss, grads, out, accept_prob = l2hmc.loss_and_grads(
+      dynamics, x, loss_fn=loss_fn)
+  optimizer.apply_gradients(
+      zip(grads, dynamics.trainable_variables), global_step=global_step)
+
+  return loss, out, accept_prob
+
+
+def compute_ac_spectrum(samples_history, target_mean, target_covar):
+  """Compute autocorrelation spectrum.
+
+  Follows equation 15 from the L2HMC paper.
+
+  Args:
+    samples_history: Numpy array of shape [T, B, D], where T is the total
+        number of time steps, B is the batch size, and D is the dimensionality
+        of sample space.
+    target_mean: 1D Numpy array of the mean of target(true) distribution.
+    target_covar: 2D Numpy array representing a symmetric matrix for variance.
+  Returns:
+    Autocorrelation spectrum, Numpy array of shape [T-1].
+  """
+
+  # Using numpy here since eager is a bit slow due to the loop
+  time_steps = samples_history.shape[0]
+  trace = np.trace(target_covar)
+
+  rhos = []
+  for t in range(time_steps - 1):
+    rho_t = 0.
+    for tau in range(time_steps - t):
+      v_tau = samples_history[tau, :, :] - target_mean
+      v_tau_plus_t = samples_history[tau + t, :, :] - target_mean
+      # Take dot product over observation dims and take mean over batch dims
+      rho_t += np.mean(np.sum(v_tau * v_tau_plus_t, axis=1))
+
+    rho_t /= trace * (time_steps - t)
+    rhos.append(rho_t)
+
+  return np.array(rhos)
+
+
+def compute_ess(ac_spectrum):
+  """Compute the effective sample size based on autocorrelation spectrum.
+
+  This follows equation 16 from the L2HMC paper.
+
+  Args:
+    ac_spectrum: Autocorrelation spectrum
+  Returns:
+    The effective sample size
+  """
+  # Cutoff from the first value less than 0.05
+  cutoff = np.argmax(ac_spectrum[1:] < .05)
+  if cutoff == 0:
+    cutoff = len(ac_spectrum)
+  ess = 1. / (1. + 2. * np.sum(ac_spectrum[1:cutoff]))
+  return ess
+
+
+if __name__ == "__main__":
+  flags.DEFINE_string(
+      "train_dir",
+      default=None,
+      help="[Optional] Directory to store the training information")
+  flags.DEFINE_boolean(
+      "restore",
+      default=False,
+      help="[Optional] Restore the latest checkpoint from `train_dir` if True")
+  flags.DEFINE_boolean(
+      "use_defun",
+      default=False,
+      help="[Optional] Use `tfe.defun` to boost performance")
+  flags.DEFINE_string(
+      "energy_fn",
+      default="scg",
+      help="[Optional] The energy function used for experimentation"
+      "Other options include `rw`")
+  FLAGS = flags.FLAGS
+  tf.app.run(main)

commit 1eeca01d5c8702764f7597b5e9745573adefc88e
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Thu Sep 28 10:06:29 2017 -0700

    Add `tf.contrib.bayesflow.metropolis_hastings`.
    The Metropolis-Hastings accept/reject framework is useful for constructing various MCMC algorithms. Many of the MCMC algorithms are Metropolis-like, i.e., a proposal is generated and then the accept/reject procedure is performed. Current implementation accepts a user-defined target energy and proposal generating function (e.g., normal or HMC proposals) to produce a Markov Chain.
    
    PiperOrigin-RevId: 170358662

diff --git a/tensorflow/contrib/bayesflow/BUILD b/tensorflow/contrib/bayesflow/BUILD
index df3f93d3f0e..06ab0a1987f 100644
--- a/tensorflow/contrib/bayesflow/BUILD
+++ b/tensorflow/contrib/bayesflow/BUILD
@@ -19,20 +19,44 @@ py_library(
         "//tensorflow/contrib/framework:framework_py",
         "//tensorflow/python:array_ops",
         "//tensorflow/python:check_ops",
+        "//tensorflow/python:control_flow_ops",
+        "//tensorflow/python:framework",
         "//tensorflow/python:framework_for_generated_wrappers",
         "//tensorflow/python:math_ops",
         "//tensorflow/python:nn",
         "//tensorflow/python:nn_ops",
         "//tensorflow/python:platform",
+        "//tensorflow/python:random_ops",
+        "//tensorflow/python:state_ops",
         "//tensorflow/python:training",
         "//tensorflow/python:util",
         "//tensorflow/python:variable_scope",
+        "//tensorflow/python:variables",
         "//tensorflow/python/ops/distributions",
         "//third_party/py/numpy",
         "@six_archive//:six",
     ],
 )
 
+cuda_py_test(
+    name = "metropolis_hastings_test",
+    size = "medium",
+    srcs = ["python/kernel_tests/metropolis_hastings_test.py"],
+    additional_deps = [
+        ":bayesflow_py",
+        "//third_party/py/numpy",
+        "//tensorflow/python:array_ops",
+        "//tensorflow/python:math_ops",
+        "//tensorflow/python:client_testlib",
+        "//tensorflow/python:framework",
+        "//tensorflow/python:framework_for_generated_wrappers",
+        "//tensorflow/python:platform_test",
+        "//tensorflow/python:random_ops",
+        "//tensorflow/python:variable_scope",
+        "//tensorflow/python:variables",
+    ],
+)
+
 cuda_py_test(
     name = "csiszar_divergence_test",
     size = "medium",
diff --git a/tensorflow/contrib/bayesflow/__init__.py b/tensorflow/contrib/bayesflow/__init__.py
index 15c1614a671..6d486e7e157 100644
--- a/tensorflow/contrib/bayesflow/__init__.py
+++ b/tensorflow/contrib/bayesflow/__init__.py
@@ -24,6 +24,7 @@ from __future__ import print_function
 from tensorflow.contrib.bayesflow.python.ops import csiszar_divergence
 from tensorflow.contrib.bayesflow.python.ops import custom_grad
 from tensorflow.contrib.bayesflow.python.ops import entropy
+from tensorflow.contrib.bayesflow.python.ops import metropolis_hastings
 from tensorflow.contrib.bayesflow.python.ops import monte_carlo
 from tensorflow.contrib.bayesflow.python.ops import stochastic_gradient_estimators
 from tensorflow.contrib.bayesflow.python.ops import stochastic_graph
@@ -36,7 +37,7 @@ from tensorflow.python.util.all_util import remove_undocumented
 
 
 _allowed_symbols = ['csiszar_divergence', 'custom_grad', 'entropy',
-                    'monte_carlo', 'special_math',
+                    'metropolis_hastings', 'monte_carlo', 'special_math',
                     'stochastic_gradient_estimators', 'stochastic_graph',
                     'stochastic_tensor', 'stochastic_variables',
                     'variational_inference']
diff --git a/tensorflow/contrib/bayesflow/python/kernel_tests/metropolis_hastings_test.py b/tensorflow/contrib/bayesflow/python/kernel_tests/metropolis_hastings_test.py
new file mode 100644
index 00000000000..0784785e97a
--- /dev/null
+++ b/tensorflow/contrib/bayesflow/python/kernel_tests/metropolis_hastings_test.py
@@ -0,0 +1,178 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for metropolis_hastings.py."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+from tensorflow.contrib.bayesflow.python.ops import metropolis_hastings_impl as mh
+from tensorflow.python.framework import dtypes
+from tensorflow.python.ops import array_ops
+from tensorflow.python.ops import math_ops
+from tensorflow.python.ops import random_ops
+from tensorflow.python.ops import variable_scope
+from tensorflow.python.ops import variables
+from tensorflow.python.platform import test
+
+
+class McmcStepTest(test.TestCase):
+
+  def test_density_increasing_step_accepted(self):
+    """Tests that if a transition increases density, it is always accepted."""
+    target_log_density = lambda x: - x * x
+    state = variable_scope.get_variable('state', initializer=10.)
+    state_log_density = variable_scope.get_variable(
+        'state_log_density',
+        initializer=target_log_density(state.initialized_value()))
+    log_accept_ratio = variable_scope.get_variable(
+        'log_accept_ratio', initializer=0.)
+
+    get_next_proposal = lambda x: (x - 1., None)
+    step = mh.evolve(state, state_log_density, log_accept_ratio,
+                     target_log_density, get_next_proposal, seed=1234)
+    init = variables.initialize_all_variables()
+    with self.test_session() as sess:
+      sess.run(init)
+      for j in range(9):
+        sess.run(step)
+        sample = sess.run(state)
+        sample_log_density = sess.run(state_log_density)
+        self.assertAlmostEqual(sample, 9 - j)
+        self.assertAlmostEqual(sample_log_density, - (9 - j) * (9 - j))
+
+  def test_sample_properties(self):
+    """Tests that the samples converge to the target distribution."""
+
+    def target_log_density(x):
+      """Log-density corresponding to a normal distribution with mean = 4."""
+      return - (x - 2.0) * (x - 2.0) * 0.5
+
+    # Use the uniform random walker to generate proposals.
+    proposal_fn = mh.uniform_random_proposal(
+        step_size=1.0, seed=1234)
+
+    state = variable_scope.get_variable('state', initializer=0.0)
+    state_log_density = variable_scope.get_variable(
+        'state_log_density',
+        initializer=target_log_density(state.initialized_value()))
+
+    log_accept_ratio = variable_scope.get_variable(
+        'log_accept_ratio', initializer=0.)
+    # Random walk MCMC converges slowly so need to put in enough iterations.
+    num_iterations = 5000
+    step = mh.evolve(state, state_log_density, log_accept_ratio,
+                     target_log_density, proposal_fn, seed=4321)
+
+    init = variables.global_variables_initializer()
+
+    sample_sum, sample_sq_sum = 0.0, 0.0
+    with self.test_session() as sess:
+      sess.run(init)
+      for _ in np.arange(num_iterations):
+        # Allow for the mixing of the chain and discard these samples.
+        sess.run(step)
+      for _ in np.arange(num_iterations):
+        sess.run(step)
+        sample = sess.run(state)
+        sample_sum += sample
+        sample_sq_sum += sample * sample
+
+    sample_mean = sample_sum / num_iterations
+    sample_variance = sample_sq_sum / num_iterations - sample_mean * sample_mean
+    # The samples have large autocorrelation which reduces the effective sample
+    # size.
+    self.assertAlmostEqual(sample_mean, 2.0, delta=0.1)
+    self.assertAlmostEqual(sample_variance, 1.0, delta=0.1)
+
+  def test_normal_proposals(self):
+    """Tests that the normal proposals are correctly distributed."""
+
+    initial_points = array_ops.ones([10000], dtype=dtypes.float32)
+    proposal_fn = mh.normal_random_proposal(
+        scale=2.0, seed=1234)
+    proposal_points, _ = proposal_fn(initial_points)
+
+    with self.test_session() as sess:
+      sample = sess.run(proposal_points)
+
+    # It is expected that the elements in proposal_points have the same mean as
+    # initial_points and have the standard deviation that was supplied to the
+    # proposal scheme.
+    self.assertAlmostEqual(np.mean(sample), 1.0, delta=0.1)
+    self.assertAlmostEqual(np.std(sample), 2.0, delta=0.1)
+
+  def test_docstring_example(self):
+    """Tests the simplified docstring example with multiple chains."""
+
+    n = 2  # dimension of the problem
+
+    # Generate 500 initial values randomly. Each of these would be an
+    # independent starting point for a Markov chain.
+    state = variable_scope.get_variable(
+        'state', initializer=random_ops.random_normal(
+            [300, n], mean=3.0, dtype=dtypes.float32, seed=42))
+
+    # Computes the log(p(x)) for the unit normal density and ignores the
+    # normalization constant.
+    def log_density(x):
+      return  - math_ops.reduce_sum(x * x, reduction_indices=-1) / 2.0
+
+    # Initial log-density value
+    state_log_density = variable_scope.get_variable(
+        'state_log_density',
+        initializer=log_density(state.initialized_value()))
+
+    # A variable to store the log_acceptance_ratio:
+    log_acceptance_ratio = variable_scope.get_variable(
+        'log_acceptance_ratio',
+        initializer=array_ops.zeros([300], dtype=dtypes.float32))
+
+    # Generates random proposals by moving each coordinate uniformly and
+    # independently in a box of size 2 centered around the current value.
+    # Returns the new point and also the log of the Hastings ratio (the
+    # ratio of the probability of going from the proposal to origin and the
+    # probability of the reverse transition). When this ratio is 1, the value
+    # may be omitted and replaced by None.
+    def random_proposal(x):
+      return (x + random_ops.random_uniform(
+          array_ops.shape(x), minval=-1, maxval=1,
+          dtype=x.dtype, seed=12)), None
+
+    #  Create the op to propagate the chain for 100 steps.
+    stepper = mh.evolve(
+        state, state_log_density, log_acceptance_ratio,
+        log_density, random_proposal, n_steps=100, seed=123)
+    init = variables.initialize_all_variables()
+    with self.test_session() as sess:
+      sess.run(init)
+      # Run the chain for a total of 1000 and print out the mean across the
+      # chains every 100 iterations
+      for _ in range(10):
+        sess.run(stepper)
+      samples = sess.run(state)
+      covariance = np.eye(n)
+      self.assertAlmostEqual(
+          np.max(np.abs(np.mean(samples, 0)
+                        - np.zeros(n))), 0,
+          delta=0.1)
+      self.assertAlmostEqual(
+          np.max(np.abs(np.reshape(np.cov(samples, rowvar=False), [n**2])
+                        - np.reshape(covariance, [n**2]))), 0,
+          delta=0.2)
+
+if __name__ == '__main__':
+  test.main()
diff --git a/tensorflow/contrib/bayesflow/python/ops/metropolis_hastings.py b/tensorflow/contrib/bayesflow/python/ops/metropolis_hastings.py
new file mode 100644
index 00000000000..7bdeaa862d5
--- /dev/null
+++ b/tensorflow/contrib/bayesflow/python/ops/metropolis_hastings.py
@@ -0,0 +1,33 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Functions to create a Markov Chain Monte Carlo Metropolis step."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# go/tf-wildcard-import
+# pylint: disable=wildcard-import
+from tensorflow.contrib.bayesflow.python.ops.metropolis_hastings_impl import *
+# pylint: enable=wildcard-import
+from tensorflow.python.util.all_util import remove_undocumented
+
+_allowed_symbols = [
+    'evolve',
+    'uniform_random_proposal',
+    'normal_random_proposal',
+]
+
+remove_undocumented(__name__, _allowed_symbols)
diff --git a/tensorflow/contrib/bayesflow/python/ops/metropolis_hastings_impl.py b/tensorflow/contrib/bayesflow/python/ops/metropolis_hastings_impl.py
new file mode 100644
index 00000000000..928fd62df1b
--- /dev/null
+++ b/tensorflow/contrib/bayesflow/python/ops/metropolis_hastings_impl.py
@@ -0,0 +1,426 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Functions to create a Markov Chain Monte Carlo Metropolis step.
+
+@@evolve
+@@uniform_random_proposal
+@@normal_random_proposal
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from tensorflow.python.framework import ops
+from tensorflow.python.ops import array_ops
+from tensorflow.python.ops import control_flow_ops
+from tensorflow.python.ops import math_ops
+from tensorflow.python.ops import random_ops
+from tensorflow.python.ops import state_ops
+
+__all__ = [
+    'evolve',
+    'uniform_random_proposal',
+    'normal_random_proposal',
+]
+
+
+def _single_iteration(current_state, current_log_density,
+                      log_unnormalized_prob_fn, proposal_fn, seed=None,
+                      name='None'):
+  """Performs a single Metropolis-Hastings step.
+
+  Args:
+    current_state: Float-like `Tensor` (i.e., `dtype` is either
+      `tf.float16`, `tf.float32` or `tf.float64`) of any shape that can
+      be consumed by the `log_unnormalized_prob_fn` and `proposal_fn`
+      callables.
+    current_log_density: Float-like `Tensor` with `dtype` and shape equivalent
+      to `log_unnormalized_prob_fn(current_state)`, i.e., matching the result of
+      `log_unnormalized_prob_fn` invoked at `current_state`.
+    log_unnormalized_prob_fn: A Python callable evaluated at
+      `current_state` and returning a float-like `Tensor` of log target-density
+      up to a normalizing constant. In other words,
+      `log_unnormalized_prob_fn(x) = log(g(x))`, where
+      `target_density = g(x)/Z` for some constant `A`. The shape of the input
+      tensor is the same as the shape of the `current_state`. The shape of the
+      output tensor is either
+        (a). Same as the input shape if the density being sampled is one
+          dimensional, or
+        (b). If the density is defined for `events` of shape
+          `event_shape = [E1, E2, ... Ee]`, then the input tensor should be of
+          shape `batch_shape + event_shape`, where `batch_shape = [B1, ..., Bb]`
+          and the result must be of shape [B1, ..., Bb]. For example, if the
+          distribution that is being sampled is a 10 dimensional normal,
+          then the input tensor may be of shape [100, 10] or [30, 20, 10]. The
+          last dimension will then be 'consumed' by `log_unnormalized_prob_fn`
+          and it should return tensors of shape [100] and [30, 20] respectively.
+    proposal_fn: A callable accepting a real valued `Tensor` of current sample
+      points and returning a tuple of two `Tensors`. The first element of the
+      pair is a `Tensor` containing the proposal state and should have
+      the same shape as the input `Tensor`. The second element of the pair gives
+      the log of the ratio of the probability of transitioning from the
+      proposal points to the input points and the probability of transitioning
+      from the input points to the proposal points. If the proposal is
+      symmetric (e.g., random walk, where the proposal is either
+      normal or uniform centered at `current_state`), i.e.,
+      Probability(Proposal -> Current) = Probability(Current -> Proposal)
+      the second value should be set to `None` instead of explicitly supplying a
+      tensor of zeros. In addition to being convenient, this also leads to a
+      more efficient graph.
+    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is
+      applied.
+    name: Python `str` name prefix for ops managed by this function.
+
+  Returns:
+    next_state: `Tensor` with `dtype` and shape matching `current_state`.
+      Created by propagating the chain by one step, starting from
+      `current_state`.
+    next_log_density: `Tensor` with `dtype` and shape matching
+      `current_log_density`, which is equal to the value of the unnormalized
+      `log_unnormalized_prob_fn` computed at `next_state`.
+    log_accept_ratio: `Tensor` with `dtype` and shape matching
+      `current_log_density`. Stands for the log of Metropolis-Hastings
+      acceptance ratio used in generating the `next_state`.
+  """
+
+  with ops.name_scope(name, 'single_iteration', [current_state]):
+    # The proposed state and the log of the corresponding Hastings ratio.
+    proposal_state, log_transit_ratio = proposal_fn(current_state)
+
+    # If the log ratio is None, assume that the transitions are symmetric,
+    # i.e., Prob(Current -> Proposed) = Prob(Proposed -> Current).
+    if log_transit_ratio is None:
+      log_transit_ratio = 0.
+
+    # Log-density of the proposal state.
+    proposal_log_density = log_unnormalized_prob_fn(proposal_state)
+
+    # Ops to compute the log of the acceptance ratio. Recall that the
+    # acceptance ratio is: [Prob(Proposed) / Prob(Current)] *
+    # [Prob(Proposed -> Current) / Prob(Current -> Proposed)]. The log of the
+    # second term is the log_transit_ratio.
+    with ops.name_scope('accept_reject'):
+      # The log of the acceptance ratio.
+      log_accept_ratio = (proposal_log_density - current_log_density
+                          + log_transit_ratio)
+
+      # A proposal is accepted or rejected depending on the acceptance ratio.
+      # If the acceptance ratio is greater than 1 then it is always accepted.
+      # If the acceptance ratio is less than 1 then the proposal is accepted
+      # with probability = acceptance ratio. As we are working in log space to
+      # prevent over/underflows, this logic is expressed in log terms below.
+      # If a proposal is accepted we place a True in the acceptance state
+      # tensor and if it is to be rejected we place a False.
+      # The log_draws below have to be compared to the log_accept_ratio so we
+      # make sure that they have the same data type.
+      log_draws = math_ops.log(random_ops.random_uniform(
+          array_ops.shape(current_log_density), seed=seed,
+          dtype=log_accept_ratio.dtype))
+      is_proposal_accepted = log_draws < log_accept_ratio
+
+    # The acceptance state decides which elements of the current state are to
+    # be replaced with the corresponding elements in the proposal state.
+    with ops.name_scope(name, 'metropolis_single_step',
+                        [current_state, current_log_density]):
+      next_log_density = array_ops.where(is_proposal_accepted,
+                                         proposal_log_density,
+                                         current_log_density)
+      next_state = array_ops.where(is_proposal_accepted, proposal_state,
+                                   current_state)
+
+    return next_state, next_log_density, log_accept_ratio
+
+
+def evolve(initial_sample,
+           initial_log_density,
+           initial_log_accept_ratio,
+           log_unnormalized_prob_fn,
+           proposal_fn,
+           n_steps=1,
+           seed=None,
+           name=None):
+  """Performs `n_steps` of the Metropolis-Hastings update.
+
+  Given a probability density function, `f(x)` and a proposal scheme which
+  generates new points from old, this `Op` returns a tensor
+  which may be used to generate approximate samples from the target distribution
+  using the Metropolis-Hastings algorithm. These samples are from a Markov chain
+  whose equilibrium distribution matches the target distribution.
+
+  The probability distribution may have an unknown normalization constan.
+  We parameterize the probability density as follows:
+    ```
+      f(x) = exp(L(x) + constant)
+    ```
+  Here `L(x)` is any continuous function with an (possibly unknown but finite)
+  upper bound, i.e. there exists a number beta such that
+  `L(x)< beta < infinity` for all x. The constant is the normalization needed
+  to make `f(x)` a probability density (as opposed to just a finite measure).
+
+  Although `initial_sample` can be arbitrary, a poor choice may result in a
+  slow-to-mix chain. In many cases the best choice is the one that maximizes
+  the target density, i.e., choose `initial_sample` such that
+  `f(initial_sample) >= f(x)` for all `x`.
+
+
+  If the support of the distribution is a strict subset of R^n (but of non zero
+  measure), then the unnormalized log-density `L(x)` should return `-infinity`
+  outside the support domain. This effectively forces the sampler to only
+  explore points in the regions of finite support.
+
+  Usage:
+  This function is meant to be wrapped up with some of the common proposal
+  schemes (e.g. random walk, Langevin diffusion etc) to produce a more user
+  friendly interface. However, it may also be used to create bespoke samplers.
+
+  The following example, demonstrates the use to generate a 1000 uniform random
+  walk Metropolis samplers run in parallel for the normal target distribution.
+  ```python
+    n = 3  # dimension of the problem
+
+    # Generate 1000 initial values randomly. Each of these would be an
+    # independent starting point for a Markov chain.
+    state = tf.get_variable(
+        'state',initializer=tf.random_normal([1000, n], mean=3.0,
+                                             dtype=tf.float64, seed=42))
+
+    # Computes the log(p(x)) for the unit normal density and ignores the
+    # normalization constant.
+    def log_density(x):
+      return  - tf.reduce_sum(x * x, reduction_indices=-1) / 2.0
+
+    # Initial log-density value
+    state_log_density = tf.get_variable(
+        'state_log_density', initializer=log_density(state.initialized_value()))
+
+    # A variable to store the log_acceptance_ratio:
+    log_acceptance_ratio = tf.get_variable(
+        'log_acceptance_ratio', initializer=tf.zeros([1000], dtype=tf.float64))
+
+    # Generates random proposals by moving each coordinate uniformly and
+    # independently in a box of size 2 centered around the current value.
+    # Returns the new point and also the log of the Hastings ratio (the
+    # ratio of the probability of going from the proposal to origin and the
+    # probability of the reverse transition). When this ratio is 1, the value
+    # may be omitted and replaced by None.
+    def random_proposal(x):
+      return (x + tf.random_uniform(tf.shape(x), minval=-1, maxval=1,
+                                    dtype=x.dtype, seed=12)), None
+
+    #  Create the op to propagate the chain for 100 steps.
+    stepper = mh.evolve(
+        state, state_log_density, log_acceptance_ratio,
+        log_density, random_proposal, n_steps=100, seed=123)
+    init = tf.initialize_all_variables()
+    with tf.Session() as sess:
+      sess.run(init)
+      # Run the chain for a total of 1000 and print out the mean across the
+      # chains every 100 iterations
+      for n_iter in range(10):
+        # Executing the stepper advances the chain to the next state.
+        sess.run(stepper)
+        # Print out the current value of the mean(sample) for every dimension.
+        print(np.mean(sess.run(state), 0))
+      # Estimated covariance matrix
+      samples = sess.run(state)
+      print('')
+      print(np.cov(samples, rowvar=False))
+  ```
+
+  Args:
+    initial_sample: A float-like `tf.Variable` of any shape that can
+      be consumed by the `log_unnormalized_prob_fn` and `proposal_fn`
+      callables.
+    initial_log_density: Float-like `tf.Variable` with `dtype` and shape
+      equivalent  to `log_unnormalized_prob_fn(initial_sample)`, i.e., matching
+        the result of `log_unnormalized_prob_fn` invoked at `current_state`.
+    initial_log_accept_ratio: A `tf.Variable` with `dtype` and shape matching
+      `initial_log_density`. Stands for the log of Metropolis-Hastings
+      acceptance ratio after propagating the chain for `n_steps`.
+    log_unnormalized_prob_fn: A Python callable evaluated at
+      `current_state` and returning a float-like `Tensor` of log target-density
+      up to a normalizing constant. In other words,
+      `log_unnormalized_prob_fn(x) = log(g(x))`, where
+      `target_density = g(x)/Z` for some constant `A`. The shape of the input
+      tensor is the same as the shape of the `current_state`. The shape of the
+      output tensor is either
+        (a). Same as the input shape if the density being sampled is one
+          dimensional, or
+        (b). If the density is defined for `events` of shape
+          `event_shape = [E1, E2, ... Ee]`, then the input tensor should be of
+          shape `batch_shape + event_shape`, here `batch_shape = [B1, ..., Bb]`
+          and the result must be of shape [B1, ..., Bb]. For example, if the
+          distribution that is being sampled is a 10 dimensional normal,
+          then the input tensor may be of shape [100, 10] or [30, 20, 10]. The
+          last dimension will then be 'consumed' by `log_unnormalized_prob_fn`
+          and it should return tensors of shape [100] and [30, 20] respectively.
+    proposal_fn: A callable accepting a real valued `Tensor` of current sample
+      points and returning a tuple of two `Tensors`. The first element of the
+      pair should be a `Tensor` containing the proposal state and should have
+      the same shape as the input `Tensor`. The second element of the pair gives
+      the log of the ratio of the probability of transitioning from the
+      proposal points to the input points and the probability of transitioning
+      from the input points to the proposal points. If the proposal is
+      symmetric, i.e.
+      Probability(Proposal -> Current) = Probability(Current -> Proposal)
+      the second value should be set to None instead of explicitly supplying a
+      tensor of zeros. In addition to being convenient, this also leads to a
+      more efficient graph.
+    n_steps: A positive `int` or a scalar `int32` tensor. Sets the number of
+      iterations of the chain.
+    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is
+      applied.
+    name: A string that sets the name for this `Op`.
+
+  Returns:
+    forward_step: an `Op` to step the Markov chain forward for `n_steps`.
+  """
+
+  with ops.name_scope(name, 'metropolis_hastings', [initial_sample]):
+    current_state = initial_sample
+    current_log_density = initial_log_density
+    log_accept_ratio = initial_log_accept_ratio
+
+    # Stop condition for the while_loop
+    def stop_condition(i, _):
+      return i < n_steps
+
+    def step(i, loop_vars):
+      """Wrap `_single_iteration` for `while_loop`."""
+      state = loop_vars[0]
+      state_log_density = loop_vars[1]
+      return i + 1, list(_single_iteration(state, state_log_density,
+                                           log_unnormalized_prob_fn,
+                                           proposal_fn, seed=seed))
+
+    loop_vars = [current_state, current_log_density, log_accept_ratio]
+    # Build an `Op` to evolve the Markov chain for `n_steps`
+    (_, [end_state, end_log_density, end_log_acceptance]) = (
+        control_flow_ops.while_loop(
+            stop_condition, step,
+            (0, loop_vars),
+            parallel_iterations=1, swap_memory=1))
+
+    forward_step = control_flow_ops.group(
+        state_ops.assign(current_log_density, end_log_density),
+        state_ops.assign(current_state, end_state),
+        state_ops.assign(log_accept_ratio, end_log_acceptance))
+
+    return forward_step
+
+
+def uniform_random_proposal(step_size=1.,
+                            seed=None,
+                            name=None):
+  """Returns a callable that adds a random uniform tensor to the input.
+
+  This function returns a callable that accepts one `Tensor` argument of any
+  shape and a real data type (i.e. `tf.float32` or `tf.float64`). It adds a
+  sample from a random uniform distribution drawn from [-stepsize, stepsize]
+  to its input. It also returns the log of the ratio of the probability of
+  moving from the input point to the proposed point, but since this log ratio is
+  identically equal to 0 (because the probability of drawing a value `x` from
+  the symmetric uniform distribution is the same as the probability of drawing
+  `-x`), it simply returns None for the second element of the returned tuple.
+
+  Args:
+    step_size: A positive `float` or a scalar tensor of real dtype
+      controlling the scale of the uniform distribution.
+      If step_size = a, then draws are made uniformly from [-a, a].
+    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is
+      applied.
+    name: A string that sets the name for this `Op`.
+
+  Returns:
+    proposal_fn:  A callable accepting one float-like `Tensor` and returning a
+    2-tuple. The first value in the tuple is a `Tensor` of the same shape and
+    dtype as the input argument and the second element of the tuple is None.
+  """
+
+  with ops.name_scope(name, 'uniform_random_proposal', [step_size]):
+    def proposal_fn(input_state, name=None):
+      """Adds a uniform perturbation to the input state.
+
+      Args:
+        input_state: A `Tensor` of any shape and real dtype.
+        name: A string that sets the name for this `Op`.
+
+      Returns:
+        proposal_state:  A float-like `Tensot` with `dtype` and shape matching
+          `input_state`.
+        log_transit_ratio: `None`. Proposal is symmetric.
+      """
+      with ops.name_scope(name, 'proposer', [input_state]):
+        input_state = ops.convert_to_tensor(input_state, name='input_state')
+        return input_state + random_ops.random_uniform(
+            array_ops.shape(input_state),
+            minval=-step_size,
+            maxval=step_size,
+            seed=seed), None
+    return proposal_fn
+
+
+def normal_random_proposal(scale=1.,
+                           seed=None,
+                           name=None):
+  """Returns a callable that adds a random normal tensor to the input.
+
+  This function returns a callable that accepts one `Tensor` argument of any
+  shape and a real data type (i.e. `tf.float32` or `tf.float64`). The callable
+  adds a sample from a normal distribution with the supplied standard deviation
+  and zero mean to its input argument (called the proposal point).
+  The callable returns a tuple with the proposal point as the first element.
+  The second element is identically `None`. It is included so the callable is
+  compatible with the expected signature of the proposal scheme argument in the
+  `metropolis_hastings` function. A value of `None` indicates that the
+  probability of going from the input point to the proposal point is equal to
+  the probability of going from the proposal point to the input point.
+
+  Args:
+    scale: A positive `float` or a scalar tensor of any real dtype controlling
+      the scale of the normal distribution.
+    seed: `int` or None. The random seed for this `Op`. If `None`, no seed is
+      applied.
+    name: A string that sets the name for this `Op`.
+
+  Returns:
+    proposal_fn: A callable accepting one float-like `Tensor` and returning a
+    2-tuple. The first value in the tuple is a `Tensor` of the same shape and
+    dtype as the input argument and the second element of the tuple is None.
+  """
+
+  with ops.name_scope(name, 'normal_random_proposal', [scale]):
+    def proposal_fn(input_state, name=None):
+      """Adds a normal perturbation to the input state.
+
+      Args:
+        input_state: A `Tensor` of any shape and real dtype.
+        name: A string that sets the name for this `Op`.
+
+      Returns:
+        proposal_state:  A float-like `Tensot` with `dtype` and shape matching
+          `input_state`.
+        log_transit_ratio: `None`. Proposal is symmetric.
+      """
+
+      with ops.name_scope(name, 'proposer', [input_state]):
+        input_state = ops.convert_to_tensor(input_state, name='input_state')
+        return input_state + random_ops.random_normal(
+            array_ops.shape(input_state),
+            mean=0.,
+            stddev=scale,
+            seed=seed), None
+    return proposal_fn
