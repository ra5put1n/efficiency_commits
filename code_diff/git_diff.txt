commit a16dd1374023d1a5f6ee7c48661e0ed53a954391
Merge: dc89c34d9e 516680ba77
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon Sep 20 15:20:44 2021 -0700

    Merge branch 'ds/mergies-with-sparse-index'
    
    Various mergy operations have been prepared to work efficiently
    with the sparse index.
    
    * ds/mergies-with-sparse-index:
      sparse-index: integrate with cherry-pick and rebase
      sequencer: ensure full index if not ORT strategy
      t1092: add cherry-pick, rebase tests
      merge-ort: expand only for out-of-cone conflicts
      merge: make sparse-aware with ORT
      diff: ignore sparse paths in diffstat

commit c7d3aabd2793ab5772627ff6da95f8f7c28258ab
Author: Jeff King <peff@peff.net>
Date:   Tue Sep 14 19:51:27 2021 -0400

    serve: provide "receive" function for object-format capability
    
    We get any "object-format" specified by the client by searching for it
    in the collected list of capabilities the client sent. We can instead
    just handle it as soon as they send it. This is slightly more efficient,
    and gets us one step closer to dropping that collected list.
    
    Note that we do still have to do our final hash check after receiving
    all capabilities (because they might not have sent an object-format line
    at all, and we still have to check that the default matches our
    repository algorithm). Since the check_algorithm() function would now be
    down to a single if() statement, I've just inlined it in its only
    caller.
    
    There should be no change of behavior here, except for two
    broken-protocol cases:
    
      - if the client sends multiple conflicting object-format capabilities
        (which they should not), we'll now choose the last one rather than
        the first. We could also detect and complain about the duplicates
        quite easily now, which we could not before, but I didn't do so
        here.
    
      - if the client sends a bogus "object-format" with no equals sign,
        we'll now say so, rather than "unknown object format: ''"
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e56e53067fc3d31899e837293a5bf72e2d086b4a
Author: Jeff King <peff@peff.net>
Date:   Tue Sep 14 11:31:07 2021 -0400

    serve: add "receive" method for v2 capabilities table
    
    We have a capabilities table that tells us what we should tell the
    client we are capable of, and what to do when a client gives us a
    particular command (e.g., "command=ls-refs"). But it doesn't tell us
    what to do when the client sends us back a capability (e.g.,
    "object-format=sha256"). We just collect them all in a strvec and hope
    somebody can use them later.
    
    Instead, let's provide a function pointer in the table to act on these.
    This will eventually help us avoid collecting the strings, which will be
    more efficient and less prone to mischief.
    
    Using the new method is optional, which helps in two ways:
    
      - we can move existing capabilities over to this new system gradually
        in individual commits
    
      - some capabilities we don't actually do anything with anyway. For
        example, the client is free to say "agent=git/1.2.3" to us, but we
        do not act on the information in any way.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 711260fd60366063e8c5253a83fc4aeb8947dc9d
Author: Taylor Blau <me@ttaylorr.com>
Date:   Tue Aug 31 16:52:16 2021 -0400

    pack-bitmap.c: introduce 'bitmap_is_preferred_refname()'
    
    In a recent commit, pack-objects learned support for the
    'pack.preferBitmapTips' configuration. This patch prepares the
    multi-pack bitmap code to respect this configuration, too.
    
    The yet-to-be implemented code will find that it is more efficient to
    check whether each reference contains a prefix found in the configured
    set of values rather than doing an additional traversal.
    
    Implement a function 'bitmap_is_preferred_refname()' which will perform
    that check. Its caller will be added in a subsequent patch.
    
    Signed-off-by: Taylor Blau <me@ttaylorr.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a9fd2f207d0318cd7a4771f13c9fb4759d280f68
Author: Taylor Blau <me@ttaylorr.com>
Date:   Sun Aug 29 22:48:54 2021 -0400

    builtin/pack-objects.c: simplify add_objects_in_unpacked_packs()
    
    This function is used to implement `pack-objects`'s `--keep-unreachable`
    option, but can be simplified in a couple of ways:
    
      - add_objects_in_unpacked_packs() iterates over all packs (and then
        all packed objects) itself, but could use for_each_packed_object()
        instead since the missing flags necessary were added in the previous
        commit
    
      - objects are added to an in_pack array which store (off_t, object)
        tuples, and then sorted in offset order when we could iterate
        objects in offset order.
    
        There is a slight behavior change here: before we would have added
        objects in sorted offset order among _all_ packs. Handing objects to
        create_object_entry() in pack order for each pack (instead of
        feeding objects from all packs simultaneously their offset relative
        to different packs) is much more reasonable, if different than how
        the code currently works.
    
      - objects in a single pack are iterated in index order and searched
        for in order to discover their offsets, which is much less efficient
        than using the on-disk reverse index
    
    Simplify the function by addressing each of the above and moving the
    core of the loop into a callback function that we then pass to
    for_each_packed_object() instead of open-coding the latter function
    ourselves.
    
    Signed-off-by: Taylor Blau <me@ttaylorr.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit ad51ae4dc0f3df5d2c85c9a4eb57ffda79a31049
Author: Carlo Marcelo Arenas Belón <carenas@gmail.com>
Date:   Thu Aug 12 13:22:00 2021 -0700

    ci: update freebsd 12 cirrus job
    
    make sure it uses a supported OS branch and uses all the resources
    that can be allocated efficiently.
    
    while only 1GB of memory is needed, 2GB is the minimum for a 2 CPU
    machine (the default), but by increasing parallelism wall time has
    been reduced by 35%.
    
    Signed-off-by: Carlo Marcelo Arenas Belón <carenas@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f559d6d45e7e58ae1f922213948723de77ea77bd
Author: Patrick Steinhardt <ps@pks.im>
Date:   Mon Aug 9 10:12:03 2021 +0200

    revision: avoid hitting packfiles when commits are in commit-graph
    
    When queueing references in git-rev-list(1), we try to optimize parsing
    of commits via the commit-graph. To do so, we first look up the object's
    type, and if it is a commit we call `repo_parse_commit()` instead of
    `parse_object()`. This is quite inefficient though given that we're
    always uncompressing the object header in order to determine the type.
    Instead, we can opportunistically search the commit-graph for the object
    ID: in case it's found, we know it's a commit and can directly fill in
    the commit object without having to uncompress the object header.
    
    Expose a new function `lookup_commit_in_graph()`, which tries to find a
    commit in the commit-graph by ID, and convert `get_reference()` to use
    this function. This provides a big performance win in cases where we
    load references in a repository with lots of references pointing to
    commits. The following has been executed in a real-world repository with
    about 2.2 million refs:
    
        Benchmark #1: HEAD~: rev-list --unsorted-input --objects --quiet --not --all --not $newrev
          Time (mean ± σ):      4.458 s ±  0.044 s    [User: 4.115 s, System: 0.342 s]
          Range (min … max):    4.409 s …  4.534 s    10 runs
    
        Benchmark #2: HEAD: rev-list --unsorted-input --objects --quiet --not --all --not $newrev
          Time (mean ± σ):      3.089 s ±  0.015 s    [User: 2.768 s, System: 0.321 s]
          Range (min … max):    3.061 s …  3.105 s    10 runs
    
        Summary
          'HEAD: rev-list --unsorted-input --objects --quiet --not --all --not $newrev' ran
            1.44 ± 0.02 times faster than 'HEAD~: rev-list --unsorted-input --objects --quiet --not --all --not $newrev'
    
    Signed-off-by: Patrick Steinhardt <ps@pks.im>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 64f0109f17ecfdd504f497b67cc066b672e00dc5
Author: Ævar Arnfjörð Bjarmason <avarab@gmail.com>
Date:   Fri Jul 16 17:41:33 2021 +0200

    test-lib-functions: use test-tool for [de]packetize()
    
    The shell+perl "[de]packetize()" helper functions were added in
    4414a150025 (t/lib-git-daemon: add network-protocol helpers,
    2018-01-24), and around the same time we added the "pkt-line" helper
    in 74e70029615 (test-pkt-line: introduce a packet-line test helper,
    2018-03-14).
    
    For some reason it seems we've mostly used the shell+perl version
    instead of the helper since then. There were discussions around
    88124ab2636 (test-lib-functions: make packetize() more efficient,
    2020-03-27) and cacae4329fa (test-lib-functions: simplify packetize()
    stdin code, 2020-03-29) to improve them and make them more efficient.
    
    There was one good reason to do so, we needed an equivalent of
    "test-tool pkt-line pack", but that command wasn't capable of handling
    input with "\n" (a feature) or "\0" (just because it happens to be
    printf-based under the hood).
    
    Let's add a "pkt-line-raw" helper for that, and expose is at a
    packetize_raw() to go with the existing packetize() on the shell
    level, this gives us the smallest amount of change to the tests
    themselves.
    
    Signed-off-by: Ævar Arnfjörð Bjarmason <avarab@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 102969c42261f3ff8597771f4da8416f048667b6
Merge: a515f26eac eb87c6f559
Author: Junio C Hamano <gitster@pobox.com>
Date:   Thu Jul 8 13:14:59 2021 -0700

    Merge branch 'jx/t6020-with-older-bash'
    
    Work around inefficient glob substitution in older versions of bash
    by rewriting parts of a test.
    
    * jx/t6020-with-older-bash:
      t6020: fix incompatible parameter expansion

commit 5726a6b4012cd41701927a6637b9f2070e7760ee
Author: Ævar Arnfjörð Bjarmason <avarab@gmail.com>
Date:   Thu Jul 1 12:51:26 2021 +0200

    *.c *_init(): define in terms of corresponding *_INIT macro
    
    Change the common patter in the codebase of duplicating the
    initialization logic between an *_INIT macro and a
    corresponding *_init() function to use the macro as the canonical
    source of truth.
    
    Now we no longer need to keep the function up-to-date with the macro
    version. This implements a suggestion by Jeff King who found that
    under -O2 [1] modern compilers will init new version in place without
    the extra copy[1]. The performance of a single *_init() won't matter
    in most cases, but even if it does we're going to be producing
    efficient machine code to perform these operations.
    
    1. https://lore.kernel.org/git/YNyrDxUO1PlGJvCn@coredump.intra.peff.net/
    
    Signed-off-by: Ævar Arnfjörð Bjarmason <avarab@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e289f681ede1aad2ded33e76518bdc80064f3c16
Merge: eede71149e 6b79818bfb
Author: Junio C Hamano <gitster@pobox.com>
Date:   Fri May 14 08:26:08 2021 +0900

    Merge branch 'jk/p4-locate-branch-point-optim'
    
    "git p4" learned to find branch points more efficiently.
    
    * jk/p4-locate-branch-point-optim:
      git-p4: speed up search for branch parent
      git-p4: ensure complex branches are cloned correctly

commit c1fa951d7ea9e943f001ac7c7502995273db5776
Author: Jeff King <peff@peff.net>
Date:   Tue Apr 13 03:17:48 2021 -0400

    revision: avoid parsing with --exclude-promisor-objects
    
    When --exclude-promisor-objects is given, before traversing any objects
    we iterate over all of the objects in any promisor packs, marking them
    as UNINTERESTING and SEEN. We turn the oid we get from iterating the
    pack into an object with parse_object(), but this has two problems:
    
      - it's slow; we are zlib inflating (and reconstructing from deltas)
        every byte of every object in the packfile
    
      - it leaves the tree buffers attached to their structs, which means
        our heap usage will grow to store every uncompressed tree
        simultaneously. This can be gigabytes.
    
    We can obviously fix the second by freeing the tree buffers after we've
    parsed them. But we can observe that the function doesn't look at the
    object contents at all! The only reason we call parse_object() is that
    we need a "struct object" on which to set the flags. There are two
    options here:
    
      - we can look up just the object type via oid_object_info(), and then
        call the appropriate lookup_foo() function
    
      - we can call lookup_unknown_object(), which gives us an OBJ_NONE
        struct (which will get auto-converted later by object_as_type() via
        calls to lookup_commit(), etc).
    
    The first one is closer to the current code, but we do pay the price to
    look up the type for each object. The latter should be more efficient in
    CPU, though it wastes a little bit of memory (the "unknown" object
    structs are a union of all object types, so some of the structs are
    bigger than they need to be). It also runs the risk of triggering a
    latent bug in code that calls lookup_object() directly but isn't ready
    to handle OBJ_NONE (such code would already be buggy, but we use
    lookup_unknown_object() infrequently enough that it might be hiding).
    
    I went with the second option here. I don't think the risk is high (and
    we'd want to find and fix any such bugs anyway), and it should be more
    efficient overall.
    
    The new tests in p5600 show off the improvement (this is on git.git):
    
      Test                                 HEAD^               HEAD
      -------------------------------------------------------------------------------
      5600.5: count commits                0.37(0.37+0.00)     0.38(0.38+0.00) +2.7%
      5600.6: count non-promisor commits   11.74(11.37+0.37)   0.04(0.03+0.00) -99.7%
    
    The improvement is particularly big in this script because _every_
    object in the newly-cloned partial repo is a promisor object. So after
    marking them all, there's nothing left to traverse.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e4de4502e6e32d5ce71fa6fcfc0703a4be411eb7
Author: Andrzej Hunt <ajrhunt@google.com>
Date:   Sun Mar 14 18:47:38 2021 +0000

    init: remove git_init_db_config() while fixing leaks
    
    The primary goal of this change is to stop leaking init_db_template_dir.
    This leak can happen because:
     1. git_init_db_config() allocates new memory into init_db_template_dir
        without first freeing the existing value.
     2. init_db_template_dir might already contain data, either because:
      2.1 git_config() can be invoked twice with this callback in a single
          process - at least 2 allocations are likely.
      2.2 A single git_config() allocation can invoke the callback multiple
          times for a given key (see further explanation in the function
          docs) - each of those calls will trigger another leak.
    
    The simplest fix for the leak would be to free(init_db_template_dir)
    before overwriting it. Instead we choose to convert to fetching
    init.templatedir via git_config_get_value() as that is more explicit,
    more efficient, and avoids allocations (the returned result is owned by
    the config cache, so we aren't responsible for freeing it).
    
    If we remove init_db_template_dir, git_init_db_config() ends up being
    responsible only for forwarding core.* config values to
    platform_core_config(). However platform_core_config() already ignores
    non-core.* config values, so we can safely remove git_init_db_config()
    and invoke git_config() directly with platform_core_config() as the
    callback.
    
    The platform_core_config forwarding was originally added in:
      287853392a (mingw: respect core.hidedotfiles = false in git-init again, 2019-03-11
    And I suspect the potential for a leak existed since the original
    implementation of git_init_db_config in:
      90b45187ba (Add `init.templatedir` configuration variable., 2010-02-17)
    
    LSAN output from t0001:
    
    Direct leak of 73 byte(s) in 1 object(s) allocated from:
        #0 0x49a859 in realloc /home/abuild/rpmbuild/BUILD/llvm-11.0.0.src/build/../projects/compiler-rt/lib/asan/asan_malloc_linux.cpp:164:3
        #1 0x9a7276 in xrealloc /home/ahunt/oss-fuzz/git/wrapper.c:126:8
        #2 0x9362ad in strbuf_grow /home/ahunt/oss-fuzz/git/strbuf.c:98:2
        #3 0x936eaa in strbuf_add /home/ahunt/oss-fuzz/git/strbuf.c:295:2
        #4 0x868112 in strbuf_addstr /home/ahunt/oss-fuzz/git/./strbuf.h:304:2
        #5 0x86a8ad in expand_user_path /home/ahunt/oss-fuzz/git/path.c:758:2
        #6 0x720bb1 in git_config_pathname /home/ahunt/oss-fuzz/git/config.c:1287:10
        #7 0x5960e2 in git_init_db_config /home/ahunt/oss-fuzz/git/builtin/init-db.c:161:11
        #8 0x7255b8 in configset_iter /home/ahunt/oss-fuzz/git/config.c:1982:7
        #9 0x7253fc in repo_config /home/ahunt/oss-fuzz/git/config.c:2311:2
        #10 0x725ca7 in git_config /home/ahunt/oss-fuzz/git/config.c:2399:2
        #11 0x593e8d in create_default_files /home/ahunt/oss-fuzz/git/builtin/init-db.c:225:2
        #12 0x5935c6 in init_db /home/ahunt/oss-fuzz/git/builtin/init-db.c:449:11
        #13 0x59588e in cmd_init_db /home/ahunt/oss-fuzz/git/builtin/init-db.c:714:9
        #14 0x4cd60d in run_builtin /home/ahunt/oss-fuzz/git/git.c:453:11
        #15 0x4cb2da in handle_builtin /home/ahunt/oss-fuzz/git/git.c:704:3
        #16 0x4ccc37 in run_argv /home/ahunt/oss-fuzz/git/git.c:771:4
        #17 0x4cac29 in cmd_main /home/ahunt/oss-fuzz/git/git.c:902:19
        #18 0x69c4de in main /home/ahunt/oss-fuzz/git/common-main.c:52:11
        #19 0x7f23552d6349 in __libc_start_main (/lib64/libc.so.6+0x24349)
    
    Signed-off-by: Andrzej Hunt <ajrhunt@google.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 339bce27f4f2a6f3bfab3e708429c810f4030c43
Author: Taylor Blau <me@ttaylorr.com>
Date:   Mon Feb 22 21:25:10 2021 -0500

    builtin/pack-objects.c: add '--stdin-packs' option
    
    In an upcoming commit, 'git repack' will want to create a pack comprised
    of all of the objects in some packs (the included packs) excluding any
    objects in some other packs (the excluded packs).
    
    This caller could iterate those packs themselves and feed the objects it
    finds to 'git pack-objects' directly over stdin, but this approach has a
    few downsides:
    
      - It requires every caller that wants to drive 'git pack-objects' in
        this way to implement pack iteration themselves. This forces the
        caller to think about details like what order objects are fed to
        pack-objects, which callers would likely rather not do.
    
      - If the set of objects in included packs is large, it requires
        sending a lot of data over a pipe, which is inefficient.
    
      - The caller is forced to keep track of the excluded objects, too, and
        make sure that it doesn't send any objects that appear in both
        included and excluded packs.
    
    But the biggest downside is the lack of a reachability traversal.
    Because the caller passes in a list of objects directly, those objects
    don't get a namehash assigned to them, which can have a negative impact
    on the delta selection process, causing 'git pack-objects' to fail to
    find good deltas even when they exist.
    
    The caller could formulate a reachability traversal themselves, but the
    only way to drive 'git pack-objects' in this way is to do a full
    traversal, and then remove objects in the excluded packs after the
    traversal is complete. This can be detrimental to callers who care
    about performance, especially in repositories with many objects.
    
    Introduce 'git pack-objects --stdin-packs' which remedies these four
    concerns.
    
    'git pack-objects --stdin-packs' expects a list of pack names on stdin,
    where 'pack-xyz.pack' denotes that pack as included, and
    '^pack-xyz.pack' denotes it as excluded. The resulting pack includes all
    objects that are present in at least one included pack, and aren't
    present in any excluded pack.
    
    To address the delta selection problem, 'git pack-objects --stdin-packs'
    works as follows. First, it assembles a list of objects that it is going
    to pack, as above. Then, a reachability traversal is started, whose tips
    are any commits mentioned in included packs. Upon visiting an object, we
    find its corresponding object_entry in the to_pack list, and set its
    namehash parameter appropriately.
    
    To avoid the traversal visiting more objects than it needs to, the
    traversal is halted upon encountering an object which can be found in an
    excluded pack (by marking the excluded packs as kept in-core, and
    passing --no-kept-objects=in-core to the revision machinery).
    
    This can cause the traversal to halt early, for example if an object in
    an included pack is an ancestor of ones in excluded packs. But stopping
    early is OK, since filling in the namehash fields of objects in the
    to_pack list is only additive (i.e., having it helps the delta selection
    process, but leaving it blank doesn't impact the correctness of the
    resulting pack).
    
    Even still, it is unlikely that this hurts us much in practice, since
    the 'git repack --geometric' caller (which is introduced in a later
    commit) marks small packs as included, and large ones as excluded.
    During ordinary use, the small packs usually represent pushes after a
    large repack, and so are unlikely to be ancestors of objects that
    already exist in the repository.
    
    (I found it convenient while developing this patch to have 'git
    pack-objects' report the number of objects which were visited and got
    their namehash fields filled in during traversal. This is also included
    in the below patch via trace2 data lines).
    
    Suggested-by: Jeff King <peff@peff.net>
    Signed-off-by: Taylor Blau <me@ttaylorr.com>
    Reviewed-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 36a317929b8f0c67d77d54235f2d20751c576cbb
Author: Jeff King <peff@peff.net>
Date:   Wed Jan 20 14:44:43 2021 -0500

    refs: switch peel_ref() to peel_iterated_oid()
    
    The peel_ref() interface is confusing and error-prone:
    
      - it's typically used by ref iteration callbacks that have both a
        refname and oid. But since they pass only the refname, we may load
        the ref value from the filesystem again. This is inefficient, but
        also means we are open to a race if somebody simultaneously updates
        the ref. E.g., this:
    
          int some_ref_cb(const char *refname, const struct object_id *oid, ...)
          {
                  if (!peel_ref(refname, &peeled))
                          printf("%s peels to %s",
                                 oid_to_hex(oid), oid_to_hex(&peeled);
          }
    
        could print nonsense. It is correct to say "refname peels to..."
        (you may see the "before" value or the "after" value, either of
        which is consistent), but mentioning both oids may be mixing
        before/after values.
    
        Worse, whether this is possible depends on whether the optimization
        to read from the current iterator value kicks in. So it is actually
        not possible with:
    
          for_each_ref(some_ref_cb);
    
        but it _is_ possible with:
    
          head_ref(some_ref_cb);
    
        which does not use the iterator mechanism (though in practice, HEAD
        should never peel to anything, so this may not be triggerable).
    
      - it must take a fully-qualified refname for the read_ref_full() code
        path to work. Yet we routinely pass it partial refnames from
        callbacks to for_each_tag_ref(), etc. This happens to work when
        iterating because there we do not call read_ref_full() at all, and
        only use the passed refname to check if it is the same as the
        iterator. But the requirements for the function parameters are quite
        unclear.
    
    Instead of taking a refname, let's instead take an oid. That fixes both
    problems. It's a little funny for a "ref" function not to involve refs
    at all. The key thing is that it's optimizing under the hood based on
    having access to the ref iterator. So let's change the name to make it
    clear why you'd want this function versus just peel_object().
    
    There are two other directions I considered but rejected:
    
      - we could pass the peel information into the each_ref_fn callback.
        However, we don't know if the caller actually wants it or not. For
        packed-refs, providing it is essentially free. But for loose refs,
        we actually have to peel the object, which would be wasteful in most
        cases. We could likewise pass in a flag to the callback indicating
        whether the peeled information is known, but that complicates those
        callbacks, as they then have to decide whether to manually peel
        themselves. Plus it requires changing the interface of every
        callback, whether they care about peeling or not, and there are many
        of them.
    
      - we could make a function to return the peeled value of the current
        iterated ref (computing it if necessary), and BUG() otherwise. I.e.:
    
          int peel_current_iterated_ref(struct object_id *out);
    
        Each of the current callers is an each_ref_fn callback, so they'd
        mostly be happy. But:
    
          - we use those callbacks with functions like head_ref(), which do
            not use the iteration code. So we'd need to handle the fallback
            case there, anyway.
    
          - it's possible that a caller would want to call into generic code
            that sometimes is used during iteration and sometimes not. This
            encapsulates the logic to do the fast thing when possible, and
            fallback when necessary.
    
    The implementation is mostly obvious, but I want to call out a few
    things in the patch:
    
      - the test-tool coverage for peel_ref() is now meaningless, as it all
        collapses to a single peel_object() call (arguably they were pretty
        uninteresting before; the tricky part of that function is the
        fast-path we see during iteration, but these calls didn't trigger
        that). I've just dropped it entirely, though note that some other
        tests relied on the tags we created; I've moved that creation to the
        tests where it matters.
    
      - we no longer need to take a ref_store parameter, since we'd never
        look up a ref now. We do still rely on a global "current iterator"
        variable which _could_ be kept per-ref-store. But in practice this
        is only useful if there are multiple recursive iterations, at which
        point the more appropriate solution is probably a stack of
        iterators. No caller used the actual ref-store parameter anyway
        (they all call the wrapper that passes the_repository).
    
      - the original only kicked in the optimization when the "refname"
        pointer matched (i.e., not string comparison). We do likewise with
        the "oid" parameter here, but fall back to doing an actual oideq()
        call. This in theory lets us kick in the optimization more often,
        though in practice no current caller cares. It should never be
        wrong, though (peeling is a property of an object, so two refs
        pointing to the same object would peel identically).
    
      - the original took care not to touch the peeled out-parameter unless
        we found something to put in it. But no caller cares about this, and
        anyway, it is enforced by peel_object() itself (and even in the
        optimized iterator case, that's where we eventually end up). We can
        shorten the code and avoid an extra copy by just passing the
        out-parameter through the stack.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Reviewed-by: Taylor Blau <me@ttaylorr.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1cbdbf3bef7e9167794cb2c12f9af1a450584ebe
Author: Jeff King <peff@peff.net>
Date:   Mon Dec 7 14:11:02 2020 -0500

    commit-graph: drop count_distinct_commits() function
    
    When writing a commit graph, we collect a list of object ids in an
    array, which we'll eventually copy into an array of "struct commit"
    pointers. Before we do that, though, we count the number of distinct
    commit entries. There's a subtle bug in this step, though.
    
    We eliminate not only duplicate oids, but also in split mode, any oids
    which are not commits or which are already in a graph file. However, the
    loop starts at index 1, always counting index 0 as distinct. And indeed
    it can't be a duplicate, since we check for those by comparing against
    the previous entry, and there isn't one for index 0. But it could be a
    commit that's already in a graph file, and we'd overcount the number of
    commits by 1 in that case.
    
    That turns out not to be a problem, though. The only things we do with
    the count are:
    
      - check if our count will overflow our data structures. But the limit
        there is 2^31 commits, so while this is a useful check, the
        off-by-one is not likely to matter.
    
      - pre-allocate the array of commit pointers. But over-allocating by
        one isn't a problem; we'll just waste a few extra bytes.
    
    The bug would be easy enough to fix, but we can observe that neither of
    those steps is necessary.
    
    After building the actual commit array, we'll likewise check its count
    for overflow. So the extra check of the distinct commit count here is
    redundant.
    
    And likewise we use ALLOC_GROW() when building the commit array, so
    there's no need to preallocate it (it's possible that doing so is
    slightly more efficient, but if we care we can just optimistically
    allocate one slot for each oid; I didn't bother here).
    
    So count_distinct_commits() isn't doing anything useful. Let's just get
    rid of that step.
    
    Note that a side effect of the function was that we sorted the list of
    oids, which we do rely on in copy_oids_to_commits(), since it must also
    skip the duplicates. So we'll move the qsort there. I didn't copy the
    "TODO" about adding more progress meters. It's actually quite hard to
    make a repository large enough for this qsort would take an appreciable
    amount of time, so this doesn't seem like a useful note.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a13e3d0ec87c9d12b93700375fceacdfb2c6885f
Author: Derrick Stolee <dstolee@microsoft.com>
Date:   Fri Sep 25 12:33:37 2020 +0000

    maintenance: auto-size incremental-repack batch
    
    When repacking during the 'incremental-repack' task, we use the
    --batch-size option in 'git multi-pack-index repack'. The initial setting
    used --batch-size=0 to repack everything into a single pack-file. This is
    not sustainable for a large repository. The amount of work required is
    also likely to use too many system resources for a background job.
    
    Update the 'incremental-repack' task by dynamically computing a
    --batch-size option based on the current pack-file structure.
    
    The dynamic default size is computed with this idea in mind for a client
    repository that was cloned from a very large remote: there is likely one
    "big" pack-file that was created at clone time. Thus, do not try
    repacking it as it is likely packed efficiently by the server.
    
    Instead, we select the second-largest pack-file, and create a batch size
    that is one larger than that pack-file. If there are three or more
    pack-files, then this guarantees that at least two will be combined into
    a new pack-file.
    
    Of course, this means that the second-largest pack-file size is likely
    to grow over time and may eventually surpass the initially-cloned
    pack-file. Recall that the pack-file batch is selected in a greedy
    manner: the packs are considered from oldest to newest and are selected
    if they have size smaller than the batch size until the total selected
    size is larger than the batch size. Thus, that oldest "clone" pack will
    be first to repack after the new data creates a pack larger than that.
    
    We also want to place some limits on how large these pack-files become,
    in order to bound the amount of time spent repacking. A maximum
    batch-size of two gigabytes means that large repositories will never be
    packed into a single pack-file using this job, but also that repack is
    rather expensive. This is a trade-off that is valuable to have if the
    maintenance is being run automatically or in the background. Users who
    truly want to optimize for space and performance (and are willing to pay
    the upfront cost of a full repack) can use the 'gc' task to do so.
    
    Create a test for this two gigabyte limit by creating an EXPENSIVE test
    that generates two pack-files of roughly 2.5 gigabytes in size, then
    performs an incremental repack. Check that the --batch-size argument in
    the subcommand uses the hard-coded maximum.
    
    Helped-by: Chris Torek <chris.torek@gmail.com>
    Reported-by: Son Luong Ngoc <sluongng@gmail.com>
    Signed-off-by: Derrick Stolee <dstolee@microsoft.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 52fe41ff1cd6e1f0b67d4e864e718d949e225f30
Author: Derrick Stolee <dstolee@microsoft.com>
Date:   Fri Sep 25 12:33:36 2020 +0000

    maintenance: add incremental-repack task
    
    The previous change cleaned up loose objects using the
    'loose-objects' that can be run safely in the background. Add a
    similar job that performs similar cleanups for pack-files.
    
    One issue with running 'git repack' is that it is designed to
    repack all pack-files into a single pack-file. While this is the
    most space-efficient way to store object data, it is not time or
    memory efficient. This becomes extremely important if the repo is
    so large that a user struggles to store two copies of the pack on
    their disk.
    
    Instead, perform an "incremental" repack by collecting a few small
    pack-files into a new pack-file. The multi-pack-index facilitates
    this process ever since 'git multi-pack-index expire' was added in
    19575c7 (multi-pack-index: implement 'expire' subcommand,
    2019-06-10) and 'git multi-pack-index repack' was added in ce1e4a1
    (midx: implement midx_repack(), 2019-06-10).
    
    The 'incremental-repack' task runs the following steps:
    
    1. 'git multi-pack-index write' creates a multi-pack-index file if
       one did not exist, and otherwise will update the multi-pack-index
       with any new pack-files that appeared since the last write. This
       is particularly relevant with the background fetch job.
    
       When the multi-pack-index sees two copies of the same object, it
       stores the offset data into the newer pack-file. This means that
       some old pack-files could become "unreferenced" which I will use
       to mean "a pack-file that is in the pack-file list of the
       multi-pack-index but none of the objects in the multi-pack-index
       reference a location inside that pack-file."
    
    2. 'git multi-pack-index expire' deletes any unreferenced pack-files
       and updaes the multi-pack-index to drop those pack-files from the
       list. This is safe to do as concurrent Git processes will see the
       multi-pack-index and not open those packs when looking for object
       contents. (Similar to the 'loose-objects' job, there are some Git
       commands that open pack-files regardless of the multi-pack-index,
       but they are rarely used. Further, a user that self-selects to
       use background operations would likely refrain from using those
       commands.)
    
    3. 'git multi-pack-index repack --bacth-size=<size>' collects a set
       of pack-files that are listed in the multi-pack-index and creates
       a new pack-file containing the objects whose offsets are listed
       by the multi-pack-index to be in those objects. The set of pack-
       files is selected greedily by sorting the pack-files by modified
       time and adding a pack-file to the set if its "expected size" is
       smaller than the batch size until the total expected size of the
       selected pack-files is at least the batch size. The "expected
       size" is calculated by taking the size of the pack-file divided
       by the number of objects in the pack-file and multiplied by the
       number of objects from the multi-pack-index with offset in that
       pack-file. The expected size approximates how much data from that
       pack-file will contribute to the resulting pack-file size. The
       intention is that the resulting pack-file will be close in size
       to the provided batch size.
    
       The next run of the incremental-repack task will delete these
       repacked pack-files during the 'expire' step.
    
       In this version, the batch size is set to "0" which ignores the
       size restrictions when selecting the pack-files. It instead
       selects all pack-files and repacks all packed objects into a
       single pack-file. This will be updated in the next change, but
       it requires doing some calculations that are better isolated to
       a separate change.
    
    These steps are based on a similar background maintenance step in
    Scalar (and VFS for Git) [1]. This was incredibly effective for
    users of the Windows OS repository. After using the same VFS for Git
    repository for over a year, some users had _thousands_ of pack-files
    that combined to up to 250 GB of data. We noticed a few users were
    running into the open file descriptor limits (due in part to a bug
    in the multi-pack-index fixed by af96fe3 (midx: add packs to
    packed_git linked list, 2019-04-29).
    
    These pack-files were mostly small since they contained the commits
    and trees that were pushed to the origin in a given hour. The GVFS
    protocol includes a "prefetch" step that asks for pre-computed pack-
    files containing commits and trees by timestamp. These pack-files
    were grouped into "daily" pack-files once a day for up to 30 days.
    If a user did not request prefetch packs for over 30 days, then they
    would get the entire history of commits and trees in a new, large
    pack-file. This led to a large number of pack-files that had poor
    delta compression.
    
    By running this pack-file maintenance step once per day, these repos
    with thousands of packs spanning 200+ GB dropped to dozens of pack-
    files spanning 30-50 GB. This was done all without removing objects
    from the system and using a constant batch size of two gigabytes.
    Once the work was done to reduce the pack-files to small sizes, the
    batch size of two gigabytes means that not every run triggers a
    repack operation, so the following run will not expire a pack-file.
    This has kept these repos in a "clean" state.
    
    [1] https://github.com/microsoft/scalar/blob/master/Scalar.Common/Maintenance/PackfileMaintenanceStep.cs
    
    Signed-off-by: Derrick Stolee <dstolee@microsoft.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 59f0d5073fd9f0497b9ff9a48fb3a7a8d82d1f9d
Author: Taylor Blau <me@ttaylorr.com>
Date:   Thu Sep 17 22:59:44 2020 -0400

    bloom: encode out-of-bounds filters as non-empty
    
    When a changed-path Bloom filter has either zero, or more than a
    certain number (commonly 512) of entries, the commit-graph machinery
    encodes it as "missing". More specifically, it sets the indices adjacent
    in the BIDX chunk as equal to each other to indicate a "length 0"
    filter; that is, that the filter occupies zero bytes on disk.
    
    This has heretofore been fine, since the commit-graph machinery has no
    need to care about these filters with too few or too many changed paths.
    Both cases act like no filter has been generated at all, and so there is
    no need to store them.
    
    In a subsequent commit, however, the commit-graph machinery will learn
    to only compute Bloom filters for some commits in the current
    commit-graph layer. This is a change from the current implementation
    which computes Bloom filters for all commits that are in the layer being
    written. Critically for this patch, only computing some of the Bloom
    filters means adding a third state for length 0 Bloom filters: zero
    entries, too many entries, or "hasn't been computed".
    
    It will be important for that future patch to distinguish between "not
    representable" (i.e., zero or too-many changed paths), and "hasn't been
    computed". In particular, we don't want to waste time recomputing
    filters that have already been computed.
    
    To that end, change how we store Bloom filters in the "computed but not
    representable" category:
    
      - Bloom filters with no entries are stored as a single byte with all
        bits low (i.e., all queries to that Bloom filter will return
        "definitely not")
    
      - Bloom filters with too many entries are stored as a single byte with
        all bits set high (i.e., all queries to that Bloom filter will
        return "maybe").
    
    These rules are sufficient to not incur a behavior change by changing
    the on-disk representation of these two classes. Likewise, no
    specification changes are necessary for the commit-graph format, either:
    
      - Filters that were previously empty will be recomputed and stored
        according to the new rules, and
    
      - old clients reading filters generated by new clients will interpret
        the filters correctly and be none the wiser to how they were
        generated.
    
    Clients will invoke the Bloom machinery in more cases than before, but
    this can be addressed by returning a NULL filter when all bits are set
    high. This can be addressed in a future patch.
    
    Note that this does increase the size of on-disk commit-graphs, but far
    less than other proposals. In particular, this is generally more
    efficient than storing a bitmap for which commits haven't computed their
    Bloom filters. Storing a bitmap incurs a penalty of one bit per commit,
    whereas storing explicit filters as above incurs a penalty of one byte
    per too-large or empty commit.
    
    In practice, these boundary commits likely occupy a small proportion of
    the overall number of commits, and so the size penalty is likely smaller
    than storing a bitmap for all commits.
    
    See, for example, these relative proportions of such boundary commits
    (collected by SZEDER Gábor):
    
                      |     Percentage of     |    commit-graph   |           |
                      |   commits modifying   |     file size     |           |
                      ├────────┬──────────────┼───────────────────┤    pct.   |
                      | 0 path | >= 512 paths | before  |  after  |   change  |
     ┌────────────────┼────────┼──────────────┼─────────┼─────────┼───────────┤
     | android-base   | 13.20% |        0.13% | 37.468M | 37.534M | +0.1741 % |
     | cmssw          |  0.15% |        0.23% | 17.118M | 17.119M | +0.0091 % |
     | cpython        |  3.07% |        0.01% |  7.967M |  7.971M | +0.0423 % |
     | elasticsearch  |  0.70% |        1.00% |  8.833M |  8.835M | +0.0128 % |
     | gcc            |  0.00% |        0.08% | 16.073M | 16.074M | +0.0030 % |
     | gecko-dev      |  0.14% |        0.64% | 59.868M | 59.874M | +0.0105 % |
     | git            |  0.11% |        0.02% |  3.895M |  3.895M | +0.0020 % |
     | glibc          |  0.02% |        0.10% |  3.555M |  3.555M | +0.0021 % |
     | go             |  0.00% |        0.07% |  3.186M |  3.186M | +0.0018 % |
     | homebrew-cask  |  0.40% |        0.02% |  7.035M |  7.035M | +0.0065 % |
     | homebrew-core  |  0.01% |        0.01% | 11.611M | 11.611M | +0.0002 % |
     | jdk            |  0.26% |        5.64% |  5.537M |  5.540M | +0.0590 % |
     | linux          |  0.01% |        0.51% | 63.735M | 63.740M | +0.0073 % |
     | llvm-project   |  0.12% |        0.03% | 25.515M | 25.516M | +0.0050 % |
     | rails          |  0.10% |        0.10% |  6.252M |  6.252M | +0.0027 % |
     | rust           |  0.07% |        0.17% |  9.364M |  9.364M | +0.0033 % |
     | tensorflow     |  0.09% |        1.02% |  7.009M |  7.010M | +0.0158 % |
     | webkit         |  0.05% |        0.31% | 17.405M | 17.406M | +0.0047 % |
    
    (where the above increase is determined by computing a non-split
    commit-graph before and after this patch).
    
    Given that these projects are all "large" by commit count, the storage
    cost by writing these filters explicitly is negligible. In the most
    extreme example, android-base (which has 494,848 commits at the time of
    writing) would have its commit-graph increase by a modest 68.4 KB.
    
    Finally, a test to exercise filters which contain too many changed path
    entries will be introduced in a subsequent patch.
    
    Suggested-by: SZEDER Gábor <szeder.dev@gmail.com>
    Suggested-by: Jakub Narębski <jnareb@gmail.com>
    Helped-by: Derrick Stolee <dstolee@microsoft.com>
    Helped-by: SZEDER Gábor <szeder.dev@gmail.com>
    Helped-by: Junio C Hamano <gitster@pobox.com>
    Signed-off-by: Taylor Blau <me@ttaylorr.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 5c454b3825777493eada736423db75e8b128dd83
Merge: a1315123e2 e00549aa9b
Author: Junio C Hamano <gitster@pobox.com>
Date:   Tue Aug 4 13:53:57 2020 -0700

    Merge branch 'jt/pack-objects-prefetch-in-batch'
    
    While packing many objects in a repository with a promissor remote,
    lazily fetching missing objects from the promissor remote one by
    one may be inefficient---the code now attempts to fetch all the
    missing objects in batch (obviously this won't work for a lazy
    clone that lazily fetches tree objects as you cannot even enumerate
    what blobs are missing until you learn which trees are missing).
    
    * jt/pack-objects-prefetch-in-batch:
      pack-objects: prefetch objects to be packed
      pack-objects: refactor to oid_object_info_extended

commit fd9a631c56ff326bea2956b675f205cd474def4e
Author: Jeff King <peff@peff.net>
Date:   Tue Aug 4 03:46:52 2020 -0400

    revision: avoid out-of-bounds read/write on empty pathspec
    
    Running t4216 with ASan results in it complaining of an out-of-bounds
    read in prepare_to_use_bloom_filter(). The issue is this code to strip a
    trailing slash:
    
      last_index = pi->len - 1;
      if (pi->match[last_index] == '/') {
    
    because we have no guarantee that pi->len isn't zero. This can happen if
    the pathspec is ".", as we translate that to an empty string. And if
    that read of random memory does trigger the conditional, we'd then do an
    out-of-bounds write:
    
      path_alloc = xstrdup(pi->match);
      path_alloc[last_index] = '\0';
    
    Let's make sure to check the length before subtracting. Note that for an
    empty pathspec, we'd end up bailing from the function a few lines later,
    which makes it tempting to just:
    
      if (!pi->len)
              return;
    
    early here. But our code here is stripping a trailing slash, and we need
    to check for emptiness after stripping that slash, too. So we'd have two
    blocks, which would require repeating some cleanup code.
    
    Instead, just skip the trailing-slash for an empty string. Setting
    last_index at all in the case is awkward since it will have a nonsense
    value (and it uses an "int", which is a too-small type for a string
    anyway). So while we're here, let's:
    
      - drop last_index entirely; it's only used in two spots right next to
        each other and writing out "pi->len - 1" in both is actually easier
        to follow
    
      - use xmemdupz() to duplicate the string. This is slightly more
        efficient, but more importantly makes the intent more clear by
        allocating the correct-sized substring in the first place. It also
        eliminates any question of whether path_alloc is as long as
        pi->match (which it would not be if pi->match has any embedded NULs,
        though in practice this is probably impossible).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7d23ff818f156da48882bec9ac84fb4e33dd619a
Author: René Scharfe <l.s.r@web.de>
Date:   Sun Aug 2 16:36:50 2020 +0200

    bisect: use oid_to_hex_r() instead of memcpy()+oid_to_hex()
    
    Write the hexadecimal object ID directly into the destination buffer
    using oid_to_hex_r() instead of writing it into a static buffer first
    using oid_to_hex() and then copying it from there using memcpy().
    This is shorter, simpler and a bit more efficient.
    
    Reviewed-by: brian m. carlson <sandals@crustytoothpaste.net>
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 98c6871fad0be42fbe045de2338d0a5bdbc5af1d
Author: René Scharfe <l.s.r@web.de>
Date:   Tue Jul 28 23:40:38 2020 +0200

    grep: avoid using oid_to_hex() with parse_object_or_die()
    
    parse_object_or_die() is passed an object ID and a name to show if the
    object cannot be parsed.  If the name is NULL then it shows the
    hexadecimal object ID.  Use that feature instead of preparing and
    passing the hexadecimal representation to the function proactively.
    That's shorter and a bit more efficient.
    
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit b6839fda6809b1de8d528837dfc99d0837f77c9d
Author: Christian Couder <christian.couder@gmail.com>
Date:   Thu Jul 16 14:19:40 2020 +0200

    ref-filter: add support for %(contents:size)
    
    It's useful and efficient to be able to get the size of the
    contents directly without having to pipe through `wc -c`.
    
    Also the result of the following:
    
    `git for-each-ref --format='%(contents)' refs/heads/my-branch | wc -c`
    
    is off by one as `git for-each-ref` appends a newline character
    after the contents, which can be seen by comparing its output
    with the output from `git cat-file`.
    
    As with %(contents), %(contents:size) is silently ignored, if a
    ref points to something other than a commit or a tag:
    
    ```
    $ git update-ref refs/mytrees/first HEAD^{tree}
    $ git for-each-ref --format='%(contents)' refs/mytrees/first
    
    $ git for-each-ref --format='%(contents:size)' refs/mytrees/first
    
    ```
    
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e76eec35540f9eca7baef3d84c164a1f8d4d3cb9
Author: Jeff King <peff@peff.net>
Date:   Thu May 7 12:20:11 2020 -0400

    ci: allow per-branch config for GitHub Actions
    
    Depending on the workflows of individual developers, it can either be
    convenient or annoying that our GitHub Actions CI jobs are run on every
    branch. As an example of annoying: if you carry many half-finished
    work-in-progress branches and rebase them frequently against master,
    you'd get tons of failure reports that aren't interesting (not to
    mention the wasted CPU).
    
    This commit adds a new job which checks a special branch within the
    repository for CI config, and then runs a shell script it finds there to
    decide whether to skip the rest of the tests. The default will continue
    to run tests for all refs if that branch or script is missing.
    
    There have been a few alternatives discussed:
    
    One option is to carry information in the commit itself about whether it
    should be tested, either in the tree itself (changing the workflow YAML
    file) or in the commit message (a "[skip ci]" flag or similar). But
    these are frustrating and error-prone to use:
    
      - you have to manually apply them to each branch that you want to mark
    
      - it's easy for them to leak into other workflows, like emailing patches
    
    We could likewise try to get some information from the branch name. But
    that leads to debates about whether the default should be "off" or "on",
    and overriding still ends up somewhat awkward. If we default to "on",
    you have to remember to name your branches appropriately to skip CI. And
    if "off", you end up having to contort your branch names or duplicate
    your pushes with an extra refspec.
    
    By comparison, this commit's solution lets you specify your config once
    and forget about it, and all of the data is off in its own ref, where it
    can be changed by individual forks without touching the main tree.
    
    There were a few design decisions that came out of on-list discussion.
    I'll summarize here:
    
     - we could use GitHub's API to retrieve the config ref, rather than a
       real checkout (and then just operate on it via some javascript). We
       still have to spin up a VM and contact GitHub over the network from
       it either way, so it ends up not being much faster. I opted to go
       with shell to keep things similar to our other tools (and really
       could implement allow-refs in any language you want). This also makes
       it easy to test your script locally, and to modify it within the
       context of a normal git.git tree.
    
     - we could keep the well-known refname out of refs/heads/ to avoid
       cluttering the branch namespace. But that makes it awkward to
       manipulate. By contrast, you can just "git checkout ci-config" to
       make changes.
    
     - we could assume the ci-config ref has nothing in it except config
       (i.e., a branch unrelated to the rest of git.git). But dealing with
       orphan branches is awkward. Instead, we'll do our best to efficiently
       check out only the ci/config directory using a shallow partial clone,
       which allows your ci-config branch to be just a normal branch, with
       your config changes on top.
    
     - we could provide a simpler interface, like a static list of ref
       patterns. But we can't get out of spinning up a whole VM anyway, so
       we might as well use that feature to make the config as flexible as
       possible. If we add more config, we should be able to reuse our
       partial-clone to set more outputs.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 9b6606f43d55bbf33b9924d16e02e60e1c09660a
Merge: cf054f817a caf388caa1
Author: Junio C Hamano <gitster@pobox.com>
Date:   Fri May 1 13:39:53 2020 -0700

    Merge branch 'gs/commit-graph-path-filter'
    
    Introduce an extension to the commit-graph to make it efficient to
    check for the paths that were modified at each commit using Bloom
    filters.
    
    * gs/commit-graph-path-filter:
      bloom: ignore renames when computing changed paths
      commit-graph: add GIT_TEST_COMMIT_GRAPH_CHANGED_PATHS test flag
      t4216: add end to end tests for git log with Bloom filters
      revision.c: add trace2 stats around Bloom filter usage
      revision.c: use Bloom filters to speed up path based revision walks
      commit-graph: add --changed-paths option to write subcommand
      commit-graph: reuse existing Bloom filters during write
      commit-graph: write Bloom filters to commit graph file
      commit-graph: examine commits by generation number
      commit-graph: examine changed-path objects in pack order
      commit-graph: compute Bloom filters for changed paths
      diff: halt tree-diff early after max_changes
      bloom.c: core Bloom filter implementation for changed paths.
      bloom.c: introduce core Bloom filter constructs
      bloom.c: add the murmur3 hash implementation
      commit-graph: define and use MAX_NUM_CHUNKS

commit bf10200871d9e7e1fc9f54aca9b2fe40bc4e4ac7
Merge: 86ab15cb15 d9f15d37f1
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Apr 29 16:15:27 2020 -0700

    Merge branch 'dl/merge-autostash'
    
    "git merge" learns the "--autostash" option.
    
    * dl/merge-autostash: (22 commits)
      pull: pass --autostash to merge
      t5520: make test_pull_autostash() accept expect_parent_num
      merge: teach --autostash option
      sequencer: implement apply_autostash_oid()
      sequencer: implement save_autostash()
      sequencer: unlink autostash in apply_autostash()
      sequencer: extract perform_autostash() from rebase
      rebase: generify create_autostash()
      rebase: extract create_autostash()
      reset: extract reset_head() from rebase
      rebase: generify reset_head()
      rebase: use apply_autostash() from sequencer.c
      sequencer: rename stash_sha1 to stash_oid
      sequencer: make apply_autostash() accept a path
      rebase: use read_oneliner()
      sequencer: make read_oneliner() extern
      sequencer: configurably warn on non-existent files
      sequencer: make read_oneliner() accept flags
      sequencer: make file exists check more efficient
      sequencer: stop leaking buf
      ...

commit 5ee5788af680ebc20b138f40fb0bfd79494ba95c
Merge: dfe48154b1 cacae4329f
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Apr 22 13:42:44 2020 -0700

    Merge branch 'jk/harden-protocol-v2-delim-handling'
    
    The server-end of the v2 protocol to serve "git clone" and "git
    fetch" was not prepared to see a delim packets at unexpected
    places, which led to a crash.
    
    * jk/harden-protocol-v2-delim-handling:
      test-lib-functions: simplify packetize() stdin code
      upload-pack: handle unexpected delim packets
      test-lib-functions: make packetize() more efficient

commit 6830c360777468434184f60023e2562348c9dacc
Author: Taylor Blau <me@ttaylorr.com>
Date:   Mon Apr 13 22:04:25 2020 -0600

    commit-graph.h: replace 'commit_hex' with 'commits'
    
    The 'write_commit_graph()' function takes in either a string list of
    pack indices, or a string list of hexadecimal commit OIDs. These
    correspond to the '--stdin-packs' and '--stdin-commits' mode(s) from
    'git commit-graph write'.
    
    Using a string_list of hexadecimal commit IDs is not the most efficient
    use of memory, since we can instead use the 'struct oidset', which is
    more well-suited for this case.
    
    This has another benefit which will become apparent in the following
    commit. This is that we are about to disambiguate the kinds of errors we
    produce with '--stdin-commits' into "non-hex input" and "hex-input, but
    referring to a non-commit object". By having 'write_commit_graph' take
    in a 'struct oidset *' of commits, we place the burden on the caller (in
    this case, the builtin) to handle the first case, and the commit-graph
    machinery can handle the second case.
    
    Suggested-by: Jeff King <peff@peff.net>
    Signed-off-by: Taylor Blau <me@ttaylorr.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f72f328bc57e1b0db380ef76e0c1e94a9ed0ac7c
Author: Johannes Schindelin <johannes.schindelin@gmx.de>
Date:   Sat Apr 11 00:18:14 2020 +0700

    ci: let GitHub Actions upload failed tests' directories
    
    Arguably, CI builds' most important task is to not only identify
    regressions, but to make it as easy as possible to investigate what went
    wrong.
    
    In that light, we will want to provide users with a way to inspect the
    tests' output as well as the corresponding directories.
    
    This commit adds build steps that are only executed when tests failed,
    uploading the relevant information as build artifacts. These artifacts
    can then be downloaded by interested parties to diagnose the failures
    more efficiently.
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Đoàn Trần Công Danh <congdanhqx@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 5b2f6d9cd50b1c8909326c7175aef288a9915f33
Author: Denton Liu <liu.denton@gmail.com>
Date:   Tue Apr 7 10:27:51 2020 -0400

    sequencer: make file exists check more efficient
    
    We currently check whether a file exists and return early before reading
    the file. Instead of accessing the file twice, always read the file and
    check `errno` to see if the file doesn't exist.
    
    Signed-off-by: Denton Liu <liu.denton@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit cacae4329fa4779127f4944e7807512e7b9e8cac
Author: Jeff King <peff@peff.net>
Date:   Sun Mar 29 11:02:26 2020 -0400

    test-lib-functions: simplify packetize() stdin code
    
    The code path in packetize() for reading stdin needs to handle NUL
    bytes, so we can't rely on shell variables. However, the current code
    takes a whopping 4 processes and uses a temporary file. We can do this
    much more simply and efficiently by using a single perl invocation (and
    we already rely on perl in the matching depacketize() function).
    
    We'll keep the non-stdin code path as it is, since that uses zero extra
    processes.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 88124ab263670b4252be7c13d03754a127cee90e
Author: Jeff King <peff@peff.net>
Date:   Fri Mar 27 04:03:00 2020 -0400

    test-lib-functions: make packetize() more efficient
    
    The packetize() function takes its input on stdin, and requires 4
    separate sub-processes to format a simple string. We can do much better
    by getting the length via the shell's "${#packet}" construct. The one
    caveat is that the shell can't put a NUL into a variable, so we'll have
    to continue to provide the stdin form for a few calls.
    
    There are a few other cleanups here in the touched code:
    
     - the stdin form of packetize() had an extra stray "%s" when printing
       the packet
    
     - the converted calls in t5562 can be made simpler by redirecting
       output as a block, rather than repeated appending
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1bdca816412910e1206c15ef47f2a8a6b369b831
Author: brian m. carlson <sandals@crustytoothpaste.net>
Date:   Sat Feb 22 20:17:49 2020 +0000

    fast-import: add options for rewriting submodules
    
    When converting a repository using submodules from one hash algorithm to
    another, it is necessary to rewrite the submodules from the old
    algorithm to the new algorithm, since only references to submodules, not
    their contents, are written to the fast-export stream. Without rewriting
    the submodules, fast-import fails with an "Invalid dataref" error when
    encountering a submodule in another algorithm.
    
    Add a pair of options, --rewrite-submodules-from and
    --rewrite-submodules-to, that take a list of marks produced by
    fast-export and fast-import, respectively, when processing the
    submodule. Use these marks to map the submodule commits from the old
    algorithm to the new algorithm.
    
    We read marks into two corresponding struct mark_set objects and then
    perform a mapping from the old to the new using a hash table. This lets
    us reuse the same mark parsing code that is used elsewhere and allows us
    to efficiently read and match marks based on their ID, since mark files
    need not be sorted.
    
    Note that because we're using a khash table for the object IDs, and this
    table copies values of struct object_id instead of taking references to
    them, it's necessary to zero the struct object_id values that we use to
    insert and look up in the table. Otherwise, we would end up with SHA-1
    values that don't match because of whatever stack garbage might be left
    in the unused area.
    
    Signed-off-by: brian m. carlson <sandals@crustytoothpaste.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit d68ce906c7e1b2bb28a389b915cb63ddbae43365
Author: René Scharfe <l.s.r@web.de>
Date:   Thu Feb 20 19:49:18 2020 +0100

    commit-graph: use progress title directly
    
    merge_commit_graphs() copies the (translated) progress message into a
    strbuf and passes the copy to start_delayed_progress() at each loop
    iteration.  The latter function takes a string pointer, so let's avoid
    the detour and hand the string to it directly.  That's shorter, simpler
    and slightly more efficient.
    
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Acked-by: Derrick Stolee <dstolee@microsoft.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 63f4a7fc0107ec240f48605a4d4f8e41b91caa41
Author: Jeff King <peff@peff.net>
Date:   Sun Feb 23 23:36:31 2020 -0500

    pack-check: push oid lookup into loop
    
    When we're checking a pack with fsck or verify-pack, we first sort the
    idx entries by offset, since accessing them in pack order is more
    efficient. To do so, we loop over them and fill in an array of structs
    with the offset, object_id, and index position of each, sort the result,
    and only then do we iterate over the sorted array and process each
    entry.
    
    In order to avoid the memory cost of storing the hash of each object, we
    just store a pointer into the copy in the mmap'd pack index file. To
    keep that property even as the rest of the code converted to "struct
    object_id", commit 9fd750461b (Convert the verify_pack callback to
    struct object_id, 2017-05-06) introduced a union in order to type-pun
    the pointer-to-hash into an object_id struct.
    
    But we can make this even simpler by observing that the sort operation
    doesn't need the object id at all! We only need them one at a time while
    we actually process each entry. So we can just omit the oid from the
    struct entirely and load it on the fly into a local variable in the
    second loop.
    
    This gets rid of the type-punning, and lets us directly use the more
    type-safe nth_packed_object_id(), simplifying the code. And as a bonus,
    it saves 8 bytes of memory per object.
    
    Note that this does mean we'll do the offset lookup for each object
    before the oid lookup. The oid lookup has more safety checks in it
    (e.g., for looking past p->num_objects) which in theory protected the
    offset lookup. But since violating those checks was already a BUG()
    condition (as described in the previous commit), it's not worth worrying
    about.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 2ce6d075fa35e4ea4a581c809eca3ad5631c9079
Author: René Scharfe <l.s.r@web.de>
Date:   Sat Feb 22 19:51:19 2020 +0100

    use strpbrk(3) to search for characters from a given set
    
    We can check if certain characters are present in a string by calling
    strchr(3) on each of them, or we can pass them all to a single
    strpbrk(3) call.  The latter is shorter, less repetitive and slightly
    more efficient, so let's do that instead.
    
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 2b3c430bcea5d8c40b218c7e2a71d261822c6a52
Author: René Scharfe <l.s.r@web.de>
Date:   Sat Feb 22 19:51:13 2020 +0100

    quote: use isalnum() to check for alphanumeric characters
    
    isalnum(c) is equivalent to isalpha(c) || isdigit(c), so use the
    former instead.  The result is shorter, simpler and slightly more
    efficient.
    
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a277d0a67f0324deb58abbeb0c5a2b3abbcde340
Author: René Scharfe <l.s.r@web.de>
Date:   Sun Feb 9 16:55:54 2020 +0100

    parse-options: use COPY_ARRAY in parse_options_concat()
    
    Use COPY_ARRAY to copy whole arrays instead of iterating through the
    elements.  That's shorter, simpler and bit more efficient.
    
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 76c57fedfa8a15bd341c0059bff164a049fe1d5c
Merge: 9a5315edfd b2627cc3d4
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Feb 5 14:34:58 2020 -0800

    Merge branch 'js/add-p-leftover-bits'
    
    The final leg of rewriting "add -i/-p" in C.
    
    * js/add-p-leftover-bits:
      ci: include the built-in `git add -i` in the `linux-gcc` job
      built-in add -p: handle Escape sequences more efficiently
      built-in add -p: handle Escape sequences in interactive.singlekey mode
      built-in add -p: respect the `interactive.singlekey` config setting
      terminal: add a new function to read a single keystroke
      terminal: accommodate Git for Windows' default terminal
      terminal: make the code of disable_echo() reusable
      built-in add -p: handle diff.algorithm
      built-in add -p: support interactive.diffFilter
      t3701: adjust difffilter test

commit 2e2aa8d9032ccdfdecaab51c60c5bada517f60bc
Author: Yang Zhao <yang.zhao@skyboxlabs.com>
Date:   Fri Dec 13 15:52:45 2019 -0800

    git-p4: use dict.items() iteration for python3 compatibility
    
    Python3 uses dict.items() instead of .iteritems() to provide
    iteratoration over dict.  Although items() is technically less efficient
    for python2.7 (allocates a new list instead of simply iterating), the
    amount of data involved is very small and the penalty negligible.
    
    Signed-off-by: Yang Zhao <yang.zhao@skyboxlabs.com>
    Reviewed-by: Ben Keene <seraphire@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 12acdf573aafb8633f710fdbe5ac1282165fc6cc
Author: Johannes Schindelin <johannes.schindelin@gmx.de>
Date:   Tue Jan 14 18:43:52 2020 +0000

    built-in add -p: handle Escape sequences more efficiently
    
    When `interactive.singlekey = true`, we react immediately to keystrokes,
    even to Escape sequences (e.g. when pressing a cursor key).
    
    The problem with Escape sequences is that we do not really know when
    they are done, and as a heuristic we poll standard input for half a
    second to make sure that we got all of it.
    
    While waiting half a second is not asking for a whole lot, it can become
    quite annoying over time, therefore with this patch, we read the
    terminal capabilities (if available) and extract known Escape sequences
    from there, then stop polling immediately when we detected that the user
    pressed a key that generated such a known sequence.
    
    This recapitulates the remaining part of b5cc003253c8 (add -i: ignore
    terminal escape sequences, 2011-05-17).
    
    Note: We do *not* query the terminal capabilities directly. That would
    either require a lot of platform-specific code, or it would require
    linking to a library such as ncurses.
    
    Linking to a library in the built-ins is something we try very hard to
    avoid (we even kicked the libcurl dependency to a non-built-in remote
    helper, just to shave off a tiny fraction of a second from Git's startup
    time). And the platform-specific code would be a maintenance nightmare.
    
    Even worse: in Git for Windows' case, we would need to query MSYS2
    pseudo terminals, which `git.exe` simply cannot do (because it is
    intentionally *not* an MSYS2 program).
    
    To address this, we simply spawn `infocmp -L -1` and parse its output
    (which works even in Git for Windows, because that helper is included in
    the end-user facing installations).
    
    This is done only once, as in the Perl version, but it is done only when
    the first Escape sequence is encountered, not upon startup of `git add
    -i`; This saves on startup time, yet makes reacting to the first Escape
    sequence slightly more sluggish. But it allows us to keep the
    terminal-related code encapsulated in the `compat/terminal.c` file.
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 6d831b8a3ec7dc06981e6347eb9a45849ef88665
Merge: 3beff388b2 ec48540fe8
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon Dec 16 13:08:31 2019 -0800

    Merge branch 'cs/store-packfiles-in-hashmap'
    
    In a repository with many packfiles, the cost of the procedure that
    avoids registering the same packfile twice was unnecessarily high
    by using an inefficient search algorithm, which has been corrected.
    
    * cs/store-packfiles-in-hashmap:
      packfile.c: speed up loading lots of packfiles

commit 288a74bcd28229a00c3632f18cba92dbfdf73ee9
Author: Johannes Schindelin <johannes.schindelin@gmx.de>
Date:   Mon Sep 23 08:58:11 2019 +0200

    is_ntfs_dotgit(): only verify the leading segment
    
    The config setting `core.protectNTFS` is specifically designed to work
    not only on Windows, but anywhere, to allow for repositories hosted on,
    say, Linux servers to be protected against NTFS-specific attack vectors.
    
    As a consequence, `is_ntfs_dotgit()` manually splits backslash-separated
    paths (but does not do the same for paths separated by forward slashes),
    under the assumption that the backslash might not be a valid directory
    separator on the _current_ Operating System.
    
    However, the two callers, `verify_path()` and `fsck_tree()`, are
    supposed to feed only individual path segments to the `is_ntfs_dotgit()`
    function.
    
    This causes a lot of duplicate scanning (and very inefficient scanning,
    too, as the inner loop of `is_ntfs_dotgit()` was optimized for
    readability rather than for speed.
    
    Let's simplify the design of `is_ntfs_dotgit()` by putting the burden of
    splitting the paths by backslashes as directory separators on the
    callers of said function.
    
    Consequently, the `verify_path()` function, which already splits the
    path by directory separators, now treats backslashes as directory
    separators _explicitly_ when `core.protectNTFS` is turned on, even on
    platforms where the backslash is _not_ a directory separator.
    
    Note that we have to repeat some code in `verify_path()`: if the
    backslash is not a directory separator on the current Operating System,
    we want to allow file names like `\`, but we _do_ want to disallow paths
    that are clearly intended to cause harm when the repository is cloned on
    Windows.
    
    The `fsck_tree()` function (the other caller of `is_ntfs_dotgit()`) now
    needs to look for backslashes in tree entries' names specifically when
    `core.protectNTFS` is turned on. While it would be tempting to
    completely disallow backslashes in that case (much like `fsck` reports
    names containing forward slashes as "full paths"), this would be
    overzealous: when `core.protectNTFS` is turned on in a non-Windows
    setup, backslashes are perfectly valid characters in file names while we
    _still_ want to disallow tree entries that are clearly designed to
    exploit NTFS-specific behavior.
    
    This simplification will make subsequent changes easier to implement,
    such as turning `core.protectNTFS` on by default (not only on Windows)
    or protecting against attack vectors involving NTFS Alternate Data
    Streams.
    
    Incidentally, this change allows for catching malicious repositories
    that contain tree entries of the form `dir\.gitmodules` already on the
    server side rather than only on the client side (and previously only on
    Windows): in contrast to `is_ntfs_dotgit()`, the
    `is_ntfs_dotgitmodules()` function already expects the caller to split
    the paths by directory separators.
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>

commit ae475afc0f8685607e2de838db7fb4bee7934d4d
Author: Denton Liu <liu.denton@gmail.com>
Date:   Wed Dec 4 14:03:09 2019 -0800

    t7700: consolidate code into test_no_missing_in_packs()
    
    The code to test that objects were not missing from the packfile was
    duplicated many times. Extract the duplicated code into
    test_no_missing_in_packs() and use that instead.
    
    Refactor the resulting extraction so that if any git commands fail,
    their return codes are not silently lost.
    
    Instead of verifying each file of `alt_objects/pack/*.idx` individually
    in a for-loop, batch them together into one verification step.
    
    The original testing construct was O(n^2): it used a grep in a loop to
    test whether any objects were missing in the packfile. Rewrite this to
    extract the hash using sed or cut, sort the files, then use `comm -23`
    so that finding missing lines from the original file is done more
    efficiently.
    
    While we're at it, add a space to `commit_and_pack ()` for style.
    
    Signed-off-by: Denton Liu <liu.denton@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit ed254710eee0a5fcde1057593d181d40b02922e1
Author: René Scharfe <l.s.r@web.de>
Date:   Wed Nov 27 08:51:43 2019 +0100

    test: use test_must_be_empty F instead of test_cmp empty F
    
    Use test_must_be_empty instead of comparing it to an empty file.  That's
    more efficient, as the function only needs built-in meta-data only check
    in the usual case, and provides nicer debug output otherwise.
    
    Helped-by: Denton Liu <liu.denton@gmail.com>
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 213dabf49dc53ae5808efbdebf0cd2bfc2c1bfa8
Author: René Scharfe <l.s.r@web.de>
Date:   Tue Nov 26 20:46:07 2019 +0100

    test: use test_must_be_empty F instead of test -z $(cat F)
    
    Use test_must_be_empty instead of reading the file and comparing its
    contents to an empty string.  That's more efficient, as the function
    only needs built-in meta-data only check in the usual case, and provides
    nicer debug output otherwise.
    
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit c93a5aaec859f93ff24277269e2fdef00c9c45e6
Author: René Scharfe <l.s.r@web.de>
Date:   Tue Nov 26 20:41:57 2019 +0100

    t1400: use test_must_be_empty
    
    Use test_must_be_empty instead of reading the file and comparing its
    contents to an empty string.  That's more efficient, as the function
    only needs built-in meta-data only check in the usual case, and provides
    nicer debug output otherwise.
    
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 679f2f9fdd2173d87251aee357dd0e46ce977f42
Author: Utsav Shah <utsav@dropbox.com>
Date:   Wed Nov 20 08:32:17 2019 +0000

    unpack-trees: skip stat on fsmonitor-valid files
    
    The index might be aware that a file hasn't modified via fsmonitor, but
    unpack-trees did not pay attention to it and checked via ie_match_stat
    which can be inefficient on certain filesystems. This significantly slows
    down commands that run oneway_merge, like checkout and reset --hard.
    
    This patch makes oneway_merge check whether a file is considered
    unchanged through fsmonitor and skips ie_match_stat on it. unpack-trees
    also now correctly copies over fsmonitor validity state from the source
    index. Finally, for correctness, we force a refresh of fsmonitor state in
    tweak_fsmonitor.
    
    After this change, commands like stash (that use reset --hard
    internally) go from 8s or more to ~2s on a 250k file repository on a
    mac.
    
    Helped-by: Junio C Hamano <gitster@pobox.com>
    Helped-by: Kevin Willford <Kevin.Willford@microsoft.com>
    Signed-off-by: Utsav Shah <utsav@dropbox.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 5c34d2f03e4a7505a6acb71b6a1be8d85d1cde05
Author: René Scharfe <l.s.r@web.de>
Date:   Mon Nov 4 20:27:54 2019 +0100

    trace2: add dots directly to strbuf in perf_fmt_prepare()
    
    The initialization function of the Trace2 performance format target sets
    aside a stash of dots for indenting output.  Get rid of it and use
    strbuf_addchars() to provide dots on demand instead.  This shortens the
    code, gets rid of a small heap allocation and is a bit more efficient.
    
    Signed-off-by: René Scharfe <l.s.r@web.de>
    Acked-by: Jeff King <peff@peff.net>
    Acked-by: Jeff Hostetler <jeffhost@microsoft.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f45f88b2e483649cd063a7dc7826c03025683e56
Author: SZEDER Gábor <szeder.dev@gmail.com>
Date:   Mon Oct 21 18:00:43 2019 +0200

    path.c: don't call the match function without value in trie_find()
    
    'logs/refs' is not a working tree-specific path, but since commit
    b9317d55a3 (Make sure refs/rewritten/ is per-worktree, 2019-03-07)
    'git rev-parse --git-path' has been returning a bogus path if a
    trailing '/' is present:
    
      $ git -C WT/ rev-parse --git-path logs/refs --git-path logs/refs/
      /home/szeder/src/git/.git/logs/refs
      /home/szeder/src/git/.git/worktrees/WT/logs/refs/
    
    We use a trie data structure to efficiently decide whether a path
    belongs to the common dir or is working tree-specific.  As it happens
    b9317d55a3 triggered a bug that is as old as the trie implementation
    itself, added in 4e09cf2acf (path: optimize common dir checking,
    2015-08-31).
    
      - According to the comment describing trie_find(), it should only
        call the given match function 'fn' for a "/-or-\0-terminated
        prefix of the key for which the trie contains a value".  This is
        not true: there are three places where trie_find() calls the match
        function, but one of them is missing the check for value's
        existence.
    
      - b9317d55a3 added two new keys to the trie: 'logs/refs/rewritten'
        and 'logs/refs/worktree', next to the already existing
        'logs/refs/bisect'.  This resulted in a trie node with the path
        'logs/refs/', which didn't exist before, and which doesn't have a
        value attached.  A query for 'logs/refs/' finds this node and then
        hits that one callsite of the match function which doesn't check
        for the value's existence, and thus invokes the match function
        with NULL as value.
    
      - When the match function check_common() is invoked with a NULL
        value, it returns 0, which indicates that the queried path doesn't
        belong to the common directory, ultimately resulting the bogus
        path shown above.
    
    Add the missing condition to trie_find() so it will never invoke the
    match function with a non-existing value.  check_common() will then no
    longer have to check that it got a non-NULL value, so remove that
    condition.
    
    I believe that there are no other paths that could cause similar bogus
    output.  AFAICT the only other key resulting in the match function
    being called with a NULL value is 'co' (because of the keys 'common'
    and 'config').  However, as they are not in a directory that belongs
    to the common directory the resulting working tree-specific path is
    expected.
    
    Signed-off-by: SZEDER Gábor <szeder.dev@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit fbab552a53a2558ec84a06034e6657fb95227266
Author: Jeff King <peff@peff.net>
Date:   Thu Sep 12 10:44:34 2019 -0400

    commit-graph: bump DIE_ON_LOAD check to actual load-time
    
    Commit 43d3561805 (commit-graph write: don't die if the existing graph
    is corrupt, 2019-03-25) added an environment variable we use only in the
    test suite, $GIT_TEST_COMMIT_GRAPH_DIE_ON_LOAD. But it put the check for
    this variable at the very top of prepare_commit_graph(), which is called
    every time we want to use the commit graph. Most importantly, it comes
    _before_ we check the fast-path "did we already try to load?", meaning
    we end up calling getenv() for every single use of the commit graph,
    rather than just when we load.
    
    getenv() is allowed to have unexpected side effects, but that shouldn't
    be a problem here; we're lazy-loading the graph so it's clear that at
    least _one_ invocation of this function is going to call it.
    
    But it is inefficient. getenv() typically has to do a linear search
    through the environment space.
    
    We could memoize the call, but it's simpler still to just bump the check
    down to the actual loading step. That's fine for our sole user in t5318,
    and produces this minor real-world speedup:
    
      [before]
      Benchmark #1: git -C linux rev-list HEAD >/dev/null
        Time (mean ± σ):      1.460 s ±  0.017 s    [User: 1.174 s, System: 0.285 s]
        Range (min … max):    1.440 s …  1.491 s    10 runs
    
      [after]
      Benchmark #1: git -C linux rev-list HEAD >/dev/null
        Time (mean ± σ):      1.391 s ±  0.005 s    [User: 1.118 s, System: 0.273 s]
        Range (min … max):    1.385 s …  1.399 s    10 runs
    
    Of course that actual speedup depends on how big your environment is. We
    can game it like this:
    
      for i in $(seq 10000); do
        export dummy$i=$i
      done
    
    in which case I get:
    
      [before]
      Benchmark #1: git -C linux rev-list HEAD >/dev/null
        Time (mean ± σ):      6.257 s ±  0.061 s    [User: 6.005 s, System: 0.250 s]
        Range (min … max):    6.174 s …  6.337 s    10 runs
    
      [after]
      Benchmark #1: git -C linux rev-list HEAD >/dev/null
      Time (mean ± σ):      1.403 s ±  0.005 s    [User: 1.146 s, System: 0.256 s]
      Range (min … max):    1.396 s …  1.412 s    10 runs
    
    So this is really more about avoiding the pathological case than
    providing a big real-world speedup.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a2bb801f6a430f6049e5c9729a8f3bf9097d9b34
Author: SZEDER Gábor <szeder.dev@gmail.com>
Date:   Wed Aug 21 13:04:24 2019 +0200

    line-log: avoid unnecessary full tree diffs
    
    With rename detection enabled the line-level log is able to trace the
    evolution of line ranges across whole-file renames [1].  Alas, to
    achieve that it uses the diff machinery very inefficiently, making the
    operation very slow [2].  And since rename detection is enabled by
    default, the line-level log is very slow by default.
    
    When the line-level log processes a commit with rename detection
    enabled, it currently does the following (see queue_diffs()):
    
      1. Computes a full tree diff between the commit and (one of) its
         parent(s), i.e. invokes diff_tree_oid() with an empty
         'diffopt->pathspec'.
      2. Checks whether any paths in the line ranges were modified.
      3. Checks whether any modified paths in the line ranges are missing
         in the parent commit's tree.
      4. If there is such a missing path, then calls diffcore_std() to
         figure out whether the path was indeed renamed based on the
         previously computed full tree diff.
      5. Continues doing stuff that are unrelated to the slowness.
    
    So basically the line-level log computes a full tree diff for each
    commit-parent pair in step (1) to be used for rename detection in step
    (4) in the off chance that an interesting path is missing from the
    parent.
    
    Avoid these expensive and mostly unnecessary full tree diffs by
    limiting the diffs to paths in the line ranges.  This is much cheaper,
    and makes step (2) unnecessary.  If it turns out that an interesting
    path is missing from the parent, then fall back and compute a full
    tree diff, so the rename detection will still work.
    
    Care must be taken when to update the pathspec used to limit the diff
    in case of renames.  A path might be renamed on one branch and
    modified on several parallel running branches, and while processing
    commits on these branches the line-level log might have to alternate
    between looking at a path's new and old name.  However, at any one
    time there is only a single 'diffopt->pathspec'.
    
    So add a step (0) to the above to ensure that the paths in the
    pathspec match the paths in the line ranges associated with the
    currently processed commit, and re-parse the pathspec from the paths
    in the line ranges if they differ.
    
    The new test cases include a specially crafted piece of history with
    two merged branches and two files, where each branch modifies both
    files, renames on of them, and then modifies both again.  Then two
    separate 'git log -L' invocations check the line-level log of each of
    those two files, which ensures that at least one of those invocations
    have to do that back-and-forth between the file's old and new name (no
    matter which branch is traversed first).  't/t4211-line-log.sh'
    already contains two tests involving renames, they don't don't trigger
    this back-and-forth.
    
    Avoiding these unnecessary full tree diffs can have huge impact on
    performance, especially in big repositories with big trees and mergy
    history.  Tracing the evolution of a function through the whole
    history:
    
      # git.git
      $ time git --no-pager log -L:read_alternate_refs:sha1-file.c v2.23.0
    
      Before:
    
        real    0m8.874s
        user    0m8.816s
        sys     0m0.057s
    
      After:
    
        real    0m2.516s
        user    0m2.456s
        sys     0m0.060s
    
      # linux.git
      $ time ~/src/git/git --no-pager log \
        -L:build_restore_work_registers:arch/mips/mm/tlbex.c v5.2
    
      Before:
    
        real    3m50.033s
        user    3m48.041s
        sys     0m0.300s
    
      After:
    
        real    0m2.599s
        user    0m2.466s
        sys     0m0.157s
    
    That's just over 88x speedup.
    
    [1] Line-level log's rename following is quite similar to 'git log
        --follow path', with the notable differences that it does handle
        multiple paths at once as well, and that it doesn't show the
        commit performing the rename if it's an exact rename.
    
    [2] This slowness might not have been apparent initially, because back
        when the line-level log feature was introduced rename detection
        was not yet enabled by default; 12da1d1f6f (Implement line-history
        search (git log -L), 2013-03-28) and 5404c116aa (diff: activate
        diff.renames by default, 2016-02-25).
    
    Signed-off-by: SZEDER Gábor <szeder.dev@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 8320b1dbe7d160ea08dec931cf17dc39682bfb91
Author: Jeff King <peff@peff.net>
Date:   Wed Apr 3 21:41:09 2019 -0400

    revision: use a prio_queue to hold rewritten parents
    
    This patch fixes a quadratic list insertion in rewrite_one() when
    pathspec limiting is combined with --parents. What happens is something
    like this:
    
      1. We see that some commit X touches the path, so we try to rewrite
         its parents.
    
      2. rewrite_one() loops forever, rewriting parents, until it finds a
         relevant parent (or hits the root and decides there are none). The
         heavy lifting is done by process_parent(), which uses
         try_to_simplify_commit() to drop parents.
    
      3. process_parent() puts any intermediate parents into the
         &revs->commits list, inserting by commit date as usual.
    
    So if commit X is recent, and then there's a large chunk of history that
    doesn't touch the path, we may add a lot of commits to &revs->commits.
    And insertion by commit date is O(n) in the worst case, making the whole
    thing quadratic.
    
    We tried to deal with this long ago in fce87ae538 (Fix quadratic
    performance in rewrite_one., 2008-07-12). In that scheme, we cache the
    oldest commit in the list; if the new commit to be added is older, we
    can start our linear traversal there. This often works well in practice
    because parents are older than their descendants, and thus we tend to
    add older and older commits as we traverse.
    
    But this isn't guaranteed, and in fact there's a simple case where it is
    not: merges. Imagine we look at the first parent of a merge and see a
    very old commit (let's say 3 years old). And on the second parent, as we
    go back 3 years in history, we might have many commits. That one
    first-parent commit has polluted our oldest-commit cache; it will remain
    the oldest while we traverse a huge chunk of history, during which we
    have to fall back to the slow, linear method of adding to the list.
    
    Naively, one might imagine that instead of caching the oldest commit,
    we'd start at the last-added one. But that just makes some cases faster
    while making others slower (and indeed, while it made a real-world test
    case much faster, it does quite poorly in the perf test include here).
    Fundamentally, these are just heuristics; our worst case is still
    quadratic, and some cases will approach that.
    
    Instead, let's use a data structure with better worst-case performance.
    Swapping out revs->commits for something else would have repercussions
    all over the code base, but we can take advantage of one fact: for the
    rewrite_one() case, nobody actually needs to see those commits in
    revs->commits until we've finished generating the whole list.
    
    That leaves us with two obvious options:
    
      1. We can generate the list _unordered_, which should be O(n), and
         then sort it afterwards, which would be O(n log n) total. This is
         "sort-after" below.
    
      2. We can insert the commits into a separate data structure, like a
         priority queue. This is "prio-queue" below.
    
    I expected that sort-after would be the fastest (since it saves us the
    extra step of copying the items into the linked list), but surprisingly
    the prio-queue seems to be a bit faster.
    
    Here are timings for the new p0001.6 for all three techniques across a
    few repositories, as compared to master:
    
    master              cache-last                sort-after              prio-queue
    --------------------------------------------------------------------------------------------
    GIT_PERF_REPO=git.git
    0.52(0.50+0.02)      0.53(0.51+0.02)  +1.9%   0.37(0.33+0.03) -28.8%  0.37(0.32+0.04) -28.8%
    
    GIT_PERF_REPO=linux.git
    20.81(20.74+0.07)   20.31(20.24+0.07) -2.4%   0.94(0.86+0.07) -95.5%  0.91(0.82+0.09) -95.6%
    
    GIT_PERF_REPO=llvm-project.git
    83.67(83.57+0.09)    4.23(4.15+0.08) -94.9%   3.21(3.15+0.06) -96.2%  2.98(2.91+0.07) -96.4%
    
    A few items to note:
    
      - the cache-list tweak does improve the bad case for llvm-project.git
        that started my digging into this problem. But it performs terribly
        on linux.git, barely helping at all.
    
      - the sort-after and prio-queue techniques work well. They approach
        the timing for running without --parents at all, which is what you'd
        expect (see below for more data).
    
      - prio-queue just barely outperforms sort-after. As I said, I'm not
        really sure why this is the case, but it is. You can see it even
        more prominently in this real-world case on llvm-project.git:
    
          git rev-list --parents 07ef786652e7 -- llvm/test/CodeGen/Generic/bswap.ll
    
        where prio-queue routinely outperforms sort-after by about 7%. One
        guess is that the prio-queue may just be more efficient because it
        uses a compact array.
    
    There are three new perf tests:
    
      - "rev-list --parents" gives us a baseline for running with --parents.
        This isn't sped up meaningfully here, because the bad case is
        triggered only with simplification. But it's good to make sure we
        don't screw it up (now, or in the future).
    
      - "rev-list -- dummy" gives us a baseline for just traversing with
        pathspec limiting. This gives a lower bound for the next test (and
        it's also a good thing for us to be checking in general for
        regressions, since we don't seem to have any existing tests).
    
      - "rev-list --parents -- dummy" shows off the problem (and our fix)
    
    Here are the timings for those three on llvm-project.git, before and
    after the fix:
    
    Test                                 master              prio-queue
    ------------------------------------------------------------------------------
    0001.3: rev-list --parents           2.24(2.12+0.12)     2.22(2.11+0.11) -0.9%
    0001.5: rev-list -- dummy            2.89(2.82+0.07)     2.92(2.89+0.03) +1.0%
    0001.6: rev-list --parents -- dummy  83.67(83.57+0.09)   2.98(2.91+0.07) -96.4%
    
    Changes in the first two are basically noise, and you can see we
    approach our lower bound in the final one.
    
    Note that we can't fully get rid of the list argument from
    process_parents(). Other callers do have lists, and it would be hard to
    convert them. They also don't seem to have this problem (probably
    because they actually remove items from the list as they loop, meaning
    it doesn't grow so large in the first place). So this basically just
    drops the "cache_ptr" parameter (which was used only by the one caller
    we're fixing here) and replaces it with a prio_queue. Callers are free
    to use either data structure, depending on what they're prepared to
    handle.
    
    Reported-by: Björn Pettersson A <bjorn.a.pettersson@ericsson.com>
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 143588949c8a0672302403c722fc433a5bb2ea2a
Author: Jeff King <peff@peff.net>
Date:   Sun Mar 3 11:58:43 2019 -0500

    fetch: ignore SIGPIPE during network operation
    
    The default SIGPIPE behavior can be useful for a command that generates
    a lot of output: if the receiver of our output goes away, we'll be
    notified asynchronously to stop generating it (typically by killing the
    program).
    
    But for a command like fetch, which is primarily concerned with
    receiving data and writing it to disk, an unexpected SIGPIPE can be
    awkward. We're already checking the return value of all of our write()
    calls, and dying due to the signal takes away our chance to gracefully
    handle the error.
    
    On Linux, we wouldn't generally see SIGPIPE at all during fetch. If the
    other side of the network connection hangs up, we'll see ECONNRESET. But
    on OS X, we get a SIGPIPE, and the process is killed. This causes t5570
    to racily fail, as we sometimes die by signal (instead of the expected
    die() call) when the server side hangs up.
    
    Let's ignore SIGPIPE during the network portion of the fetch, which will
    cause our write() to return EPIPE, giving us consistent behavior across
    platforms.
    
    This fixes the test flakiness, but note that it stops short of fixing
    the larger problem. The server side hit a fatal error, sent us an "ERR"
    packet, and then hung up. We notice the failure because we're trying to
    write to a closed socket. But by dying immediately, we never actually
    read the ERR packet and report its content to the user. This is a (racy)
    problem on all platforms. So this patch lays the groundwork from which
    that problem might be fixed consistently, but it doesn't actually fix
    it.
    
    Note the placement of the SIGPIPE handling. The absolute minimal change
    would be to ignore SIGPIPE only when we're writing. But twiddling the
    signal handler for each write call is inefficient and maintenance
    burden. On the opposite end of the spectrum, we could simply declare
    that fetch does not need SIGPIPE handling, since it doesn't generate a
    lot of output, and we could just ignore it at the start of cmd_fetch().
    
    This patch takes a middle ground. It ignores SIGPIPE during the network
    operation (which is admittedly most of the program, since the actual
    network operations are all done under the hood by the transport code).
    So it's still pretty coarse.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit fde67d68966e2acc4f2790d0a69991ab1f89a042
Author: Jeff King <peff@peff.net>
Date:   Wed Feb 13 23:37:43 2019 -0500

    prune: use bitmaps for reachability traversal
    
    Pruning generally has to traverse the whole commit graph in order to
    see which objects are reachable. This is the exact problem that
    reachability bitmaps were meant to solve, so let's use them (if they're
    available, of course).
    
    Here are timings on git.git:
    
      Test                            HEAD^             HEAD
      ------------------------------------------------------------------------
      5304.6: prune with bitmaps      3.65(3.56+0.09)   1.01(0.92+0.08) -72.3%
    
    And on linux.git:
    
      Test                            HEAD^               HEAD
      --------------------------------------------------------------------------
      5304.6: prune with bitmaps      35.05(34.79+0.23)   3.00(2.78+0.21) -91.4%
    
    The tests show a pretty optimal case, as we'll have just repacked and
    should have pretty good coverage of all refs with our bitmaps. But
    that's actually pretty realistic: normally prune is run via "gc" right
    after repacking.
    
    A few notes on the implementation:
    
      - the change is actually in reachable.c, so it would improve
        reachability traversals by "reflog expire --stale-fix", as well.
        Those aren't performed regularly, though (a normal "git gc" doesn't
        use --stale-fix), so they're not really worth measuring. There's a
        low chance of regressing that caller, since the use of bitmaps is
        totally transparent from the caller's perspective.
    
      - The bitmap case could actually get away without creating a "struct
        object", and instead the caller could just look up each object id in
        the bitmap result. However, this would be a marginal improvement in
        runtime, and it would make the callers much more complicated. They'd
        have to handle both the bitmap and non-bitmap cases separately, and
        in the case of git-prune, we'd also have to tweak prune_shallow(),
        which relies on our SEEN flags.
    
      - Because we do create real object structs, we go through a few
        contortions to create ones of the right type. This isn't strictly
        necessary (lookup_unknown_object() would suffice), but it's more
        memory efficient to use the correct types, since we already know
        them.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 28c23cd4c3902449aff72cb9a4a703220be0d6ac
Author: SZEDER Gábor <szeder.dev@gmail.com>
Date:   Fri Jan 25 13:25:17 2019 +0100

    strbuf.cocci: suggest strbuf_addbuf() to add one strbuf to an other
    
    The best way to add one strbuf to an other is via:
    
      strbuf_addbuf(&sb, &sb2);
    
    This is a bit more idiomatic and efficient than:
    
      strbuf_addstr(&sb, sb2.buf);
    
    because the size of the second strbuf is known and thus it can spare a
    strlen() call, and much more so than:
    
      strbuf_addf(&sb, "%s", sb2.buf);
    
    because it can spare the whole vsnprintf() formatting magic.
    
    Add new semantic patches to 'contrib/coccinelle/strbuf.cocci' to catch
    these undesired patterns and to suggest strbuf_addbuf() instead.
    
    Luckily, our codebase is already clean from any such undesired
    patterns (but one of the in-flight topics just tried to sneak in such
    a strbuf_addf() call).
    
    Signed-off-by: SZEDER Gábor <szeder.dev@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit d7574c95bb74cd189950cc6e9cad762099382162
Author: Ævar Arnfjörð Bjarmason <avarab@gmail.com>
Date:   Sat Jan 19 21:21:12 2019 +0100

    commit-graph write: use pack order when finding commits
    
    Slightly optimize the "commit-graph write" step by using
    FOR_EACH_OBJECT_PACK_ORDER with for_each_object_in_pack(). See commit
    [1] and [2] for the facility and a similar optimization for "cat-file".
    
    On Linux it is around 5% slower to run:
    
        echo 1 >/proc/sys/vm/drop_caches &&
        cat .git/objects/pack/* >/dev/null &&
        git cat-file --batch-all-objects --batch-check --unordered
    
    Than the same thing with the "cat" omitted. This is as expected, since
    we're iterating in pack order and the "cat" is extra work.
    
    Before this change the opposite was true of "commit-graph write". We
    were 6% faster if we first ran "cat" to efficiently populate the FS
    cache for our sole big pack on linux.git, than if we had populated it
    via for_each_object_in_pack(). Now we're 3% faster without the "cat"
    instead.
    
    My tests were done on an unloaded Linux 3.10 system with 10 runs for
    each. Derrick Stolee did his own tests on Windows[3] showing a 2%
    improvement with a high degree of accuracy.
    
    1. 736eb88fdc ("for_each_packed_object: support iterating in
       pack-order", 2018-08-10)
    
    2. 0750bb5b51 ("cat-file: support "unordered" output for
       --batch-all-objects", 2018-08-10)
    
    3. https://public-inbox.org/git/f71fa868-25e8-a9c9-46a6-611b987f1a8f@gmail.com/
    
    Signed-off-by: Ævar Arnfjörð Bjarmason <avarab@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit ea82b2a0857e3e0449bdce4e3987dee6adbc51ae
Author: brian m. carlson <sandals@crustytoothpaste.net>
Date:   Tue Jan 15 00:39:44 2019 +0000

    tree-walk: store object_id in a separate member
    
    When parsing a tree, we read the object ID directly out of the tree
    buffer. This is normally fine, but such an object ID cannot be used with
    oidcpy, which copies GIT_MAX_RAWSZ bytes, because if we are using SHA-1,
    there may not be that many bytes to copy.
    
    Instead, store the object ID in a separate struct member. Since we can
    no longer efficiently compute the path length, store that information as
    well in struct name_entry. Ensure we only copy the object ID into the
    new buffer if the path length is nonzero, as some callers will pass us
    an empty path with no object ID following it, and we will not want to
    read past the end of the buffer.
    
    Signed-off-by: brian m. carlson <sandals@crustytoothpaste.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 0dab7129ab122d7ac5249fc7f6890302f29cd9e7
Author: brian m. carlson <sandals@crustytoothpaste.net>
Date:   Wed Nov 14 04:09:30 2018 +0000

    cache: make hashcmp and hasheq work with larger hashes
    
    In 183a638b7d ("hashcmp: assert constant hash size", 2018-08-23), we
    modified hashcmp to assert that the hash size was always 20 to help it
    optimize and inline calls to memcmp.  In a future series, we replaced
    many calls to hashcmp and oidcmp with calls to hasheq and oideq to
    improve inlining further.
    
    However, we want to support hash algorithms other than SHA-1, namely
    SHA-256.  When doing so, we must handle the case where these values are
    32 bytes long as well as 20.  Adjust hashcmp to handle two cases:
    20-byte matches, and maximum-size matches.  Therefore, when we include
    SHA-256, we'll automatically handle it properly, while at the same time
    teaching the compiler that there are only two possible options to
    consider.  This will allow the compiler to write the most efficient
    possible code.
    
    Copy similar code into hasheq and perform an identical transformation.
    At least with GCC 8.2.0, making hasheq defer to hashcmp when there are
    two branches prevents the compiler from inlining the comparison, while
    the code in this patch is inlined properly.  Add a comment to avoid an
    accidental performance regression from well-intentioned refactoring.
    
    Signed-off-by: brian m. carlson <sandals@crustytoothpaste.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f3f043a103b7a7da57272ecf3252bda6089e41ae
Author: Jeff King <peff@peff.net>
Date:   Mon Nov 12 09:49:35 2018 -0500

    handle alternates paths the same as the main object dir
    
    When we generate loose file paths for the main object directory, the
    caller provides a buffer to loose_object_path (formerly sha1_file_name).
    The callers generally keep their own static buffer to avoid excessive
    reallocations.
    
    But for alternate directories, each struct carries its own scratch
    buffer. This is needlessly different; let's unify them.
    
    We could go either direction here, but this patch moves the alternates
    struct over to the main directory style (rather than vice-versa).
    Technically the alternates style is more efficient, as it avoids
    rewriting the object directory name on each call. But this is unlikely
    to matter in practice, as we avoid reallocations either way (and nobody
    has ever noticed or complained that the main object directory is copying
    a few extra bytes before making a much more expensive system call).
    
    And this has the advantage that the reusable buffers are tied to
    particular calls, which makes the invalidation rules simpler (for
    example, the return value from stat_sha1_file() used to be invalidated
    by basically any other object call, but now it is affected only by other
    calls to stat_sha1_file()).
    
    We do steal the trick from alt_sha1_path() of returning a pointer to the
    filled buffer, which makes a few conversions more convenient.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 3255089ada6fc8f18d41dfc37cf66d7af994603d
Author: Ben Peart <benpeart@microsoft.com>
Date:   Wed Oct 10 11:59:37 2018 -0400

    ieot: add Index Entry Offset Table (IEOT) extension
    
    This patch enables addressing the CPU cost of loading the index by adding
    additional data to the index that will allow us to efficiently multi-
    thread the loading and conversion of cache entries.
    
    It accomplishes this by adding an (optional) index extension that is a
    table of offsets to blocks of cache entries in the index file.  To make
    this work for V4 indexes, when writing the cache entries, it periodically
    "resets" the prefix-compression by encoding the current entry as if the
    path name for the previous entry is completely different and saves the
    offset of that entry in the IEOT.  Basically, with V4 indexes, it
    generates offsets into blocks of prefix-compressed entries.
    
    Signed-off-by: Ben Peart <benpeart@microsoft.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 252d079cbd27ca442d94535e3979145eceaf082b
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Wed Sep 26 15:54:36 2018 -0400

    read-cache.c: optimize reading index format v4
    
    Index format v4 requires some more computation to assemble a path
    based on a previous one. The current code is not very efficient
    because
    
     - it doubles memory copy, we assemble the final path in a temporary
       first before putting it back to a cache_entry
    
     - strbuf_remove() in expand_name_field() is not exactly a good fit
       for stripping a part at the end, _setlen() would do the same job
       and is much cheaper.
    
     - the open-coded loop to find the end of the string in
       expand_name_field() can't beat an optimized strlen()
    
    This patch avoids the temporary buffer and writes directly to the new
    cache_entry, which addresses the first two points. The last point
    could also be avoided if the total string length fits in the first 12
    bits of ce_flags, if not we fall back to strlen().
    
    Running "test-tool read-cache 100" on webkit.git (275k files), reading
    v2 only takes 4.226 seconds, while v4 takes 5.711 seconds, 35% more
    time. The patch reduces read time on v4 to 4.319 seconds.
    
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7e794d0a3f7ad4a37541539b823d5b9afdc10ce3
Merge: 1b7a91da71 5f4436a721
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon Sep 17 13:53:53 2018 -0700

    Merge branch 'nd/unpack-trees-with-cache-tree'
    
    The unpack_trees() API used in checking out a branch and merging
    walks one or more trees along with the index.  When the cache-tree
    in the index tells us that we are walking a tree whose flattened
    contents is known (i.e. matches a span in the index), as linearly
    scanning a span in the index is much more efficient than having to
    open tree objects recursively and listing their entries, the walk
    can be optimized, which is done in this topic.
    
    * nd/unpack-trees-with-cache-tree:
      Document update for nd/unpack-trees-with-cache-tree
      cache-tree: verify valid cache-tree in the test suite
      unpack-trees: add missing cache invalidation
      unpack-trees: reuse (still valid) cache-tree from src_index
      unpack-trees: reduce malloc in cache-tree walk
      unpack-trees: optimize walking same trees with cache-tree
      unpack-trees: add performance tracing
      trace.h: support nested performance tracing

commit 6a1e32d532c5948071e322cefc7052be6228adc3
Author: Jeff King <peff@peff.net>
Date:   Tue Aug 21 15:07:05 2018 -0400

    pack-objects: reuse on-disk deltas for thin "have" objects
    
    When we serve a fetch, we pass the "wants" and "haves" from
    the fetch negotiation to pack-objects. That tells us not
    only which objects we need to send, but we also use the
    boundary commits as "preferred bases": their trees and blobs
    are candidates for delta bases, both for reusing on-disk
    deltas and for finding new ones.
    
    However, this misses some opportunities. Modulo some special
    cases like shallow or partial clones, we know that every
    object reachable from the "haves" could be a preferred base.
    We don't use all of them for two reasons:
    
      1. It's expensive to traverse the whole history and
         enumerate all of the objects the other side has.
    
      2. The delta search is expensive, so we want to keep the
         number of candidate bases sane. The boundary commits
         are the most likely to work.
    
    When we have reachability bitmaps, though, reason 1 no
    longer applies. We can efficiently compute the set of
    reachable objects on the other side (and in fact already did
    so as part of the bitmap set-difference to get the list of
    interesting objects). And using this set conveniently
    covers the shallow and partial cases, since we have to
    disable the use of bitmaps for those anyway.
    
    The second reason argues against using these bases in the
    search for new deltas. But there's one case where we can use
    this information for free: when we have an existing on-disk
    delta that we're considering reusing, we can do so if we
    know the other side has the base object. This in fact saves
    time during the delta search, because it's one less delta we
    have to compute.
    
    And that's exactly what this patch does: when we're
    considering whether to reuse an on-disk delta, if bitmaps
    tell us the other side has the object (and we're making a
    thin-pack), then we reuse it.
    
    Here are the results on p5311 using linux.git, which
    simulates a client fetching after `N` days since their last
    fetch:
    
     Test                         origin              HEAD
     --------------------------------------------------------------------------
     5311.3: server   (1 days)    0.27(0.27+0.04)     0.12(0.09+0.03) -55.6%
     5311.4: size     (1 days)               0.9M              237.0K -73.7%
     5311.5: client   (1 days)    0.04(0.05+0.00)     0.10(0.10+0.00) +150.0%
     5311.7: server   (2 days)    0.34(0.42+0.04)     0.13(0.10+0.03) -61.8%
     5311.8: size     (2 days)               1.5M              347.7K -76.5%
     5311.9: client   (2 days)    0.07(0.08+0.00)     0.16(0.15+0.01) +128.6%
     5311.11: server   (4 days)   0.56(0.77+0.08)     0.13(0.10+0.02) -76.8%
     5311.12: size     (4 days)              2.8M              566.6K -79.8%
     5311.13: client   (4 days)   0.13(0.15+0.00)     0.34(0.31+0.02) +161.5%
     5311.15: server   (8 days)   0.97(1.39+0.11)     0.30(0.25+0.05) -69.1%
     5311.16: size     (8 days)              4.3M                1.0M -76.0%
     5311.17: client   (8 days)   0.20(0.22+0.01)     0.53(0.52+0.01) +165.0%
     5311.19: server  (16 days)   1.52(2.51+0.12)     0.30(0.26+0.03) -80.3%
     5311.20: size    (16 days)              8.0M                2.0M -74.5%
     5311.21: client  (16 days)   0.40(0.47+0.03)     1.01(0.98+0.04) +152.5%
     5311.23: server  (32 days)   2.40(4.44+0.20)     0.31(0.26+0.04) -87.1%
     5311.24: size    (32 days)             14.1M                4.1M -70.9%
     5311.25: client  (32 days)   0.70(0.90+0.03)     1.81(1.75+0.06) +158.6%
     5311.27: server  (64 days)   11.76(26.57+0.29)   0.55(0.50+0.08) -95.3%
     5311.28: size    (64 days)             89.4M               47.4M -47.0%
     5311.29: client  (64 days)   5.71(9.31+0.27)     15.20(15.20+0.32) +166.2%
     5311.31: server (128 days)   16.15(36.87+0.40)   0.91(0.82+0.14) -94.4%
     5311.32: size   (128 days)            134.8M              100.4M -25.5%
     5311.33: client (128 days)   9.42(16.86+0.49)    25.34(25.80+0.46) +169.0%
    
    In all cases we save CPU time on the server (sometimes
    significant) and the resulting pack is smaller. We do spend
    more CPU time on the client side, because it has to
    reconstruct more deltas. But that's the right tradeoff to
    make, since clients tend to outnumber servers. It just means
    the thin pack mechanism is doing its job.
    
    From the user's perspective, the end-to-end time of the
    operation will generally be faster. E.g., in the 128-day
    case, we saved 15s on the server at a cost of 16s on the
    client. Since the resulting pack is 34MB smaller, this is a
    net win if the network speed is less than 270Mbit/s. And
    that's actually the worst case. The 64-day case saves just
    over 11s at a cost of just under 11s. So it's a slight win
    at any network speed, and the 40MB saved is pure bonus. That
    trend continues for the smaller fetches.
    
    The implementation itself is mostly straightforward, with
    the new logic going into check_object(). But there are two
    tricky bits.
    
    The first is that check_object() needs access to the
    relevant information (the thin flag and bitmap result). We
    can do this by pushing these into program-lifetime globals.
    
    The second is that the rest of the code assumes that any
    reused delta will point to another "struct object_entry" as
    its base. But of course the case we are interested in here
    is the one where don't have such an entry!
    
    I looked at a number of options that didn't quite work:
    
     - we could use a flag to signal a reused delta, but it's
       not a single bit. We have to actually store the oid of
       the base, which is normally done by pointing to the
       existing object_entry. And we'd have to modify all the
       code which looks at deltas.
    
     - we could add the reused bases to the end of the existing
       object_entry array. While this does create some extra
       work as later stages consider the extra entries, it's
       actually not too bad (we're not sending them, so they
       don't cost much in the delta search, and at most we'd
       have 2*N of them).
    
       But there's a more subtle problem. Adding to the existing
       array means we might need to grow it with realloc, which
       could move the earlier entries around. While many of the
       references to other entries are done by integer index,
       some (including ones on the stack) use pointers, which
       would become invalidated.
    
       This isn't insurmountable, but it would require quite a
       bit of refactoring (and it's hard to know that you've got
       it all, since it may work _most_ of the time and then
       fail subtly based on memory allocation patterns).
    
     - we could allocate a new one-off entry for the base. In
       fact, this is what an earlier version of this patch did.
       However, since the refactoring brought in by ad635e82d6
       (Merge branch 'nd/pack-objects-pack-struct', 2018-05-23),
       the delta_idx code requires that both entries be in the
       main packing list.
    
    So taking all of those options into account, what I ended up
    with is a separate list of "external bases" that are not
    part of the main packing list. Each delta entry that points
    to an external base has a single-bit flag to do so; we have a
    little breathing room in the bitfield section of
    object_entry.
    
    This lets us limit the change primarily to the oe_delta()
    and oe_set_delta_ext() functions. And as a bonus, most of
    the rest of the code does not consider these dummy entries
    at all, saving both runtime CPU and code complexity.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit fe0ac2fb7f8e87d37ef83dcee2d93901d58d8277
Author: Christian Couder <christian.couder@gmail.com>
Date:   Thu Aug 16 08:13:13 2018 +0200

    pack-objects: move 'layer' into 'struct packing_data'
    
    This reduces the size of 'struct object_entry' from 88 bytes
    to 80 and therefore makes packing objects more efficient.
    
    For example on a Linux repo with 12M objects,
    `git pack-objects --all` needs extra 96MB memory even if the
    layer feature is not used.
    
    Helped-by: Jeff King <peff@peff.net>
    Helped-by: Duy Nguyen <pclouds@gmail.com>
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 108f530385e969feab343b2b8acadeb7bb670009
Author: Christian Couder <christian.couder@gmail.com>
Date:   Thu Aug 16 08:13:12 2018 +0200

    pack-objects: move tree_depth into 'struct packing_data'
    
    This reduces the size of 'struct object_entry' and therefore
    makes packing objects more efficient.
    
    This also renames cmp_tree_depth() into tree_depth_compare(),
    as it is more modern to have the name of the compare functions
    end with "compare".
    
    Helped-by: Jeff King <peff@peff.net>
    Helped-by: Duy Nguyen <pclouds@gmail.com>
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 0750bb5b51f021ecad6f33b7ec88cdfc2a8cdff4
Author: Jeff King <peff@peff.net>
Date:   Fri Aug 10 19:24:57 2018 -0400

    cat-file: support "unordered" output for --batch-all-objects
    
    If you're going to access the contents of every object in a
    packfile, it's generally much more efficient to do so in
    pack order, rather than in hash order. That increases the
    locality of access within the packfile, which in turn is
    friendlier to the delta base cache, since the packfile puts
    related deltas next to each other. By contrast, hash order
    is effectively random, since the sha1 has no discernible
    relationship to the content.
    
    This patch introduces an "--unordered" option to cat-file
    which iterates over packs in pack-order under the hood. You
    can see the results when dumping all of the file content:
    
      $ time ./git cat-file --batch-all-objects --buffer --batch | wc -c
      6883195596
    
      real  0m44.491s
      user  0m42.902s
      sys   0m5.230s
    
      $ time ./git cat-file --unordered \
                            --batch-all-objects --buffer --batch | wc -c
      6883195596
    
      real  0m6.075s
      user  0m4.774s
      sys   0m3.548s
    
    Same output, different order, way faster. The same speed-up
    applies even if you end up accessing the object content in a
    different process, like:
    
      git cat-file --batch-all-objects --buffer --batch-check |
      grep blob |
      git cat-file --batch='%(objectname) %(rest)' |
      wc -c
    
    Adding "--unordered" to the first command drops the runtime
    in git.git from 24s to 3.5s.
    
      Side note: there are actually further speedups available
      for doing it all in-process now. Since we are outputting
      the object content during the actual pack iteration, we
      know where to find the object and could skip the extra
      lookup done by oid_object_info(). This patch stops short
      of that optimization since the underlying API isn't ready
      for us to make those sorts of direct requests.
    
    So if --unordered is so much better, why not make it the
    default? Two reasons:
    
      1. We've promised in the documentation that --batch-all-objects
         outputs in hash order. Since cat-file is plumbing,
         people may be relying on that default, and we can't
         change it.
    
      2. It's actually _slower_ for some cases. We have to
         compute the pack revindex to walk in pack order. And
         our de-duplication step uses an oidset, rather than a
         sort-and-dedup, which can end up being more expensive.
         If we're just accessing the type and size of each
         object, for example, like:
    
           git cat-file --batch-all-objects --buffer --batch-check
    
         my best-of-five warm cache timings go from 900ms to
         1100ms using --unordered. Though it's possible in a
         cold-cache or under memory pressure that we could do
         better, since we'd have better locality within the
         packfile.
    
    And one final question: why is it "--unordered" and not
    "--pack-order"? The answer is again two-fold:
    
      1. "pack order" isn't a well-defined thing across the
         whole set of objects. We're hitting loose objects, as
         well as objects in multiple packs, and the only
         ordering we're promising is _within_ a single pack. The
         rest is apparently random.
    
      2. The point here is optimization. So we don't want to
         promise any particular ordering, but only to say that
         we will choose an ordering which is likely to be
         efficient for accessing the object content. That leaves
         the door open for further changes in the future without
         having to add another compatibility option.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 736eb88fdc8a2dea4302114d2f74b580d0f83cfe
Author: Jeff King <peff@peff.net>
Date:   Fri Aug 10 19:15:49 2018 -0400

    for_each_packed_object: support iterating in pack-order
    
    We currently iterate over objects within a pack in .idx
    order, which uses the object hashes. That means that it
    is effectively random with respect to the location of the
    object within the pack. If you're going to access the actual
    object data, there are two reasons to move linearly through
    the pack itself:
    
      1. It improves the locality of access in the packfile. In
         the cold-cache case, this may mean fewer disk seeks, or
         better usage of disk cache.
    
      2. We store related deltas together in the packfile. Which
         means that the delta base cache can operate much more
         efficiently if we visit all of those related deltas in
         sequence, as the earlier items are likely to still be
         in the cache.  Whereas if we visit the objects in
         random order, our cache entries are much more likely to
         have been evicted by unrelated deltas in the meantime.
    
    So in general, if you're going to access the object contents
    pack order is generally going to end up more efficient.
    
    But if you're simply generating a list of object names, or
    if you're going to end up sorting the result anyway, you're
    better off just using the .idx order, as finding the pack
    order means generating the in-memory pack-revindex.
    According to the numbers in 8b8dfd5132 (pack-revindex:
    radix-sort the revindex, 2013-07-11), that takes about 200ms
    for linux.git, and 20ms for git.git (those numbers are a few
    years old but are still a good ballpark).
    
    That makes it a good optimization for some cases (we can
    save tens of seconds in git.git by having good locality of
    delta access, for a 20ms cost), but a bad one for others
    (e.g., right now "cat-file --batch-all-objects
    --batch-check="%(objectname)" is 170ms in git.git, so adding
    20ms to that is noticeable).
    
    Hence this patch makes it an optional flag. You can't
    actually do any interesting timings yet, as it's not plumbed
    through to any user-facing tools like cat-file. That will
    come in a later patch.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 73c3f0f704a91b6792e0199a3f3ab6e3a1971675
Author: Jeff King <peff@peff.net>
Date:   Fri May 4 19:45:01 2018 -0400

    index-pack: check .gitmodules files with --strict
    
    Now that the internal fsck code has all of the plumbing we
    need, we can start checking incoming .gitmodules files.
    Naively, it seems like we would just need to add a call to
    fsck_finish() after we've processed all of the objects. And
    that would be enough to cover the initial test included
    here. But there are two extra bits:
    
      1. We currently don't bother calling fsck_object() at all
         for blobs, since it has traditionally been a noop. We'd
         actually catch these blobs in fsck_finish() at the end,
         but it's more efficient to check them when we already
         have the object loaded in memory.
    
      2. The second pass done by fsck_finish() needs to access
         the objects, but we're actually indexing the pack in
         this process. In theory we could give the fsck code a
         special callback for accessing the in-pack data, but
         it's actually quite tricky:
    
           a. We don't have an internal efficient index mapping
              oids to packfile offsets. We only generate it on
              the fly as part of writing out the .idx file.
    
           b. We'd still have to reconstruct deltas, which means
              we'd basically have to replicate all of the
              reading logic in packfile.c.
    
         Instead, let's avoid running fsck_finish() until after
         we've written out the .idx file, and then just add it
         to our internal packed_git list.
    
         This does mean that the objects are "in the repository"
         before we finish our fsck checks. But unpack-objects
         already exhibits this same behavior, and it's an
         acceptable tradeoff here for the same reason: the
         quarantine mechanism means that pushes will be
         fully protected.
    
    In addition to a basic push test in t7415, we add a sneaky
    pack that reverses the usual object order in the pack,
    requiring that index-pack access the tree and blob during
    the "finish" step.
    
    This already works for unpack-objects (since it will have
    written out loose objects), but we'll check it with this
    sneaky pack for good measure.
    
    Signed-off-by: Jeff King <peff@peff.net>

commit 1dfb929a37bcb1456832753ec073a3fb5a27ffd0
Merge: 90186fa057 94408dc71c
Author: Junio C Hamano <gitster@pobox.com>
Date:   Tue May 8 15:59:25 2018 +0900

    Merge branch 'sg/completion-clear-cached'
    
    The completion script (in contrib/) learned to clear cached list of
    command line options upon dot-sourcing it again in a more efficient
    way.
    
    * sg/completion-clear-cached:
      completion: reduce overhead of clearing cached --options

commit 55dc227d16cb0ca79ca25e89433d85bd5f806c22
Author: brian m. carlson <sandals@crustytoothpaste.net>
Date:   Wed May 2 00:25:51 2018 +0000

    upload-pack: replace use of several hard-coded constants
    
    Update several uses of hard-coded 40-based constants to use either
    the_hash_algo or GIT_MAX_HEXSZ, as appropriate.  Replace a combined use
    of oid_to_hex and memcpy with oid_to_hex_r, which not only avoids the
    need for a constant, but is more efficient.  Make use of parse_oid_hex
    to eliminate the need for constants and simplify the code at the same
    time.  Update some comments to no longer refer to SHA-1 as well.
    
    Signed-off-by: brian m. carlson <sandals@crustytoothpaste.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7b0034206843c38dc96e700a527e06b88ac35d69
Author: SZEDER Gábor <szeder.dev@gmail.com>
Date:   Tue Apr 17 00:42:36 2018 +0200

    completion: fill COMPREPLY directly when completing paths
    
    During git-aware path completion, when a lot of path components have
    to be listed, a significant amount of time is spent in
    __gitcomp_file(), or more accurately in the shell loop of
    __gitcompappend(), iterating over all the path components filtering
    path components matching the current word to be completed, adding
    prefix path components, and placing the resulting matching paths into
    the COMPREPLY array.
    
    Now, a previous patch in this series made 'git ls-files' and 'git
    diff-index' list only paths matching the current word to be completed,
    so an additional filtering in __gitcomp_file() is not necessary
    anymore.  Adding the prefix path components could be done much more
    efficiently in __git_index_files()'s 'awk' script while stripping
    trailing path components and removing duplicates and quoting.  And
    then the resulting paths won't require any more filtering or
    processing before being handed over to Bash, so we could fill the
    COMPREPLY array directly.
    
    Unfortunately, we can't simply use the __gitcomp_direct() helper
    function to do that, because __gitcomp_file() does one additional
    thing: it tells Bash that we are doing filename completion, so the
    shell will kindly do four important things for us:
    
      1. Append a trailing space to all filenames.
      2. Append a trailing '/' to all directory names.
      3. Escape any meta, globbing, separator, etc. characters.
      4. List only the current path component when listing possible
         completions (i.e. 'dir/subdir/f<TAB>' will list 'file1', 'file2',
         etc. instead of the whole 'dir/subdir/file1',
         'dir/subdir/file2').
    
    While we could let __git_index_files()'s 'awk' script take care of the
    first two points, the third one gets tricky, and we absolutely need
    the shell's support for the fourth.
    
    Add the helper function __gitcomp_file_direct(), which, just like
    __gitcomp_direct(), fills the COMPREPLY array with prefiltered and
    preprocessed paths without any additional processing, without a shell
    loop, with just one single compound assignment, and, similar to
    __gitcomp_file(), tells Bash and ZSH that we are doing filename
    completion.  Extend __git_index_files()'s 'awk' script a bit to
    prepend any prefix path components to all listed paths.  Finally,
    modify __git_complete_index_file() to feed __git_index_files()'s
    output to ___gitcomp_file_direct() instead of __gitcomp_file().
    
    After this patch there is no shell loop left in the path completion
    code path.
    
    This speeds up path completion when there are a lot of paths matching
    the current word to be completed.  In a pathological repository with
    100k files in a single directory, listing all those files:
    
      Before this patch, best of five, using GNU awk on Linux:
    
        $ time cur=dir/ __git_complete_index_file
    
        real    0m0.983s
        user    0m1.004s
        sys     0m0.033s
    
      After:
    
        real    0m0.313s
        user    0m0.341s
        sys     0m0.029s
    
      Difference: -68.2%
      Speedup:      3.1x
    
      To see the benefits of the whole patch series, the same command with
      v2.17.0:
    
        real    0m2.736s
        user    0m2.472s
        sys     0m0.610s
    
      Difference: -88.6%
      Speedup:      8.7x
    
    Note that this patch changes the output of the __git_index_files()
    helper function by unconditionally prepending the prefix path
    components to every listed path.  This would break users' completion
    scriptlets that directly run:
    
      __gitcomp_file "$(__git_index_files ...)" "$pfx" "$cur_"
    
    because that would add the prefix path components once more.
    However, __git_index_files() is kind of a "helper function of a helper
    function", and users' completion scriptlets should have been using
    __git_complete_index_file() for git-aware path completion in the first
    place, so this is likely doesn't worth worrying about.
    
    Signed-off-by: SZEDER Gábor <szeder.dev@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 4669e7d68ec8fb6ff3572a1193ae75ae2094b8e6
Author: Jonathan Tan <jonathantanmy@google.com>
Date:   Tue Feb 13 10:39:38 2018 -0800

    packfile: remove GIT_DEBUG_LOOKUP log statements
    
    In commit 628522ec1439 ("sha1-lookup: more memory efficient search in
    sorted list of SHA-1", 2008-04-09), a different algorithm for searching
    a sorted list was introduced, together with a set of log statements
    guarded by GIT_DEBUG_LOOKUP that are invoked both when using that
    algorithm and when using the existing binary search. Those log
    statements was meant for experiments and debugging, but with the removal
    of the aforementioned different algorithm in commit f1068efefe6d
    ("sha1_file: drop experimental GIT_USE_LOOKUP search", 2017-08-09),
    those log statements are probably no longer necessary.
    
    Remove those statements.
    
    Signed-off-by: Jonathan Tan <jonathantanmy@google.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a8e7a2bf0fe843836f4ec286c6cdba24bf40fac8
Author: Jeff King <peff@peff.net>
Date:   Mon Feb 12 12:23:06 2018 -0500

    describe: confirm that blobs actually exist
    
    Prior to 644eb60bd0 (builtin/describe.c: describe a blob,
    2017-11-15), we noticed and complained about missing
    objects, since they were not valid commits:
    
      $ git describe 0000000000000000000000000000000000000000
      fatal: 0000000000000000000000000000000000000000 is not a valid 'commit' object
    
    After that commit, we feed any non-commit to lookup_blob(),
    and complain only if it returns NULL. But the lookup_*
    functions do not actually look at the on-disk object
    database at all. They return an entry from the in-memory
    object hash if present (and if it matches the requested
    type), and otherwise auto-create a "struct object" of the
    requested type.
    
    A missing object would hit that latter case: we create a
    bogus blob struct, walk all of history looking for it, and
    then exit successfully having produced no output.
    
    One reason nobody may have noticed this is that some related
    cases do still work OK:
    
      1. If we ask for a tree by sha1, then the call to
         lookup_commit_referecne_gently() would have parsed it,
         and we would have its true type in the in-memory object
         hash.
    
      2. If we ask for a name that doesn't exist but isn't a
         40-hex sha1, then get_oid() would complain before we
         even look at the objects at all.
    
    We can fix this by replacing the lookup_blob() call with a
    check of the true type via sha1_object_info(). This is not
    quite as efficient as we could possibly make this check. We
    know in most cases that the object was already parsed in the
    earlier commit lookup, so we could call lookup_object(),
    which does auto-create, and check the resulting struct's
    type (or NULL).  However it's not worth the fragility nor
    code complexity to save a single object lookup.
    
    The new tests cover this case, as well as that of a
    tree-by-sha1 (which does work as described above, but was
    not explicitly tested).
    
    Noticed-by: Michael Haggerty <mhagger@alum.mit.edu>
    Signed-off-by: Jeff King <peff@peff.net>
    Acked-by: Stefan Beller <sbeller@google.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 02adf84ab8996cb80d7468f1d3c13be19f929294
Author: Jeff King <peff@peff.net>
Date:   Wed Jan 24 19:55:05 2018 -0500

    t5570: use ls-remote instead of clone for interp tests
    
    We don't actually care about the clone operation here; we
    just want to know if we were able to actually contact the
    remote repository. Using ls-remote does that more
    efficiently, and without us having to worry about managing
    the tmp.git directory.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit fbac558a9bde48fa23b9c536357ab17c71b4bd22
Author: René Scharfe <l.s.r@web.de>
Date:   Mon Jan 15 18:10:32 2018 +0100

    describe: use strbuf_add_unique_abbrev() for adding short hashes
    
    Call strbuf_add_unique_abbrev() to add an abbreviated hash to a strbuf
    instead of taking a detour through find_unique_abbrev() and its static
    buffer.  This is shorter and a bit more efficient.
    
    Patch generated by Coccinelle (and contrib/coccinelle/strbuf.cocci).
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 8b4c0103a98239287176a537f7a04d9b07b49125
Author: Jonathan Tan <jonathantanmy@google.com>
Date:   Fri Dec 8 15:27:14 2017 +0000

    sha1_file: support lazily fetching missing objects
    
    Teach sha1_file to fetch objects from the remote configured in
    extensions.partialclone whenever an object is requested but missing.
    
    The fetching of objects can be suppressed through a global variable.
    This is used by fsck and index-pack.
    
    However, by default, such fetching is not suppressed. This is meant as a
    temporary measure to ensure that all Git commands work in such a
    situation. Future patches will update some commands to either tolerate
    missing objects (without fetching them) or be more efficient in fetching
    them.
    
    In order to determine the code changes in sha1_file.c necessary, I
    investigated the following:
     (1) functions in sha1_file.c that take in a hash, without the user
         regarding how the object is stored (loose or packed)
     (2) functions in packfile.c (because I need to check callers that know
         about the loose/packed distinction and operate on both differently,
         and ensure that they can handle the concept of objects that are
         neither loose nor packed)
    
    (1) is handled by the modification to sha1_object_info_extended().
    
    For (2), I looked at for_each_packed_object and others.  For
    for_each_packed_object, the callers either already work or are fixed in
    this patch:
     - reachable - only to find recent objects
     - builtin/fsck - already knows about missing objects
     - builtin/cat-file - warning message added in this commit
    
    Callers of the other functions do not need to be changed:
     - parse_pack_index
       - http - indirectly from http_get_info_packs
       - find_pack_entry_one
         - this searches a single pack that is provided as an argument; the
           caller already knows (through other means) that the sought object
           is in a specific pack
     - find_sha1_pack
       - fast-import - appears to be an optimization to not store a file if
         it is already in a pack
       - http-walker - to search through a struct alt_base
       - http-push - to search through remote packs
     - has_sha1_pack
       - builtin/fsck - already knows about promisor objects
       - builtin/count-objects - informational purposes only (check if loose
         object is also packed)
       - builtin/prune-packed - check if object to be pruned is packed (if
         not, don't prune it)
       - revision - used to exclude packed objects if requested by user
       - diff - just for optimization
    
    Signed-off-by: Jonathan Tan <jonathantanmy@google.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 87b5e236a14d4783b063041588c2f77f3cc6ee89
Author: Jeff King <peff@peff.net>
Date:   Tue Nov 21 18:17:39 2017 -0500

    sha1_file: fast-path null sha1 as a missing object
    
    In theory nobody should ever ask the low-level object code
    for a null sha1. It's used as a sentinel for "no such
    object" in lots of places, so leaking through to this level
    is a sign that the higher-level code is not being careful
    about its error-checking.  In practice, though, quite a few
    code paths seem to rely on the null sha1 lookup failing as a
    way to quietly propagate non-existence (e.g., by feeding it
    to lookup_commit_reference_gently(), which then returns
    NULL).
    
    When this happens, we do two inefficient things:
    
      1. We actually search for the null sha1 in packs and in
         the loose object directory.
    
      2. When we fail to find it, we re-scan the pack directory
         in case a simultaneous repack happened to move it from
         loose to packed. This can be very expensive if you have
         a large number of packs.
    
    Only the second one actually causes noticeable performance
    problems, so we could treat them independently. But for the
    sake of simplicity (both of code and of reasoning about it),
    it makes sense to just declare that the null sha1 cannot be
    a real on-disk object, and looking it up will always return
    "no such object".
    
    There's no real loss of functionality to do so Its use as a
    sentinel value means that anybody who is unlucky enough to
    hit the 2^-160th chance of generating an object with that
    sha1 is already going to find the object largely unusable.
    
    In an ideal world, we'd simply fix all of the callers to
    notice the null sha1 and avoid passing it to us. But a
    simple experiment to catch this with a BUG() shows that
    there are a large number of code paths that do so.
    
    So in the meantime, let's fix the performance problem by
    taking a fast exit from the object lookup when we see a null
    sha1. p5551 shows off the improvement (when a fetched ref is
    new, the "old" sha1 is 0{40}, which ends up being passed for
    fast-forward checks, the status table abbreviations, etc):
    
      Test            HEAD^             HEAD
      --------------------------------------------------------
      5551.4: fetch   5.51(5.03+0.48)   0.17(0.10+0.06) -96.9%
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit c602d3a9897a408ce0db543860d472332f79d045
Author: Jeff King <peff@peff.net>
Date:   Mon Oct 9 13:48:52 2017 -0400

    write_entry: avoid reading blobs in CE_RETRY case
    
    When retrying a delayed filter-process request, we don't
    need to send the blob to the filter a second time. However,
    we read it unconditionally into a buffer, only to later
    throw away that buffer. We can make this more efficient by
    skipping the read in the first place when it isn't
    necessary.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 38bdf62b738bb93f7e1a6af8058dc31f27c91d4e
Author: René Scharfe <l.s.r@web.de>
Date:   Sun Oct 1 16:45:45 2017 +0200

    graph: use strbuf_addchars() to add spaces
    
    strbuf_addf() can be used to add a specific number of space characters
    by using the format "%*s" with an empty string and specifying the
    desired width.  Use strbuf_addchars() instead as it's shorter, makes the
    intent clearer and is a bit more efficient.
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit d1cf15516feb024f34ae5fbbdad8f538b4e62fce
Author: Michael Haggerty <mhagger@alum.mit.edu>
Date:   Mon Sep 25 10:00:12 2017 +0200

    packed_ref_iterator_begin(): iterate using `mmapped_ref_iterator`
    
    Now that we have an efficient way to iterate, in order, over the
    mmapped contents of the `packed-refs` file, we can use that directly
    to implement reference iteration for the `packed_ref_store`, rather
    than iterating over the `ref_cache`. This is the next step towards
    getting rid of the `ref_cache` entirely.
    
    Signed-off-by: Michael Haggerty <mhagger@alum.mit.edu>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit acd9544a8fdcf8095c82c91365c45dcb93112be3
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Wed Aug 23 19:37:01 2017 +0700

    revision.c: --reflog add HEAD reflog from all worktrees
    
    Note that add_other_reflogs_to_pending() is a bit inefficient, since
    it scans reflog for all refs of each worktree, including shared refs,
    so the shared ref's reflog is scanned over and over again.
    
    We could update refs API to pass "per-worktree only" flag to avoid
    that. But long term we should be able to obtain a "per-worktree only"
    ref store and would need to revert the changes in reflog iteration
    API. So let's just wait until then.
    
    add_reflogs_to_pending() is called by reachable.c so by default "git
    prune" will examine reflog from all worktrees.
    
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 58311c66fd316dff8f2c68a634ca0cf968227870
Author: Jeff King <peff@peff.net>
Date:   Tue Aug 15 06:25:27 2017 -0400

    pretty: support normalization options for %(trailers)
    
    The interpret-trailers command recently learned some options
    to make its output easier to parse (for a caller whose only
    interested in picking out the trailer values). But it's not
    very efficient for asking for the trailers of many commits
    in a single invocation.
    
    We already have "%(trailers)" to do that, but it doesn't
    know about unfolding or omitting non-trailers. Let's plumb
    those options through, so you can have the best of both.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f1068efefe6dd3beaa89484db5e2db730b094e0b
Author: Jeff King <peff@peff.net>
Date:   Wed Aug 9 06:14:32 2017 -0400

    sha1_file: drop experimental GIT_USE_LOOKUP search
    
    Long ago in 628522ec14 (sha1-lookup: more memory efficient
    search in sorted list of SHA-1, 2007-12-29) we added
    sha1_entry_pos(), a binary search that uses the uniform
    distribution of sha1s to scale the selection of mid-points.
    As this was a performance experiment, we tied it to the
    GIT_USE_LOOKUP environment variable and never enabled it by
    default.
    
    This code was successful in reducing the number of steps in
    each search. But the overhead of the scaling ends up making
    it slower when the cache is warm. Here are best-of-five
    timings for running rev-list on linux.git, which will have
    to look up every object:
    
      $ time git rev-list --objects --all >/dev/null
      real  0m35.357s
      user  0m35.016s
      sys   0m0.340s
    
      $ time GIT_USE_LOOKUP=1 git rev-list --objects --all >/dev/null
      real  0m37.364s
      user  0m37.045s
      sys   0m0.316s
    
    The USE_LOOKUP version might have more benefit on a cold
    cache, as the time to fault in each page would dominate. But
    that would be for a single lookup. In practice, most
    operations tend to look up many objects, and the whole pack
    .idx will end up warm.
    
    It's possible that the code could be better optimized to
    compete with a naive binary search for the warm-cache case,
    and we could have the best of both worlds. But over the
    years nobody has done so, and this is largely dead code that
    is rarely run outside of the test suite. Let's drop it in
    the name of simplicity.
    
    This lets us remove sha1_entry_pos() entirely, as the .idx
    lookup code was the only caller.  Note that sha1-lookup.c
    still contains sha1_pos(), which differs from
    sha1_entry_pos() in two ways:
    
      - it has a different interface; it uses a function pointer
        to access sha1 entries rather than a size/offset pair
        describing the table's memory layout
    
      - it only scales the initial selection of "mi", rather
        than each iteration of the search
    
    We can't get rid of this function, as it's called from
    several places. It may be that we could replace it with a
    simple binary search, but that's out of scope for this patch
    (and would need benchmarking).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f23092f19e73f5d8c3480ef02104af627a90361f
Author: Michael Haggerty <mhagger@alum.mit.edu>
Date:   Mon May 22 16:17:55 2017 +0200

    cache_ref_iterator_begin(): avoid priming unneeded directories
    
    When iterating over references, reference priming is used to make sure
    that loose references are read into the ref-cache before packed
    references, to avoid races. It used to be that the prefix passed to
    reference iterators almost always ended in `/`, for example
    `refs/heads/`. In that case, the priming code would read all loose
    references under `find_containing_dir("refs/heads/")`, which is
    "refs/heads/". That's just what we want.
    
    But now that `ref-filter` knows how to pass refname prefixes to
    `for_each_fullref_in()`, the prefix might come from user input; for
    example,
    
        git for-each-ref refs/heads
    
    Since the argument doesn't include a trailing slash, the reference
    iteration code would prime all of the loose references under
    `find_containing_dir("refs/heads")`, which is "refs/". Thus we would
    unnecessarily read tags, remote-tracking references, etc., when the
    user is only interested in branches.
    
    It is a bit awkward to get around this problem. We can't just append a
    slash to the argument, because we don't know ab initio whether an
    argument like `refs/tags/release` corresponds to a single tag or to a
    directory containing tags.
    
    Moreover, until now a `prefix_ref_iterator` was used to make the final
    decision about which references fall within the prefix (the
    `cache_ref_iterator` only did a rough cut). This is also inefficient,
    because the `prefix_ref_iterator` can't know, for example, that while
    you are in a subdirectory that is completely within the prefix, you
    don't have to do the prefix check.
    
    So:
    
    * Move the responsibility for doing the prefix check directly to
      `cache_ref_iterator`. This means that `cache_ref_iterator_begin()`
      never has to wrap its return value in a `prefix_ref_iterator`.
    
    * Teach `cache_ref_iterator_begin()` (and `prime_ref_dir()`) to be
      stricter about what they iterate over and what directories they
      prime.
    
    * Teach `cache_ref_iterator` to keep track of whether the current
      `cache_ref_iterator_level` is fully within the prefix. If so, skip
      the prefix checks entirely.
    
    The main benefit of these optimizations is for loose references, since
    packed references are always read all at once.
    
    Note that after this change, `prefix_ref_iterator` is only ever used
    for its trimming feature and not for its "prefix" feature. But I'm not
    ripping out the latter yet, because it might be useful for another
    patch series that I'm working on.
    
    Signed-off-by: Michael Haggerty <mhagger@alum.mit.edu>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1f9e18b77282070e8fef6dbe6983a8c94a3b0efa
Author: Jeff King <peff@peff.net>
Date:   Mon Apr 24 07:49:20 2017 -0400

    prio_queue_reverse: don't swap elements with themselves
    
    Our array-reverse algorithm does the usual "walk from both
    ends, swapping elements". We can quit when the two indices
    are equal, since:
    
      1. Swapping an element with itself is a noop.
    
      2. If i and j are equal, then in the next iteration i is
         guaranteed to be bigge than j, and we will exit the
         loop.
    
    So exiting the loop on equality is slightly more efficient.
    And more importantly, the new SWAP() macro does not expect
    to handle noop swaps; it will call memcpy() with the same src
    and dst pointers in this case. It's unclear whether that
    causes a problem on any platforms by violating the
    "overlapping memory" constraint of memcpy, but it does cause
    valgrind to complain.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 8b6bba66633cf16807ae962e0b1126af164f0e78
Merge: a2e2c04683 950a234cbd
Author: Junio C Hamano <gitster@pobox.com>
Date:   Sun Apr 23 22:07:47 2017 -0700

    Merge branch 'jh/string-list-micro-optim'
    
    The string-list API used a custom reallocation strategy that was
    very inefficient, instead of using the usual ALLOC_GROW() macro,
    which has been fixed.
    
    * jh/string-list-micro-optim:
      string-list: use ALLOC_GROW macro when reallocing string_list

commit 16d2676c9ee996208277772fdf81dda212355440
Author: Jeff King <peff@peff.net>
Date:   Thu Apr 20 17:09:35 2017 -0400

    am: drop "dir" parameter from am_state_init
    
    The only caller of this function passes in a static buffer
    returned from git_path(). This looks dangerous at first
    glance, but turns out to be OK because the first thing we do
    is xstrdup() the result.
    
    Let's turn this into a git_pathdup(). That's slightly more
    efficient (no extra copy), and makes it easier to audit for
    dangerous git_path() invocations.
    
    Since there's only a single caller, let's just set this
    default path inside the init function. That makes the memory
    ownership clear.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit d9c69644b27eb59fe16c1931580e6fce4abbdc65
Author: Jeff King <peff@peff.net>
Date:   Thu Apr 20 17:09:09 2017 -0400

    replace xstrdup(git_path(...)) with git_pathdup(...)
    
    It's more efficient to use git_pathdup(), as it skips an
    extra copy of the path. And by removing some calls to
    git_path(), it makes it easier to audit for dangerous uses.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 446d5d911214fd3d61921478c98d4a88f84e410c
Author: Jeff King <peff@peff.net>
Date:   Tue Mar 28 15:46:47 2017 -0400

    receive-pack: print --pack-header directly into argv array
    
    After receive-pack reads the pack header from the client, it
    feeds the already-read part to index-pack and unpack-objects
    via their --pack-header command-line options.  To do so, we
    format it into a fixed buffer, then duplicate it into the
    child's argv_array.
    
    Our buffer is long enough to handle any possible input, so
    this isn't wrong. But it's more complicated than it needs to
    be; we can just argv_array_pushf() the final value and avoid
    the intermediate copy. This drops the magic number and is
    more efficient, too.
    
    Note that we need to push to the argv_array in order, which
    means we can't do the push until we are in the "unpack-objects
    versus index-pack" conditional.  Rather than duplicate the
    slightly complicated format specifier, I pushed it into a
    helper function.
    
    Signed-off-by: Jeff King <peff@peff.net>

commit fef56eb0063dcf61796bf812ba980c99ce85f6b0
Author: SZEDER Gábor <szeder.dev@gmail.com>
Date:   Thu Mar 23 16:29:22 2017 +0100

    completion: fill COMPREPLY directly when completing refs
    
    __gitcomp_nl() iterates over all the possible completion words it gets
    as argument
    
      - filtering matching words,
      - appending a trailing space to each matching word (in all but two
        cases),
      - prepending a prefix to each matching word (when completing words
        after e.g. '--option=<TAB>' or 'master..<TAB>'), and
      - adding each matching word to the COMPREPLY array.
    
    This takes a while when a lot of refs are passed to __gitcomp_nl().
    
    The previous changes in this series ensure that __git_refs() lists
    only refs matching the current word to be completed, making a second
    filtering in __gitcomp_nl() redundant.
    
    Adding the necessary prefix and suffix could be done in __git_refs()
    as well:
    
      - When refs come from 'git for-each-ref', then that prefix and
        suffix could be added much more efficiently using a 'git
        for-each-ref' format containing said prefix and suffix.  Care
        should be taken, though, because that prefix might contain
        'for-each-ref' format specifiers as part of the left hand side of
        a '..' range or '...' symmetric difference notation or
        fetch/push/etc. refspec, e.g. 'git log "evil-%(refname)..br<TAB>'.
        Doubling every '%' in the prefix will prevent 'git for-each-ref'
        from interpolating any of those contained specifiers.
      - When refs come from 'git ls-remote', then that prefix and suffix
        can be added in the shell loop that has to process 'git
        ls-remote's output anyway.
      - Finally, the prefix and suffix can be added to that handful of
        potentially matching symbolic and pseudo refs right away in the
        shell loop listing them.
    
    And then all what is still left to do is to assign a bunch of
    newline-separated words to a shell array, which can be done without a
    shell loop iterating over each word, basically making all of
    __gitcomp_nl() unnecessary for refs completion.
    
    Add the helper function __gitcomp_direct() to fill the COMPREPLY array
    with prefiltered and preprocessed words without any additional
    processing, without a shell loop, with just one single compound
    assignment.  Modify __git_refs() to accept prefix and suffix
    parameters and add them to each and every listed ref as described
    above.  Modify __git_complete_refs() to pass the prefix and suffix
    parameters to __git_refs() and to feed __git_refs()'s output to
    __gitcomp_direct() instead of __gitcomp_nl().
    
    This speeds up refs completion when there are a lot of refs matching
    the current word to be completed.  Listing all branches for completion
    in a repo with 100k local branches, all packed, best of five:
    
      On Linux, near the beginning of this series, for reference:
    
        $ time __git_complete_refs
    
        real    0m2.028s
        user    0m1.692s
        sys     0m0.344s
    
      Before this patch:
    
        real    0m1.135s
        user    0m1.112s
        sys     0m0.024s
    
      After:
    
        real    0m0.367s
        user    0m0.352s
        sys     0m0.020s
    
      On Windows, near the beginning:
    
        real    0m13.078s
        user    0m1.609s
        sys     0m0.060s
    
      Before this patch:
    
        real    0m2.093s
        user    0m1.641s
        sys     0m0.060s
    
      After:
    
        real    0m0.683s
        user    0m0.203s
        sys     0m0.076s
    
    Signed-off-by: SZEDER Gábor <szeder.dev@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 824388d54b238b0d0cdc3f584dd27cb334df4ebc
Author: SZEDER Gábor <szeder.dev@gmail.com>
Date:   Thu Mar 23 16:29:20 2017 +0100

    completion: let 'for-each-ref' filter remote branches for 'checkout' DWIMery
    
    The code listing unique remote branches for 'git checkout's tracking
    DWIMery outputs only remote branches that match the current word to be
    completed, but the filtering is done in a shell loop iterating over
    all remote refs.
    
    Let 'git for-each-ref' do the filtering, as it can do so much more
    efficiently and we can remove that shell loop entirely.
    
    This speeds up refs completion for 'git checkout' considerably when
    there are a lot of non-matching remote refs to be filtered out.
    Uniquely completing a branch in a repository with 100k remote
    branches, all packed, best of five:
    
      On Linux, before:
    
        $ time __git_complete_refs --cur=maste --track
    
        real    0m1.993s
        user    0m1.740s
        sys     0m0.304s
    
      After:
    
        real    0m0.266s
        user    0m0.248s
        sys     0m0.012s
    
      On Windows, before:
    
        real    0m6.187s
        user    0m3.358s
        sys     0m2.121s
    
      After:
    
        real    0m0.750s
        user    0m0.015s
        sys     0m0.090s
    
    Signed-off-by: SZEDER Gábor <szeder.dev@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 96f6d3f61ad02ef2fd0393765207233845a7c7e0
Author: René Scharfe <l.s.r@web.de>
Date:   Fri Feb 10 21:03:30 2017 +0100

    ls-files: move only kept cache entries in prune_cache()
    
    prune_cache() first identifies those entries at the start of the sorted
    array that can be discarded.  Then it moves the rest of the entries up.
    Last it identifies the unwanted trailing entries among the moved ones
    and cuts them off.
    
    Change the order: Identify both start *and* end of the range to keep
    first and then move only those entries to the top.  The resulting code
    is slightly shorter and a bit more efficient.
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Reviewed-by: Brandon Williams <bmwill@google.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 9da9965ba6672dc0016a5ac694271bbdd4589e15
Merge: 0f47d3d78e 250ab24ab3
Author: Junio C Hamano <gitster@pobox.com>
Date:   Tue Jan 17 14:49:26 2017 -0800

    Merge branch 'hv/submodule-not-yet-pushed-fix' into maint
    
    The code in "git push" to compute if any commit being pushed in the
    superproject binds a commit in a submodule that hasn't been pushed
    out was overly inefficient, making it unusable even for a small
    project that does not have any submodule but have a reasonable
    number of refs.
    
    * hv/submodule-not-yet-pushed-fix:
      submodule_needs_pushing(): explain the behaviour when we cannot answer
      batch check whether submodule needs pushing into one call
      serialize collection of refs that contain submodule changes
      serialize collection of changed submodules

commit af952dac7a6997e587ded97e4051a927e5384423
Merge: a616162909 250ab24ab3
Author: Junio C Hamano <gitster@pobox.com>
Date:   Fri Dec 16 15:27:47 2016 -0800

    Merge branch 'hv/submodule-not-yet-pushed-fix'
    
    The code in "git push" to compute if any commit being pushed in the
    superproject binds a commit in a submodule that hasn't been pushed
    out was overly inefficient, making it unusable even for a small
    project that does not have any submodule but have a reasonable
    number of refs.
    
    * hv/submodule-not-yet-pushed-fix:
      submodule_needs_pushing(): explain the behaviour when we cannot answer
      batch check whether submodule needs pushing into one call
      serialize collection of refs that contain submodule changes
      serialize collection of changed submodules

commit 722ff7f876c8a2ad99c42434f58af098e61b96e8
Author: Jeff King <peff@peff.net>
Date:   Mon Oct 3 16:49:14 2016 -0400

    receive-pack: quarantine objects until pre-receive accepts
    
    When a client pushes objects to us, index-pack checks the
    objects themselves and then installs them into place. If we
    then reject the push due to a pre-receive hook, we cannot
    just delete the packfile; other processes may be depending
    on it. We have to do a normal reachability check at this
    point via `git gc`.
    
    But such objects may hang around for weeks due to the
    gc.pruneExpire grace period. And worse, during that time
    they may be exploded from the pack into inefficient loose
    objects.
    
    Instead, this patch teaches receive-pack to put the new
    objects into a "quarantine" temporary directory. We make
    these objects available to the connectivity check and to the
    pre-receive hook, and then install them into place only if
    it is successful (and otherwise remove them as tempfiles).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 670c359da357639f9f9a814ed646b4d854ec5d55
Author: Jeff King <peff@peff.net>
Date:   Mon Oct 3 16:34:17 2016 -0400

    link_alt_odb_entry: handle normalize_path errors
    
    When we add a new alternate to the list, we try to normalize
    out any redundant "..", etc. However, we do not look at the
    return value of normalize_path_copy(), and will happily
    continue with a path that could not be normalized. Worse,
    the normalizing process is done in-place, so we are left
    with whatever half-finished working state the normalizing
    function was in.
    
    Fortunately, this cannot cause us to read past the end of
    our buffer, as that working state will always leave the
    NUL from the original path in place. And we do tend to
    notice problems when we check is_directory() on the path.
    But you can see the nonsense that we feed to is_directory
    with an entry like:
    
      this/../../is/../../way/../../too/../../deep/../../to/../../resolve
    
    in your objects/info/alternates, which yields:
    
      error: object directory
      /to/e/deep/too/way//ects/this/../../is/../../way/../../too/../../deep/../../to/../../resolve
      does not exist; check .git/objects/info/alternates.
    
    We can easily fix this just by checking the return value.
    But that makes it hard to generate a good error message,
    since we're normalizing in-place and our input value has
    been overwritten by cruft.
    
    Instead, let's provide a strbuf helper that does an in-place
    normalize, but restores the original contents on error. This
    uses a second buffer under the hood, which is slightly less
    efficient, but this is not a performance-critical code path.
    
    The strbuf helper can also properly set the "len" parameter
    of the strbuf before returning. Just doing:
    
      normalize_path_copy(buf.buf, buf.buf);
    
    will shorten the string, but leave buf.len at the original
    length. That may be confusing to later code which uses the
    strbuf.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 597f9134ded20f882e2bf6bca8b0e1f03981b98d
Author: Jeff King <peff@peff.net>
Date:   Mon Oct 3 16:35:51 2016 -0400

    alternates: use a separate scratch space
    
    The alternate_object_database struct uses a single buffer
    both for storing the path to the alternate, and as a scratch
    buffer for forming object names. This is efficient (since
    otherwise we'd end up storing the path twice), but it makes
    life hard for callers who just want to know the path to the
    alternate. They have to remember to stop reading after
    "alt->name - alt->base" bytes, and to subtract one for the
    trailing '/'.
    
    It would be much simpler if they could simply access a
    NUL-terminated path string. We could encapsulate this in a
    function which puts a NUL in the scratch buffer and returns
    the string, but that opens up questions about the lifetime
    of the result. The first time another caller uses the
    alternate, the scratch buffer may get other data tacked onto
    it.
    
    Let's instead just store the root path separately from the
    scratch buffer. There aren't enough alternates being stored
    for the duplicated data to matter for performance, and this
    keeps things simple and safe for the callers.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a94bb683970a111b467a36590ca36e52754ad504
Author: René Scharfe <l.s.r@web.de>
Date:   Sat Oct 8 17:38:47 2016 +0200

    use strbuf_add_unique_abbrev() for adding short hashes, part 3
    
    Call strbuf_add_unique_abbrev() to add abbreviated hashes to strbufs
    instead of taking detours through find_unique_abbrev() and its static
    buffer.  This is shorter in most cases and a bit more efficient.
    
    The changes here are not easily handled by a semantic patch because
    they involve removing temporary variables and deconstructing format
    strings for strbuf_addf().
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f937d78553ce22505543580ae7958d9f5ffeeb89
Author: René Scharfe <l.s.r@web.de>
Date:   Tue Sep 27 21:11:58 2016 +0200

    use strbuf_add_unique_abbrev() for adding short hashes, part 2
    
    Call strbuf_add_unique_abbrev() to add abbreviated hashes to strbufs
    instead of taking detours through find_unique_abbrev() and its static
    buffer.  This is shorter and a bit more efficient.
    
    1eb47f167d65d1d305b9c196a1bb40eb96117cb1 already converted six cases,
    this patch covers three more.
    
    A semantic patch for Coccinelle is included for easier checking for
    new cases that might be introduced in the future.
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 4d0efa101bbbc63d8bd1ec0477f027f23b9f573b
Author: Jeff King <peff@peff.net>
Date:   Mon Sep 12 20:23:12 2016 -0700

    t1007: factor out repeated setup
    
    We have a series of 3 CRLF tests that do exactly the same
    (long) setup sequence. Let's pull it out into a common setup
    test, which is shorter, more efficient, and will make it
    easier to add new tests.
    
    Note that we don't have to worry about cleaning up any of
    the setup which was previously per-test; we call pop_repo
    after the CRLF tests, which cleans up everything.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 475b362c2a326ddc9987ee0ff6448e795f009d5d
Author: Jeff King <peff@peff.net>
Date:   Mon Sep 12 20:23:27 2016 -0700

    diff: skip implicit no-index check when given --no-index
    
    We can invoke no-index mode in two ways: by an explicit
    request from the user, or implicitly by noticing that we
    have two paths, and at least one is outside the repository.
    
    If the user already told us --no-index, there is no need for
    us to do the implicit test at all.  However, we currently
    do, and downgrade our "explicit" to DIFF_NO_INDEX_IMPLICIT.
    
    This doesn't have any user-visible behavior, though it's not
    immediately obvious why. We only trigger the implicit check
    when we have exactly two non-option arguments. And the only
    code that cares about implicit versus explicit is an error
    message that we show when we _don't_ have two non-option
    arguments.
    
    However, it's worth fixing anyway. Besides being slightly
    more efficient, it makes the code easier to follow, which
    will help when we modify it in future patches.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit edfac5ebffd41b1f83b31d042e9534a0254a05fe
Author: Christian Couder <christian.couder@gmail.com>
Date:   Sun Sep 4 22:18:33 2016 +0200

    builtin/am: use apply API in run_apply()
    
    This replaces run_apply() implementation with a new one that
    uses the apply API that has been previously prepared in
    apply.c and apply.h.
    
    This shoud improve performance a lot in certain cases.
    
    As the previous implementation was creating a new `git apply`
    process to apply each patch, it could be slow on systems like
    Windows where it is costly to create new processes.
    
    Also the new `git apply` process had to read the index from
    disk, and when the process was done the calling process
    discarded its own index and read back from disk the new
    index that had been created by the `git apply` process.
    
    This could be very inefficient with big repositories that
    have big index files, especially when the system decided
    that it was a good idea to run the `git apply` processes on
    a different processor core.
    
    Also eliminating index reads enables further performance
    improvements by using:
    
    `git update-index --split-index`
    
    For example here is a benchmark of a multi hundred commit
    rebase on the Linux kernel on a Debian laptop with SSD:
    
    command: git rebase --onto 1993b17 52bef0c 29dde7c
    
    Vanilla "next" without split index:                1m54.953s
    Vanilla "next" with split index:                   1m22.476s
    This series on top of "next" without split index:  1m12.034s
    This series on top of "next" with split index:     0m15.678s
    
    (using branch "next" from mid April 2016.)
    
    Benchmarked-by: Ævar Arnfjörð Bjarmason <avarab@gmail.com>
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1ecdecce621009a4d039d061d514056501d0ed8f
Author: Jeff Hostetler <jeffhost@microsoft.com>
Date:   Thu Aug 11 10:45:57 2016 -0400

    status: collect per-file data for --porcelain=v2
    
    Collect extra per-file data for porcelain V2 format.
    
    The output of `git status --porcelain` leaves out many
    details about the current status that clients might like
    to have.  This can force them to be less efficient as they
    may need to launch secondary commands (and try to match
    the logic within git) to accumulate this extra information.
    For example, a GUI IDE might want the file mode to display
    the correct icon for a changed item (without having to stat
    it afterwards).
    
    Signed-off-by: Jeff Hostetler <jeffhost@microsoft.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1eb47f167d65d1d305b9c196a1bb40eb96117cb1
Author: René Scharfe <l.s.r@web.de>
Date:   Sat Aug 6 17:41:01 2016 +0200

    use strbuf_add_unique_abbrev() for adding short hashes
    
    Call strbuf_add_unique_abbrev() to add abbreviated hashes to strbufs
    instead of taking detours through find_unique_abbrev() and its static
    buffer.  This is shorter and a bit more efficient.
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 67b3a5d4c084c3f579205a07b65169e742d39c02
Merge: 5569c01be8 da470981de
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Aug 3 15:10:27 2016 -0700

    Merge branch 'jt/fetch-large-handshake-window-on-http'
    
    "git fetch" exchanges batched have/ack messages between the sender
    and the receiver, initially doubling every time and then falling
    back to enlarge the window size linearly.  The "smart http"
    transport, being an half-duplex protocol, outgrows the preset limit
    too quickly and becomes inefficient when interacting with a large
    repository.  The internal mechanism learned to grow the window size
    more aggressively when working with the "smart http" transport.
    
    * jt/fetch-large-handshake-window-on-http:
      fetch-pack: grow stateless RPC windows exponentially

commit a73cdd21c4514bf0890d4c1ac756fe02262b1192
Author: Jeff King <peff@peff.net>
Date:   Fri Jul 29 00:09:46 2016 -0400

    find_pack_entry: replace last_found_pack with MRU cache
    
    Each pack has an index for looking up entries in O(log n)
    time, but if we have multiple packs, we have to scan through
    them linearly. This can produce a measurable overhead for
    some operations.
    
    We dealt with this long ago in f7c22cc (always start looking
    up objects in the last used pack first, 2007-05-30), which
    keeps what is essentially a 1-element most-recently-used
    cache. In theory, we should be able to do better by keeping
    a similar but longer cache, that is the same length as the
    pack-list itself.
    
    Since we now have a convenient generic MRU structure, we can
    plug it in and measure. Here are the numbers for running
    p5303 against linux.git:
    
    Test                      HEAD^                HEAD
    ------------------------------------------------------------------------
    5303.3: rev-list (1)      31.56(31.28+0.27)    31.30(31.08+0.20) -0.8%
    5303.4: repack (1)        40.62(39.35+2.36)    40.60(39.27+2.44) -0.0%
    5303.6: rev-list (50)     31.31(31.06+0.23)    31.23(31.00+0.22) -0.3%
    5303.7: repack (50)       58.65(69.12+1.94)    58.27(68.64+2.05) -0.6%
    5303.9: rev-list (1000)   38.74(38.40+0.33)    31.87(31.62+0.24) -17.7%
    5303.10: repack (1000)    367.20(441.80+4.62)  342.00(414.04+3.72) -6.9%
    
    The main numbers of interest here are the rev-list ones
    (since that is exercising the normal object lookup code
    path).  The single-pack case shouldn't improve at all; the
    260ms speedup there is just part of the run-to-run noise
    (but it's important to note that we didn't make anything
    worse with the overhead of maintaining our cache). In the
    50-pack case, we see similar results. There may be a slight
    improvement, but it's mostly within the noise.
    
    The 1000-pack case does show a big improvement, though. That
    carries over to the repack case, as well. Even though we
    haven't touched its pack-search loop yet, it does still do a
    lot of normal object lookups (e.g., for the internal
    revision walk), and so improves.
    
    As a point of reference, I also ran the 1000-pack test
    against a version of HEAD^ with the last_found_pack
    optimization disabled. It takes ~60s, so that gives an
    indication of how much even the single-element cache is
    helping.
    
    For comparison, here's a smaller repository, git.git:
    
    Test                      HEAD^               HEAD
    ---------------------------------------------------------------------
    5303.3: rev-list (1)      1.56(1.54+0.01)    1.54(1.51+0.02) -1.3%
    5303.4: repack (1)        1.84(1.80+0.10)    1.82(1.80+0.09) -1.1%
    5303.6: rev-list (50)     1.58(1.55+0.02)    1.59(1.57+0.01) +0.6%
    5303.7: repack (50)       2.50(3.18+0.04)    2.50(3.14+0.04) +0.0%
    5303.9: rev-list (1000)   2.76(2.71+0.04)    2.24(2.21+0.02) -18.8%
    5303.10: repack (1000)    13.21(19.56+0.25)  11.66(18.01+0.21) -11.7%
    
    You can see that the percentage improvement is similar.
    That's because the lookup we are optimizing is roughly
    O(nr_objects * nr_packs). Since the number of packs is
    constant in both tests, we'd expect the improvement to be
    linear in the number of objects. But the whole process is
    also linear in the number of objects, so the improvement
    is a constant factor.
    
    The exact improvement does also depend on the contents of
    the packs. In p5303, the extra packs all have 5 first-parent
    commits in them, which is a reasonable simulation of a
    pushed-to repository. But it also means that only 250
    first-parent commits are in those packs (compared to almost
    50,000 total in linux.git), and the rest are in the huge
    "base" pack. So once we start looking at history in taht big
    pack, that's where we'll find most everything, and even the
    1-element cache gets close to 100% cache hits.  You could
    almost certainly show better numbers with a more
    pathological case (e.g., distributing the objects more
    evenly across the packs). But that's simply not that
    realistic a scenario, so it makes more sense to focus on
    these numbers.
    
    The implementation itself is a straightforward application
    of the MRU code. We provide an MRU-ordered list of packs
    that shadows the packed_git list. This is easy to do because
    we only create and revise the pack list in one place. The
    "reprepare" code path actually drops the whole MRU and
    replaces it for simplicity. It would be more efficient to
    just add new entries, but there's not much point in
    optimizing here; repreparing happens rarely, and only after
    doing a lot of other expensive work.  The key things to keep
    optimized are traversal (which is just a normal linked list,
    albeit with one extra level of indirection over the regular
    packed_git list), and marking (which is a constant number of
    pointer assignments, though slightly more than the old
    last_found_pack was; it doesn't seem to create a measurable
    slowdown, though).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 77023ea3c3951be97286bc241ae88bc6c860e2b7
Author: Jeff King <peff@peff.net>
Date:   Fri Jul 29 00:06:09 2016 -0400

    t/perf: add tests for many-pack scenarios
    
    Git's pack storage does efficient (log n) lookups in a
    single packfile's index, but if we have multiple packfiles,
    we have to linearly search each for a given object.  This
    patch introduces some timing tests for cases where we have a
    large number of packs, so that we can measure any
    improvements we make in the following patches.
    
    The main thing we want to time is object lookup. To do this,
    we measure "git rev-list --objects --all", which does a
    fairly large number of object lookups (essentially one per
    object in the repository).
    
    However, we also measure the time to do a full repack, which
    is interesting for two reasons. One is that in addition to
    the usual pack lookup, it has its own linear iteration over
    the list of packs. And two is that because it it is the tool
    one uses to go from an inefficient many-pack situation back
    to a single pack, we care about its performance not only at
    marginal numbers of packs, but at the extreme cases (e.g.,
    if you somehow end up with 5,000 packs, it is the only way
    to get back to 1 pack, so we need to make sure it performs
    well).
    
    We measure the performance of each command in three
    scenarios: 1 pack, 50 packs, and 1,000 packs.
    
    The 1-pack case is a baseline; any optimizations we do to
    handle multiple packs cannot possibly perform better than
    this.
    
    The 50-pack case is as far as Git should generally allow
    your repository to go, if you have auto-gc enabled with the
    default settings. So this represents the maximum performance
    improvement we would expect under normal circumstances.
    
    The 1,000-pack case is hopefully rare, though I have seen it
    in the wild where automatic maintenance was broken for some
    time (and the repository continued to receive pushes). This
    represents cases where we care less about general
    performance, but want to make sure that a full repack
    command does not take excessively long.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f241ff0d0a9ddb84b2f673a1b7a92fea0d6add3a
Author: Johannes Schindelin <johannes.schindelin@gmx.de>
Date:   Tue Jul 26 18:06:02 2016 +0200

    prepare the builtins for a libified merge_recursive()
    
    Previously, callers of merge_trees() or merge_recursive() expected that
    code to die() with an error message. This used to be okay because we
    called those commands from scripts, and had a chance to print out a
    message in case the command failed fatally (read: with exit code 128).
    
    As scripting incurs its own set of problems (portability, speed,
    idiosyncrasies of different shells, limited data structures leading to
    inefficient code), we are converting more and more of these scripts into
    builtins, using library functions directly.
    
    We already tried to use merge_recursive() directly in the builtin
    git-am, for example. Unfortunately, we had to roll it back temporarily
    because some of the code in merge-recursive.c still deemed it okay to
    call die(), when the builtin am code really wanted to print out a useful
    advice after the merge failed fatally. In the next commits, we want to
    fix that.
    
    The code touched by this commit expected merge_trees() to die() with
    some useful message when there is an error condition, but merge_trees()
    is going to be improved by converting all die() calls to return error()
    instead (i.e. return value -1 after printing out the message as before),
    so that the caller can react more flexibly.
    
    This is a step to prepare for the version of merge_trees() that no
    longer dies,  even if we just imitate the previous behavior by calling
    exit(128): this is what callers of e.g. `git merge` have come to expect.
    
    Note that the callers of the sequencer (revert and cherry-pick) already
    fail fast even for the return value -1; The only difference is that they
    now get a chance to say "<command> failed".
    
    A caller of merge_trees() might want handle error messages themselves
    (or even suppress them). As this patch is already complex enough, we
    leave that change for a later patch.
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 8109984d619467ad26c6a76d59b0a5de8a86e6d6
Author: René Scharfe <l.s.r@web.de>
Date:   Tue Jul 19 20:36:29 2016 +0200

    use strbuf_addbuf() for appending a strbuf to another
    
    Use strbuf_addbuf() where possible; it's shorter and more efficient.
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7eb6e10c9d7f43913615667740d1b22055cfba1f
Author: Jeff King <peff@peff.net>
Date:   Fri Jul 8 05:16:53 2016 -0400

    branch: use write_file_buf instead of write_file
    
    If we already have a strbuf, then using write_file_buf is a
    little nicer to read (no wondering whether "%s" will eat
    your NULs), and it's more efficient (no extra formatting
    step).
    
    We don't care about the newline magic of write_file(), as we
    have our own multi-line content.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 52563d7ecc8f3f38cb1c0521294c5f6a0a475637
Author: Jeff King <peff@peff.net>
Date:   Fri Jul 8 05:12:22 2016 -0400

    write_file: add pointer+len variant
    
    There are many callsites which could use write_file, but for
    which it is a little awkward because they have a strbuf or
    other pointer/len combo. Specifically:
    
     1. write_file() takes a format string, so we have to use
        "%s" or "%.*s", which are ugly.
    
     2. Using any form of "%s" does not handle embedded NULs in
        the output. That probably doesn't matter for our
        call-sites, but it's nicer not to have to worry.
    
     3. It's less efficient; we format into another strbuf
        just to do the write. That's probably not measurably
        slow for our uses, but it's simply inelegant.
    
    We can fix this by providing a helper to write out the
    formatted buffer, and just calling it from write_file().
    
    Note that we don't do the usual "complete with a newline"
    that write_file does. If the caller has their own buffer,
    there's a reasonable chance they're doing something more
    complicated than a single line, and they can call
    strbuf_complete_line() themselves.
    
    We could go even further and add strbuf_write_file(), but it
    doesn't save much:
    
      -  write_file_buf(path, sb.buf, sb.len);
      +  strbuf_write_file(&sb, path);
    
    It would also be somewhat asymmetric with strbuf_read_file,
    which actually returns errors rather than dying (and the
    error handling is most of the benefit of write_file() in the
    first place).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e51217e15c3eb89d18b313dad32db1787d4f2a44
Author: Jeff King <peff@peff.net>
Date:   Thu Jun 30 05:08:57 2016 -0400

    t5000: test tar files that overflow ustar headers
    
    The ustar format only has room for 11 (or 12, depending on
    some implementations) octal digits for the size and mtime of
    each file. For values larger than this, we have to add pax
    extended headers to specify the real data, and git does not
    yet know how to do so.
    
    Before fixing that, let's start off with some test
    infrastructure, as designing portable and efficient tests
    for this is non-trivial.
    
    We want to use the system tar to check our output (because
    what we really care about is interoperability), but we can't
    rely on it:
    
      1. being able to read pax headers
    
      2. being able to handle huge sizes or mtimes
    
      3. supporting a "t" format we can parse
    
    So as a prerequisite, we can feed the system tar a reference
    tarball to make sure it can handle these features. The
    reference tar here was created with:
    
      dd if=/dev/zero seek=64G bs=1 count=1 of=huge
      touch -d @68719476737 huge
      tar cf - --format=pax |
      head -c 2048
    
    using GNU tar. Note that this is not a complete tarfile, but
    it's enough to contain the headers we want to examine.
    
    Likewise, we need to convince git that it has a 64GB blob to
    output. Running "git add" on that 64GB file takes many
    minutes of CPU, and even compressed, the result is 64MB. So
    again, I pre-generated that loose object, and then took only
    the first 2k of it. That should be enough to generate 2MB of
    data before hitting an inflate error, which is plenty for us
    to generate the tar header (and then die of SIGPIPE while
    streaming the rest out).
    
    The tests are split so that we test as much as we can even
    with an uncooperative system tar. This actually catches the
    current breakage (which is that we die("BUG") trying to
    write the ustar header) on every system, and then on systems
    where we can, we go farther and actually verify the result.
    
    Helped-by: Robin H. Johnson <robbat2@gentoo.org>
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 4c4de89573fa29b7f97e7a9a3d0674dbdb6f2a28
Author: Michael Haggerty <mhagger@alum.mit.edu>
Date:   Sat Jun 18 06:15:16 2016 +0200

    do_for_each_ref(): reimplement using reference iteration
    
    Use the reference iterator interface to implement do_for_each_ref().
    Delete a bunch of code supporting the old for_each_ref() implementation.
    And now that do_for_each_ref() is generic code (it is no longer tied to
    the files backend), move it to refs.c.
    
    The implementation is via a new function, do_for_each_ref_iterator(),
    which takes a reference iterator as argument and calls a callback
    function for each of the references in the iterator.
    
    This change requires the current_ref performance hack for peel_ref() to
    be implemented via ref_iterator_peel() rather than peel_entry() because
    we don't have a ref_entry handy (it is hidden under three layers:
    file_ref_iterator, merge_ref_iterator, and cache_ref_iterator). So:
    
    * do_for_each_ref_iterator() records the active iterator in
      current_ref_iter while it is running.
    
    * peel_ref() checks whether current_ref_iter is pointing at the
      requested reference. If so, it asks the iterator to peel the
      reference (which it can do efficiently via its "peel" virtual
      function). For extra safety, we do the optimization only if the
      refname *addresses* are the same, not only if the refname *strings*
      are the same, to forestall possible mixups between refnames that come
      from different ref_iterators.
    
    Please note that this optimization of peel_ref() is only available when
    iterating via do_for_each_ref_iterator() (including all of the
    for_each_ref() functions, which call it indirectly). It would be
    complicated to implement a similar optimization when iterating directly
    using a reference iterator, because multiple reference iterators can be
    in use at the same time, with interleaved calls to
    ref_iterator_advance(). (In fact we do exactly that in
    merge_ref_iterator.)
    
    But that is not necessary. peel_ref() is only called while iterating
    over references. Callers who iterate using the for_each_ref() functions
    benefit from the optimization described above. Callers who iterate using
    reference iterators directly have access to the ref_iterator, so they
    can call ref_iterator_peel() themselves to get an analogous optimization
    in a more straightforward manner.
    
    If we rewrite all callers to use the reference iteration API, then we
    can remove the current_ref_iter hack permanently.
    
    Signed-off-by: Michael Haggerty <mhagger@alum.mit.edu>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit d9545c7f465ed103df44cd93caddfdd265757779
Author: Eric Wong <normalperson@yhbt.net>
Date:   Mon Apr 25 21:17:28 2016 +0000

    fast-import: implement unpack limit
    
    With many incremental imports, small packs become highly
    inefficient due to the need to readdir scan and load many
    indices to locate even a single object.  Frequent repacking and
    consolidation may be prohibitively expensive in terms of disk
    I/O, especially in large repositories where the initial packs
    were aggressively optimized and marked with .keep files.
    
    In those cases, users may be better served with loose objects
    and relying on "git gc --auto".
    
    This changes the default behavior of fast-import for small
    imports found in test cases, so adjustments to t9300 were
    necessary.
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 2824e1841b99393d2469c495253d547c643bd8f1
Author: Jeff King <peff@peff.net>
Date:   Thu Feb 11 17:28:36 2016 -0500

    list-objects: pass full pathname to callbacks
    
    When we find a blob at "a/b/c", we currently pass this to
    our show_object_fn callbacks as two components: "a/b/" and
    "c". Callbacks which want the full value then call
    path_name(), which concatenates the two. But this is an
    inefficient interface; the path is a strbuf, and we could
    simply append "c" to it temporarily, then roll back the
    length, without creating a new copy.
    
    So we could improve this by teaching the callsites of
    path_name() this trick (and there are only 3). But we can
    also notice that no callback actually cares about the
    broken-down representation, and simply pass each callback
    the full path "a/b/c" as a string. The callback code becomes
    even simpler, then, as we do not have to worry about freeing
    an allocated buffer, nor rolling back our modification to
    the strbuf.
    
    This is theoretically less efficient, as some callbacks
    would not bother to format the final path component. But in
    practice this is not measurable. Since we use the same
    strbuf over and over, our work to grow it is amortized, and
    we really only pay to memcpy a few bytes.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f3badaed5106a16499d0fae31a382f9047b272d7
Author: Jeff King <peff@peff.net>
Date:   Thu Feb 11 17:26:18 2016 -0500

    list-objects: convert name_path to a strbuf
    
    The "struct name_path" data is examined in only two places:
    we generate it in process_tree(), and we convert it to a
    single string in path_name(). Everyone else just passes it
    through to those functions.
    
    We can further note that process_tree() already keeps a
    single strbuf with the leading tree path, for use with
    tree_entry_interesting().
    
    Instead of building a separate name_path linked list, let's
    just use the one we already build in "base". This reduces
    the amount of code (especially tricky code in path_name()
    which did not check for integer overflows caused by deep
    or large pathnames).
    
    It is also more efficient in some instances.  Any time we
    were using tree_entry_interesting, we were building up the
    strbuf anyway, so this is an immediate and obvious win
    there. In cases where we were not, we trade off storing
    "pathname/" in a strbuf on the heap for each level of the
    path, instead of two pointers and an int on the stack (with
    one pointer into the tree object). On a 64-bit system, the
    latter is 20 bytes; so if path components are less than that
    on average, this has lower peak memory usage.  In practice
    it probably doesn't matter either way; we are already
    holding in memory all of the tree objects leading up to each
    pathname, and for normal-depth pathnames, we are only
    talking about hundreds of bytes.
    
    This patch leaves "struct name_path" as a thin wrapper
    around the strbuf, to avoid disrupting callbacks. We should
    fix them, but leaving it out makes this diff easier to view.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 8eee9f9277b6e38ec46c84f4ca3be5d988ca0a33
Author: Jeff King <peff@peff.net>
Date:   Thu Feb 11 17:24:18 2016 -0500

    show_object_with_name: simplify by using path_name()
    
    When "git rev-list" shows an object with its associated path
    name, it does so by walking the name_path linked list and
    printing each component (stopping at any embedded NULs or
    newlines).
    
    We'd like to eventually get rid of name_path entirely in
    favor of a single buffer, and dropping this custom printing
    code is part of that. As a first step, let's use path_name()
    to format the list into a single buffer, and print that.
    This is strictly less efficient than the original, but it's
    a temporary step in the refactoring; our end game will be to
    get the fully formatted name in the first place.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 85975c0c7f46aeb5010121959e38c339038758a7
Author: Jeff King <peff@peff.net>
Date:   Mon Mar 7 10:51:21 2016 -0500

    grep: turn off gitlink detection for --no-index
    
    If we are running "git grep --no-index" outside of a git
    repository, we behave roughly like "grep -r", examining all
    files in the current directory and its subdirectories.
    However, because we use fill_directory() to do the
    recursion, it will skip over any directories which look like
    sub-repositories.
    
    For a normal git operation (like "git grep" in a repository)
    this makes sense; we do not want to cross the boundary out
    of our current repository into a submodule. But for
    "--no-index" without a repository, we should look at all
    files, including embedded repositories.
    
    There is one exception, though: we probably should _not_
    descend into ".git" directories. Doing so is inefficient and
    unlikely to turn up useful hits.
    
    This patch drops our use of dir.c's gitlink-detection, but
    we do still avoid ".git". That makes us more like tools such
    as "ack" or "ag", which also know to avoid cruft in .git.
    
    As a bonus, this also drops our usage of the ref code
    when we are outside of a repository, making the transition
    to pluggable ref backends cleaner.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 9831e92bfa833ee9c0ce464bbc2f941ae6c2698d
Merge: e84d5e9fa1 de1e67d070
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Feb 24 13:25:55 2016 -0800

    Merge branch 'jk/lose-name-path'
    
    The "name_path" API was an attempt to reduce the need to construct
    the full path out of a series of path components while walking a
    tree hierarchy, but over time made less efficient because the path
    needs to be flattened, e.g. to be compared with another path that
    is already flat.  The API has been removed and its users have been
    rewritten to simplify the overall code complexity.
    
    * jk/lose-name-path:
      list-objects: pass full pathname to callbacks
      list-objects: drop name_path entirely
      list-objects: convert name_path to a strbuf
      show_object_with_name: simplify by using path_name()
      http-push: stop using name_path

commit 1a92e53ba3614105b7a58a4820f4d33f3cf33a3a
Author: Jeff King <peff@peff.net>
Date:   Tue Feb 23 01:04:41 2016 -0500

    merge-one-file: use empty blob for add/add base
    
    When we see an add/add conflict on a file, we generate the
    conflicted content by doing a 3-way merge with a "virtual"
    base consisting of the common lines of the two sides. This
    strategy dates back to cb93c19 (merge-one-file: use common
    as base, instead of emptiness., 2005-11-09).
    
    Back then, the next step was to call rcs merge to generate
    the 3-way conflicts. Using the virtual base produced much
    better results, as rcs merge does not attempt to minimize
    the hunks. As a result, you'd get a conflict with the
    entirety of the files on either side.
    
    Since then, though, we've switched to using git-merge-file,
    which uses xdiff's "zealous" merge. This will find the
    minimal hunks even with just the simple, empty base.
    
    Let's switch to using that empty base. It's simpler, more
    efficient, and reduces our dependencies (we no longer need a
    working diff binary). It's also how the merge-recursive
    strategy handles this same case.
    
    We can almost get rid of git-sh-setup's create_virtual_base,
    but we don't here, for two reasons:
    
      1. The functions in git-sh-setup are part of our public
         interface, so it's possible somebody is depending on
         it. We'd at least need to deprecate it first.
    
      2. It's also used by mergetool's p4merge driver. It's
         unknown whether its 3-way merge is as capable as git's;
         if not, then it is benefiting from the function.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit de1e67d0703894cb6ea782e36abb63976ab07e60
Author: Jeff King <peff@peff.net>
Date:   Thu Feb 11 17:28:36 2016 -0500

    list-objects: pass full pathname to callbacks
    
    When we find a blob at "a/b/c", we currently pass this to
    our show_object_fn callbacks as two components: "a/b/" and
    "c". Callbacks which want the full value then call
    path_name(), which concatenates the two. But this is an
    inefficient interface; the path is a strbuf, and we could
    simply append "c" to it temporarily, then roll back the
    length, without creating a new copy.
    
    So we could improve this by teaching the callsites of
    path_name() this trick (and there are only 3). But we can
    also notice that no callback actually cares about the
    broken-down representation, and simply pass each callback
    the full path "a/b/c" as a string. The callback code becomes
    even simpler, then, as we do not have to worry about freeing
    an allocated buffer, nor rolling back our modification to
    the strbuf.
    
    This is theoretically less efficient, as some callbacks
    would not bother to format the final path component. But in
    practice this is not measurable. Since we use the same
    strbuf over and over, our work to grow it is amortized, and
    we really only pay to memcpy a few bytes.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 13528ab37cadb4d4f7384d0449489760912904b8
Author: Jeff King <peff@peff.net>
Date:   Thu Feb 11 17:26:18 2016 -0500

    list-objects: convert name_path to a strbuf
    
    The "struct name_path" data is examined in only two places:
    we generate it in process_tree(), and we convert it to a
    single string in path_name(). Everyone else just passes it
    through to those functions.
    
    We can further note that process_tree() already keeps a
    single strbuf with the leading tree path, for use with
    tree_entry_interesting().
    
    Instead of building a separate name_path linked list, let's
    just use the one we already build in "base". This reduces
    the amount of code (especially tricky code in path_name()
    which did not check for integer overflows caused by deep
    or large pathnames).
    
    It is also more efficient in some instances.  Any time we
    were using tree_entry_interesting, we were building up the
    strbuf anyway, so this is an immediate and obvious win
    there. In cases where we were not, we trade off storing
    "pathname/" in a strbuf on the heap for each level of the
    path, instead of two pointers and an int on the stack (with
    one pointer into the tree object). On a 64-bit system, the
    latter is 20 bytes; so if path components are less than that
    on average, this has lower peak memory usage.  In practice
    it probably doesn't matter either way; we are already
    holding in memory all of the tree objects leading up to each
    pathname, and for normal-depth pathnames, we are only
    talking about hundreds of bytes.
    
    This patch leaves "struct name_path" as a thin wrapper
    around the strbuf, to avoid disrupting callbacks. We should
    fix them, but leaving it out makes this diff easier to view.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f9fb9d0e3caebe97c54c101d8235e6529d5a1273
Author: Jeff King <peff@peff.net>
Date:   Thu Feb 11 17:24:18 2016 -0500

    show_object_with_name: simplify by using path_name()
    
    When "git rev-list" shows an object with its associated path
    name, it does so by walking the name_path linked list and
    printing each component (stopping at any embedded NULs or
    newlines).
    
    We'd like to eventually get rid of name_path entirely in
    favor of a single buffer, and dropping this custom printing
    code is part of that. As a first step, let's use path_name()
    to format the list into a single buffer, and print that.
    This is strictly less efficient than the original, but it's
    a temporary step in the refactoring; our end game will be to
    get the fully formatted name in the first place.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 0d0bac67ce3b3f2301702573f6acc100798d7edd
Author: Jeff King <peff@peff.net>
Date:   Sat Jan 30 02:21:26 2016 -0500

    transport: drop support for git-over-rsync
    
    The git-over-rsync protocol is inefficient and broken, and
    has been for a long time. It transfers way more objects than
    it needs (grabbing all of the remote's "objects/",
    regardless of which objects we need). It does its own ad-hoc
    parsing of loose and packed refs from the remote, but
    doesn't properly override packed refs with loose ones,
    leading to garbage results (e.g., expecting the other side
    to have an object pointed to by a stale packed-refs entry,
    or complaining that the other side has two copies of the
    refs[1]).
    
    This latter breakage means that nobody could have
    successfully pulled from a moderately active repository
    since cd547b4 (fetch/push: readd rsync support, 2007-10-01).
    
    We never made an official deprecation notice in the release
    notes for git's rsync protocol, but the tutorial has marked
    it as such since 914328a (Update tutorial., 2005-08-30).
    And on the mailing list as far back as Oct 2005, we can find
    Junio mentioning it as having "been deprecated for quite
    some time."[2,3,4]. So it was old news then; cogito had
    deprecated the transport in July of 2005[5] (though it did
    come back briefly when Linus broke git-http-pull!).
    
    Of course some people professed their love of rsync through
    2006, but Linus clarified in his usual gentle manner[6]:
    
      > Thanks!  This is why I still use rsync, even though
      > everybody and their mother tells me "Linus says rsync is
      > deprecated."
    
      No. You're using rsync because you're actively doing
      something _wrong_.
    
    The deprecation sentiment was reinforced in 2008, with a
    mention that cloning via rsync is broken (with no fix)[7].
    
    Even the commit porting rsync over to C from shell (cd547b4)
    lists it as deprecated! So between the 10 years of informal
    warnings, and the fact that it has been severely broken
    since 2007, it's probably safe to simply remove it without
    further deprecation warnings.
    
    [1] http://article.gmane.org/gmane.comp.version-control.git/285101
    [2] http://article.gmane.org/gmane.comp.version-control.git/10093
    [3] http://article.gmane.org/gmane.comp.version-control.git/17734
    [4] http://article.gmane.org/gmane.comp.version-control.git/18911
    [5] http://article.gmane.org/gmane.comp.version-control.git/5617
    [6] http://article.gmane.org/gmane.comp.version-control.git/19354
    [7] http://article.gmane.org/gmane.comp.version-control.git/103635
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 2db6b83d189bb82d1d45805fa6c85a9c8b507920
Author: Jeff King <peff@peff.net>
Date:   Mon Jan 18 15:02:48 2016 -0500

    shortlog: replace hand-parsing of author with pretty-printer
    
    When gathering the author and oneline subject for each
    commit, we hand-parse the commit headers to find the
    "author" line, and then continue past to the blank line at
    the end of the header.
    
    We can replace this tricky hand-parsing by simply asking the
    pretty-printer for the relevant items. This also decouples
    the author and oneline parsing, opening up some new
    optimizations in further commits.
    
    One reason to avoid the pretty-printer is that it might be
    less efficient than hand-parsing. However, I measured no
    slowdown at all running "git shortlog -ns HEAD" on
    linux.git.
    
    As a bonus, we also fix a memory leak in the (uncommon) case
    that the author field is blank.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 28de56856a62a459cfb2276e517bac5af9147ab2
Author: Beat Bolli <dev+git@drbeat.li>
Date:   Wed Sep 30 21:50:11 2015 +0200

    gitk: Add missing accelerators
    
    In d99b4b0de27a ("gitk: Accelerators for the main menu", 2015-09-09),
    accelerators were added to allow efficient keyboard navigation. One
    instance of the strings "Edit view..." and "Delete view" were left
    without the ampersand.
    
    Add the missing ampersand characters to unbreak our international
    users.
    
    Signed-off-by: Beat Bolli <dev+git@drbeat.li>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit 3131977de1476185209d4d56a0494f5b30ee5557
Author: Jeff King <peff@peff.net>
Date:   Thu Sep 24 17:05:57 2015 -0400

    progress: store throughput display in a strbuf
    
    Coverity noticed that we strncpy() into a fixed-size buffer
    without making sure that it actually ended up
    NUL-terminated. This is unlikely to be a bug in practice,
    since throughput strings rarely hit 32 characters, but it
    would be nice to clean it up.
    
    The most obvious way to do so is to add a NUL-terminator.
    But instead, this patch switches the fixed-size buffer out
    for a strbuf. At first glance this seems much less
    efficient, until we realize that filling in the fixed-size
    buffer is done by writing into a strbuf and copying the
    result!
    
    By writing straight to the buffer, we actually end up more
    efficient:
    
      1. We avoid an extra copy of the bytes.
    
      2. Rather than malloc/free each time progress is shown, we
         can strbuf_reset and use the same buffer each time.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit ef8b53e78c534d732fe58a46da8dbd922287a56b
Author: Edward Thomson <ethomson@microsoft.com>
Date:   Sat Sep 12 17:50:26 2015 +0000

    poll: honor the timeout on Win32
    
    Ensure that when passing a pipe, the gnulib poll replacement will not
    return 0 before the timeout has passed.
    
    Not obeying the timeout (and merely returning 0) causes pathological
    behavior when preparing a packfile for a repository and taking a
    long time to do so.  If poll were to return 0 immediately, this would
    cause keep-alives to get sent as quickly as possible until the packfile
    was created.  Such deviance from the standard would cause megabytes (or
    more) of keep-alive packets to be sent.
    
    GetTickCount is used as it is efficient, stable and monotonically
    increasing.  (Neither GetSystemTime nor QueryPerformanceCounter have
    all three of these properties.)
    
    Signed-off-by: Edward Thomson <ethomson@microsoft.com>
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f9610bcae907a38636174a688c1aa89c63cc9c1a
Merge: d3ac359841 0df3245721
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Aug 19 14:41:29 2015 -0700

    Merge branch 'mh/fast-import-optimize-current-from' into maint
    
    Often a fast-import stream builds a new commit on top of the
    previous commit it built, and it often unconditionally emits a
    "from" command to specify the first parent, which can be omitted in
    such a case.  This caused fast-import to forget the tree of the
    previous commit and then re-read it from scratch, which was
    inefficient.  Optimize for this common case.
    
    * mh/fast-import-optimize-current-from:
      fast-import: do less work when given "from" matches current branch head

commit f5b2dec1657e09a22f8b2aefa25d022988e3e467
Author: Jeff King <peff@peff.net>
Date:   Mon Aug 10 05:36:19 2015 -0400

    refs.c: remove extra git_path calls from read_loose_refs
    
    In iterating over the loose refs in "refs/foo/", we keep a
    running strbuf with "refs/foo/one", "refs/foo/two", etc. But
    we also need to access these files in the filesystem, as
    ".git/refs/foo/one", etc. For this latter purpose, we make a
    series of independent calls to git_path(). These are safe
    (we only use the result to call stat()), but assigning the
    result of git_path is a suspicious pattern that we'd rather
    avoid.
    
    This patch keeps a running buffer with ".git/refs/foo/", and
    we can just append/reset each directory element as we loop.
    This matches how we handle the refnames. It should also be
    more efficient, as we do not keep formatting the same
    ".git/refs/foo" prefix (which can be arbitrarily deep).
    
    Technically we are dropping a call to strbuf_cleanup() on
    each generated filename, but that's OK; it wasn't doing
    anything, as we are putting in single-level names we read
    from the filesystem (so it could not possibly be cleaning up
    cruft like "./" in this instance).
    
    A clever reader may also note that the running refname
    buffer ("refs/foo/") is actually a subset of the filesystem
    path buffer (".git/refs/foo/"). We could get by with one
    buffer, indexing the length of $GIT_DIR when we want the
    refname. However, having tried this, the resulting code
    actually ends up a little more confusing, and the efficiency
    improvement is tiny (and almost certainly dwarfed by the
    system calls we are making).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 77b9b1d13ac9e6b78ba676d4edb221b7d2273c62
Author: Jeff King <peff@peff.net>
Date:   Mon Aug 10 05:34:46 2015 -0400

    add_to_alternates_file: don't add duplicate entries
    
    The add_to_alternates_file function blindly uses
    hold_lock_file_for_append to copy the existing contents, and
    then adds the new line to it. This has two minor problems:
    
      1. We might add duplicate entries, which are ugly and
         inefficient.
    
      2. We do not check that the file ends with a newline, in
         which case we would bogusly append to the final line.
         This is quite unlikely in practice, though, as we call
         this function only from git-clone, so presumably we are
         the only writers of the file (and we always add a
         newline).
    
    Instead of using hold_lock_file_for_append, let's copy the
    file line by line, which ensures all records are properly
    terminated. If we see an extra line, we can simply abort the
    update (there is no point in even copying the rest, as we
    know that it would be identical to the original).
    
    As a bonus, we also get rid of some calls to the
    static-buffer mkpath and git_path functions.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 3ecca8879a07889884dcdc90066c76c4e948856a
Merge: 85df7cd487 0df3245721
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon Aug 3 11:01:24 2015 -0700

    Merge branch 'mh/fast-import-optimize-current-from'
    
    Often a fast-import stream builds a new commit on top of the
    previous commit it built, and it often unconditionally emits a
    "from" command to specify the first parent, which can be omitted in
    such a case.  This caused fast-import to forget the tree of the
    previous commit and then re-read it from scratch, which was
    inefficient.  Optimize for this common case.
    
    * mh/fast-import-optimize-current-from:
      fast-import: do less work when given "from" matches current branch head

commit 1c601af25a1d182420da11d20bc8862823e47012
Author: Eric Sunshine <sunshine@sunshineco.com>
Date:   Tue Jul 28 16:06:15 2015 -0400

    Documentation/git-tools: retire manually-maintained list
    
    When Git was young, people looking for third-party Git-related tools
    came to the Git project itself to find them, so it made sense to
    maintain a list of tools here. These days, however, search engines fill
    that role much more efficiently, so retire the manually-maintained
    list.
    
    The list of front-ends and tools on the Git wiki rates perhaps a distant
    second to search engines, and may still have value, so retain a
    reference to it.
    
    Signed-off-by: Eric Sunshine <sunshine@sunshineco.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 3096b2ecdb91c27cb9e64b692c480deed6c7e77e
Author: Jeff King <peff@peff.net>
Date:   Wed Jul 8 16:33:52 2015 -0400

    check_and_freshen_file: fix reversed success-check
    
    When we want to write out a loose object file, we have
    always first made sure we don't already have the object
    somewhere. Since 33d4221 (write_sha1_file: freshen existing
    objects, 2014-10-15), we also update the timestamp on the
    file, so that a simultaneous prune knows somebody is
    likely to reference it soon.
    
    If our utime() call fails, we treat this the same as not
    having the object in the first place; the safe thing to do
    is write out another copy. However, the loose-object check
    accidentally inverts the utime() check; it returns failure
    _only_ when the utime() call actually succeeded. Thus it was
    failing to protect us there, and in the normal case where
    utime() succeeds, it caused us to pointlessly write out and
    link the object.
    
    This passed our freshening tests, because writing out the
    new object is certainly _one_ way of updating its utime. So
    the normal case was inefficient, but not wrong.
    
    While we're here, let's also drop a comment in front of the
    check_and_freshen functions, making a note of their return
    type (since it is not our usual "0 for success, -1 for
    error").
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 6a951937ae1abb5fe438bfb41ebb28c5abe0419d
Author: Jeff King <peff@peff.net>
Date:   Mon Jun 22 06:45:59 2015 -0400

    cat-file: add --batch-all-objects option
    
    It can sometimes be useful to examine all objects in the
    repository. Normally this is done with "git rev-list --all
    --objects", but:
    
      1. That shows only reachable objects. You may want to look
         at all available objects.
    
      2. It's slow. We actually open each object to walk the
         graph. If your operation is OK with seeing unreachable
         objects, it's an order of magnitude faster to just
         enumerate the loose directories and pack indices.
    
    You can do this yourself using "ls" and "git show-index",
    but it's non-obvious.  This patch adds an option to
    "cat-file --batch-check" to operate on all available
    objects (rather than reading names from stdin).
    
    This is based on a proposal by Charles Bailey to provide a
    separate "git list-all-objects" command. That is more
    orthogonal, as it splits enumerating the objects from
    getting information about them. However, in practice you
    will either:
    
      a. Feed the list of objects directly into cat-file anyway,
         so you can find out information about them. Keeping it
         in a single process is more efficient.
    
      b. Ask the listing process to start telling you more
         information about the objects, in which case you will
         reinvent cat-file's batch-check formatter.
    
    Adding a cat-file option is simple and efficient. And if you
    really do want just the object names, you can always do:
    
      git cat-file --batch-check='%(objectname)' --batch-all-objects
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e479c5f8f380ea54353b96364f66a4496d213733
Author: Jeff King <peff@peff.net>
Date:   Wed Jun 17 14:46:08 2015 -0400

    docs: clarify that --encoding can produce invalid sequences
    
    In the common case that the commit encoding matches the
    output encoding, we do not touch the buffer at all, which
    makes things much more efficient. But it might be unclear to
    a consumer that we will pass through bogus sequences.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit df08eb357dd7f432c3dcbe0ef4b3212a38b4aeff
Merge: 1e6c8babf8 b6e8a3b540
Author: Junio C Hamano <gitster@pobox.com>
Date:   Tue May 26 13:49:26 2015 -0700

    Merge branch 'jk/still-interesting' into maint
    
    "git rev-list --objects $old --not --all" to see if everything that
    is reachable from $old is already connected to the existing refs
    was very inefficient.
    
    * jk/still-interesting:
      limit_list: avoid quadratic behavior from still_interesting

commit 9e3751d4437b43e72497178774c74be1ceac28b9
Author: Jeff King <peff@peff.net>
Date:   Thu May 21 00:45:13 2015 -0400

    remote.c: drop "remote" pointer from "struct branch"
    
    When we create each branch struct, we fill in the
    "remote_name" field from the config, and then fill in the
    actual "remote" field (with a "struct remote") based on that
    name. However, it turns out that nobody really cares about
    the latter field. The only two sites that access it at all
    are:
    
      1. git-merge, which uses it to notice when the branch does
         not have a remote defined. But we can easily replace this
         with looking at remote_name instead.
    
      2. remote.c itself, when setting up the @{upstream} merge
         config. But we don't need to save the "remote" in the
         "struct branch" for that; we can just look it up for
         the duration of the operation.
    
    So there is no need to have both fields; they are redundant
    with each other (the struct remote contains the name, or you
    can look up the struct from the name). It would be nice to
    simplify this, especially as we are going to add matching
    pushremote config in a future patch (and it would be nice to
    keep them consistent).
    
    So which one do we keep and which one do we get rid of?
    
    If we had a lot of callers accessing the struct, it would be
    more efficient to keep it (since you have to do a lookup to
    go from the name to the struct, but not vice versa). But we
    don't have a lot of callers; we have exactly one, so
    efficiency doesn't matter. We can decide this based on
    simplicity and readability.
    
    And the meaning of the struct value is somewhat unclear. Is
    it always the remote matching remote_name? If remote_name is
    NULL (i.e., no per-branch config), does the struct fall back
    to the "origin" remote, or is it also NULL? These questions
    will get even more tricky with pushremotes, whose fallback
    behavior is more complicated. So let's just store the name,
    which pretty clearly represents the branch.*.remote config.
    Any lookup or fallback behavior can then be implemented in
    helper functions.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit cc969c8dc1cfd1f60b1dab8ee3117172d9d1bb4f
Author: Jeff King <peff@peff.net>
Date:   Wed May 20 03:36:43 2015 -0400

    t5551: factor out tag creation
    
    One of our tests in t5551 creates a large number of tags,
    and jumps through some hoops to do it efficiently. Let's
    factor that out into a function so we can make other similar
    tests.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 84e55dcb344974e99843440c2cfa6df7eb3969ba
Merge: 6cc983d0ad b6e8a3b540
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon May 11 14:23:43 2015 -0700

    Merge branch 'jk/still-interesting'
    
    "git rev-list --objects $old --not --all" to see if everything that
    is reachable from $old is already connected to the existing refs
    was very inefficient.
    
    * jk/still-interesting:
      limit_list: avoid quadratic behavior from still_interesting

commit 6cc983d0adc9fca975d977b4432c4645db9de464
Merge: 66ff763ebb a337292675
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon May 11 14:23:42 2015 -0700

    Merge branch 'jk/reading-packed-refs'
    
    An earlier rewrite to use strbuf_getwholeline() instead of fgets(3)
    to read packed-refs file revealed that the former is unacceptably
    inefficient.
    
    * jk/reading-packed-refs:
      t1430: add another refs-escape test
      read_packed_refs: avoid double-checking sane refs
      strbuf_getwholeline: use getdelim if it is available
      strbuf_getwholeline: avoid calling strbuf_grow
      strbuf_addch: avoid calling strbuf_grow
      config: use getc_unlocked when reading from file
      strbuf_getwholeline: use getc_unlocked
      git-compat-util: add fallbacks for unlocked stdio
      strbuf_getwholeline: use getc macro

commit c6458e60ed0f3e26a1df88bf5a3da8b091b0ce15
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Sat Apr 18 17:47:05 2015 +0700

    index-pack: kill union delta_base to save memory
    
    Once we know the number of objects in the input pack, we allocate an
    array of nr_objects of struct delta_entry. On x86-64, this struct is
    32 bytes long. The union delta_base, which is part of struct
    delta_entry, provides enough space to store either ofs-delta (8 bytes)
    or ref-delta (20 bytes).
    
    Because ofs-delta encoding is more efficient space-wise and more
    performant at runtime than ref-delta encoding, Git packers try to use
    ofs-delta whenever possible, and it is expected that objects encoded
    as ref-delta are minority.
    
    In the best clone case where no ref-delta object is present, we waste
    (20-8) * nr_objects bytes because of this union. That's about 38MB out
    of 100MB for deltas[] with 3.4M objects, or 38%. deltas[] would be
    around 62MB without the waste.
    
    This patch attempts to eliminate that. deltas[] array is split into
    two: one for ofs-delta and one for ref-delta. Many functions are also
    duplicated because of this split. With this patch, ofs_deltas[] array
    takes 51MB. ref_deltas[] should remain unallocated in clone case (0
    bytes). This array grows as we see ref-delta. We save about half in
    this case, or 25% of total bookkeeping.
    
    The saving is more than the calculation above because some padding in
    the old delta_entry struct is removed. ofs_delta_entry is 16 bytes,
    including the 4 bytes padding. That's 13MB for padding, but packing
    the struct could break platforms that do not support unaligned
    access. If someone on 32-bit is really low on memory and only deals
    with packs smaller than 2G, using 32-bit off_t would eliminate the
    padding and save 27MB on top.
    
    A note about ofs_deltas allocation. We could use ref_deltas memory
    allocation strategy for ofs_deltas. But that probably just adds more
    overhead on top. ofs-deltas are generally the majority (1/2 to 2/3) in
    any pack. Incremental realloc may lead to too many memcpy. And if we
    preallocate, say 1/2 or 2/3 of nr_objects initially, the growth rate
    of ALLOC_GROW() could make this array larger than nr_objects, wasting
    more memory.
    
    Brought-up-by: Matthew Sporleder <msporleder@gmail.com>
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 26cb0182b8b2e119f469750b3511fac4624f6667
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Sun Mar 8 17:12:30 2015 +0700

    untracked cache: mark what dirs should be recursed/saved
    
    If we redo this thing in a functional style, we would have one struct
    untracked_dir as input tree and another as output. The input is used
    for verification. The output is a brand new tree, reflecting current
    worktree.
    
    But that means recreate a lot of dir nodes even if a lot could be
    shared between input and output trees in good cases. So we go with the
    messy but efficient way, combining both input and output trees into
    one. We need a way to know which node in this combined tree belongs to
    the output. This is the purpose of this "recurse" flag.
    
    "valid" bit can't be used for this because it's about data of the node
    except the subdirs. When we invalidate a directory, we want to keep
    cached data of the subdirs intact even though we don't really know
    what subdir still exists (yet). Then we check worktree to see what
    actual subdir remains on disk. Those will have 'recurse' bit set
    again. If cached data for those are still valid, we may be able to
    avoid computing exclude files for them. Those subdirs that are deleted
    will have 'recurse' remained clear and their 'valid' bits do not
    matter.
    
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit b5007211b6582fc38647ff695b5ac51541ea9de8
Author: Karsten Blees <blees@dcon.de>
Date:   Thu Nov 27 00:24:01 2014 -0500

    pack-bitmap: do not use gcc packed attribute
    
    The "__attribute__" flag may be a noop on some compilers.
    That's OK as long as the code is correct without the
    attribute, but in this case it is not. We would typically
    end up with a struct that is 2 bytes too long due to struct
    padding, breaking both reading and writing of bitmaps.
    
    Instead of marshalling the data in a struct, let's just
    provide helpers for reading and writing the appropriate
    types. Besides being correct on all platforms, the result is
    more efficient and simpler to read.
    
    Signed-off-by: Karsten Blees <blees@dcon.de>
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 695d95df19b74485be62aba0f978044ff1215ea0
Author: Jeff King <peff@peff.net>
Date:   Thu Nov 20 10:17:05 2014 -0500

    parse_color: refactor color storage
    
    When we parse a color name like "red" into its ANSI color
    value, we pack the storage into a single int that may take
    on many values:
    
      1. If it's "-2", no value has been specified.
    
      2. If it's "-1", the value is "normal" (i.e., no color).
    
      3. If it's 0 through 7, the value is a standard ANSI
         color.
    
      4. If it's larger (up to 255), it is a 256-color extended
         value.
    
    Given these magic numbers, it is often hard to see what is
    going on in the code. Let's refactor this into a struct with
    a flag that tells which scheme we are using, along with a
    numeric value. This is more verbose, but should hopefully be
    simpler to follow. It will also allow us to easily add
    support for more schemes, like 24-bit RGB values.
    
    The result is also slightly less efficient to store, but
    that's OK; we only store this intermediate state during the
    parse, after which we write out the actual ANSI bytes.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7ffa35b0479dac547659f06b8a6ea7d31c57cc05
Author: Eric Wong <normalperson@yhbt.net>
Date:   Fri Oct 31 10:34:03 2014 +0000

    git-svn: use SVN::Ra::get_dir2 when possible
    
    This avoids the following failure with normal "get_dir" on newer
    versions of SVN (tested with SVN 1.8.8-1ubuntu3.1):
    
      Incorrect parameters given: Could not convert '%ld' into a number
    
    get_dir2 also has the potential to be more efficient by requesting
    less data.
    
    ref: <1414636504.45506.YahooMailBasic@web172304.mail.ir2.yahoo.com>
    ref: <1414722617.89476.YahooMailBasic@web172305.mail.ir2.yahoo.com>
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>
    Cc: Hin-Tak Leung <htl10@users.sourceforge.net>

commit 207394908e9465d0169608725aeaa5bb355086e0
Author: Jeff King <peff@peff.net>
Date:   Wed Oct 15 18:43:19 2014 -0400

    traverse_commit_list: support pending blobs/trees with paths
    
    When we call traverse_commit_list, we may have trees and
    blobs in the pending array. As we process these, we pass the
    "name" field from the pending entry as the path of the
    object within the tree (which then becomes the root path if
    we recurse into subtrees).
    
    When we set up the traversal in prepare_revision_walk,
    though, the "name" field of any pending trees and blobs is
    likely to be the ref at which we found the object. We would
    not want to make this part of the path (e.g., doing so would
    make "git rev-list --objects v2.6.11-tree" in linux.git show
    paths like "v2.6.11-tree/Makefile", which is nonsensical).
    Therefore prepare_revision_walk sets the name field of each
    pending tree and blobs to the empty string.
    
    However, this leaves no room for a caller who does know the
    correct path of a pending object to propagate that
    information to the revision walker. We can fix this by
    making two related changes:
    
      1. Use the "path" field as the path instead of the "name"
         field in traverse_commit_list. If the path is not set,
         default to "" (which is what we always ended up with in
         the current code, because of prepare_revision_walk).
    
      2. In prepare_revision_walk, make a complete copy of the
         entry. This makes the path field available to the
         walker (if there is one), solving our problem.
         Leaving the name field intact is now OK, as we do not
         use it as a path due to point (1) above (and we can use
         it to make more meaningful error messages if we want).
         We also make the original "mode" field available to the
         walker, though it does not actually use it.
    
    Note that we still re-add the pending objects and free the
    old ones (so we may strdup the path and name only to free
    the old ones). This could be made more efficient by simply
    copying the object_array entries that we are keeping.
    However, that would require more restructuring of the code,
    and is not done here.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit d0d46abc167d18fdbdbf2ece8ca6df704517c62f
Author: Jeff King <peff@peff.net>
Date:   Wed Oct 15 18:41:53 2014 -0400

    pack-objects: refactor unpack-unreachable expiration check
    
    When we are loosening unreachable packed objects, we do not
    bother to process objects that would simply be pruned
    immediately anyway. The "would be pruned" check is a simple
    comparison, but is about to get more complicated. Let's pull
    it out into a separate function.
    
    Note that this is slightly less efficient than the original,
    which avoided even opening old packs, since no object in
    them could pass the current check, which cares only about
    the pack mtime.  But the new rules will depend on the exact
    object, so we need to perform the check even for old packs.
    
    Note also that we fix a minor buglet when the pack mtime is
    exactly the same as the expiration time. The prune code
    considers that worth pruning, whereas our check here
    considered it worth keeping. This wasn't a big deal. Besides
    being unlikely to happen, the result was simply that the
    object was loosened and then pruned, missing the
    optimization. Still, we can easily fix it while we are here.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1da1e07c835e900337714cfad6c32a8dc0b36ac3
Author: Jeff King <peff@peff.net>
Date:   Wed Oct 15 18:35:12 2014 -0400

    clean up name allocation in prepare_revision_walk
    
    When we enter prepare_revision_walk, we have zero or more
    entries in our "pending" array. We disconnect that array
    from the rev_info, and then process each entry:
    
      1. If the entry is a commit and the --source option is in
         effect, we keep a pointer to the object name.
    
      2. Otherwise, we re-add the item to the pending list with
         a blank name.
    
    We then throw away the old array by freeing the array
    itself, but do not touch the "name" field of each entry. For
    any items of type (2), we leak the memory associated with
    the name. This commit fixes that by calling object_array_clear,
    which handles the cleanup for us.
    
    That breaks (1), though, because it depends on the memory
    pointed to by the name to last forever. We can solve that by
    making a copy of the name. This is slightly less efficient,
    but it shouldn't matter in practice, as we do it only for
    the tip commits of the traversal.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit cbe73331812ed0ac3c3b680ab3aab4e6d22a98ad
Author: Jeff King <peff@peff.net>
Date:   Wed Sep 10 07:11:55 2014 -0400

    refs: speed up is_refname_available
    
    Our filesystem ref storage does not allow D/F conflicts; so
    if "refs/heads/a/b" exists, we do not allow "refs/heads/a"
    to exist (and vice versa). This falls out naturally for
    loose refs, where the filesystem enforces the condition. But
    for packed-refs, we have to make the check ourselves.
    
    We do so by iterating over the entire packed-refs namespace
    and checking whether each name creates a conflict. If you
    have a very large number of refs, this is quite inefficient,
    as you end up doing a large number of comparisons with
    uninteresting bits of the ref tree (e.g., we know that all
    of "refs/tags" is uninteresting in the example above, yet we
    check each entry in it).
    
    Instead, let's take advantage of the fact that we have the
    packed refs stored as a trie of ref_entry structs. We can
    find each component of the proposed refname as we walk
    through the trie, checking for D/F conflicts as we go. For a
    refname of depth N (i.e., 4 in the above example), we only
    have to visit N nodes. And at each visit, we can binary
    search the M names at that level, for a total complexity of
    O(N lg M). ("M" is different at each level, of course, but
    we can take the worst-case "M" as a bound).
    
    In a pathological case of fetching 30,000 fresh refs into a
    repository with 8.5 million refs, this dropped the time to
    run "git fetch" from tens of minutes to ~30s.
    
    This may also help smaller cases in which we check against
    loose refs (which we do when renaming a ref), as we may
    avoid a disk access for unrelated loose directories.
    
    Note that the tests we add appear at first glance to be
    redundant with what is already in t3210. However, the early
    tests are not robust; they are run with reflogs turned on,
    meaning that we are not actually testing
    is_refname_available at all! The operations will still fail
    because the reflogs will hit D/F conflicts in the
    filesystem. To get a true test, we must turn off reflogs
    (but we don't want to do so for the entire script, because
    the point of turning them on was to cover some other cases).
    
    Reviewed-by: Michael Haggerty <mhagger@alum.mit.edu>
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 2e3dfb216991974b60fdb1933eb3331e03383e61
Author: Jeff King <peff@peff.net>
Date:   Tue Aug 26 06:24:20 2014 -0400

    log-tree: use FLEX_ARRAY in name_decoration
    
    We are already using the flex-array technique; let's
    annotate it with our usual FLEX_ARRAY macro. Besides being
    more readable, this is slightly more efficient on compilers
    that understand flex-arrays.
    
    Note that we need to bump the allocation in add_name_decoration,
    which did not explicitly add one byte for the NUL terminator
    of the string we are putting into the flex-array (it did not
    need to before, because the struct itself was over-allocated
    by one byte).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 3bc7a05b1a78b850da94ca85267ca279489ce70f
Author: René Scharfe <l.s.r@web.de>
Date:   Thu Aug 21 20:30:24 2014 +0200

    walker: avoid quadratic list insertion in mark_complete
    
    Similar to 16445242 (fetch-pack: avoid quadratic list insertion in
    mark_complete), sort only after all refs are collected instead of while
    inserting.  The result is the same, but it's more efficient that way.
    The difference will only be measurable in repositories with a large
    number of refs.
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Acked-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e8d1dfe639f71dc957c30c1eaa82a3ef0010cd8e
Author: René Scharfe <l.s.r@web.de>
Date:   Thu Aug 21 20:30:29 2014 +0200

    sha1_name: avoid quadratic list insertion in handle_one_ref
    
    Similar to 16445242 (fetch-pack: avoid quadratic list insertion in
    mark_complete), sort only after all refs are collected instead of while
    inserting.  The result is the same, but it's more efficient that way.
    The difference will only be measurable in repositories with a large
    number of refs.
    
    Signed-off-by: Rene Scharfe <l.s.r@web.de>
    Acked-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 6bf3b813486b4528feca39d599c256f662defc14
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Sat Aug 16 10:08:05 2014 +0700

    diff --stat: mark any file larger than core.bigfilethreshold binary
    
    Too large files may lead to failure to allocate memory. If it happens
    here, it could impact quite a few commands that involve
    diff. Moreover, too large files are inefficient to compare anyway (and
    most likely non-text), so mark them binary and skip looking at their
    content.
    
    Noticed-by: Dale R. Worley <worley@alum.mit.edu>
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 35480f0b23f2c1824109ddae24392a70d19c6b9c
Author: Jeff King <peff@peff.net>
Date:   Mon Jun 30 12:57:51 2014 -0400

    add strip_suffix function
    
    Many callers of ends_with want to not only find out whether
    a string has a suffix, but want to also strip it off. Doing
    that separately has two minor problems:
    
      1. We often run over the string twice (once to find
         the suffix, and then once more to find its length to
         subtract the suffix length).
    
      2. We have to specify the suffix length again, which means
         either a magic number, or repeating ourselves with
         strlen("suffix").
    
    Just as we have skip_prefix to avoid these cases with
    starts_with, we can add a strip_suffix to avoid them with
    ends_with.
    
    Note that we add two forms of strip_suffix here: one that
    takes a string, with the resulting length as an
    out-parameter; and one that takes a pointer/length pair, and
    reuses the length as an out-parameter. The latter is more
    efficient when the caller already has the length (e.g., when
    using strbufs), but it can be easy to confuse the two, as
    they take the same number and types of parameters.
    
    For that reason, the "mem" form puts its length parameter
    next to the buffer (since they are a pair), and the string
    form puts it at the end (since it is an out-parameter). The
    compiler can notice when you get the order wrong, which
    should help prevent writing one when you meant the other.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit c4f79d13b93a7ce53c932c3970403d8f10904d30
Merge: ada8710e63 e6bea66db6
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Jun 25 11:49:01 2014 -0700

    Merge branch 'jl/remote-rm-prune' into maint
    
    "git remote rm" and "git remote prune" can involve removing many
    refs at once, which is not a very efficient thing to do when very
    many refs exist in the packed-refs file.
    
    * jl/remote-rm-prune:
      remote prune: optimize "dangling symref" check/warning
      remote: repack packed-refs once when deleting multiple refs
      remote rm: delete remote configuration as the last

commit 474df928b11975d8c0ef532a3b61e1b86143406a
Merge: 5cf2c571d0 e6bea66db6
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon Jun 16 12:17:58 2014 -0700

    Merge branch 'jl/remote-rm-prune'
    
    "git remote rm" and "git remote prune" can involve removing many
    refs at once, which is not a very efficient thing to do when very
    many refs exist in the packed-refs file.
    
    * jl/remote-rm-prune:
      remote prune: optimize "dangling symref" check/warning
      remote: repack packed-refs once when deleting multiple refs
      remote rm: delete remote configuration as the last

commit 88d5a6f6cd1b63e1637027322cdfdbeefe38c3ed
Author: Jeff King <peff@peff.net>
Date:   Thu May 22 05:44:09 2014 -0400

    daemon/config: factor out duplicate xstrdup_tolower
    
    We have two implementations of the same function; let's drop
    that to one. We take the name from daemon.c, but the
    implementation (which is just slightly more efficient) from
    the config code.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 750b2e4785e5956122b3c565af65eb1929714fba
Author: Jeff King <peff@peff.net>
Date:   Mon Apr 28 12:16:31 2014 -0400

    t3910: show failure of core.precomposeunicode with decomposed filenames
    
    If you have existing decomposed filenames in your git
    repository (e.g., that were created with older versions of
    git that did not precompose unicode), a modern git with
    core.precomposeunicode set does not handle them well.
    
    The problem is that we normalize the paths coming from the
    disk into their precomposed form, and then compare them
    against the literal bytes in the index. This makes things
    better if you have the precomposed form in the index. It
    makes things worse if you actually have the decomposed form
    in the index.
    
    As a result, paths with decomposed filenames may have their
    precomposed variants listed as untracked files (even though
    the precomposed variants do not exist on-disk at all).
    
    This patch just adds a test to demonstrate the breakage.
    Some possible fixes are:
    
      1. Tell everyone that NFD in the git repo is wrong, and
         they should make a new commit to normalize all their
         in-repo files to be precomposed.
    
         This is probably not the right thing to do, because it
         still doesn't fix checkouts of old history. And it
         spreads the problem to people on byte-preserving
         filesystems (like ext4), because now they have to start
         precomposing their filenames as they are adde to git.
    
      2. Do all index filename comparisons using a UTF-8 aware
         comparison function when core.precomposeunicode is set.
         This would probably have bad performance, and somewhat
         defeats the point of converting the filenames at the
         readdir level in the first place.
    
      3. Convert index filenames to their precomposed form when
         we read the index from disk. This would be efficient,
         but we would have to be careful not to write the
         precomposed forms back out to disk.
    
      4. Introduce some infrastructure to efficiently match up
         the precomposed/decomposed forms. We already do
         something similar for case-insensitive files using
         name-hash.c. We might be able to adapt that strategy
         here.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 72441af7c4e3bde33cdf7edafcf09c227d5d5296
Author: Kirill Smelkov <kirr@mns.spb.ru>
Date:   Mon Apr 7 01:46:26 2014 +0400

    tree-diff: rework diff_tree() to generate diffs for multiparent cases as well
    
    Previously diff_tree(), which is now named ll_diff_tree_sha1(), was
    generating diff_filepair(s) for two trees t1 and t2, and that was
    usually used for a commit as t1=HEAD~, and t2=HEAD - i.e. to see changes
    a commit introduces.
    
    In Git, however, we have fundamentally built flexibility in that a
    commit can have many parents - 1 for a plain commit, 2 for a simple merge,
    but also more than 2 for merging several heads at once.
    
    For merges there is a so called combine-diff, which shows diff, a merge
    introduces by itself, omitting changes done by any parent. That works
    through first finding paths, that are different to all parents, and then
    showing generalized diff, with separate columns for +/- for each parent.
    The code lives in combine-diff.c .
    
    There is an impedance mismatch, however, in that a commit could
    generally have any number of parents, and that while diffing trees, we
    divide cases for 2-tree diffs and more-than-2-tree diffs. I mean there
    is no special casing for multiple parents commits in e.g.
    revision-walker .
    
    That impedance mismatch *hurts* *performance* *badly* for generating
    combined diffs - in "combine-diff: optimize combine_diff_path
    sets intersection" I've already removed some slowness from it, but from
    the timings provided there, it could be seen, that combined diffs still
    cost more than an order of magnitude more cpu time, compared to diff for
    usual commits, and that would only be an optimistic estimate, if we take
    into account that for e.g. linux.git there is only one merge for several
    dozens of plain commits.
    
    That slowness comes from the fact that currently, while generating
    combined diff, a lot of time is spent computing diff(commit,commit^2)
    just to only then intersect that huge diff to almost small set of files
    from diff(commit,commit^1).
    
    That's because at present, to compute combine-diff, for first finding
    paths, that "every parent touches", we use the following combine-diff
    property/definition:
    
    D(A,P1...Pn) = D(A,P1) ^ ... ^ D(A,Pn)      (w.r.t. paths)
    
    where
    
    D(A,P1...Pn) is combined diff between commit A, and parents Pi
    
    and
    
    D(A,Pi) is usual two-tree diff Pi..A
    
    So if any of that D(A,Pi) is huge, tracting 1 n-parent combine-diff as n
    1-parent diffs and intersecting results will be slow.
    
    And usually, for linux.git and other topic-based workflows, that
    D(A,P2) is huge, because, if merge-base of A and P2, is several dozens
    of merges (from A, via first parent) below, that D(A,P2) will be diffing
    sum of merges from several subsystems to 1 subsystem.
    
    The solution is to avoid computing n 1-parent diffs, and to find
    changed-to-all-parents paths via scanning A's and all Pi's trees
    simultaneously, at each step comparing their entries, and based on that
    comparison, populate paths result, and deduce we could *skip*
    *recursing* into subdirectories, if at least for 1 parent, sha1 of that
    dir tree is the same as in A. That would save us from doing significant
    amount of needless work.
    
    Such approach is very similar to what diff_tree() does, only there we
    deal with scanning only 2 trees simultaneously, and for n+1 tree, the
    logic is a bit more complex:
    
    D(T,P1...Pn) calculation scheme
    -------------------------------
    
    D(T,P1...Pn) = D(T,P1) ^ ... ^ D(T,Pn)  (regarding resulting paths set)
    
        D(T,Pj)             - diff between T..Pj
        D(T,P1...Pn)        - combined diff from T to parents P1,...,Pn
    
    We start from all trees, which are sorted, and compare their entries in
    lock-step:
    
         T     P1       Pn
         -     -        -
        |t|   |p1|     |pn|
        |-|   |--| ... |--|      imin = argmin(p1...pn)
        | |   |  |     |  |
        |-|   |--|     |--|
        |.|   |. |     |. |
         .     .        .
         .     .        .
    
    at any time there could be 3 cases:
    
        1)  t < p[imin];
        2)  t > p[imin];
        3)  t = p[imin].
    
    Schematic deduction of what every case means, and what to do, follows:
    
    1)  t < p[imin]  ->  ∀j t ∉ Pj  ->  "+t" ∈ D(T,Pj)  ->  D += "+t";  t↓
    
    2)  t > p[imin]
    
        2.1) ∃j: pj > p[imin]  ->  "-p[imin]" ∉ D(T,Pj)  ->  D += ø;  ∀ pi=p[imin]  pi↓
        2.2) ∀i  pi = p[imin]  ->  pi ∉ T  ->  "-pi" ∈ D(T,Pi)  ->  D += "-p[imin]";  ∀i pi↓
    
    3)  t = p[imin]
    
        3.1) ∃j: pj > p[imin]  ->  "+t" ∈ D(T,Pj)  ->  only pi=p[imin] remains to investigate
        3.2) pi = p[imin]  ->  investigate δ(t,pi)
         |
         |
         v
    
        3.1+3.2) looking at δ(t,pi) ∀i: pi=p[imin] - if all != ø  ->
    
                          ⎧δ(t,pi)  - if pi=p[imin]
                 ->  D += ⎨
                          ⎩"+t"     - if pi>p[imin]
    
        in any case t↓  ∀ pi=p[imin]  pi↓
    
    ~
    
    For comparison, here is how diff_tree() works:
    
    D(A,B) calculation scheme
    -------------------------
    
        A     B
        -     -
       |a|   |b|    a < b   ->  a ∉ B   ->   D(A,B) +=  +a    a↓
       |-|   |-|    a > b   ->  b ∉ A   ->   D(A,B) +=  -b    b↓
       | |   | |    a = b   ->  investigate δ(a,b)            a↓ b↓
       |-|   |-|
       |.|   |.|
        .     .
        .     .
    
    ~~~~~~~~
    
    This patch generalizes diff tree-walker to work with arbitrary number of
    parents as described above - i.e. now there is a resulting tree t, and
    some parents trees tp[i] i=[0..nparent). The generalization builds on
    the fact that usual diff
    
    D(A,B)
    
    is by definition the same as combined diff
    
    D(A,[B]),
    
    so if we could rework the code for common case and make it be not slower
    for nparent=1 case, usual diff(t1,t2) generation will not be slower, and
    multiparent diff tree-walker would greatly benefit generating
    combine-diff.
    
    What we do is as follows:
    
    1) diff tree-walker ll_diff_tree_sha1() is internally reworked to be
       a paths generator (new name diff_tree_paths()), with each generated path
       being `struct combine_diff_path` with info for path, new sha1,mode and for
       every parent which sha1,mode it was in it.
    
    2) From that info, we can still generate usual diff queue with
       struct diff_filepairs, via "exporting" generated
       combine_diff_path, if we know we run for nparent=1 case.
       (see emit_diff() which is now named emit_diff_first_parent_only())
    
    3) In order for diff_can_quit_early(), which checks
    
           DIFF_OPT_TST(opt, HAS_CHANGES))
    
       to work, that exporting have to be happening not in bulk, but
       incrementally, one diff path at a time.
    
       For such consumers, there is a new callback in diff_options
       introduced:
    
           ->pathchange(opt, struct combine_diff_path *)
    
       which, if set to !NULL, is called for every generated path.
    
       (see new compat ll_diff_tree_sha1() wrapper around new paths
        generator for setup)
    
    4) The paths generation itself, is reworked from previous
       ll_diff_tree_sha1() code according to "D(A,P1...Pn) calculation
       scheme" provided above:
    
       On the start we allocate [nparent] arrays in place what was
       earlier just for one parent tree.
    
       then we just generalize loops, and comparison according to the
       algorithm.
    
    Some notes(*):
    
    1) alloca(), for small arrays, is used for "runs not slower for
       nparent=1 case than before" goal - if we change it to xmalloc()/free()
       the timings get ~1% worse. For alloca() we use just-introduced
       xalloca/xalloca_free compatibility wrappers, so it should not be a
       portability problem.
    
    2) For every parent tree, we need to keep a tag, whether entry from that
       parent equals to entry from minimal parent. For performance reasons I'm
       keeping that tag in entry's mode field in unused bit - see S_IFXMIN_NEQ.
       Not doing so, we'd need to alloca another [nparent] array, which hurts
       performance.
    
    3) For emitted paths, memory could be reused, if we know the path was
       processed via callback and will not be needed later. We use efficient
       hand-made realloc-style path_appendnew(), that saves us from ~1-1.5%
       of potential additional slowdown.
    
    4) goto(s) are used in several places, as the code executes a little bit
       faster with lowered register pressure.
    
    Also
    
    - we should now check for FIND_COPIES_HARDER not only when two entries
      names are the same, and their hashes are equal, but also for a case,
      when a path was removed from some of all parents having it.
    
      The reason is, if we don't, that path won't be emitted at all (see
      "a > xi" case), and we'll just skip it, and FIND_COPIES_HARDER wants
      all paths - with diff or without - to be emitted, to be later analyzed
      for being copies sources.
    
      The new check is only necessary for nparent >1, as for nparent=1 case
      xmin_eqtotal always =1 =nparent, and a path is always added to diff as
      removal.
    
    ~~~~~~~~
    
    Timings for
    
        # without -c, i.e. testing only nparent=1 case
        `git log --raw --no-abbrev --no-renames`
    
    before and after the patch are as follows:
    
                    navy.git        linux.git v3.10..v3.11
    
        before      0.611s          1.889s
        after       0.619s          1.907s
        slowdown    1.3%            0.9%
    
    This timings show we did no harm to usual diff(tree1,tree2) generation.
    From the table we can see that we actually did ~1% slowdown, but I think
    I've "earned" that 1% in the previous patch ("tree-diff: reuse base
    str(buf) memory on sub-tree recursion", HEAD~~) so for nparent=1 case,
    net timings stays approximately the same.
    
    The output also stayed the same.
    
    (*) If we revert 1)-4) to more usual techniques, for nparent=1 case,
        we'll get ~2-2.5% of additional slowdown, which I've tried to avoid, as
       "do no harm for nparent=1 case" rule.
    
    For linux.git, combined diff will run an order of magnitude faster and
    appropriate timings will be provided in the next commit, as we'll be
    taking advantage of the new diff tree-walker for combined-diff
    generation there.
    
    P.S. and combined diff is not some exotic/for-play-only stuff - for
    example for a program I write to represent Git archives as readonly
    filesystem, there is initial scan with
    
        `git log --reverse --raw --no-abbrev --no-renames -c`
    
    to extract log of what was created/changed when, as a result building a
    map
    
        {}  sha1    ->  in which commit (and date) a content was added
    
    that `-c` means also show combined diff for merges, and without them, if
    a merge is non-trivial (merges changes from two parents with both having
    separate changes to a file), or an evil one, the map will not be full,
    i.e. some valid sha1 would be absent from it.
    
    That case was my initial motivation for combined diffs speedup.
    
    Signed-off-by: Kirill Smelkov <kirr@mns.spb.ru>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1a6d8b91489ad4a7b7d267b46a3e838b004157f1
Author: Jeff King <peff@peff.net>
Date:   Wed Jan 15 06:17:48 2014 -0500

    do not discard revindex when re-preparing packfiles
    
    When an object lookup fails, we re-read the objects/pack
    directory to pick up any new packfiles that may have been
    created since our last read. We also discard any pack
    revindex structs we've allocated.
    
    The discarding is a problem for the pack-bitmap code, which keeps
    a pointer to the revindex for the bitmapped pack. After the
    discard, the pointer is invalid, and we may read free()d
    memory.
    
    Other revindex users do not keep a bare pointer to the
    revindex; instead, they always access it through
    revindex_for_pack(), which lazily builds the revindex. So
    one solution is to teach the pack-bitmap code a similar
    trick. It would be slightly less efficient, but probably not
    all that noticeable.
    
    However, it turns out this discarding is not actually
    necessary. When we call reprepare_packed_git, we do not
    throw away our old pack list. We keep the existing entries,
    and only add in new ones. So there is no safety problem; we
    will still have the pack struct that matches each revindex.
    The packfile itself may go away, of course, but we are
    already prepared to handle that, and it may happen outside
    of reprepare_packed_git anyway.
    
    Throwing away the revindex may save some RAM if the pack
    never gets reused (about 12 bytes per object). But it also
    wastes some CPU time (to regenerate the index) if the pack
    does get reused. It's hard to say which is more valuable,
    but in either case, it happens very rarely (only when we
    race with a simultaneous repack). Just leaving the revindex
    in place is simple and safe both for current and future
    code.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7cc8f9710857ed69d34c696330f7fd0367a5a29c
Author: Vicent Marti <tanoku@gmail.com>
Date:   Sat Dec 21 09:00:16 2013 -0500

    pack-objects: implement bitmap writing
    
    This commit extends more the functionality of `pack-objects` by allowing
    it to write out a `.bitmap` index next to any written packs, together
    with the `.idx` index that currently gets written.
    
    If bitmap writing is enabled for a given repository (either by calling
    `pack-objects` with the `--write-bitmap-index` flag or by having
    `pack.writebitmaps` set to `true` in the config) and pack-objects is
    writing a packfile that would normally be indexed (i.e. not piping to
    stdout), we will attempt to write the corresponding bitmap index for the
    packfile.
    
    Bitmap index writing happens after the packfile and its index has been
    successfully written to disk (`finish_tmp_packfile`). The process is
    performed in several steps:
    
        1. `bitmap_writer_set_checksum`: this call stores the partial
           checksum for the packfile being written; the checksum will be
           written in the resulting bitmap index to verify its integrity
    
        2. `bitmap_writer_build_type_index`: this call uses the array of
           `struct object_entry` that has just been sorted when writing out
           the actual packfile index to disk to generate 4 type-index bitmaps
           (one for each object type).
    
           These bitmaps have their nth bit set if the given object is of
           the bitmap's type. E.g. the nth bit of the Commits bitmap will be
           1 if the nth object in the packfile index is a commit.
    
           This is a very cheap operation because the bitmap writing code has
           access to the metadata stored in the `struct object_entry` array,
           and hence the real type for each object in the packfile.
    
        3. `bitmap_writer_reuse_bitmaps`: if there exists an existing bitmap
           index for one of the packfiles we're trying to repack, this call
           will efficiently rebuild the existing bitmaps so they can be
           reused on the new index. All the existing bitmaps will be stored
           in a `reuse` hash table, and the commit selection phase will
           prioritize these when selecting, as they can be written directly
           to the new index without having to perform a revision walk to
           fill the bitmap. This can greatly speed up the repack of a
           repository that already has bitmaps.
    
        4. `bitmap_writer_select_commits`: if bitmap writing is enabled for
           a given `pack-objects` run, the sequence of commits generated
           during the Counting Objects phase will be stored in an array.
    
           We then use that array to build up the list of selected commits.
           Writing a bitmap in the index for each object in the repository
           would be cost-prohibitive, so we use a simple heuristic to pick
           the commits that will be indexed with bitmaps.
    
           The current heuristics are a simplified version of JGit's
           original implementation. We select a higher density of commits
           depending on their age: the 100 most recent commits are always
           selected, after that we pick 1 commit of each 100, and the gap
           increases as the commits grow older. On top of that, we make sure
           that every single branch that has not been merged (all the tips
           that would be required from a clone) gets their own bitmap, and
           when selecting commits between a gap, we tend to prioritize the
           commit with the most parents.
    
           Do note that there is no right/wrong way to perform commit
           selection; different selection algorithms will result in
           different commits being selected, but there's no such thing as
           "missing a commit". The bitmap walker algorithm implemented in
           `prepare_bitmap_walk` is able to adapt to missing bitmaps by
           performing manual walks that complete the bitmap: the ideal
           selection algorithm, however, would select the commits that are
           more likely to be used as roots for a walk in the future (e.g.
           the tips of each branch, and so on) to ensure a bitmap for them
           is always available.
    
        5. `bitmap_writer_build`: this is the computationally expensive part
           of bitmap generation. Based on the list of commits that were
           selected in the previous step, we perform several incremental
           walks to generate the bitmap for each commit.
    
           The walks begin from the oldest commit, and are built up
           incrementally for each branch. E.g. consider this dag where A, B,
           C, D, E, F are the selected commits, and a, b, c, e are a chunk
           of simplified history that will not receive bitmaps.
    
                A---a---B--b--C--c--D
                         \
                          E--e--F
    
           We start by building the bitmap for A, using A as the root for a
           revision walk and marking all the objects that are reachable
           until the walk is over. Once this bitmap is stored, we reuse the
           bitmap walker to perform the walk for B, assuming that once we
           reach A again, the walk will be terminated because A has already
           been SEEN on the previous walk.
    
           This process is repeated for C, and D, but when we try to
           generate the bitmaps for E, we can reuse neither the current walk
           nor the bitmap we have generated so far.
    
           What we do now is resetting both the walk and clearing the
           bitmap, and performing the walk from scratch using E as the
           origin. This new walk, however, does not need to be completed.
           Once we hit B, we can lookup the bitmap we have already stored
           for that commit and OR it with the existing bitmap we've composed
           so far, allowing us to limit the walk early.
    
           After all the bitmaps have been generated, another iteration
           through the list of commits is performed to find the best XOR
           offsets for compression before writing them to disk. Because of
           the incremental nature of these bitmaps, XORing one of them with
           its predecesor results in a minimal "bitmap delta" most of the
           time. We can write this delta to the on-disk bitmap index, and
           then re-compose the original bitmaps by XORing them again when
           loaded.
    
           This is a phase very similar to pack-object's `find_delta` (using
           bitmaps instead of objects, of course), except the heuristics
           have been greatly simplified: we only check the 10 bitmaps before
           any given one to find best compressing one. This gives good
           results in practice, because there is locality in the ordering of
           the objects (and therefore bitmaps) in the packfile.
    
         6. `bitmap_writer_finish`: the last step in the process is
            serializing to disk all the bitmap data that has been generated
            in the two previous steps.
    
            The bitmap is written to a tmp file and then moved atomically to
            its final destination, using the same process as
            `pack-write.c:write_idx_file`.
    
    Signed-off-by: Vicent Marti <tanoku@gmail.com>
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e1273106f62927e3efdb1cfa107cb1a9f913274c
Author: Vicent Marti <tanoku@gmail.com>
Date:   Thu Nov 14 07:43:51 2013 -0500

    ewah: compressed bitmap implementation
    
    EWAH is a word-aligned compressed variant of a bitset (i.e. a data
    structure that acts as a 0-indexed boolean array for many entries).
    
    It uses a 64-bit run-length encoding (RLE) compression scheme,
    trading some compression for better processing speed.
    
    The goal of this word-aligned implementation is not to achieve
    the best compression, but rather to improve query processing time.
    As it stands right now, this EWAH implementation will always be more
    efficient storage-wise than its uncompressed alternative.
    
    EWAH arrays will be used as the on-disk format to store reachability
    bitmaps for all objects in a repository while keeping reasonable sizes,
    in the same way that JGit does.
    
    This EWAH implementation is a mostly straightforward port of the
    original `javaewah` library that JGit currently uses. The library is
    self-contained and has been embedded whole (4 files) inside the `ewah`
    folder to ease redistribution.
    
    The library is re-licensed under the GPLv2 with the permission of Daniel
    Lemire, the original author. The source code for the C version can
    be found on GitHub:
    
            https://github.com/vmg/libewok
    
    The original Java implementation can also be found on GitHub:
    
            https://github.com/lemire/javaewah
    
    [jc: stripped debug-only code per Peff's $gmane/239768]
    
    Signed-off-by: Vicent Marti <tanoku@gmail.com>
    Signed-off-by: Jeff King <peff@peff.net>
    Helped-by: Ramsay Jones <ramsay@ramsay1.demon.co.uk>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 5d642e75069334944fcc795a80cf04749dd12857
Author: Jeff King <peff@peff.net>
Date:   Sat Dec 21 09:24:20 2013 -0500

    sha1_object_info_extended: provide delta base sha1s
    
    A caller of sha1_object_info_extended technically has enough
    information to determine the base sha1 from the results of
    the call. It knows the pack, offset, and delta type of the
    object, which is sufficient to find the base.
    
    However, the functions to do so are not publicly available,
    and the code itself is intimate enough with the pack details
    that it should be abstracted away. We could add a public
    helper to allow callers to query the delta base separately,
    but it is simpler and slightly more efficient to optionally
    grab it along with the rest of the object_info data.
    
    For cases where the object is not stored as a delta, we
    write the null sha1 into the query field. A careful caller
    could check "oi.whence == OI_PACKED && oi.u.packed.is_delta"
    before looking at the base sha1, but using the null sha1
    provides a simple alternative (and gives a better sanity
    check for a non-careful caller than simply returning random
    bytes).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1190a1acf800acdcfd7569f87ac1560e2d077414
Author: Jeff King <peff@peff.net>
Date:   Thu Dec 5 15:28:07 2013 -0500

    pack-objects: name pack files after trailer hash
    
    Our current scheme for naming packfiles is to calculate the
    sha1 hash of the sorted list of objects contained in the
    packfile. This gives us a unique name, so we are reasonably
    sure that two packs with the same name will contain the same
    objects.
    
    It does not, however, tell us that two such packs have the
    exact same bytes. This makes things awkward if we repack the
    same set of objects. Due to run-to-run variations, the bytes
    may not be identical (e.g., changed zlib or git versions,
    different source object reuse due to new packs in the
    repository, or even different deltas due to races during a
    multi-threaded delta search).
    
    In theory, this could be helpful to a program that cares
    that the packfile contains a certain set of objects, but
    does not care about the particular representation. In
    practice, no part of git makes use of that, and in many
    cases it is potentially harmful. For example, if a dumb http
    client fetches the .idx file, it must be sure to get the
    exact .pack that matches it. Similarly, a partial transfer
    of a .pack file cannot be safely resumed, as the actual
    bytes may have changed.  This could also affect a local
    client which opened the .idx and .pack files, closes the
    .pack file (due to memory or file descriptor limits), and
    then re-opens a changed packfile.
    
    In all of these cases, git can detect the problem, as we
    have the sha1 of the bytes themselves in the pack trailer
    (which we verify on transfer), and the .idx file references
    the trailer from the matching packfile. But it would be
    simpler and more efficient to actually get the correct
    bytes, rather than noticing the problem and having to
    restart the operation.
    
    This patch simply uses the pack trailer sha1 as the pack
    name. It should be similarly unique, but covers the exact
    representation of the objects. Other parts of git should not
    care, as the pack name is returned by pack-objects and is
    essentially opaque.
    
    One test needs to be updated, because it actually corrupts a
    pack and expects that re-packing the corrupted bytes will
    use the same name. It won't anymore, but we can easily just
    use the name that pack-objects hands back.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 92e5c77c3791567e299cc858a7ddfed410e2cec5
Author: Vicent Marti <tanoku@gmail.com>
Date:   Thu Oct 24 14:00:36 2013 -0400

    revindex: export new APIs
    
    Allow users to efficiently lookup consecutive entries that are expected
    to be found on the same revindex by exporting `find_revindex_position`:
    this function takes a pointer to revindex itself, instead of looking up
    the proper revindex for a given packfile on each call.
    
    Signed-off-by: Vicent Marti <tanoku@gmail.com>
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e03a5010b3fc17a1d559ae0fdbcdad63af8e30c9
Merge: 74051fa805 680be044d9
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Oct 23 13:33:08 2013 -0700

    Merge branch 'jc/ls-files-killed-optim' into maint
    
    "git ls-files -k" needs to crawl only the part of the working tree
    that may overlap the paths in the index to find killed files, but
    shared code with the logic to find all the untracked files, which
    made it unnecessarily inefficient.
    
    * jc/ls-files-killed-optim:
      dir.c::test_one_path(): work around directory_exists_in_index_icase() breakage
      t3010: update to demonstrate "ls-files -k" optimization pitfalls
      ls-files -k: a directory only can be killed if the index has a non-directory
      dir.c: use the cache_* macro to access the current index

commit c93c92f30977adb2eb385a851f9f5e9975da7d5e
Author: Jeff King <peff@peff.net>
Date:   Sat Sep 28 04:34:05 2013 -0400

    http: update base URLs when we see redirects
    
    If a caller asks the http_get_* functions to go to a
    particular URL and we end up elsewhere due to a redirect,
    the effective_url field can tell us where we went.
    
    It would be nice to remember this redirect and short-cut
    further requests for two reasons:
    
      1. It's more efficient. Otherwise we spend an extra http
         round-trip to the server for each subsequent request,
         just to get redirected.
    
      2. If we end up with an http 401 and are going to ask for
         credentials, it is to feed them to the redirect target.
         If the redirect is an http->https upgrade, this means
         our credentials may be provided on the http leg, just
         to end up redirected to https. And if the redirect
         crosses server boundaries, then curl will drop the
         credentials entirely as it follows the redirect.
    
    However, it, it is not enough to simply record the effective
    URL we saw and use that for subsequent requests. We were
    originally fed a "base" url like:
    
       http://example.com/foo.git
    
    and we want to figure out what the new base is, even though
    the URLs we see may be:
    
         original: http://example.com/foo.git/info/refs
        effective: http://example.com/bar.git/info/refs
    
    Subsequent requests will not be for "info/refs", but for
    other paths relative to the base. We must ask the caller to
    pass in the original base, and we must pass the redirected
    base back to the caller (so that it can generate more URLs
    from it). Furthermore, we need to feed the new base to the
    credential code, so that requests to credential helpers (or
    to the user) match the URL we will be requesting.
    
    This patch teaches http_request_reauth to do this munging.
    Since it is the caller who cares about making more URLs, it
    seems at first glance that callers could simply check
    effective_url themselves and handle it. However, since we
    need to update the credential struct before the second
    re-auth request, we have to do it inside http_request_reauth.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Jonathan Nieder <jrnieder@gmail.com>

commit 4c4d9d9b654db3eecb6e1107e814a737eafce0d6
Merge: 135be1ee2b 680be044d9
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Sep 11 15:03:28 2013 -0700

    Merge branch 'jc/ls-files-killed-optim'
    
    "git ls-files -k" needs to crawl only the part of the working tree
    that may overlap the paths in the index to find killed files, but
    shared code with the logic to find all the untracked files, which
    made it unnecessarily inefficient.
    
    * jc/ls-files-killed-optim:
      dir.c::test_one_path(): work around directory_exists_in_index_icase() breakage
      t3010: update to demonstrate "ls-files -k" optimization pitfalls
      ls-files -k: a directory only can be killed if the index has a non-directory
      dir.c: use the cache_* macro to access the current index

commit c587d655122a287a6796d493d176128f05374156
Author: Felipe Contreras <felipe.contreras@gmail.com>
Date:   Thu Aug 29 17:29:50 2013 -0500

    remote-hg: use notes to keep track of Hg revisions
    
    Keep track of Mercurial revisions as Git notes under the 'refs/notes/hg'
    ref.  This way, the user can easily see which Mercurial revision
    corresponds to certain Git commit.
    
    Unfortunately, there's no way to efficiently update the notes after
    doing an export (push), so they'll have to be updated when importing
    (fetching).
    
    Signed-off-by: Felipe Contreras <felipe.contreras@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f972a1658a30809db113a3c486b1fe95b56633bf
Author: Jeff King <peff@peff.net>
Date:   Tue Aug 27 21:41:39 2013 -0400

    mailmap: handle mailmap blobs without trailing newlines
    
    The read_mailmap_buf function reads each line of the mailmap
    using strchrnul, like:
    
        const char *end = strchrnul(buf, '\n');
        unsigned long linelen = end - buf + 1;
    
    But that's off-by-one when we actually hit the NUL byte; our
    line does not have a terminator, and so is only "end - buf"
    bytes long. As a result, when we subtract the linelen from
    the total len, we end up with (unsigned long)-1 bytes left
    in the buffer, and we start reading random junk from memory.
    
    We could fix it with:
    
        unsigned long linelen = end - buf + !!*end;
    
    but let's take a step back for a moment. It's questionable
    in the first place for a function that takes a buffer and
    length to be using strchrnul. But it works because we only
    have one caller (and are only likely to ever have this one),
    which is handing us data from read_sha1_file. Which means
    that it's always NUL-terminated.
    
    Instead of tightening the assumptions to make the
    buffer/length pair work for a caller that doesn't actually
    exist, let's let loosen the assumptions to what the real
    caller has: a modifiable, NUL-terminated string.
    
    This makes the code simpler and shorter (because we don't
    have to correlate strchrnul with the length calculation),
    correct (because the code with the off-by-one just goes
    away), and more efficient (we can drop the extra allocation
    we needed to create NUL-terminated strings for each line,
    and just terminate in place).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit fbd4a7036dfa71ec89e7c441cef1ac9aaa59a315
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Fri Aug 16 16:52:07 2013 +0700

    list-objects: mark more commits as edges in mark_edges_uninteresting
    
    The purpose of edge commits is to let pack-objects know what objects
    it can use as base, but does not need to include in the thin pack
    because the other side is supposed to already have them. So far we
    mark uninteresting parents of interesting commits as edges. But even
    an unrelated uninteresting commit (that the other side has) may
    become a good base for pack-objects and help produce more efficient
    packs.
    
    This is especially true for shallow clone, when the client issues a
    fetch with a depth smaller or equal to the number of commits the
    server is ahead of the client. For example, in this commit history
    the client has up to "A" and the server has up to "B":
    
        -------A---B
         have--^   ^
                  /
           want--+
    
    If depth 1 is requested, the commit list to send to the client
    includes only B. The way m_e_u is working, it checks if parent
    commits of B are uninteresting, if so mark them as edges.  Due to
    shallow effect, commit B is grafted to have no parents and the
    revision walker never sees A as the parent of B. In fact it marks no
    edges at all in this simple case and sends everything B has to the
    client even if it could have excluded what A and also the client
    already have.
    
    In a slightly different case where A is not a direct parent of B
    (iow there are commits in between A and B), marking A as an edge can
    still save some because B may still have stuff from the far ancestor
    A.
    
    There is another case from the earlier patch, when we deepen a ref
    from C->E to A->E:
    
        ---A---B   C---D---E
         want--^   ^       ^
           shallow-+      /
              have-------+
    
    In this case we need to send A and B to the client, and C (i.e. the
    current shallow point that the client informs the server) is a very
    good base because it's closet to A and B. Normal m_e_u won't recognize
    C as an edge because it only looks back to parents (i.e. A<-B) not the
    opposite way B->C even if C is already marked as uninteresting commit
    by the previous patch.
    
    This patch includes all uninteresting commits from command line as
    edges and lets pack-objects decide what's best to do. The upside is we
    have better chance of producing better packs in certain cases. The
    downside is we may need to process some extra objects on the server
    side.
    
    For the shallow case on git.git, when the client is 5 commits behind
    and does "fetch --depth=3", the result pack is 99.26 KiB instead of
    4.92 MiB.
    
    Reported-and-analyzed-by: Matthijs Kooijman <matthijs@stdin.nl>
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit cdab485853b405d6454d4974bdc3825134d85249
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Fri Aug 16 16:52:05 2013 +0700

    upload-pack: delegate rev walking in shallow fetch to pack-objects
    
    upload-pack has a special revision walking code for shallow
    recipients. It works almost like the similar code in pack-objects
    except:
    
    1. in upload-pack, graft points could be added for deepening;
    
    2. also when the repository is deepened, the shallow point will be
       moved further away from the tip, but the old shallow point will be
       marked as edge to produce more efficient packs. See 6523078 (make
       shallow repository deepening more network efficient - 2009-09-03).
    
    Pass the file to pack-objects via --shallow-file. This will override
    $GIT_DIR/shallow and give pack-objects the exact repository shape
    that upload-pack has.
    
    mark edge commits by revision command arguments. Even if old shallow
    points are passed as "--not" revisions as in this patch, they will not
    be picked up by mark_edges_uninteresting() because this function looks
    up to parents for edges, while in this case the edge is the children,
    in the opposite direction. This will be fixed in an later patch when
    all given uninteresting commits are marked as edges.
    
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 012b32bb46459f96509669c2f5be0a93a95a2b43
Author: Jeff King <peff@peff.net>
Date:   Wed Jul 10 07:50:26 2013 -0400

    pack-revindex: use unsigned to store number of objects
    
    A packfile may have up to 2^32-1 objects in it, so the
    "right" data type to use is uint32_t. We currently use a
    signed int, which means that we may behave incorrectly for
    packfiles with more than 2^31-1 objects on 32-bit systems.
    
    Nobody has noticed because having 2^31 objects is pretty
    insane. The linux.git repo has on the order of 2^22 objects,
    which is hundreds of times smaller than necessary to trigger
    the bug.
    
    Let's bump this up to an "unsigned". On 32-bit systems, this
    gives us the correct data-type, and on 64-bit systems, it is
    probably more efficient to use the native "unsigned" than a
    true uint32_t.
    
    While we're at it, we can fix the binary search not to
    overflow in such a case if our unsigned is 32 bits.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit c334b87b30c1464a1ab563fe1fb8de5eaf0e5bac
Author: Jeff King <peff@peff.net>
Date:   Thu Jul 11 16:45:59 2013 -0400

    cat-file: split --batch input lines on whitespace
    
    If we get an input line to --batch or --batch-check that
    looks like "HEAD foo bar", we will currently feed the whole
    thing to get_sha1(). This means that to use --batch-check
    with `rev-list --objects`, one must pre-process the input,
    like:
    
      git rev-list --objects HEAD |
      cut -d' ' -f1 |
      git cat-file --batch-check
    
    Besides being more typing and slightly less efficient to
    invoke `cut`, the result loses information: we no longer
    know which path each object was found at.
    
    This patch teaches cat-file to split input lines at the
    first whitespace. Everything to the left of the whitespace
    is considered an object name, and everything to the right is
    made available as the %(reset) atom. So you can now do:
    
      git rev-list --objects HEAD |
      git cat-file --batch-check='%(objectsize) %(rest)'
    
    to collect object sizes at particular paths.
    
    Even if %(rest) is not used, we always do the whitespace
    split (which means you can simply eliminate the `cut`
    command from the first example above).
    
    This whitespace split is backwards compatible for any
    reasonable input. Object names cannot contain spaces, so any
    input with spaces would have resulted in a "missing" line.
    The only input hurt is if somebody really expected input of
    the form "HEAD is a fine-looking ref!" to fail; it will now
    parse HEAD, and make "is a fine-looking ref!" available as
    %(rest).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit b91b935f04e8dcb1cc9f247627fbd0346ce949f4
Author: SZEDER Gábor <szeder@ira.uka.de>
Date:   Fri Apr 1 00:25:16 2011 +0200

    bash prompt: use bash builtins to find out rebase state
    
    During an ongoing interactive rebase __git_ps1() finds out the name of
    the rebased branch, the total number of patches and the number of the
    current patch by executing a '$(cat .git/rebase-merge/<FILE>)' command
    substitution for each.  That is not quite the most efficient way to
    read single line single word files, because it imposes the overhead of
    fork()ing a subshell and fork()+exec()ing 'cat' several times.
    
    Use the 'read' bash builtin instead to avoid those overheads.
    
    Signed-off-by: SZEDER Gábor <szeder@ira.uka.de>

commit 19a8cefc44b216db5cf8faf790d197c85db6395e
Author: Felipe Contreras <felipe.contreras@gmail.com>
Date:   Fri May 24 21:29:50 2013 -0500

    remote-hg: implement custom checkheads()
    
    The version from Mercurial is extremely inefficient and convoluted, this
    version achieves basically the same, at least for our purposes.
    
    No functional changes.
    
    Signed-off-by: Felipe Contreras <felipe.contreras@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 45c5d4a56bc3ef3b5088a07bdab12cef8163e61d
Author: Felipe Contreras <felipe.contreras@gmail.com>
Date:   Sun May 5 17:38:52 2013 -0500

    fast-{import,export}: use get_sha1_hex() to read from marks file
    
    It's wrong to call get_sha1() if they should be SHA-1s, plus
    inefficient.
    
    Signed-off-by: Felipe Contreras <felipe.contreras@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 38cecbdf52a081b732726180ad5e182d15117914
Author: Felipe Contreras <felipe.contreras@gmail.com>
Date:   Tue Apr 30 20:10:08 2013 -0500

    remote-bzr: iterate revisions properly
    
    This way we don't need to store the list of all the revisions, which
    doesn't seem to be very memory efficient with bazaar's design, for
    whatever reason.
    
    Signed-off-by: Felipe Contreras <felipe.contreras@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 850dd25c9a245da0e6b23c36cf9f45d6ee08b237
Author: Felipe Contreras <felipe.contreras@gmail.com>
Date:   Tue Apr 30 20:10:05 2013 -0500

    remote-bzr: add custom method to find branches
    
    The official method is incredibly inefficient and slow.
    
    Signed-off-by: Felipe Contreras <felipe.contreras@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 181662080098e77a0fd814ffe84be605a058b5d1
Author: Felipe Contreras <felipe.contreras@gmail.com>
Date:   Tue Apr 30 20:09:54 2013 -0500

    remote-bzr: delay blob fetching until the very end
    
    Might be more efficient, but the real reason to use the marks will be
    revealed in upcoming patches.
    
    Signed-off-by: Felipe Contreras <felipe.contreras@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit d8febde370aa464a5208cf094f306343e4ecb6dc
Author: René Scharfe <rene.scharfe@lsrfire.ath.cx>
Date:   Sun Mar 24 23:46:28 2013 +0100

    match-trees: simplify score_trees() using tree_entry()
    
    Convert the loop in score_trees() to tree_entry().  The code becomes
    shorter and simpler because the calls to update_tree_entry() are not
    needed any more.
    
    Another benefit is that we need less variables to track the current
    tree entries; as a side-effect of that the compiler has an easier
    job figuring out the control flow and thus can avoid false warnings
    about uninitialized variables.
    
    Using struct name_entry also allows the use of tree_entry_len() for
    finding the path length instead of strlen(), which may be slightly
    more efficient.
    
    Also unify the handling of missing entries in one of the two trees
    (i.e. added or removed files): Just set cmp appropriately first, no
    matter if we ran off the end of a tree or if we actually have two
    entries to compare, and check its value a bit later without
    duplicating the handler code.
    
    Signed-off-by: Rene Scharfe <rene.scharfe@lsrfire.ath.cx>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 131b8fcbfbea0acee8d78a8f9b2b3fee4285aee5
Author: Jeff King <peff@peff.net>
Date:   Sat Jan 26 17:40:38 2013 -0500

    fetch: run gc --auto after fetching
    
    We generally try to run "gc --auto" after any commands that
    might introduce a large number of new objects. An obvious
    place to do so is after running "fetch", which may introduce
    new loose objects or packs (depending on the size of the
    fetch).
    
    While an active developer repository will probably
    eventually trigger a "gc --auto" on another action (e.g.,
    git-rebase), there are two good reasons why it is nicer to
    do it at fetch time:
    
      1. Read-only repositories which track an upstream (e.g., a
         continuous integration server which fetches and builds,
         but never makes new commits) will accrue loose objects
         and small packs, but never coalesce them into a more
         efficient larger pack.
    
      2. Fetching is often already perceived to be slow to the
         user, since they have to wait on the network. It's much
         more pleasant to include a potentially slow auto-gc as
         part of the already-long network fetch than in the
         middle of productive work with git-rebase or similar.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 6bfe19ee168cd47295e9d25b4343ec318fab3790
Author: Jeff King <peff@peff.net>
Date:   Wed Jan 23 01:26:42 2013 -0500

    submodule: simplify memory handling in config parsing
    
    We keep a strbuf for the name of the submodule, even though
    we only ever add one string to it. Let's just use xmemdupz
    instead, which is slightly more efficient and makes it
    easier to follow what is going on.
    
    Unfortunately, we still end up having to deal with some
    memory ownership issues in some code branches, as we have to
    allocate the string in order to do a string list lookup, and
    then only sometimes want to hand ownership of that string
    over to the string_list. Still, making that explicit in the
    code (as opposed to sometimes detaching the strbuf, and then
    always releasing it) makes it a little more obvious what is
    going on.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Reviewed-by: Jonathan Nieder <jrnieder@gmail.com>
    Acked-by: Jens Lehmann <Jens.Lehmann@web.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit cc3046271d6b832f0fd6d3f704c5f9f653d2b682
Author: Peter Wu <lekensteyn@gmail.com>
Date:   Thu Jan 17 23:07:31 2013 +0100

    git-svn: do not escape certain characters in paths
    
    Subversion 1.7 and newer implement HTTPv2, an extension that should make HTTP
    more efficient. Servers with support for this protocol will make the subversion
    client library take an alternative code path that checks (with assertions)
    whether the URL is "canonical" or not.
    
    This patch fixes an issue I encountered while trying to `git svn dcommit` a
    rename action for a file containing a single quote character ("User's Manual"
    to "UserMan.tex"). It does not happen for older subversion 1.6 servers nor
    non-HTTP(S) protocols such as the native svn protocol, only on an Apache server
    shipping SVN 1.7. Trying to `git svn dcommit` under the aforementioned
    conditions yields the following error which aborts the commit process:
    
        Committing to http://example.com/svn ...
        perl: subversion/libsvn_subr/dirent_uri.c:1520: uri_skip_ancestor:
    Assertion `svn_uri_is_canonical(child_uri, ((void *)0))' failed.
        error: git-svn died of signal 6
    
    An analysis of the subversion source for the cause:
    
    - The assertion originates from uri_skip_ancestor which calls
      svn_uri_is_canonical, which fails when the URL contains percent-encoded values
      that do not necessarily have to be encoded (not "canonical" enough). This is
      done by a table lookup in libsvn_subr/path.c. Putting some debugging prints
      revealed that the character ' is indeed encoded to %27 which is not
      considered canonical.
    - url_skip_ancestor is called by svn_ra_neon__get_baseline_info with the root
      repository URL and path as parameters;
    - which is called by copy_resource (libsvn_ra_neon/commit.c) for a copy action
      (or in my case, renaming which is actually copy + delete old);
    - which is called by commit_add_dir;
    - which is assigned as a structure method "add_file" in
      svn_ra_neon__get_commit_editor.
    
    In the whole path, the path argument is not modified.
    
    Through some more uninteresting wrapper functions, the Perl bindings gives you
    access to the add_file method which will pass the path argument without
    modifications to svn.
    
    git-svn calls the "R"(ename) subroutine in Git::SVN::Editor which contains:
    326         my $fbat = $self->add_file($self->repo_path($m->{file_b}), $pbat,
    327                                 $self->url_path($m->{file_a}), $self->{r});
    "repo_path" basically returns the path as-is, unless the "svn.pathnameencoding"
    configuration property is set. "url_path" tries to escape some special
    characters, but does not take all special characters into account, thereby
    causing the path to contain some escaped characters which do not have to be
    escaped.
    
    The list of characters not to be escaped are taken from the
    subversion/libsvn_subr/path.c file to fully account for all characters. Tested
    with a filename containing all characters in the range 0x20 to 0x78 (inclusive).
    
    Signed-off-by: Peter Wu <lekensteyn@gmail.com>
    Signed-off-by: Eric Wong <normalperson@yhbt.net>

commit 4dcb167fc3536db0e78c50f239cd3a19afd383fa
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Fri Jan 11 16:05:46 2013 +0700

    fetch: add --unshallow for turning shallow repo into complete one
    
    The user can do --depth=2147483647 (*) for restoring complete repo
    now. But it's hard to remember. Any other numbers larger than the
    longest commit chain in the repository would also do, but some
    guessing may be involved. Make easy-to-remember --unshallow an alias
    for --depth=2147483647.
    
    Make upload-pack recognize this special number as infinite depth. The
    effect is essentially the same as before, except that upload-pack is
    more efficient because it does not have to traverse to the bottom
    anymore.
    
    The chance of a user actually wanting exactly 2147483647 commits
    depth, not infinite, on a repository with a history that long, is
    probably too small to consider. The client can learn to add or
    subtract one commit to avoid the special treatment when that actually
    happens.
    
    (*) This is the largest positive number a 32-bit signed integer can
        contain. JGit and older C Git store depth as "int" so both are OK
        with this number. Dulwich does not support shallow clone.
    
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit df7428eca46ec25df5ef14bc7e90120bdc267328
Author: Florian Achleitner <florian.achleitner.2.6.31@gmail.com>
Date:   Wed Sep 19 17:21:18 2012 +0200

    Add argv_array_detach and argv_array_free_detached
    
    Allow detaching of ownership of the argv_array's contents and add a
    function to free those detached argv_arrays later.
    
    This makes it possible to use argv_array efficiently with the exiting
    struct child_process which only contains a member char **argv.
    
    Add to documentation.
    
    Signed-off-by: Florian Achleitner <florian.achleitner.2.6.31@gmail.com>
    Acked-by: David Michael Barr <b@rr-dav.id.au>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a731fa916e5ce000eeb23f3c858c9218c78633d6
Author: Elia Pinto <gitter.spiros@gmail.com>
Date:   Fri Sep 14 09:54:22 2012 -0700

    Add MALLOC_CHECK_ and MALLOC_PERTURB_ libc env to the test suite for detecting heap corruption
    
    Recent versions of Linux libc (later than 5.4.23) and glibc (2.x)
    include a malloc() implementation which is tunable via environment
    variables. When MALLOC_CHECK_ is set, a special (less efficient)
    implementation is used which is designed to be tolerant against
    simple errors, such as double calls of free() with the same argument,
    or overruns of a single byte (off-by-one bugs). When MALLOC_CHECK_
    is set to 3, a diagnostic message is printed on stderr
    and the program is aborted.
    
    Setting the MALLOC_PERTURB_ environment variable causes the malloc
    functions in libc to return memory which has been wiped and clear
    memory when it is returned.
    Of course this does not affect calloc which always does clear the memory.
    
    The reason for this exercise is, of course, to find code which uses
    memory returned by malloc without initializing it and code which uses
    code after it is freed. valgrind can do this but it's costly to run.
    The MALLOC_PERTURB_ exchanges the ability to detect problems in 100%
    of the cases with speed.
    
    The byte value used to initialize values returned by malloc is the byte
    value of the environment value. The value used to clear memory is the
    bitwise inverse. Setting MALLOC_PERTURB_ to zero disables the feature.
    
    This technique can find hard to detect bugs.
    It is therefore suggested to always use this flag (at least temporarily)
    when testing out code or a new distribution.
    
    But the test suite can use also valgrind(memcheck) via 'make valgrind'
    or 'make GIT_TEST_OPTS="--valgrind"'.
    
    Memcheck wraps client calls to malloc(), and puts a "red zone" on
    each end of each block in order to detect access overruns.
    Memcheck already detects double free() (up to the limit of the buffer
    which remembers pending free()). Thus memcheck subsumes all the
    documented coverage of MALLOC_CHECK_.
    
    If MALLOC_CHECK_ is set non-zero when running memcheck, then the
    overruns that might be detected by MALLOC_CHECK_ would be overruns
    on the wrapped blocks which include the red zones.  Thus MALLOC_CHECK_
    would be checking memcheck, and not the client.  This is not useful,
    and actually is wasteful.  The only possible [documented] advantage
    of using MALLOC_CHECK_ and memcheck together, would be if MALLOC_CHECK_
    detected duplicate free() in more cases than memcheck because memcheck's
    buffer is too small.
    
    Therefore we don't use MALLOC_CHECK_ and valgrind(memcheck) at the
    same time.
    
    Signed-off-by: Elia Pinto <gitter.spiros@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 6a9aa0c9b224517db0549d9252fdbc5177e6c0e2
Merge: fde1cc1dc2 ff0bfd754d
Author: Junio C Hamano <gitster@pobox.com>
Date:   Fri Jul 13 15:37:46 2012 -0700

    Merge branch 'mm/mediawiki-tests'
    
    * mm/mediawiki-tests:
      git-remote-mediawiki: be more defensive when requests fail
      git-remote-mediawiki: more efficient 'pull' in the best case
      git-remote-mediawiki: extract revision-importing loop to a function
      git-remote-mediawiki: refactor loop over revision ids
      git-remote-mediawiki: change return type of get_mw_pages
      git-remote-mediawiki (t9363): test 'File:' import and export
      git-remote-mediawiki: support for uploading file in test environment
      git-remote-mediawiki (t9362): test git-remote-mediawiki with UTF8 characters
      git-remote-mediawiki (t9361): test git-remote-mediawiki pull and push
      git-remote-mediawiki (t9360): test git-remote-mediawiki clone
      git-remote-mediawiki: test environment of git-remote-mediawiki
      git-remote-mediawiki: scripts to install, delete and clear a MediaWiki

commit 5a29217dda0625707a46f527d17489c0bff0c2c9
Author: Matthieu Moy <Matthieu.Moy@imag.fr>
Date:   Fri Jul 6 12:03:14 2012 +0200

    git-remote-mediawiki: more efficient 'pull' in the best case
    
    The only way to fetch new revisions from a wiki before this patch was to
    query each page for new revisions. This is good when tracking a small set
    of pages on a large wiki, but very inefficient when tracking many pages
    on a wiki with little activity.
    
    Implement a new strategy that queries the wiki for its last global
    revision, queries each new revision, and filter out pages that are not
    tracked.
    
    Signed-off-by: Simon Perrat <simon.perrat@ensimag.imag.fr>
    Signed-off-by: Simon CATHEBRAS <Simon.Cathebras@ensimag.imag.fr>
    Signed-off-by: Julien KHAYAT <Julien.Khayat@ensimag.imag.fr>
    Signed-off-by: Charles ROUSSEL <Charles.Roussel@ensimag.imag.fr>
    Signed-off-by: Guillaume SASDY <Guillaume.Sasdy@ensimag.imag.fr>
    Signed-off-by: Matthieu Moy <Matthieu.Moy@imag.fr>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit b1ede9a9f232b1416d1487bba3f69a8e88ec75ee
Author: Matthieu Moy <Matthieu.Moy@imag.fr>
Date:   Fri Jul 6 12:03:11 2012 +0200

    git-remote-mediawiki: change return type of get_mw_pages
    
    The previous version was returning the list of pages to be fetched, but
    we are going to need an efficient membership test (i.e. is the page
    $title tracked), hence exposing a hash will be more convenient.
    
    Signed-off-by: Matthieu Moy <Matthieu.Moy@imag.fr>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 12d7d150743acebe9684100e98979f2d0188114e
Merge: a7060009e1 3d2a33e57f
Author: Junio C Hamano <gitster@pobox.com>
Date:   Tue May 29 13:09:08 2012 -0700

    Merge branch 'jk/fetch-pack-remove-dups-optim'
    
    The way "fetch-pack" that is given multiple references to fetch tried to
    remove duplicates was very inefficient.
    
    By Jeff King
    * jk/fetch-pack-remove-dups-optim:
      fetch-pack: sort incoming heads list earlier
      fetch-pack: avoid quadratic loop in filter_refs
      fetch-pack: sort the list of incoming refs
      add sorting infrastructure for list refs
      fetch-pack: avoid quadratic behavior in remove_duplicates
      fetch-pack: sort incoming heads

commit 654ad400c27653a138b028cdd35a09b0c69c7ef0
Author: Michael Haggerty <mhagger@alum.mit.edu>
Date:   Thu May 24 14:16:50 2012 +0200

    Avoid sorting if references are added to ref_cache in order
    
    The old code allowed many references to be efficiently added to a
    single directory, because it just appended the references to the
    containing directory unsorted without doing any searching (and
    therefore without requiring any intermediate sorting).  But the old
    code was inefficient when a large number of subdirectories were added
    to a directory, because the directory always had to be searched to see
    if the new subdirectory already existed, and this search required the
    directory to be sorted first.  The same was repeated for every new
    subdirectory, so the time scaled like O(N^2), where N is the number of
    subdirectories within a single directory.
    
    In practice, references are often added to the ref_cache in
    lexicographic order, for example when reading the packed-refs file.
    So build some intelligence into add_entry_to_dir() to optimize for the
    case of references and/or subdirectories being added in lexicographic
    order: if the existing entries were already sorted, and the new entry
    comes after the last existing entry, then adjust ref_dir::sorted to
    reflect the fact that the ref_dir is still sorted.
    
    Thanks to Peff for pointing out the performance regression that
    inspired this change.
    
    Signed-off-by: Michael Haggerty <mhagger@alum.mit.edu>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit ed81c76bc3a9440e37b3512c9c2b742c6ca92c6f
Author: Jeff King <peff@peff.net>
Date:   Mon May 21 18:19:28 2012 -0400

    add sorting infrastructure for list refs
    
    Since we store lists of refs as linked lists, we can use
    llist_mergesort to efficiently sort them.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 6a8989709efbd6d5187005497907264fde026ae9
Merge: 5fa8bf6bf9 a81a7fbc1a
Author: Junio C Hamano <gitster@pobox.com>
Date:   Sun Apr 29 17:51:30 2012 -0700

    Merge branch 'rs/commit-list-append'
    
    There is no need for "commit_list_reverse()" function that only invites
    inefficient code.
    
    By René Scharfe
    * rs/commit-list-append:
      commit: remove commit_list_reverse()
      revision: append to list instead of insert and reverse
      sequencer: export commit_list_append()

commit ba8e6326f16748d67fbeda65ffde4729760c64f0
Merge: 58bbace89d 7365c95d2d
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon Apr 23 12:52:54 2012 -0700

    Merge branch 'rs/commit-list-sort-in-batch'
    
    Setting up a revision traversal with many starting points was inefficient
    as these were placed in a date-order priority queue one-by-one.
    
    By René Scharfe (3) and Junio C Hamano (1)
    * rs/commit-list-sort-in-batch:
      mergesort: rename it to llist_mergesort()
      revision: insert unsorted, then sort in prepare_revision_walk()
      commit: use mergesort() in commit_list_sort_by_date()
      add mergesort() for linked lists

commit 7e52f5660e542cf801e8fc6902a30cc572c13736
Author: Jeff King <peff@peff.net>
Date:   Sat Apr 7 06:30:09 2012 -0400

    gc: do not explode objects which will be immediately pruned
    
    When we pack everything into one big pack with "git repack
    -Ad", any unreferenced objects in to-be-deleted packs are
    exploded into loose objects, with the intent that they will
    be examined and possibly cleaned up by the next run of "git
    prune".
    
    Since the exploded objects will receive the mtime of the
    pack from which they come, if the source pack is old, those
    loose objects will end up pruned immediately. In that case,
    it is much more efficient to skip the exploding step
    entirely for these objects.
    
    This patch teaches pack-objects to receive the expiration
    information and avoid writing these objects out. It also
    teaches "git gc" to pass the value of gc.pruneexpire to
    repack (which in turn learns to pass it along to
    pack-objects) so that this optimization happens
    automatically during "git gc" and "git gc --auto".
    
    Signed-off-by: Jeff King <peff@peff.net>
    Acked-by: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 078b895fefdca94995862a4cc8644198b00a89bf
Author: Ivan Todoroski <grnch@gmx.net>
Date:   Mon Apr 2 17:13:48 2012 +0200

    fetch-pack: new --stdin option to read refs from stdin
    
    If a remote repo has too many tags (or branches), cloning it over the
    smart HTTP transport can fail because remote-curl.c puts all the refs
    from the remote repo on the fetch-pack command line. This can make the
    command line longer than the global OS command line limit, causing
    fetch-pack to fail.
    
    This is especially a problem on Windows where the command line limit is
    orders of magnitude shorter than Linux. There are already real repos out
    there that msysGit cannot clone over smart HTTP due to this problem.
    
    Here is an easy way to trigger this problem:
    
            git init too-many-refs
            cd too-many-refs
            echo bla > bla.txt
            git add .
            git commit -m test
            sha=$(git rev-parse HEAD)
            tag=$(perl -e 'print "bla" x 30')
            for i in `seq 50000`; do
                    echo $sha refs/tags/$tag-$i >> .git/packed-refs
            done
    
    Then share this repo over the smart HTTP protocol and try cloning it:
    
            $ git clone http://localhost/.../too-many-refs/.git
            Cloning into 'too-many-refs'...
            fatal: cannot exec 'fetch-pack': Argument list too long
    
    50k tags is obviously an absurd number, but it is required to
    demonstrate the problem on Linux because it has a much more generous
    command line limit. On Windows the clone fails with as little as 500
    tags in the above loop, which is getting uncomfortably close to the
    number of tags you might see in real long lived repos.
    
    This is not just theoretical, msysGit is already failing to clone our
    company repo due to this. It's a large repo converted from CVS, nearly
    10 years of history.
    
    Four possible solutions were discussed on the Git mailing list (in no
    particular order):
    
    1) Call fetch-pack multiple times with smaller batches of refs.
    
    This was dismissed as inefficient and inelegant.
    
    2) Add option --refs-fd=$n to pass a an fd from where to read the refs.
    
    This was rejected because inheriting descriptors other than
    stdin/stdout/stderr through exec() is apparently problematic on Windows,
    plus it would require changes to the run-command API to open extra
    pipes.
    
    3) Add option --refs-from=$tmpfile to pass the refs using a temp file.
    
    This was not favored because of the temp file requirement.
    
    4) Add option --stdin to pass the refs on stdin, one per line.
    
    In the end this option was chosen as the most efficient and most
    desirable from scripting perspective.
    
    There was however a small complication when using stdin to pass refs to
    fetch-pack. The --stateless-rpc option to fetch-pack also uses stdin for
    communication with the remote server.
    
    If we are going to sneak refs on stdin line by line, it would have to be
    done very carefully in the presence of --stateless-rpc, because when
    reading refs line by line we might read ahead too much data into our
    buffer and eat some of the remote protocol data which is also coming on
    stdin.
    
    One way to solve this would be to refactor get_remote_heads() in
    fetch-pack.c to accept a residual buffer from our stdin line parsing
    above, but this function is used in several places so other callers
    would be burdened by this residual buffer interface even when most of
    them don't need it.
    
    In the end we settled on the following solution:
    
    If --stdin is specified without --stateless-rpc, fetch-pack would read
    the refs from stdin one per line, in a script friendly format.
    
    However if --stdin is specified together with --stateless-rpc,
    fetch-pack would read the refs from stdin in packetized format
    (pkt-line) with a flush packet terminating the list of refs. This way we
    can read the exact number of bytes that we need from stdin, and then
    get_remote_heads() can continue reading from the same fd without losing
    a single byte of remote protocol data.
    
    This way the --stdin option only loses generality and scriptability when
    used together with --stateless-rpc, which is not easily scriptable
    anyway because it also uses pkt-line when talking to the remote server.
    
    Signed-off-by: Ivan Todoroski <grnch@gmx.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit c3117b21666f2a68aebd5fce0bcdc4ab705c299a
Merge: 9721d2fb10 d21c463d55
Author: Junio C Hamano <gitster@pobox.com>
Date:   Fri Mar 16 08:23:53 2012 -0700

    Merge branch 'jc/maint-verify-objects-remove-pessimism'
    
    The code to validate the history connectivity between old refs and new
    refs used by fetch and receive-pack, introduced in 1.7.8, was grossly
    inefficient and unnecessarily tried to re-validate integrity of individual
    objects. This essentially reverts that performance regression.
    
    * jc/maint-verify-objects-remove-pessimism:
      fetch/receive: remove over-pessimistic connectivity check

commit 3de89c9d4216d0fdc11bd1141c419ac4d0d35fed
Author: Junio C Hamano <gitster@pobox.com>
Date:   Fri Jun 3 15:32:17 2011 -0700

    verify-pack: use index-pack --verify
    
    This finally gets rid of the inefficient verify-pack implementation that
    walks objects in the packfile in their object name order and replaces it
    with a call to index-pack --verify. As a side effect, it also removes
    packed_object_info_detail() API which is rather expensive.
    
    As this changes the way errors are reported (verify-pack used to rely on
    the usual runtime error detection routine unpack_entry() to diagnose the
    CRC errors in an entry in the *.idx file; index-pack --verify checks the
    whole *.idx file in one go), update a test that expected the string "CRC"
    to appear in the error message.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit cff38a5e11217dc997c1ffae8d0856023e05554a
Author: Jeff King <peff@peff.net>
Date:   Thu May 19 17:34:46 2011 -0400

    receive-pack: eliminate duplicate .have refs
    
    When receiving a push, we advertise ref tips from any
    alternate repositories, in case that helps the client send a
    smaller pack. Since these refs don't actually exist in the
    destination repository, we don't transmit the real ref
    names, but instead use the pseudo-ref ".have".
    
    If your alternate has a large number of duplicate refs (for
    example, because it is aggregating objects from many related
    repositories, some of which will have the same tags and
    branch tips), then we will send each ".have $sha1" line
    multiple times. This is a pointless waste of bandwidth, as
    we are simply repeating the same fact to the client over and
    over.
    
    This patch eliminates duplicate .have refs early on. It does
    so efficiently by sorting the complete list and skipping
    duplicates. This has the side effect of re-ordering the
    .have lines by ascending sha1; this isn't a problem, though,
    as the original order was meaningless.
    
    There is a similar .have system in fetch-pack, but it
    does not suffer from the same problem. For each alternate
    ref we consider in fetch-pack, we actually open the object
    and mark it with the SEEN flag, so duplicates are
    automatically culled.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 95a1d12e9b9faddc02187ca28fdeb4fddd354c59
Author: Jonathan Nieder <jrnieder@gmail.com>
Date:   Tue Mar 15 05:10:45 2011 -0500

    tests: scrub environment of GIT_* variables
    
    Variables from the inherited environment that are meaningful to git
    can break tests in undesirable ways.  For example,
    
            GIT_PAGER=more sh t5400-send-pack.sh -v -i
    
    hangs.  So unset all environment variables in the GIT_ namespace in
    test-lib, with a few exceptions:
    
    - GIT_TRACE* are useful for tracking down bugs exhibited by a failing
      test;
    
    - GIT_DEBUG* are GIT_TRACE variables by another name, practically
      speaking.  They should probably be tweaked to follow the
      GIT_TRACE_foo scheme and use trace_printf machinery some time.
    
    - GIT_USE_LOOKUP from v1.5.6-rc0~134^2~1 (sha1-lookup: more memory
      efficient search in sorted list of SHA-1, 2007-12-29) is about
      trying an alternate implementation strategy rather than changing
      semantics and it can be useful to compare performance with and
      without it set.
    
    Signed-off-by: Jonathan Nieder <jrnieder@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 94b3b3746456949d834ec7bf454da3db4eb439cf
Author: Jeff King <peff@peff.net>
Date:   Thu Feb 24 09:29:50 2011 -0500

    trace: add trace_strbuf
    
    If you happen to have a strbuf, it is a little more readable
    and a little more efficient to be able to print it directly
    instead of jamming it through the trace_printf interface.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e337a04de298f8c3e64ee1a187423203406b9bae
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Feb 2 17:29:01 2011 -0800

    index-pack: --verify
    
    Given an existing .pack file and the .idx file that describes it,
    this new mode of operation reads and re-index the packfile and makes
    sure the existing .idx file matches the result byte-for-byte.
    
    All the objects in the .pack file are validated during this operation as
    well.  Unlike verify-pack, which visits each object described in the .idx
    file in the SHA-1 order, index-pack efficiently exploits the delta-chain
    to avoid rebuilding the objects that are used as the base of deltified
    objects over and over again while validating the objects, resulting in
    much quicker verification of the .pack file and its .idx file.
    
    This version however cannot verify a .pack/.idx pair with a handcrafted v2
    index that uses 64-bit offset representation for offsets that would fit
    within 31-bit. You can create such an .idx file by giving a custom offset
    to --index-version option to the command.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 9037026d34907c0c94e3ab46f96068fa57da6ebc
Author: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
Date:   Sat Nov 27 01:17:46 2010 +0700

    unpack-trees: fix sparse checkout's "unable to match directories"
    
    Matching index entries against an excludes file currently has two
    problems.
    
    First, there's no function to do it.  Code paths (like sparse
    checkout) that wanted to try it would iterate over index entries and
    for each index entry pass that path to excluded_from_list().  But that
    is not how excluded_from_list() works; one is supposed to feed in each
    ancester of a path before a given path to find out if it was excluded
    because of some parent or grandparent matching a
    
      bigsubdirectory/
    
    pattern despite the path not matching any .gitignore pattern directly.
    
    Second, it's inefficient.  The excludes mechanism is supposed to let
    us block off vast swaths of the filesystem as uninteresting; separately
    checking every index entry doesn't fit that model.
    
    Introduce a new function to take care of both these problems.  This
    traverses the index in depth-first order (well, that's what order the
    index is in) to mark un-excluded entries.
    
    Maybe some day the in-core index format will be restructured to make
    this sort of operation easier.  Or maybe we will want to try some
    binary search based thing.  The interface is simple enough to allow
    all those things.  Example:
    
      clear_ce_flags(the_index.cache, the_index.cache_nr,
                     CE_CANDIDATE, CE_CLEARME, exclude_list);
    
    would clear the CE_CLEARME flag on all index entries with
    CE_CANDIDATE flag and not matched by exclude_list.
    
    Signed-off-by: Nguyễn Thái Ngọc Duy <pclouds@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 951f316470acc7c785c460a4e40735b22822349f
Author: Jason Evans <jasone@canonware.com>
Date:   Mon Aug 9 17:17:34 2010 -0500

    Add treap implementation
    
    Provide macros to generate a type-specific treap implementation and
    various functions to operate on it. It uses obj_pool.h to store memory
    nodes in a treap.  Previously committed nodes are never removed from
    the pool; after any *_commit operation, it is assumed (correctly, in
    the case of svn-fast-export) that someone else must care about them.
    
    Treaps provide a memory-efficient binary search tree structure.
    Insertion/deletion/search are about as about as fast in the average
    case as red-black trees and the chances of worst-case behavior are
    vanishingly small, thanks to (pseudo-)randomness.  The bad worst-case
    behavior is a small price to pay, given that treaps are much simpler
    to implement.
    
    >From http://www.canonware.com/download/trp/trp_hash/trp.h
    
    [db: Altered to reference nodes by offset from a common base pointer]
    [db: Bob Jenkins' hashing implementation dropped for Knuth's]
    [db: Methods unnecessary for search and insert dropped]
    [rr: Squelched compiler warnings]
    [db: Added support for immutable treap nodes]
    [jn: Reintroduced treap_nsearch(); with tests]
    
    Signed-off-by: David Barr <david.barr@cordelta.com>
    Signed-off-by: Ramkumar Ramachandra <artagnon@gmail.com>
    Signed-off-by: Jonathan Nieder <jrnieder@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a2430dde8ceaaaabf05937438249397b883ca77a
Author: Nicolas Pitre <nico@fluxnic.net>
Date:   Wed Feb 3 22:48:27 2010 -0500

    pack-objects: fix pack generation when using pack_size_limit
    
    Current handling of pack_size_limit is quite suboptimal.  Let's consider
    a list of objects to pack which contain alternatively big and small
    objects (which pretty matches reality when big blobs are interlaced
    with tree objects).  Currently, the code simply close the pack and opens
    a new one when the next object in line breaks the size limit.
    
    The current code may degenerate into:
    
      - small tree object => store into pack #1
      - big blob object busting the pack size limit => store into pack #2
      - small blob but pack #2 is over the limit already => pack #3
      - big blob busting the size limit => pack #4
      - small tree but pack #4 is over the limit => pack #5
      - big blob => pack #6
      - small tree => pack #7
      - ... and so on.
    
    The reality is that the content of packs 1, 3, 5 and 7 could well be
    stored more efficiently (and delta compressed) together in pack #1 if
    the big blobs were not forcing an immediate transition to a new pack.
    
    Incidentally this can be fixed pretty easily by simply skipping over
    those objects that are too big to fit in the current pack while trying
    the whole list of unwritten objects, and then that list considered from
    the beginning again when a new pack is opened.  This creates much fewer
    smallish pack files and help making more predictable test cases for the
    test suite.
    
    This change made one of the self sanity checks useless so it is removed
    as well. That check was rather redundant already anyway.
    
    Signed-off-by: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit d0f379c2dcf7198d373b3c64444019bed2e24336
Author: Stephan Beyer <s-beyer@gmx.net>
Date:   Wed Dec 30 06:54:47 2009 +0100

    reset: use "unpack_trees()" directly instead of "git read-tree"
    
    This patch makes "reset_index_file()" call "unpack_trees()" directly
    instead of forking and execing "git read-tree". So the code is more
    efficient.
    
    And it's also easier to see which unpack_tree() options will be used,
    as we don't need to follow "git read-tree"'s command line parsing
    which is quite complex.
    
    As Daniel Barkalow found, there is a difference between this new
    version and the old one. The old version gives an error for
    "git reset --merge" with unmerged entries, and the new version does
    not when we reset the entries to some states that differ from HEAD.
    Instead, it resets the index entry and succeeds, while leaving the
    conflict markers in the corresponding file in the work tree (which
    will be corrected by the next patch).
    
    The code comes from the sequencer GSoC project:
    
    git://repo.or.cz/git/sbeyer.git
    
    (at commit 5a78908b70ceb5a4ea9fd4b82f07ceba1f019079)
    
    Mentored-by: Daniel Barkalow <barkalow@iabervon.org>
    Mentored-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Stephan Beyer <s-beyer@gmx.net>
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 809809bb75e8a65ef543ab706aab4791459be95c
Author: Junio C Hamano <gitster@pobox.com>
Date:   Fri Nov 20 22:13:47 2009 -0800

    diffcore-rename: reduce memory footprint by freeing blob data early
    
    After running one round of estimate_similarity(), filespecs on either
    side will have populated their cnt_data fields, and we do not need
    the blob text anymore.  We used to retain the blob data to optimize
    for smaller projects (not freeing the blob data here would mean that
    the final output phase would not have to re-read it), but we are
    efficient enough without such optimization for smaller projects anyway,
    and freeing memory early will help larger projects.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 5abb013b3ddfb42e5baa3c7de052af596a0ee82f
Author: Shawn O. Pearce <spearce@spearce.org>
Date:   Wed Nov 4 17:16:37 2009 -0800

    http-backend: Use http.getanyfile to disable dumb HTTP serving
    
    Some repository owners may wish to enable smart HTTP, but disallow
    dumb content serving.  Disallowing dumb serving might be because
    the owners want to rely upon reachability to control which objects
    clients may access from the repository, or they just want to
    encourage clients to use the more bandwidth efficient transport.
    
    If http.getanyfile is set to false the backend CGI will return with
    '403 Forbidden' when an object file is accessed by a dumb client.
    
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 73cf0822b2a4ffa7ad559d1f0772e39718fc7776
Author: Julian Phillips <julian@quantumfyre.co.uk>
Date:   Sun Oct 25 21:28:11 2009 +0000

    remote: Make ref_remove_duplicates faster for large numbers of refs
    
    The ref_remove_duplicates function was very slow at dealing with very
    large numbers of refs.  This is because it was using a linear search
    through all remaining refs to find any duplicates of the current ref.
    
    Rewriting it to use a string list to keep track of which refs have
    already been seen and removing duplicates when they are found is much
    more efficient.
    
    Signed-off-by: Julian Phillips <julian@quantumfyre.co.uk>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 3ed24b6aaf35d6ca1eef2643cd0b9128eb152cda
Author: Johan Herland <johan@herland.net>
Date:   Fri Oct 9 12:22:03 2009 +0200

    t3302-notes-index-expensive: Speed up create_repo()
    
    Creating repos with 10/100/1000/10000 commits and notes takes a lot of time.
    However, using git-fast-import to do the job is a lot more efficient than
    using plumbing commands to do the same.
    
    This patch decreases the overall run-time of this test on my machine from
    ~3 to ~1 minutes.
    
    Signed-off-by: Johan Herland <johan@herland.net>
    Acked-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a97a74686d70a318cd802003498054cc1e8b0ae2
Author: Johannes Schindelin <Johannes.Schindelin@gmx.de>
Date:   Fri Oct 9 12:21:57 2009 +0200

    Introduce commit notes
    
    Commit notes are blobs which are shown together with the commit
    message.  These blobs are taken from the notes ref, which you can
    configure by the config variable core.notesRef, which in turn can
    be overridden by the environment variable GIT_NOTES_REF.
    
    The notes ref is a branch which contains "files" whose names are
    the names of the corresponding commits (i.e. the SHA-1).
    
    The rationale for putting this information into a ref is this: we
    want to be able to fetch and possibly union-merge the notes,
    maybe even look at the date when a note was introduced, and we
    want to store them efficiently together with the other objects.
    
    This patch has been improved by the following contributions:
    - Thomas Rast: fix core.notesRef documentation
    - Tor Arne Vestbø: fix printing of multi-line notes
    - Alex Riesen: Using char array instead of char pointer costs less BSS
    - Johan Herland: Plug leak when msg is good, but msglen or type causes return
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Thomas Rast <trast@student.ethz.ch>
    Signed-off-by: Tor Arne Vestbø <tavestbo@trolltech.com>
    Signed-off-by: Johan Herland <johan@herland.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>
    
    get_commit_notes(): Plug memory leak when 'if' triggers, but not because of read_sha1_file() failure

commit 5bdc32d3e50d8335c65e136e6b5234c5dd92a7a9
Author: Nicolas Pitre <nico@fluxnic.net>
Date:   Fri Sep 25 23:54:42 2009 -0400

    make 'git clone' ask the remote only for objects it cares about
    
    Current behavior of 'git clone' when not using --mirror is to fetch
    everything from the peer, and then filter out unwanted refs just before
    writing them out to the cloned repository.  This may become highly
    inefficient if the peer has an unusual ref namespace, or if it simply
    has "remotes" refs of its own, and those locally unwanted refs are
    connecting to a large set of objects which becomes unreferenced as soon
    as they are fetched.
    
    Let's filter out those unwanted refs from the peer _before_ asking it
    what refs we want to fetch instead, which is the most logical thing to
    do anyway.
    
    Signed-off-by: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>

commit 8e4384fd4438a143af7125eb0f03312a318319fb
Merge: 6ea71fe7d3 0ef95f72f8
Author: Junio C Hamano <gitster@pobox.com>
Date:   Mon Sep 7 15:23:50 2009 -0700

    Merge branch 'np/maint-1.6.3-deepen'
    
    * np/maint-1.6.3-deepen:
      pack-objects: free preferred base memory after usage
      make shallow repository deepening more network efficient

commit 6523078b96cd39f681e6fa11135049808591fb95
Author: Nicolas Pitre <nico@cam.org>
Date:   Thu Sep 3 19:08:33 2009 -0400

    make shallow repository deepening more network efficient
    
    First of all, I can't find any reason why thin pack generation is
    explicitly disabled when dealing with a shallow repository.  The
    possible delta base objects are collected from the edge commits which
    are always obtained through history walking with the same shallow refs
    as the client, Therefore the client is always going to have those base
    objects available. So let's remove that restriction.
    
    Then we can make shallow repository deepening much more efficient by
    using the remote's unshallowed commits as edge commits to get preferred
    base objects for thin pack generation.  On git.git, this makes the data
    transfer for the deepening of a shallow repository from depth 1 to depth 2
    around 134 KB instead of 3.68 MB.
    
    Signed-off-by: Nicolas Pitre <nico@cam.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7a4ee28f41270bf032d0dd0bfb17f601b9b3971a
Author: Jeff King <peff@peff.net>
Date:   Wed Aug 26 15:05:08 2009 -0400

    clone: add --branch option to select a different HEAD
    
    We currently point the HEAD of a newly cloned repo to the
    same ref as the parent repo's HEAD. While a user can then
    "git checkout -b foo origin/foo" whichever branch they
    choose, it is more convenient and more efficient to tell
    clone which branch you want in the first place.
    
    Based on a patch by Kirill A. Korinskiy.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 79559f27be7e2963213d840a857dee92c579843f
Author: Geoffrey Irving <irving@naml.us>
Date:   Mon Jul 27 22:20:22 2009 -0400

    git fast-export: add --no-data option
    
    When using git fast-export and git fast-import to rewrite the history
    of a repository with large binary files, almost all of the time is
    spent dealing with blobs.  This is extremely inefficient if all we want
    to do is rewrite the commits and tree structure.  --no-data skips the
    output of blobs and writes SHA-1s instead of marks, which provides a
    massive speedup.
    
    Signed-off-by: Geoffrey Irving <irving@naml.us>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f62ce3de9dd4803f50f65e17f5fc03c7bdb49c40
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 9 13:37:02 2009 -0700

    Make index preloading check the whole path to the file
    
    This uses the new thread-safe 'threaded_has_symlink_leading_path()'
    function to efficiently verify that the whole path leading up to the
    filename is a proper path, and does not contain symlinks.
    
    This makes 'ce_uptodate()' a much stronger guarantee: it no longer just
    guarantees that the 'lstat()' of the path would match, it also means
    that we know that people haven't played games with moving directories
    around and covered it up with symlinks.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 6e353a5e5de9da021c7c6c0bc2dc5f95a39900a1
Merge: a54c4edc51 13858e5770
Author: Junio C Hamano <gitster@pobox.com>
Date:   Sun Apr 12 16:46:40 2009 -0700

    Merge branch 'cc/bisect-filter'
    
    * cc/bisect-filter: (21 commits)
      rev-list: add "int bisect_show_flags" in "struct rev_list_info"
      rev-list: remove last static vars used in "show_commit"
      list-objects: add "void *data" parameter to show functions
      bisect--helper: string output variables together with "&&"
      rev-list: pass "int flags" as last argument of "show_bisect_vars"
      t6030: test bisecting with paths
      bisect: use "bisect--helper" and remove "filter_skipped" function
      bisect: implement "read_bisect_paths" to read paths in "$GIT_DIR/BISECT_NAMES"
      bisect--helper: implement "git bisect--helper"
      bisect: use the new generic "sha1_pos" function to lookup sha1
      rev-list: call new "filter_skip" function
      patch-ids: use the new generic "sha1_pos" function to lookup sha1
      sha1-lookup: add new "sha1_pos" function to efficiently lookup sha1
      rev-list: pass "revs" to "show_bisect_vars"
      rev-list: make "show_bisect_vars" non static
      rev-list: move code to show bisect vars into its own function
      rev-list: move bisect related code into its own file
      rev-list: make "bisect_list" variable local to "cmd_rev_list"
      refs: add "for_each_ref_in" function to refactor "for_each_*_ref" functions
      quote: add "sq_dequote_to_argv" to put unwrapped args in an argv array
      ...

commit 4eb5b64631d281f3789b052efac53f4c1ec2c1b6
Author: Christian Couder <chriscool@tuxfamily.org>
Date:   Sat Apr 4 22:59:36 2009 +0200

    bisect: use the new generic "sha1_pos" function to lookup sha1
    
    instead of the specific one that was simpler but less efficient.
    
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 5aaa507b067ca22324c34308cb56bcb64392a5db
Merge: 7428d754e2 5289bae17f
Author: Junio C Hamano <gitster@pobox.com>
Date:   Sat Apr 4 23:04:50 2009 -0700

    Merge branch 'cc/sha1-bsearch' into HEAD
    
    * cc/sha1-bsearch: (95 commits)
      patch-ids: use the new generic "sha1_pos" function to lookup sha1
      sha1-lookup: add new "sha1_pos" function to efficiently lookup sha1
      Update draft release notes to 1.6.3
      GIT 1.6.2.2
      send-email: ensure quoted addresses are rfc2047 encoded
      send-email: correct two tests which were going interactive
      Documentation: git-svn: fix trunk/fetch svn-remote key typo
      Mailmap: Allow empty email addresses to be mapped
      Cleanup warning about known issues in cvsimport documentation
      Documentation: Remove an odd "instead"
      send-email: ask_default should apply to all emails, not just the first
      send-email: don't attempt to prompt if tty is closed
      fix portability problem with IS_RUN_COMMAND_ERR
      Documentation: use "spurious .sp" XSLT if DOCBOOK_SUPPRESS_SP is set
      mailmap: resurrect lower-casing of email addresses
      builtin-clone.c: no need to strdup for setenv
      builtin-clone.c: make junk_pid static
      git-svn: add a double quiet option to hide git commits
      Update draft release notes to 1.6.2.2
      Documentation: push.default applies to all remotes
      ...

commit 96beef8c2efaab06f703991ed7802b8cef4c00e3
Author: Christian Couder <chriscool@tuxfamily.org>
Date:   Sat Apr 4 22:59:26 2009 +0200

    sha1-lookup: add new "sha1_pos" function to efficiently lookup sha1
    
    This function has been copied from the "patch_pos" function in
    "patch-ids.c" but an additional parameter has been added.
    
    The new parameter is a function pointer, that is used to access the
    sha1 of an element in the table.
    
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 3bd925636cd11400d1840b39d0d18b640f32bdd2
Author: Jay Soffian <jaysoffian@gmail.com>
Date:   Wed Feb 25 03:32:23 2009 -0500

    builtin-remote: fix two inconsistencies in the output of "show <remote>"
    
    Remote and stale branches are emitted in alphabetical order, but new and
    tracked branches are not. So sort the latter to be consistent with the
    former. This also lets us use more efficient string_list_has_string()
    instead of unsorted_string_list_has_string().
    
    "show <remote>" prunes symrefs, but "show <remote> -n" does not. Fix the
    latter to match the former.
    
    Signed-off-by: Jay Soffian <jaysoffian@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1ef626b4b6c70fc13062faafdccb2f0da7578a29
Author: Eric Wong <normalperson@yhbt.net>
Date:   Sat Jan 17 22:11:44 2009 -0800

    git-svn: fix SVN 1.1.x compatibility
    
    The get_log() function in the Perl SVN API introduced the limit
    parameter in 1.2.0.  However, this got discarded in our SVN::Ra
    compatibility layer when used with SVN 1.1.x.  We now emulate
    the limit functionality in older SVN versions by preventing the
    original callback from being called if the given limit has been
    reached.  This emulation is less bandwidth efficient, but SVN
    1.1.x is becoming rarer now.
    
    Additionally, the --limit parameter in svn(1) uses the
    aforementioned get_log() functionality change in SVN 1.2.x.
    t9129 no longer depends on --limit to work and instead uses
    Perl to parse out the commit message.
    
    Thanks to Tom G. Christensen for the bug report.
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>

commit baf5fa8a7fdefc696fb1d79c95ae15a4cc779c1e
Author: Eric Wong <normalperson@yhbt.net>
Date:   Sun Jan 11 16:51:11 2009 -0800

    git-svn: better attempt to handle broken symlink updates
    
    This is a followup to 7fc35e0e94782bbbefb920875813519038659930,
    (workaround a for broken symlinks in SVN).
    
    Since broken SVN clients can commit svn:special files without
    the magic "link " prefix, this can affect delta application
    when we update the broken svn:special file.  So now we fall
    back and retry the delta application on symlinks if having
    a "link " prefix fails.
    
    Our behavior differs from svn(1) (v1.5.1) slightly:
    
      When a svn:special file is created w/o a "link " prefix, svn
      will create a regular file (mode 100644 to git) with the
      contents of the blob as-is.
    
      Our behavior is to continue creating the symlink (mode 120000
      to git) with the contents of the blob as-is.  While this
      differs from current svn(1) behavior, this is easier and more
      efficient to implement (and the correctness of the svn(1) is
      debatable, since it's a workaround for a bug in the first
      place).
    
    More information on this SVN bug is described here:
      http://subversion.tigris.org/issues/show_bug.cgi?id=2692
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>

commit 5ef8d77a752884f3c5fad9f143ce5cc72ff1340c
Author: Jeff King <peff@peff.net>
Date:   Sat Jan 17 10:32:30 2009 -0500

    color: make it easier for non-config to parse color specs
    
    We have very featureful color-parsing routines which are
    used for color.diff.* and other options. Let's make it
    easier to use those routines from other parts of the code.
    
    This patch adds a color_parse_mem() helper function which
    takes a length-bounded string instead of a NUL-terminated
    one. While the helper is only a few lines long, it is nice
    to abstract this out so that:
    
     - callers don't forget to free() the temporary buffer
    
     - right now, it is implemented in terms of color_parse().
       But it would be more efficient to reverse this and
       implement color_parse in terms of color_parse_mem.
    
    This also changes the error string for an invalid color not
    to mention the word "config", since it is not always
    appropriate (and when it is, the context is obvious since
    the offending config variable is given).
    
    Finally, while we are in the area, we clean up the parameter
    names in the declaration of color_parse; the var and value
    parameters were reversed from the actual implementation.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 879ef2485d6ced20845ca626ecb45a9b65aa3a70
Author: Johannes Schindelin <Johannes.Schindelin@gmx.de>
Date:   Sat Dec 20 13:05:14 2008 +0100

    Introduce commit notes
    
    Commit notes are blobs which are shown together with the commit
    message.  These blobs are taken from the notes ref, which you can
    configure by the config variable core.notesRef, which in turn can
    be overridden by the environment variable GIT_NOTES_REF.
    
    The notes ref is a branch which contains "files" whose names are
    the names of the corresponding commits (i.e. the SHA-1).
    
    The rationale for putting this information into a ref is this: we
    want to be able to fetch and possibly union-merge the notes,
    maybe even look at the date when a note was introduced, and we
    want to store them efficiently together with the other objects.
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 39c19ce2755830dd1dfdabf36e2b0166df3546f8
Author: Jakub Narebski <jnareb@gmail.com>
Date:   Thu Dec 11 01:33:29 2008 +0100

    gitweb: cache $parent_commit info in git_blame()
    
    Luben Tuikov changed 'lineno' link from leading to commit which gave
    current version of given block of lines, to leading to parent of this
    commit in 244a70e (Blame "linenr" link jumps to previous state at
    "orig_lineno").  This made possible data mining using 'blame' view.
    
    The current implementation calls rev-parse once per each blamed line
    to find parent revision of blamed commit, even when the same commit
    appears more than once, which is inefficient.
    
    This patch mitigates this issue by caching $parent_commit info in
    %metainfo, which makes gitweb call rev-parse only once per each
    unique commit in the output from "git blame".
    
    In the tables below you can see simple benchmark comparing gitweb
    performance before and after this patch
    
    File               | L[1] | C[2] || Time0[3] | Before[4] | After[4]
    ====================================================================
    blob.h             |   18 |    4 || 0m1.727s |  0m2.545s |  0m2.474s
    GIT-VERSION-GEN    |   42 |   13 || 0m2.165s |  0m2.448s |  0m2.071s
    README             |   46 |    6 || 0m1.593s |  0m2.727s |  0m2.242s
    revision.c         | 1923 |  121 || 0m2.357s | 0m30.365s |  0m7.028s
    gitweb/gitweb.perl | 6291 |  428 || 0m8.080s | 1m37.244s | 0m20.627s
    
    File               | L/C  | Before/After
    =========================================
    blob.h             |  4.5 |         1.03
    GIT-VERSION-GEN    |  3.2 |         1.18
    README             |  7.7 |         1.22
    revision.c         | 15.9 |         4.32
    gitweb/gitweb.perl | 14.7 |         4.71
    
    As you can see the greater ratio of lines in file to unique commits
    in blame output, the greater gain from the new implementation.
    
      Legend:
    
      [1] Number of lines:
          $ wc -l <file>
      [2] Number of unique commits in the blame output:
          $ git blame -p <file> | grep author-time | wc -l
      [3] Time for running "git blame -p" (user time, single run):
          $ time git blame -p <file> >/dev/null
      [4] Time to run gitweb as Perl script from command line:
          $ gitweb-run.sh "p=.git;a=blame;f=<file>" > /dev/null 2>&1
    
    The gitweb-run.sh script includes slightly modified (with adjusted
    pathnames) code from gitweb_run() function from the test script
    t/t9500-gitweb-standalone-no-errors.sh; gitweb config file
    gitweb_config.perl contents (again up to adjusting pathnames; in
    particular $projectroot variable should point to top directory of git
    repository) can be found in the same place.
    
    Discussion
    ~~~~~~~~~~
    
    A possible future improvement would be to open a bidi pipe to
    "git cat-file --batch-check", (like in Git::Repo in gitweb caching by
    Lea Wiemann), feed $long_rev^ to it, and parse its output, which is
    in the following form:
    
      926b07e694599d86cec668475071b32147c95034 commit 637
    
    This would mean one call to git-cat-file for the whole 'blame' view,
    instead of one call to git-rev-parse per each unique commit in blame
    output.
    
    Yet another solution would be to change use of validate_refname() to
    validate_revision() when checking script parameters (CGI query or
    path_info), with validate_revision being something like the following:
    
      sub validate_revision {
            my $rev = shift;
            return validate_refname(strip_rev_suffixes($rev));
      }
    
    so we don't need to calculate $long_rev^, but can pass "$long_rev^" as
    'hb' parameter.
    
    This solution has the advantage that it can be easily adapted to future
    incremental blame output.
    
    Signed-off-by: Jakub Narebski <jnareb@gmail.com>
    Acked-by: Luben Tuikov <ltuikov@yahoo.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 0c01857df57fe8723714e49459e0c061fcaf056b
Author: Jeff King <peff@peff.net>
Date:   Tue Dec 9 03:12:28 2008 -0500

    diff: fix handling of binary rewrite diffs
    
    The current emit_rewrite_diff code always writes a text patch without
    checking whether the content is binary. This means that if you end up with
    a rewrite diff for a binary file, you get lots of raw binary goo in your
    patch.
    
    Instead, if we have binary files, then let's just skip emit_rewrite_diff
    altogether. We will already have shown the "dissimilarity index" line, so
    it is really about the diff contents. If binary diffs are turned off, the
    "Binary files a/file and b/file differ" message should be the same in
    either case. If we do have binary patches turned on, there isn't much
    point in making a less-efficient binary patch that does a total rewrite;
    no human is going to read it, and since binary patches don't apply with
    any fuzz anyway, the result of application should be the same.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 8fd5bb7f44b7192a564ebe4f9db376af9fe148ec
Author: Pierre Habouzit <madcoder@debian.org>
Date:   Tue Nov 11 00:54:01 2008 +0100

    git send-email: add --annotate option
    
    This allows to review every patch (and fix various aspects of them, or
    comment them) in an editor just before being sent. Combined to the fact
    that git send-email can now process revision lists, this makes git
    send-email and efficient way to review and send patches interactively.
    
    Signed-off-by: Pierre Habouzit <madcoder@debian.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7f96e2e25aa008556a4ede7a65de8488eb9890e6
Author: John Chapman <thestar@fussycoder.id.au>
Date:   Sat Nov 8 14:22:48 2008 +1100

    git-p4: Support purged files and optimize memory usage
    
    Purged files are handled as if they are merely deleted, which is not
    entirely optimal, but I don't know of any other way to handle them.
    File data is deleted from memory as early as they can, and they are more
    efficiently handled, at (significant) cost to CPU usage.
    
    Still need to handle p4 branches with spaces in their names.
    Still need to make git-p4 clone more reliable.
     - Perhaps with a --continue option. (Sometimes the p4 server kills
     the connection)
    
    Signed-off-by: John Chapman <thestar@fussycoder.id.au>
    Acked-by: Simon Hausmann <simon@lst.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 4db09304f97b12d7db5d199eebb1f24c74e12d17
Author: Alexander Gavrilov <angavrilov@gmail.com>
Date:   Mon Oct 13 12:12:33 2008 +0400

    gitk: Implement batch lookup and caching of encoding attrs
    
    When the diff contains thousands of files, calling git-check-attr once
    per file is very slow.  With this patch gitk does attribute lookup in
    batches of 30 files while reading the diff file list, which leads to a
    very noticeable speedup.
    
    It may be possible to reimplement this even more efficiently, if
    git-check-attr is modified to support a --stdin-paths option.
    Additionally, it should quote the ':' character in file paths, or
    provide a more robust way of column separation.
    
    Signed-off-by: Alexander Gavrilov <angavrilov@gmail.com>
    Tested-by: Johannes Sixt <johannes.sixt@telecom.at>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit c8ef0383aca38bfedf85a311bbdd30008fcbebcf
Author: Pierre Habouzit <madcoder@debian.org>
Date:   Thu Oct 2 14:59:18 2008 +0200

    parse-opt: migrate fmt-merge-msg.
    
    Also fix an inefficient printf("%s", ...) where we can use write_in_full.
    
    Signed-off-by: Pierre Habouzit <madcoder@debian.org>
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>

commit 696235c6c1c85085fd55ff849663d44c51464e69
Author: Shawn O. Pearce <spearce@spearce.org>
Date:   Wed Jan 23 00:39:50 2008 -0500

    git-gui: Assume `blame --incremental` output is in UTF-8
    
    Most commits have author name encoded in UTF-8, but the incremental
    blame output dumps raw bytes and doesn't give us the encoding header
    from the commit.  Rather than fixing up tooltip data after we have
    viewed that particular commit in the blame viewer we can assume all
    names are in UTF-8.
    
    This is still going to cause problems when the author name is not
    encoded in UTF-8, but the only (efficient) way to solve that is to
    add an "encoding" header to the blame --incremental mode output,
    as otherwise we need to run `git cat-file commit $sha1` for each
    and every commit identified and that would be horribly expensive
    on any platform.
    
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>

commit d35825da6d5570524234e1bfe4ff0f57957b6db3
Author: Nicolas Pitre <nico@cam.org>
Date:   Fri Aug 29 16:08:02 2008 -0400

    fixup_pack_header_footer(): use nicely aligned buffer sizes
    
    It should be more efficient to use nicely aligned buffer sizes, either
    for filesystem operations or SHA1 checksums.  Also, using a relatively
    small nominal size might allow for the data to remain in L1 cache
    between both SHA1_Update() calls.
    
    Signed-off-by: Nicolas Pitre <nico@cam.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit ea27a18ce2bf5860974745c04c24864231029e1d
Author: Jeff King <peff@peff.net>
Date:   Tue Jul 22 03:14:12 2008 -0400

    spawn pager via run_command interface
    
    This has two important effects:
    
     1. The pager is now the _child_ process, instead of the
        parent. This means that whatever spawned git (e.g., the
        shell) will see the exit code of the git process, and
        not the pager.
    
     2. The mingw and regular code are now unified, which makes
        the setup_pager function much simpler.
    
    There are two caveats:
    
     1. We used to call execlp directly on the pager, followed
        by trying to exec it via the shall. We now just use the
        shell (which is what mingw has always done). This may
        have different results for pager names which contain
        shell metacharacters.
    
        It is also slightly less efficient because we
        unnecessarily run the shell; however, pager spawning is
        by definition an interactive task, so it shouldn't be
        a huge problem.
    
     2. The git process will remain in memory while the user
        looks through the pager. This is potentially wasteful.
        We could get around this by turning the parent into a
        meta-process which spawns _both_ git and the pager,
        collects the exit status from git, waits for both to
        end, and then exits with git's exit code.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1e5f764c93edfd8f6575b6ede769b079a1fc5a21
Author: Junio C Hamano <gitster@pobox.com>
Date:   Tue Jul 22 22:30:40 2008 -0700

    builtin-add.c: optimize -A option and "git add ."
    
    The earlier "git add -A" change was done in a quite inefficient
    way (i.e. it is as unefficient as "git add -u && git add ." modulo
    one fork/exec and read/write index).
    
    When the user asks "git add .", we do not have to examine all paths
    we encounter and perform the excluded() and dir_add_name()
    processing, both of which are slower code and use slower data structure
    by git standards, especially when the index is already populated.
    
    Instead, we implement "git add $pathspec..." as:
    
     - read the index;
    
     - read_directory() to process untracked, unignored files the current
       way, that is, recursively doing readdir(), filtering them by pathspec
       and excluded(), queueing them via dir_add_name() and finally do
       add_files(); and
    
     - iterate over the index, filtering them by pathspec, and update only
       the modified/type changed paths but not deleted ones.
    
    And "git add -A" becomes exactly the same as above, modulo:
    
     - missing $pathspec means "." instead of being an error; and
    
     - "iterate over the index" part handles deleted ones as well,
       i.e. exactly what the current update_callback() in builtin-add.c does.
    
    In either case, because fill_directory() does not use read_directory() to
    read everything in, we need to add an extra logic to iterate over the
    index to catch mistyped pathspec.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1f5c74f6cf918d317c73b328dcd4cf6f55c44d8a
Author: Nicolas Pitre <nico@cam.org>
Date:   Mon Jun 23 21:22:14 2008 -0400

    call init_pack_revindex() lazily
    
    This makes life much easier for next patch, as well as being more efficient
    when the revindex is actually not used.
    
    Signed-off-by: Nicolas Pitre <nico@cam.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit ffe256f9bac8a40ff751a9341a5869d98f72c285
Author: Adam Roben <aroben@apple.com>
Date:   Fri May 23 16:19:41 2008 +0200

    git-svn: Speed up fetch
    
    We were spending a lot of time forking/execing git-cat-file and
    git-hash-object. We now maintain a global Git repository object in order to use
    Git.pm's more efficient hash_and_insert_object and cat_blob methods.
    
    Signed-off-by: Adam Roben <aroben@apple.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7182530d8cad5ffc396bed5d37f97cfb14b7e599
Author: Adam Roben <aroben@apple.com>
Date:   Fri May 23 16:19:40 2008 +0200

    Git.pm: Add hash_and_insert_object and cat_blob
    
    These functions are more efficient ways of executing `git hash-object -w` and
    `git cat-file blob` when you are dealing with many files/objects.
    
    Signed-off-by: Adam Roben <aroben@apple.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit ca11b212eb3e31d6fee12e9974c67dc774c1bc7c
Author: Nicolas Pitre <nico@cam.org>
Date:   Wed May 14 01:33:53 2008 -0400

    let pack-objects do the writing of unreachable objects as loose objects
    
    Commit ccc1297226b184c40459e9d373cc9eebfb7bd898 changed the behavior
    of 'git repack -A' so unreachable objects are stored as loose objects.
    However it did so in a naive and inn efficient way by making packs
    about to be deleted inaccessible and feeding their content through
    'git unpack-objects'.  While this works, there are major flaws with
    this approach:
    
    - It is unacceptably sloooooooooooooow.
    
      In the Linux kernel repository with no actual unreachable objects,
      doing 'git repack -A -d' before:
    
            real    2m33.220s
            user    2m21.675s
            sys     0m3.510s
    
      And with this change:
    
            real    0m36.849s
            user    0m24.365s
            sys     0m1.950s
    
      For reference, here's the timing for 'git repack -a -d':
    
            real    0m35.816s
            user    0m22.571s
            sys     0m2.011s
    
      This is explained by the fact that 'git unpack-objects' was used to
      unpack _every_ objects even if (almost) 100% of them were thrown away.
    
    - There is a black out period.
    
      Between the removal of the .idx file for the redundant pack and the
      completion of its unpacking, the unreachable objects become completely
      unaccessible.  This is not a big issue as we're talking about unreachable
      objects, but some consistency is always good.
    
    - There is no way to easily set a sensible mtime for the newly created
      unreachable loose objects.
    
    So, while having a command called "pack-objects" to perform object
    unpacking looks really odd, this is probably the best compromize to be
    able to solve the above issues in an efficient way.
    
    Signed-off-by: Nicolas Pitre <nico@cam.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit c40641b77b0274186fd1b327d5dc3246f814aaaf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 9 09:21:07 2008 -0700

    Optimize symlink/directory detection
    
    This is the base for making symlink detection in the middle fo a pathname
    saner and (much) more efficient.
    
    Under various loads, we want to verify that the full path leading up to a
    filename is a real directory tree, and that when we successfully do an
    'lstat()' on a filename, we don't get a false positive due to a symlink in
    the middle of the path that git should have seen as a symlink, not as a
    normal path component.
    
    The 'has_symlink_leading_path()' function already did this, and cached
    a single level of symlink information, but didn't cache the _lack_ of a
    symlink, so the normal behaviour was actually the wrong way around, and we
    ended up doing an 'lstat()' on each path component to check that it was a
    real directory.
    
    This caches the last detected full directory and symlink entries, and
    speeds up especially deep directory structures a lot by avoiding to
    lstat() all the directories leading up to each entry in the index.
    
    [ This can - and should - probably be extended upon so that we eventually
      never do a bare 'lstat()' on any path entries at *all* when checking the
      index, but always check the full path carefully. Right now we do not
      generally check the whole path for all our normal quick index
      revalidation.
    
      We should also make sure that we're careful about all the invalidation,
      ie when we remove a link and replace it by a directory we should
      invalidate the symlink cache if it matches (and vice versa for the
      directory cache).
    
      But regardless, the basic function needs to be sane to do that. The old
      'has_symlink_leading_path()' was not capable enough - or indeed the code
      readable enough - to really do that sanely. So I'm pushing this as not
      just an optimization, but as a base for further work. ]
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit dbe48256b41c1e94d81f2458d7e84b1fdcb47026
Author: Clifford Caoile <piyo@users.sourceforge.net>
Date:   Fri Apr 18 22:07:12 2008 +0900

    git.el: Set process-environment instead of invoking env
    
    According to the similar patch from David Kågedal [1], "this will make
    it a little less posix-dependent and more efficient." However, there
    are two other areas that need to replaced, namely
    git-run-command-region and git-run-hooks. This patch implements the
    changes of [1] onto those Emacs Lisp functions.
    
    If unpatched, using the git port "msysgit" on Windows will require
    defadvice changes as shown at [2] (also explained at 4msysgit.git
    [3]).
    
    I have tested git-run-command-region on msysgit, because this is
    always called by git-commit (via git-commit-tree <- git-do-commit <-
    git-commit-file). However, I could not test git-run-hooks because it
    currently does not work on the Emacs Windows port. The latter reports
    the hooks files as a+rw and a-x, despite msysgit and cygwin chmod
    setting on the respective files.
    
    References:
    [1] f27e55864317611385be4d33b3c53ca787379df9
    [2] http://groups.google.com/group/msysgit/browse_thread/thread/b852fef689817707
    [3] http://repo.or.cz/w/git/mingw/4msysgit.git?a=commit;h=3c30e5e87358eba7b6d7dcd6301ae8438f0c30ea
    
    Signed-off-by: Clifford Caoile <piyo@users.sourceforge.net>
    Acked-by: David Kågedal <davidk@lysator.liu.se>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit e9f9d4f4b7558fc0f90268760f1b64e5d67c185c
Merge: 4581f4020a 12ecb01107
Author: Junio C Hamano <gitster@pobox.com>
Date:   Sat Apr 19 21:12:52 2008 -0700

    Merge branch 'jc/sha1-lookup'
    
    * jc/sha1-lookup:
      sha1-lookup: make selection of 'middle' less aggressive
      sha1-lookup: more memory efficient search in sorted list of SHA-1

commit 12ecb01107c4e77d3bccb5be5a0230c4546dafaf
Author: Junio C Hamano <gitster@pobox.com>
Date:   Sun Dec 30 03:13:27 2007 -0800

    sha1-lookup: make selection of 'middle' less aggressive
    
    If we pick 'mi' between 'lo' and 'hi' at 50%, which was what the
    simple binary search did, we are halving the search space
    whether the entry at 'mi' is lower or higher than the target.
    
    The previous patch was about picking not the middle but closer
    to 'hi', when we know the target is a lot closer to 'hi' than it
    is to 'lo'.  However, if it turns out that the entry at 'mi' is
    higher than the target, we would end up reducing the search
    space only by the difference between 'mi' and 'hi' (which by
    definition is less than 50% --- that was the whole point of not
    using the simple binary search), which made the search less
    efficient.  And the risk of overshooting becomes very high, if
    we try to be too precise.
    
    This tweaks the selection of 'mi' to be a bit closer to the
    middle than we would otherwise pick to avoid the problem.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 628522ec1439f414dcb1e71e300eb84a37ad1af9
Author: Junio C Hamano <gitster@pobox.com>
Date:   Sat Dec 29 02:05:47 2007 -0800

    sha1-lookup: more memory efficient search in sorted list of SHA-1
    
    Currently, when looking for a packed object from the pack idx, a
    simple binary search is used.
    
    A conventional binary search loop looks like this:
    
            unsigned lo, hi;
            do {
                    unsigned mi = (lo + hi) / 2;
                    int cmp = "entry pointed at by mi" minus "target";
                    if (!cmp)
                            return mi; "mi is the wanted one"
                    if (cmp > 0)
                            hi = mi; "mi is larger than target"
                    else
                            lo = mi+1; "mi is smaller than target"
            } while (lo < hi);
            "did not find what we wanted"
    
    The invariants are:
    
      - When entering the loop, 'lo' points at a slot that is never
        above the target (it could be at the target), 'hi' points at
        a slot that is guaranteed to be above the target (it can
        never be at the target).
    
      - We find a point 'mi' between 'lo' and 'hi' ('mi' could be
        the same as 'lo', but never can be as high as 'hi'), and
        check if 'mi' hits the target.  There are three cases:
    
         - if it is a hit, we have found what we are looking for;
    
         - if it is strictly higher than the target, we set it to
           'hi', and repeat the search.
    
         - if it is strictly lower than the target, we update 'lo'
           to one slot after it, because we allow 'lo' to be at the
           target and 'mi' is known to be below the target.
    
        If the loop exits, there is no matching entry.
    
    When choosing 'mi', we do not have to take the "middle" but
    anywhere in between 'lo' and 'hi', as long as lo <= mi < hi is
    satisfied.  When we somehow know that the distance between the
    target and 'lo' is much shorter than the target and 'hi', we
    could pick 'mi' that is much closer to 'lo' than (hi+lo)/2,
    which a conventional binary search would pick.
    
    This patch takes advantage of the fact that the SHA-1 is a good
    hash function, and as long as there are enough entries in the
    table, we can expect uniform distribution.  An entry that begins
    with for example "deadbeef..." is much likely to appear much
    later than in the midway of a reasonably populated table.  In
    fact, it can be expected to be near 87% (222/256) from the top
    of the table.
    
    This is a work-in-progress and has switches to allow easier
    experiments and debugging.  Exporting GIT_USE_LOOKUP environment
    variable enables this code.
    
    On my admittedly memory starved machine, with a partial KDE
    repository (3.0G pack with 95M idx):
    
        $ GIT_USE_LOOKUP=t git log -800 --stat HEAD >/dev/null
        3.93user 0.16system 0:04.09elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
        0inputs+0outputs (0major+55588minor)pagefaults 0swaps
    
    Without the patch, the numbers are:
    
        $ git log -800 --stat HEAD >/dev/null
        4.00user 0.15system 0:04.17elapsed 99%CPU (0avgtext+0avgdata 0maxresident)k
        0inputs+0outputs (0major+60258minor)pagefaults 0swaps
    
    In the same repository:
    
        $ GIT_USE_LOOKUP=t git log -2000 HEAD >/dev/null
        0.12user 0.00system 0:00.12elapsed 97%CPU (0avgtext+0avgdata 0maxresident)k
        0inputs+0outputs (0major+4241minor)pagefaults 0swaps
    
    Without the patch, the numbers are:
    
        $ git log -2000 HEAD >/dev/null
        0.05user 0.01system 0:00.07elapsed 100%CPU (0avgtext+0avgdata 0maxresident)k
        0inputs+0outputs (0major+8506minor)pagefaults 0swaps
    
    There isn't much time difference, but the number of minor faults
    seems to show that we are touching much smaller number of pages,
    which is expected.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit f27e55864317611385be4d33b3c53ca787379df9
Author: David Kågedal <davidk@lysator.liu.se>
Date:   Tue Feb 19 15:01:53 2008 +0100

    git.el: Set process-environment instead of invoking env
    
    This will make it a little less posix-dependent, and more efficient.
    
    Included is also a minor doc improvement.
    
    Signed-off-by: David Kågedal <davidk@lysator.liu.se>
    Acked-by: Alexandre Julliard <julliard@winehq.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 17e464266701bc1453f60a80cd71d8ba55b528e6
Author: Daniel Barkalow <barkalow@iabervon.org>
Date:   Thu Feb 7 11:39:52 2008 -0500

    Add flag to make unpack_trees() not print errors.
    
    (This applies only to errors where a plausible operation is impossible due
    to the particular data, not to errors resulting from misuse of the merge
    functions.)
    
    This will allow builtin-checkout to suppress merge errors if it's
    going to try more merging methods.
    
    Additionally, if unpack_trees() returns with an error, but without
    printing anything, it will roll back any changes to the index (by
    rereading the index, currently). This obviously could be done by the
    caller, but chances are that the caller would forget and debugging
    this is difficult. Also, future implementations may give unpack_trees() a
    more efficient way of undoing its changes than the caller could.
    
    Signed-off-by: Daniel Barkalow <barkalow@iabervon.org>

commit cf558704fb68514a820e3666968967c900e0fd29
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 22 18:41:14 2008 -0800

    Create pathname-based hash-table lookup into index
    
    This creates a hash index of every single file added to the index.
    Right now that hash index isn't actually used for much: I implemented a
    "cache_name_exists()" function that uses it to efficiently look up a
    filename in the index without having to do the O(logn) binary search,
    but quite frankly, that's not why this patch is interesting.
    
    No, the whole and only reason to create the hash of the filenames in the
    index is that by modifying the hash function, you can fairly easily do
    things like making it always hash equivalent names into the same bucket.
    
    That, in turn, means that suddenly questions like "does this name exist
    in the index under an _equivalent_ name?" becomes much much cheaper.
    
    Guiding principles behind this patch:
    
     - it shouldn't be too costly. In fact, my primary goal here was to
       actually speed up "git commit" with a fully populated kernel tree, by
       being faster at checking whether a file already existed in the index. I
       did succeed, but only barely:
    
            Best before:
                    [torvalds@woody linux]$ time git commit > /dev/null
                    real    0m0.255s
                    user    0m0.168s
                    sys     0m0.088s
    
            Best after:
    
                    [torvalds@woody linux]$ time ~/git/git commit > /dev/null
                    real    0m0.233s
                    user    0m0.144s
                    sys     0m0.088s
    
       so some things are actually faster (~8%).
    
       Caveat: that's really the best case. Other things are invariably going
       to be slightly slower, since we populate that index cache, and quite
       frankly, few things really use it to look things up.
    
       That said, the cost is really quite small. The worst case is probably
       doing a "git ls-files", which will do very little except puopulate the
       index, and never actually looks anything up in it, just lists it.
    
            Before:
                    [torvalds@woody linux]$ time git ls-files > /dev/null
                    real    0m0.016s
                    user    0m0.016s
                    sys     0m0.000s
    
            After:
                    [torvalds@woody linux]$ time ~/git/git ls-files > /dev/null
                    real    0m0.021s
                    user    0m0.012s
                    sys     0m0.008s
    
       and while the thing has really gotten relatively much slower, we're
       still talking about something almost unmeasurable (eg 5ms). And that
       really should be pretty much the worst case.
    
       So we lose 5ms on one "benchmark", but win 22ms on another. Pick your
       poison - this patch has the advantage that it will _likely_ speed up
       the cases that are complex and expensive more than it slows down the
       cases that are already so fast that nobody cares. But if you look at
       relative speedups/slowdowns, it doesn't look so good.
    
     - It should be simple and clean
    
       The code may be a bit subtle (the reasons I do hash removal the way I
       do etc), but it re-uses the existing hash.c files, so it really is
       fairly small and straightforward apart from a few odd details.
    
    Now, this patch on its own doesn't really do much, but I think it's worth
    looking at, if only because if done correctly, the name hashing really can
    make an improvement to the whole issue of "do we have a filename that
    looks like this in the index already". And at least it gets real testing
    by being used even by default (ie there is a real use-case for it even
    without any insane filesystems).
    
    NOTE NOTE NOTE! The current hash is a joke. I'm ashamed of it, I'm just
    not ashamed of it enough to really care. I took all the numbers out of my
    nether regions - I'm sure it's good enough that it works in practice, but
    the whole point was that you can make a really much fancier hash that
    hashes characters not directly, but by their upper-case value or something
    like that, and thus you get a case-insensitive hash, while still keeping
    the name and the index itself totally case sensitive.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 3e5f260144514f305a529bb2fe2efa7138a6b10a
Merge: d9cf4ec9f7 66ab84b99c
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Dec 12 16:53:06 2007 -0800

    Merge branch 'ew/svn-rev-db'
    
    * ew/svn-rev-db:
      git-svn: reinstate old rev_db optimization in new rev_map
      git-svn: replace .rev_db with a more space-efficient .rev_map format

commit d4110a9726c7cd5cda35b7dd03dc8f85fe3dff0c
Author: Charles Bailey <charles@hashpling.org>
Date:   Tue Dec 11 06:47:31 2007 +0000

    Fix clone not to ignore depth when performing a local clone
    
    When git-clone detects that it can perform a local clone it
    follows a path that silently ignores the depth parameter.
    
    Presumably if the user explicitly requests a shallow clone they
    have a reason to prefer a space efficient clone of just the recent
    history so bypass the local magic if the user specifies the depth
    parameter.
    
    Signed-off-by: Charles Bailey <charles@hashpling.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 060610c572b21d00bb09ebbab664c0452c7eac9d
Author: Eric Wong <normalperson@yhbt.net>
Date:   Sat Dec 8 23:27:41 2007 -0800

    git-svn: replace .rev_db with a more space-efficient .rev_map format
    
    Migrations are done automatically on an as-needed basis when new
    revisions are to be fetched.  Stale remote branches do not get
    migrated, yet.
    
    However, unless you set noMetadata or useSvkProps it's safe to
    just do:
    
      find $GIT_DIR/svn -name '.rev_db*' -print0 | xargs rm -f
    
    to purge all the old .rev_db files.
    
    The new format is a one-way migration and is NOT compatible with
    old versions of git-svn.
    
    This is the replacement for the rev_db format, which was too big
    and inefficient for large repositories with a lot of sparse history
    (mainly tags).
    
    The format is this:
    
      - 24 bytes for every record,
        * 4 bytes for the integer representing an SVN revision number
        * 20 bytes representing the sha1 of a git commit
    
      - No empty padding records like the old format
    
      - new records are written append-only since SVN revision numbers
        increase monotonically
    
      - lookups on SVN revision number are done via a binary search
    
      - Piping the file to xxd(1) -c24 is a good way of dumping it for
        viewing or editing, should the need ever arise.
    
    As with .rev_db, these files are disposable unless noMetadata or
    useSvmProps is set.
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 9fb88419ba85e641006c80db53620423f37f1c93
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 25 11:19:10 2007 -0700

    Ref-count the filespecs used by diffcore
    
    Rather than copy the filespecs when introducing new versions of them
    (for rename or copy detection), use a refcount and increment the count
    when reusing the diff_filespec.
    
    This avoids unnecessary allocations, but the real reason behind this is
    a future enhancement: we will want to track shared data across the
    copy/rename detection.  In order to efficiently notice when a filespec
    is used by a rename, the rename machinery wants to keep track of a
    rename usage count which is shared across all different users of the
    filespec.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 9ff74e95da5605778bdc7567ae63d70c96e6bed4
Author: Steven Walter <stevenrwalter@gmail.com>
Date:   Fri Sep 28 13:24:19 2007 -0400

    Don't checkout the full tree if avoidable
    
    In most cases of branching, the tree is copied unmodified from the trunk
    to the branch.  When that is done, we can simply start with the parent's
    index and apply the changes on the branch as usual.
    
    [ew: rewritten from Steven's original to use SVN::Client instead
         of the command-line svn client.
    
         Since SVN::Client connects separately, we'll share our
         authentication providers array between our usages of
         SVN::Client and SVN::Ra, too.  Bypassing the high-level
         SVN::Client library can avoid this, but the code will be
         much more complex.  Regardless, any implementation of this
         seems to require restarting a connection to the remote
         server.
    
         Also of note is that SVN 1.4 and later allows a more
         efficient diff_summary to be done instead of a full diff,
         but since this code is only to support SVN < 1.4.4, we'll
         ignore it for now.]
    
    Signed-off-by: Steven Walter <stevenrwalter@gmail.com>
    Signed-off-by: Eric Wong <normalperson@yhbt.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 66d4035e1099477aa488c24c0c081ee764f85868
Merge: e66273a6ab 9a76adebd6
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed Oct 3 03:06:02 2007 -0700

    Merge branch 'ph/strbuf'
    
    * ph/strbuf: (44 commits)
      Make read_patch_file work on a strbuf.
      strbuf_read_file enhancement, and use it.
      strbuf change: be sure ->buf is never ever NULL.
      double free in builtin-update-index.c
      Clean up stripspace a bit, use strbuf even more.
      Add strbuf_read_file().
      rerere: Fix use of an empty strbuf.buf
      Small cache_tree_write refactor.
      Make builtin-rerere use of strbuf nicer and more efficient.
      Add strbuf_cmp.
      strbuf_setlen(): do not barf on setting length of an empty buffer to 0
      sq_quote_argv and add_to_string rework with strbuf's.
      Full rework of quote_c_style and write_name_quoted.
      Rework unquote_c_style to work on a strbuf.
      strbuf API additions and enhancements.
      nfv?asprintf are broken without va_copy, workaround them.
      Fix the expansion pattern of the pseudo-static path buffer.
      builtin-for-each-ref.c::copy_name() - do not overstep the buffer.
      builtin-apply.c: fix a tiny leak introduced during xmemdupz() conversion.
      Use xmemdupz() in many places.
      ...

commit 95af39fcb2d84c8ef2844a9d890e3c67a2e0e1ec
Author: Keith Packard <keithp@keithp.com>
Date:   Tue Oct 2 22:44:15 2007 -0700

    Must not modify the_index.cache as it may be passed to realloc at some point.
    
    The index cache is not static, growing as new entries are added. If
    entries are added after prune_cache is called, cache will no longer
    point at the base of the allocation, and realloc will not be happy.
    
    I verified that this was the only place in the current source which
    modified any index_state.cache elements aside from the alloc/realloc
    calls in read-cache by changing the type of the element to 'struct
    cache_entry ** const cache' and recompiling.
    
    A more efficient patch would create a separate 'cache_base' value to
    track the allocation and then fix things up when reallocation was
    necessary, instead of the brute-force memmove used here.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a17ba31b0b75365e1245e494d46abae4afc57480
Merge: 0f729f2134 6d69b6f6ac
Author: Junio C Hamano <gitster@pobox.com>
Date:   Thu Sep 27 00:52:47 2007 -0700

    Merge branch 'ph/strbuf' into kh/commit
    
    * ph/strbuf:
      Clean up stripspace a bit, use strbuf even more.
      Add strbuf_read_file().
      rerere: Fix use of an empty strbuf.buf
      Small cache_tree_write refactor.
      Make builtin-rerere use of strbuf nicer and more efficient.
      Add strbuf_cmp.
      strbuf_setlen(): do not barf on setting length of an empty buffer to 0
      sq_quote_argv and add_to_string rework with strbuf's.
      Full rework of quote_c_style and write_name_quoted.
      Rework unquote_c_style to work on a strbuf.
      strbuf API additions and enhancements.
      nfv?asprintf are broken without va_copy, workaround them.
      Fix the expansion pattern of the pseudo-static path buffer.

commit 8289b6209552a57c255561a2585d0edbe96d62d3
Author: Pierre Habouzit <madcoder@debian.org>
Date:   Mon Sep 24 11:25:04 2007 +0200

    Make builtin-rerere use of strbuf nicer and more efficient.
    
    memory is now reused across hunks.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7fb1011e610a28518959b1d2d48cea17ecc32048
Author: Pierre Habouzit <madcoder@debian.org>
Date:   Thu Sep 20 00:42:14 2007 +0200

    Rework unquote_c_style to work on a strbuf.
    
    If the gain is not obvious in the diffstat, the resulting code is more
    readable, _and_ in checkout-index/update-index we now reuse the same buffer
    to unquote strings instead of always freeing/mallocing.
    
    This also is more coherent with the next patch that reworks quoting
    functions.
    
    The quoting function is also made more efficient scanning for backslashes
    and treating portions of strings without a backslash at once.
    
    Signed-off-by: Pierre Habouzit <madcoder@debian.org>

commit e3490268120946cfae737ab3a884b082e962209d
Author: Jeff King <peff@peff.net>
Date:   Tue Sep 18 03:26:27 2007 -0400

    contrib/fast-import: add perl version of simple example
    
    This is based on the git-import.sh script, but is a little
    more robust and efficient. More importantly, it should
    serve as a quick template for interfacing fast-import with
    perl scripts.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 1b655040bec1ff2f44fe309ac0f26085ed6372e9
Author: Alexandre Julliard <julliard@winehq.org>
Date:   Thu Sep 13 11:49:40 2007 +0200

    git.el: Keep the status buffer sorted by filename.
    
    This makes insertions and updates much more efficient.
    
    Signed-off-by: Alexandre Julliard <julliard@winehq.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a44131181a13eb599ed35647709692b1699706eb
Author: Junio C Hamano <gitster@pobox.com>
Date:   Tue Aug 14 01:40:45 2007 -0700

    attr.c: refactoring
    
    This splits out a common routine that parses a single line of
    attribute file and adds it to the attr_stack.  It should not
    change any behaviour, other than attrs array in the attr_stack
    structure is now grown with alloc_nr() macro, instead of one by
    one, which relied on xrealloc() to give enough slack to be
    efficient enough.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit becafaace69980d2980802432e86e7873317a4b6
Author: Shawn O. Pearce <spearce@spearce.org>
Date:   Wed Jul 25 04:20:02 2007 -0400

    git-gui: Show ref last update times in revision chooser tooltips
    
    If we can we now show the last modification date of a loose ref as
    part of the tooltip information shown in the revision picker.  This
    gives the user an indication of when was the last time that the ref
    was modified locally, and may especially be of interest when looking
    at a tracking branch.
    
    If we cannot find the loose ref file than we try to fallback on the
    reflog and scan it for the date of the last record.  We don't start
    with the reflog however as scanning it backwards from the end is not
    an easy thing to do in Tcl.  So I'm being lazy here and just going
    through the entire file, line by line.  Since that is less efficient
    than a single stat system call, its our fallback strategy.
    
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>

commit 16a7fcfe5e568b50ddebe2369600e71da67d1405
Author: Johannes Schindelin <Johannes.Schindelin@gmx.de>
Date:   Sun Jul 22 21:20:26 2007 +0100

    fsck --lost-found: write blob's contents, not their SHA-1
    
    When looking for a lost blob, it is much nicer to be able to grep
    through .git/lost-found/other/* than to write an inefficient loop
    over the file names.  So write the contents of the dangling blobs,
    not their object names.
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit b6f3481bb456acbbb990a1045344bb06e5a40283
Author: Shawn O. Pearce <spearce@spearce.org>
Date:   Sun Jul 15 01:40:37 2007 -0400

    Teach fast-import to recursively copy files/directories
    
    Some source material (e.g. Subversion dump files) perform directory
    renames by telling us the directory was copied, then deleted in the
    same revision.  This makes it difficult for a frontend to convert
    such data formats to a fast-import stream, as all the frontend has
    on hand is "Copy a/ to b/; Delete a/" with no details about what
    files are in a/, unless the frontend also kept track of all files.
    
    The new 'C' subcommand within a commit allows the frontend to make a
    recursive copy of one path to another path within the branch, without
    needing to keep track of the individual file paths.  The metadata
    copy is performed in memory efficiently, but is implemented as a
    copy-immediately operation, rather than copy-on-write.
    
    With this new 'C' subcommand frontends could obviously implement an
    'R' (rename) on their own as a combination of 'C' and 'D' (delete),
    but since we have already offered up 'R' in the past and it is a
    trivial thing to keep implemented I'm not going to deprecate it.
    
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>

commit f39a946a1fb0fa4856cd0027b9da3603a1b06fdc
Author: Shawn O. Pearce <spearce@spearce.org>
Date:   Mon Jul 9 22:58:23 2007 -0400

    Support wholesale directory renames in fast-import
    
    Some source material (e.g. Subversion dump files) perform directory
    renames without telling us exactly which files in that subdirectory
    were moved.  This makes it hard for a frontend to convert such data
    formats to a fast-import stream, as all the frontend has on hand
    is "Rename a/ to b/" with no details about what files are in a/,
    unless the frontend also kept track of all files.
    
    The new 'R' subcommand within a commit allows the frontend to
    rename either a file or an entire subdirectory, without needing to
    know the object's SHA-1 or the specific files contained within it.
    The rename is performed as efficiently as possible internally,
    making it cheaper than a 'D'/'M' pair for a file rename.
    
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>

commit 7618e6b1c1676dfdc2cc4c8af9e259c3e885825f
Author: Shawn O. Pearce <spearce@spearce.org>
Date:   Wed Jul 4 16:38:13 2007 -0400

    git-gui: Enhance choose_rev to handle hundreds of branches
    
    One of my production repositories has hundreds of remote tracking
    branches.  Trying to navigate these through a popup menu is just
    not possible.  The list is far larger than the screen and it does
    not scroll fast enough to efficiently select a branch name when
    trying to create a branch or delete a branch.
    
    This is major rewrite of the revision chooser mega-widget.  We
    now use a single listbox for all three major types of named refs
    (heads, tracking branches, tags) and a radio button group to pick
    which of those namespaces should be shown in the listbox.  A filter
    field is shown to the right allowing the end-user to key in a glob
    specification to filter the list they are viewing.  The filter is
    always taken as substring, so we assume * both starts and ends the
    pattern the user wanted but otherwise treat it as a glob pattern.
    
    This new picker works out really nicely.  What used to take me at
    least a minute to find and select a branch now takes mere seconds.
    
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>

commit 36e5e70e0f40cf7ca4351b8159d68f8560a2805f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 30 11:49:17 2007 -0700

    Start deprecating "git-command" in favor of "git command"
    
    I realize that a lot of people use the "git-xyzzy" format, and we have
    various historical reasons for it, but I also think that most people have
    long since started thinking of the git command as a single command with
    various subcommands, and we've long had the documentation talk about it
    that way.
    
    Slowly migrating away from the git-xyzzy format would allow us to
    eventually no longer install hundreds of binaries (even if most of them
    are symlinks or hardlinks) in users $PATH, and the _original_ reasons for
    it (implementation issues and bash completion) are really long long gone.
    
    Using "git xyzzy" also has some fundamental advantages, like the ability
    to specify things like paging ("git -p xyzzy") and making the whole notion
    of aliases act like other git commands (which they already do, but they do
    *not* have a "git-xyzzy" form!)
    
    Anyway, while actually removing the "git-xyzzy" things is not practical
    right now, we can certainly start slowly to deprecate it internally inside
    git itself - in the shell scripts we use, and the test vectors.
    
    This patch adds a "remove-dashes" makefile target, which does that. It
    isn't particularly efficient or smart, but it *does* successfully rewrite
    a lot of our shell scripts to use the "git xyzzy" form for all built-in
    commands.
    
    (For non-builtins, the "git xyzzy" format implies an extra execve(), so
    this script leaves those alone).
    
    So apply this patch, and then run
    
            make remove-dashes
            make test
            git commit -a
    
    to generate a much larger patch that actually starts this transformation.
    
    (The only half-way subtle thing about this is that it also fixes up
    git-filter-branch.sh for the new world order by adding quoting around
    the use of "git-commit-tree" as an argument. It doesn't need it in that
    format, but when changed into "git commit-tree" it is no longer a single
    word, and the quoting maintains the old behaviour).
    
    NOTE! This does not yet mean that you can actually stop installing the
    "git-xyzzy" binaries for the builtins. There are some remaining places
    that want to use the old form, this just removes the most obvious ones
    that can easily be done automatically.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit 7eb3cb9c683624681541972910328054e9431b43
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Jun 17 14:45:00 2007 +1000

    gitk: Implement a simple scheduler for the compute-intensive stuff
    
    This allows us to do compute-intensive processing, such as laying out
    the graph, relatively efficiently while also having the GUI be
    reasonably responsive.  The problem previously was that file events
    were serviced before X events, so reading from another process which
    supplies data quickly (hi git rev-list :) could mean that X events
    didn't get processed for a long time.
    
    With this, gitk finishes laying out the graph slightly sooner and
    still responds to the GUI while doing so.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit 3af51928ab7cefa35048e3a5a9e78a127749d405
Author: Alexandre Julliard <julliard@winehq.org>
Date:   Sun Jun 3 20:21:41 2007 +0200

    pack-check: Sort entries by pack offset before unpacking them.
    
    Because of the way objects are sorted in a pack, unpacking them in
    disk order is much more efficient than random access. Tests on the
    Wine repository show a gain in pack validation time of about 35%.
    
    Signed-off-by: Alexandre Julliard <julliard@winehq.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

commit a59b276e18f3d4a548caf549e05188cb1bd3a709
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 16 16:03:15 2007 -0700

    Add a generic "object decorator" interface, and make object refs use it
    
    This allows you to add an arbitrary "decoration" of your choice to any
    object.  It's a space- and time-efficient way to add information to
    arbitrary objects, especially if most objects probably do not have the
    decoration.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit ce4b4af7ff36d4e4999da937dffd2f9a3a420277
Author: Eric Wong <normalperson@yhbt.net>
Date:   Wed Jan 31 17:57:36 2007 -0800

    git-svn: use sys* IO functions for reading rev_db
    
    Using buffered IO for reading 40-41 bytes at a time isn't very
    efficient.  Buffering writes for a short duration is alright
    since we close() right away and buffers will be flushed.
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>

commit 3ebe8df7f690281c21e330eec156098c14f4e685
Author: Eric Wong <normalperson@yhbt.net>
Date:   Thu Jan 25 17:35:40 2007 -0800

    git-svn: fix segfaults from accessing svn_log_changed_path_t
    
    svn_log_changed_path_t structs were being used out of scope
    outside of svn_ra_get_log (because I wanted to eventually be
    able to use git-svn with only a single connection to the
    repository).  So now we dup them into a hash.
    
    This was fixed while making --follow-parent fetches more
    efficient.  I've moved parsing of the command-line --revision
    argument outside of the Git::SVN module so Git::SVN::fetch() can
    be used in more places (such as find_parent_branch).
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>

commit e6434f876097f196acbd9a806637d0f6076752fd
Author: Eric Wong <normalperson@yhbt.net>
Date:   Tue Jan 23 16:29:23 2007 -0800

    git-svn: 'init' attempts to connect to the repository root if possible
    
    This allows connections to be used more efficiently and not require
    users to run 'git-svn migrate --minimize' for new repositories.
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>

commit 47e39c55c91993b94824b7a317ebeb965aaeb45a
Author: Eric Wong <normalperson@yhbt.net>
Date:   Sun Jan 21 04:27:09 2007 -0800

    git-svn: enable --minimize to simplify the config and connections
    
    --minimize will update the git-svn configuration to attempt to
    connect to the repository root (instead of directly to the
    path(s) we are tracking) in order to allow more efficient reuse
    of connections (for multi-fetch and follow-parent).
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>

commit d489bc14919cdd37d3978065591199d21d6719f8
Author: Shawn O. Pearce <spearce@spearce.org>
Date:   Sun Jan 14 06:20:23 2007 -0500

    Improve reuse of sha1_file library within fast-import.
    
    Now that the sha1_file.c library routines use the sliding mmap
    routines to perform efficient access to portions of a packfile
    I can remove that code from fast-import.c and just invoke it.
    One benefit is we now have reloading support for any packfile which
    uses OBJ_OFS_DELTA.  Another is we have significantly less code
    to maintain.
    
    This code reuse change *requires* that fast-import generate only
    an OBJ_OFS_DELTA format packfile, as there is absolutely no index
    available to perform OBJ_REF_DELTA lookup in while unpacking
    an object.  This is probably reasonable to require as the delta
    offsets result in smaller packfiles and are faster to unpack,
    as no index searching is required.  Its also only a temporary
    requirement as users could always repack without offsets before
    making the import available to older versions of Git.
    
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>

commit 9bc879c1ced505089e2a1e420d32599bb15b35b5
Author: Shawn O. Pearce <spearce@spearce.org>
Date:   Sat Dec 23 02:34:01 2006 -0500

    Refactor how we open pack files to prepare for multiple windows.
    
    To efficiently support mmaping of multiple regions of the same pack
    file we want to keep the pack's file descriptor open while we are
    actively working with that pack.  So we are now keeping that file
    descriptor in packed_git.pack_fd and closing it only after we unmap
    the last window.
    
    This is going to increase the number of file descriptors that are
    in use at once, however that will be bounded by the total number of
    pack files present and therefore should not be very high.  It is
    a small tradeoff which we may need to revisit after some testing
    can be done on various repositories and systems.
    
    For code clarity we also want to seperate out the implementation
    of how we open a pack file from the implementation which locates
    a suitable window (or makes a new one) from the given pack file.
    Since this is a rather large delta I'm taking advantage of doing
    it now, in a fairly isolated change.
    
    When we open a pack file we need to examine the header and trailer
    without having a mmap in place, as we may only need to mmap
    the middle section of this particular pack.  Consequently the
    verification code has been refactored to make use of the new
    read_or_die function.
    
    Signed-off-by: Shawn O. Pearce <spearce@spearce.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 0864e3ba12ed946f0082297530584a77e2a7097b
Author: Eric Wong <normalperson@yhbt.net>
Date:   Tue Nov 28 02:50:17 2006 -0800

    git-svn: fix output reporting from the delta fetcher
    
    There was nothing printed in the code originally because I left
    out a pair of parentheses.  Nevertheless, the affected code has
    been replaced with a more efficient version that respects the -q
    flag as well as requiring less bandwidth.
    
    We save some bandwidth by not requesting changed paths
    information when calling get_log() since we're using the delta
    fetcher.
    
    Signed-off-by: Eric Wong <normalperson@yhbt.net>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit f6c0e191020ad330c06438c144e0ea787ca964fd
Author: Junio C Hamano <junkio@cox.net>
Date:   Sat Oct 21 02:56:33 2006 -0700

    git-pickaxe: get rid of wasteful find_origin().
    
    After finding out which path in the parent to scan to pass
    blames, using get_tree_entry() to extract the blob information
    again was quite wasteful, since diff-tree already gave us that
    information.  Separate the function to create an origin out as
    get_origin().
    
    You'll never know what is more efficient unless you try and/or
    think hard.  I somehow thought that extracting one known path
    out of commit's tree is cheaper than running a diff-tree for the
    current path between the commit and its parent, but it is not
    the case.  In real, non-toy projects, most commits do not touch
    the path you are interested in, and if the path is a few levels
    away from the toplevel, whole-subdirectory comparison logic
    diff-tree allows us to skip opening lower subdirectories.
    
    This commit rewrites find_origin() function to use a single-path
    diff-tree to see if the parent has the same blob as the current
    suspect, which is cheaper than extracting the blob information
    using get_tree_entry() and comparing it with what the current
    suspect has.  This shaves about 6% overhead when annotating
    kernel/sched.c in the Linux kernel repository on my machine.
    The saving rises to 25% for arch/i386/kernel/Makefile.
    
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 5d6f0935e6df017fcc446e348aebf4da2d210a27
Author: Junio C Hamano <junkio@cox.net>
Date:   Tue Sep 5 21:28:36 2006 -0700

    revision.c: allow injecting revision parameters after setup_revisions().
    
    setup_revisions() wants to get all the parameters at once and
    then postprocesses the resulting revs structure after it is done
    with them.  This code structure is a bit cumbersome to deal with
    efficiently when we want to inject revision parameters from the
    side (e.g. read from standard input).
    
    Fortunately, the nature of this postprocessing is not affected by
    revision parameters; they are affected only by flags.  So it is
    Ok to do add_object() after the it returns.
    
    This splits out the code that deals with the revision parameter
    out of the main loop of setup_revisions(), so that we can later
    call it from elsewhere after it returns.
    
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit ba84a797e76c27932e0b317c7ce54925e81093f7
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Thu Jul 6 10:16:22 2006 -0700

    builtin "git prune"
    
    This actually removes the objects to be pruned, unless you specify "-n"
    (at which point it will just tell you which files it would prune).
    
    This doesn't do the pack-file pruning that the shell-script used to do,
    but if somebody really wants to, they could add it easily enough. I wonder
    how useful it is, though, considering that "git repack -a -d" is just a
    lot more efficient and generates a better end result.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 855419f764a65e92f1d5dd1b3d50ee987db1d9de
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Mon Jun 19 10:44:15 2006 -0700

    Add specialized object allocator
    
    This creates a simple specialized object allocator for basic
    objects.
    
    This avoids wasting space with malloc overhead (metadata and
    extra alignment), since the specialized allocator knows the
    alignment, and that objects, once allocated, are never freed.
    
    It also allows us to track some basic statistics about object
    allocations. For example, for the mozilla import, it shows
    object usage as follows:
    
         blobs:   627629 (14710 kB)
         trees:  1119035 (34969 kB)
       commits:   196423  (8440 kB)
          tags:     1336    (46 kB)
    
    and the simpler allocator shaves off about 2.5% off the memory
    footprint off a "git-rev-list --all --objects", and is a bit
    faster too.
    
    [ Side note: this concludes the series of "save memory in object storage".
      The thing is, there simply isn't much more to be saved on the objects.
    
      Doing "git-rev-list --all --objects" on the mozilla archive has a final
      total RSS of 131498 pages for me: that's about 513MB. Of that, the
      object overhead is now just 56MB, the rest is going somewhere else (put
      another way: the fact that this patch shaves off 2.5% of the total
      memory overhead, considering that objects are now not much more than 10%
      of the total shows how big the wasted space really was: this makes
      object allocations much more memory- and time-efficient).
    
      I haven't looked at where the rest is, but I suspect the bulk of it is
      just the pack-file loading. It may be that we should pack the tree
      objects separately from the blob objects: for git-rev-list --objects, we
      don't actually ever need to even look at the blobs, but since trees and
      blobs are interspersed in the pack-file, we end up not being dense in
      the tree accesses, so we end up looking at more pages than we strictly
      need to.
    
      So with a 535MB pack-file, it's entirely possible - even likely - that
      most of the remaining RSS is just the mmap of the pack-file itself. We
      don't need to map in _all_ of it, but we do end up mapping a fair
      amount. ]
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 2d9c58c69d1bab601e67b036d0546e85abcee7eb
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Mon May 29 12:18:33 2006 -0700

    Remove "tree->entries" tree-entry list from tree parser
    
    Instead, just use the tree buffer directly, and use the tree-walk
    infrastructure to walk the buffers instead of the tree-entry list.
    
    The tree-entry list is inefficient, and generates tons of small
    allocations for no good reason. The tree-walk infrastructure is
    generally no harder to use than following a linked list, and allows
    us to do most tree parsing in-place.
    
    Some programs still use the old tree-entry lists, and are a bit
    painful to convert without major surgery. For them we have a helper
    function that creates a temporary tree-entry list on demand.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 3a7c352bd0ecac4b4c96c0995d61de9ef8d814f9
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Mon May 29 12:16:46 2006 -0700

    Make "tree_entry" have a SHA1 instead of a union of object pointers
    
    This is preparatory work for further cleanups, where we try to make
    tree_entry look more like the more efficient tree-walk descriptor.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 097dc3d8c32f4b85bf9701d5e1de98999ac25c1c
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Sun May 28 15:13:53 2006 -0700

    Remove "tree->entries" tree-entry list from tree parser
    
    This finally removes the tree-entry list from "struct tree", since most of
    the users can just use the tree-walk infrastructure to walk the raw tree
    buffers instead of the tree-entry list.
    
    The tree-entry list is inefficient, and generates tons of small
    allocations for no good reason. The tree-walk infrastructure is generally
    no harder to use than following a linked list, and allows us to do most
    tree parsing in-place.
    
    Some programs still use the old tree-entry lists, and are a bit painful to
    convert without major surgery. For them we have a helper function that
    creates a temporary tree-entry list on demand. We can convert those too
    eventually, but with this they no longer affect any users who don't need
    the explicit lists.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit a755dfe45c10ccd9f180d3c267602ad18d127d6a
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Sun May 28 15:10:04 2006 -0700

    Make "tree_entry" have a SHA1 instead of a union of object pointers
    
    This is preparatory work for further cleanups, where we try to make
    tree_entry look more like the more efficient tree-walk descriptor.
    
    Instead of having a union of pointers to blob/tree/objects, this just
    makes "struct tree_entry" have the raw SHA1, and makes all the users use
    that instead (often that implies adding a "lookup_tree(..)" on the sha1,
    but sometimes the user just wanted the SHA1 in the first place, and it
    just avoids an unnecessary indirection).
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 0fa6417c4911e8d252e562d3f58e737d0fdd0228
Merge: 94cdb38258 c3b06a69ff
Author: Junio C Hamano <junkio@cox.net>
Date:   Tue May 16 17:20:24 2006 -0700

    Merge branch 'np/pack'
    
    * np/pack:
      improve depth heuristic for maximum delta size
      pack-object: slightly more efficient
      simple euristic for further free packing improvements

commit 8dd84b0169864f638516b82df40b0a3ef6e2cb92
Merge: f6fb133b84 ff45715ce5
Author: Junio C Hamano <junkio@cox.net>
Date:   Mon May 15 13:51:09 2006 -0700

    Merge branch 'np/pack' into next
    
    * np/pack:
      pack-object: slightly more efficient
      simple euristic for further free packing improvements

commit ff45715ce50b80ab16ee0d0dc7fff0c47a51959a
Author: Nicolas Pitre <nico@cam.org>
Date:   Mon May 15 13:47:16 2006 -0400

    pack-object: slightly more efficient
    
    Avoid creating a delta index for objects with maximum depth since they
    are not going to be used as delta base anyway.  This also reduce peak
    memory usage slightly as the current object's delta index is not useful
    until the next object in the loop is considered for deltification. This
    saves a bit more than 1% on CPU usage.
    
    Signed-off-by: Nicolas Pitre <nico@cam.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 7927a55d5bde25702dca4fb1a7d6eb7ef61110ba
Author: Junio C Hamano <junkio@cox.net>
Date:   Thu Apr 27 01:33:07 2006 -0700

    read-tree: teach 1-way merege and plain read to prime cache-tree.
    
    This teaches read-tree to fully populate valid cache-tree when
    reading a tree from scratch, or reading a single tree into an
    existing index, reusing only the cached stat information (i.e.
    one-way merge).  We have already taught update-index about cache-tree,
    so "git checkout" followed by updates to a few path followed by
    a "git commit" would become very efficient.
    
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 79b2c75e043ad85f9a6b1a8d890b601a2f761a0e
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Apr 2 20:47:40 2006 +1000

    gitk: replace parent and children arrays with lists
    
    This will make it easier to switch between views efficiently, and
    turns out to be slightly faster as well.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit aa1dbc9897822c8acb284b35c40da60f3debca91
Author: Nick Hengeveld <nickh@reactrix.com>
Date:   Tue Mar 7 16:13:20 2006 -0800

    Update http-push functionality
    
    This brings http-push functionality more in line with the ssh/git version,
    by borrowing bits from send-pack and rev-list to process refspecs and
    revision history in more standard ways.  Also, the status of remote objects
    is determined using PROPFIND requests for the object directory rather than
    HEAD requests for each object - while it may be less efficient for small
    numbers of objects, this approach is able to get the status of all remote
    loose objects in a maximum of 256 requests.
    
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 8676eb43133cebe5121b8426f0e67f32c5cdefaa
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Sun Feb 26 15:51:24 2006 -0800

    Make git diff-generation use a simpler spawn-like interface
    
    Instead of depending of fork() and execve() and doing things in between
    the two, make the git diff functions do everything up front, and then do
    a single "spawn_prog()" invocation to run the actual external diff
    program (if any is even needed).
    
    This actually ends up simplifying the code, and should make it much
    easier to make it efficient under broken operating systems (read: Windows).
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 907380eeff27e9a07d6f1c03847c3d80f9e0e79a
Author: Junio C Hamano <junkio@cox.net>
Date:   Thu Feb 23 02:58:37 2006 -0800

    count-delta: tweak counting of copied source material.
    
    With the finer grained delta algorithm, count-delta algorithm
    started overcounting copied source material, since the new delta
    output tends to reuse the same source range more than once and
    more aggressively.  This broke an earlier assumption that the
    number of bytes copied out from the source buffer is a good
    approximation how much source material is actually remaining in
    the result.
    
    This uses fairly inefficient algorithm to keep track of ranges
    of source material that are actually copied out to the
    destination buffer.  With this tweak, the obvious rename/break
    detection tests in the testsuite start to work again.
    
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 164dcb97f0cd1accaec4ed40c24881f8e9ae4371
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Wed Feb 15 19:25:32 2006 -0800

    git-merge-tree: generalize the "traverse <n> trees in sync" functionality
    
    It's actually very useful for other things too. Notably, we could do the
    combined diff a lot more efficiently with this.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 461cf59f8924f174d7a0dcc3d77f576d93ed29a4
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Wed Jan 18 14:47:30 2006 -0800

    rev-list: stop when the file disappears
    
    The one thing I've considered doing (I really should) is to add a "stop
    when you don't find the file" option to "git-rev-list". This patch does
    some of the work towards that: it removes the "parent" thing when the
    file disappears, so a "git annotate" could do do something like
    
            git-rev-list --remove-empty --parents HEAD -- "$filename"
    
    and it would get a good graph that stops when the filename disappears
    (it's not perfect though: it won't remove all the unintersting commits).
    
    It also simplifies the logic of finding tree differences a bit, at the
    cost of making it a tad less efficient.
    
    The old logic was two-phase: it would first simplify _only_ merges tree as
    it traversed the tree, and then simplify the linear parts of the remainder
    independently. That was pretty optimal from an efficiency standpoint
    because it avoids doing any comparisons that we can see are unnecessary,
    but it made it much harder to understand than it really needed to be.
    
    The new logic is a lot more straightforward, and compares the trees as it
    traverses the graph (ie everything is a single phase). That makes it much
    easier to stop graph traversal at any point where a file disappears.
    
    As an example, let's say that you have a git repository that has had a
    file called "A" some time in the past. That file gets renamed to B, and
    then gets renamed back again to A. The old "git-rev-list" would show two
    commits: the commit that renames B to A (because it changes A) _and_ as
    its parent the commit that renames A to B (because it changes A).
    
    With the new --remove-empty flag, git-rev-list will show just the commit
    that renames B to A as the "root" commit, and stop traversal there
    (because that's what you want for "annotate" - you want to stop there, and
    for every "root" commit you then separately see if it really is a new
    file, or if the paths history disappeared because it was renamed from some
    other file).
    
    With this patch, you should be able to basically do a "poor mans 'git
    annotate'" with a fairly simple loop:
    
            push("HEAD", "$filename")
            while (revision,filename = pop()) {
                    for each i in $(git-rev-list --parents --remove-empty $revision -- "$filename")
    
                    pseudo-parents($i) = git-rev-list parents for that line
    
                    if (pseudo-parents($i) is non-empty) {
                            show diff of $i against pseudo-parents
                            continue
                    }
    
                    /* See if the _real_ parents of $i had a rename */
                    parent($i) = real-parent($i)
                    if (find-rename in $parent($i)->$i)
                            push $parent($i), "old-name"
            }
    
    which should be doable in perl or something (doing stacks in shell is just
    too painful to be worth it, so I'm not going to do this).
    
    Anybody want to try?
    
                    Linus

commit f0243f26f6d8403cd8e83c7d498c81e01a1f1735
Author: Johannes Schindelin <Johannes.Schindelin@gmx.de>
Date:   Fri Oct 28 04:48:32 2005 +0200

    git-upload-pack: More efficient usage of the has_sha1 array
    
    This patch is based on Junio's proposal. It marks parents of common revs
    so that they do not clutter up the has_sha1 array.
    
    Signed-off-by: Johannes Schindelin <Johannes.Schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 794f9fe7db30e2aff8f8f0543c6d18bf579cdbc2
Author: Johannes Schindelin <Johannes.Schindelin@gmx.de>
Date:   Sun Oct 23 03:36:06 2005 +0200

    git-upload-pack: More efficient usage of the has_sha1 array
    
    This patch is based on Junio's proposal. It marks parents of common revs
    so that they do not clutter up the has_sha1 array.
    
    Signed-off-by: Johannes Schindelin <Johannes.Schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 0910e8cab828b53fd7188a93c0476cab0af81cdc
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Wed Oct 19 00:01:01 2005 -0700

    Optimize common case of git-rev-list
    
    I took a look at webgit, and it looks like at least for the "projects"
    page, the most common operation ends up being basically
    
            git-rev-list --header --parents --max-count=1 HEAD
    
    Now, the thing is, the way "git-rev-list" works, it always keeps on
    popping the parents and parsing them in order to build the list of
    parents, and it turns out that even though we just want a single commit,
    git-rev-list will invariably look up _three_ generations of commits.
    
    It will parse:
     - the commit we want (it obviously needs this)
     - it's parent(s) as part of the "pop_most_recent_commit()" logic
     - it will then pop one of the parents before it notices that it doesn't
       need any more
     - and as part of popping the parent, it will parse the grandparent (again
       due to "pop_most_recent_commit()".
    
    Now, I've strace'd it, and it really is pretty efficient on the whole, but
    if things aren't nicely cached, and with long-latency IO, doing those two
    extra objects (at a minimum - if the parent is a merge it will be more) is
    just wasted time, and potentially a lot of it.
    
    So here's a quick special-case for the trivial case of "just one commit,
    and no date-limits or other special rules".
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 844ac7f81874f7acf22a15ffd985048d930c8341
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Tue Oct 18 23:52:08 2005 -0700

    git-fetch-pack: avoid unnecessary zero packing
    
    If everything is up-to-date locally, we don't need to even ask for a
    pack-file from the remote, or try to unpack it.
    
    This is especially important for tags - since the pack-file common commit
    logic is based purely on the commit history, it will never be able to find
    a common tag, and will thus always end up re-fetching them.
    
    Especially notably, if the tag points to a non-commit (eg a tagged tree),
    the pack-file would be unnecessarily big, just because it cannot any most
    recent common point between commits for pruning.
    
    Short-circuiting the case where we already have that reference means that
    we avoid a lot of these in the common case.
    
    NOTE! This only matches remote ref names against the same local name,
    which works well for tags, but is not as generic as it could be. If we
    ever need to, we could match against _any_ local ref (if we have it, we
    have it), but this "match against same name" is simpler and more
    efficient, and covers the common case.
    
    Renaming of refs is common for branch heads, but since those are always
    commits, the pack-file generation can optimize that case.
    
    In some cases we might still end up fetching pack-files unnecessarily, but
    this at least avoids the re-fetching of tags over and over if you use a
    regular
    
            git fetch --tags ...
    
    which was the main reason behind the change.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit fe5f51ce277df00cb01dfc984dba1e7128718e41
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Tue Oct 18 18:29:17 2005 -0700

    Optimize common case of git-rev-list
    
    I took a look at webgit, and it looks like at least for the "projects"
    page, the most common operation ends up being basically
    
            git-rev-list --header --parents --max-count=1 HEAD
    
    Now, the thing is, the way "git-rev-list" works, it always keeps on
    popping the parents and parsing them in order to build the list of
    parents, and it turns out that even though we just want a single commit,
    git-rev-list will invariably look up _three_ generations of commits.
    
    It will parse:
     - the commit we want (it obviously needs this)
     - it's parent(s) as part of the "pop_most_recent_commit()" logic
     - it will then pop one of the parents before it notices that it doesn't
       need any more
     - and as part of popping the parent, it will parse the grandparent (again
       due to "pop_most_recent_commit()".
    
    Now, I've strace'd it, and it really is pretty efficient on the whole, but
    if things aren't nicely cached, and with long-latency IO, doing those two
    extra objects (at a minimum - if the parent is a merge it will be more) is
    just wasted time, and potentially a lot of it.
    
    So here's a quick special-case for the trivial case of "just one commit,
    and no date-limits or other special rules".
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 2759cbc7741d7ce7bf44c2433fa1312d3fc7e64a
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Tue Oct 18 11:35:17 2005 -0700

    git-fetch-pack: avoid unnecessary zero packing
    
    If everything is up-to-date locally, we don't need to even ask for a
    pack-file from the remote, or try to unpack it.
    
    This is especially important for tags - since the pack-file common commit
    logic is based purely on the commit history, it will never be able to find
    a common tag, and will thus always end up re-fetching them.
    
    Especially notably, if the tag points to a non-commit (eg a tagged tree),
    the pack-file would be unnecessarily big, just because it cannot any most
    recent common point between commits for pruning.
    
    Short-circuiting the case where we already have that reference means that
    we avoid a lot of these in the common case.
    
    NOTE! This only matches remote ref names against the same local name,
    which works well for tags, but is not as generic as it could be. If we
    ever need to, we could match against _any_ local ref (if we have it, we
    have it), but this "match against same name" is simpler and more
    efficient, and covers the common case.
    
    Renaming of refs is common for branch heads, but since those are always
    commits, the pack-file generation can optimize that case.
    
    In some cases we might still end up fetching pack-files unnecessarily, but
    this at least avoids the re-fetching of tags over and over if you use a
    regular
    
            git fetch --tags ...
    
    which was the main reason behind the change.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit caf4f582b2bb4d5582540aa49d29749b2600cd03
Author: Junio C Hamano <junkio@cox.net>
Date:   Fri Oct 14 21:56:46 2005 -0700

    Improve "git add" again.
    
    This makes it possible to add paths that have funny characters (TAB
    and LF) in them, and makes adding many paths more efficient in
    general.
    
    New flag "--stdin" to update-index was initially added for different
    purpose, but it turns out to be a perfect match for feeding "ls-files
    --others -z" output to improve "git add".
    
    It also adds "--verbose" flag to update-index for use with "git add"
    command.
    
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 5be4efbefafcd5b81fe3d97e8395da1887b4902a
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Sun Aug 21 12:55:33 2005 -0700

    [PATCH] Make "git-ls-files" work in subdirectories
    
    This makes git-ls-files work inside a relative directory, and also adds
    some rudimentary filename globbing support. For example, in the kernel you
    can now do
    
            cd arch/i386
            git-ls-files
    
    and it will show all files under that subdirectory (and it will have
    removed the "arch/i386/" prefix unless you give it the "--full-name"
    option, so that you can feed the result to "xargs grep" or similar).
    
    The filename globbing is kind of strange: it does _not_ follow normal
    globbing rules, although it does look "almost" like a normal file glob
    (and it uses the POSIX.2 "fnmatch()" function).
    
    The glob pattern (there can be only one) is always split into a "directory
    part" and a "glob part", where the directory part is defined as any full
    directory path without any '*' or '?' characters. The "glob" part is
    whatever is left over.
    
    For example, when doing
    
            git-ls-files 'arch/i386/p*/*.c'
    
    the "directory part" is is "arch/i386/", and the "glob part" is "p*/*.c".
    The directory part will be added to the prefix, and handled efficiently
    (ie we will not be searching outside of that subdirectory), while the glob
    part (if anything is left over) will be used to trigger "fnmatch()"
    matches.
    
    This is efficient and very useful, but can result in somewhat
    non-intuitive behaviour.
    
    For example:
    
            git-ls-files 'arch/i386/*.[ch]'
    
    will find all .c and .h files under arch/i386/, _including_ things in
    lower subdirectories (ie it will match "arch/i386/kernel/process.c",
    because "kernel/process.c" will match the "*.c" specifier).
    
    Also, while
    
            git-ls-files arch/i386/
    
    will show all files under that subdirectory, doing the same without the
    final slash would try to show the file "i386" under the "arch/"
    subdirectory, and since there is no such file (even if there is such a
    _directory_) it will not match anything at all.
    
    These semantics may not seem intuitive, but they are actually very
    practical. In particular, it makes it very simple to do
    
            git-ls-files fs/*.c | xargs grep some_pattern
    
    and it does what you want.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Junio C Hamano <junkio@cox.net>

commit 12d2a18780321e0d00a0656f4965716ada910f66
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Sun Jul 3 13:29:54 2005 -0700

    "git rev-list --unpacked" shows only unpacked commits
    
    More infrastructure to do efficient incremental packs.

commit a733cb606fed08130317d803956e946f73b0b88e
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Tue Jun 28 14:21:02 2005 -0700

    Change pack file format. Hopefully for the last time.
    
    This also adds a header with a signature, version info, and the number
    of objects to the pack file.  It also encodes the file length and type
    more efficiently.

commit c323ac7d9c573c5ee8b45b9b9def92a4d4d8204d
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Jun 25 14:42:43 2005 -0700

    git-pack-objects: create a packed object representation.
    
    This is kind of like a tar-ball for a set of objects, ready to be
    shipped off to another end.  Alternatively, you could use is as a packed
    representation of the object database directly, if you changed
    "read_sha1_file()" to read these kinds of packs.
    
    The latter is partiularly useful to generate a "packed history", ie you
    could pack up your old history efficiently, but still have it available
    (at a performance hit, of course).
    
    I haven't actually written an unpacker yet, so the end result has not
    been verified in any way yet.  I obviously always write bug-free code,
    so it just has to work, no?

commit 438195ccedce7270cf5ba167a940c90467cb72d7
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Thu Jun 9 12:51:01 2005 -0700

    git-read-tree: add "--reset" flag
    
    This is the same as "-m", but it will silently ignore any unmerged
    entries, which makes it useful for efficiently forcing a new position
    regardless of the state of the current index file.
    
    IOW, to reset to a previous HEAD (in case you have had a failed
    merge, for example), you'd just do
    
            git-read-tree -u --reset HEAD
    
    which will also update your working tree to the right state.
    
    NOTE! The "update" will not remove files that may have been added by the
    merge.  Yet.

commit d9839e030592292267e6317fba308383e0012001
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Wed Apr 13 09:57:30 2005 -0700

    Make "fsck-cache" use the same revision tracking structure as "rev-tree".
    
    This makes things a lot more efficient, and makes it trivial to do things
    like reachability analysis.
    
    Add command line flags to tell what the head is, and whether to warn
    about unreachable objects.

commit 97d9c3cdeafb9f48075987c76373a71e6874c6eb
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Mon Apr 11 16:42:13 2005 -0700

    Make "rev-tree" more efficient and more useful.
    
    Slight change of output format: it now lists all parents on the same line.
    
    This allows it to work on initial commits too (which have no parents), and
    also makes the output format a lot more intuitive.

commit 711cf3a026a539f68ab647e012f145a03e12a5e7
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Mon Apr 11 09:39:21 2005 -0700

    Make "update-cache --refresh" do what it really should do: just
    refresh the "stat" information.
    
    We need this after having done a "read-tree", for example, when the
    stat information does not match the checked-out tree, and we want to
    start getting efficient cache matching against the parts of the tree
    that are already up-to-date.

commit 9174026cfe69d73ef80b27890615f8b2ef5c265a
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 9 13:00:54 2005 -0700

    Add "diff-tree" program to show which files have changed between two trees.
    
    Very useful for creating diffs efficiently, and in general to see what has
    changed in the namespace.
commit 7463064b28086c0a765e247bc8336f8e32356494
Author: Jeff King <peff@peff.net>
Date:   Tue Jun 22 12:06:41 2021 -0400

    object.h: add lookup_object_by_type() function
    
    In some cases it's useful for efficiency reasons to get the type of an
    object before deciding whether to parse it, but we still want an object
    struct. E.g., in reachable.c, bitmaps give us the type, but we just want
    to mark flags on each object. Likewise, we may loop over every object
    and only parse tags in order to peel them; checking the type first lets
    us avoid parsing the non-tags.
    
    But our lookup_blob(), etc, functions make getting an object struct
    annoying: we have to call the right function for every type. And we
    cannot just use the generic lookup_object(), because it only returns an
    already-seen object; it won't allocate a new object struct.
    
    Let's provide a function that dispatches to the correct lookup_*
    function based on a run-time type. In fact, reachable.c already has such
    a helper, so we'll just make that public.
    
    I did change the return type from "void *" to "struct object *". While
    the former is a clever way to avoid casting inside the function, it's
    less safe and less informative to people reading the function
    declaration.
    
    The next commit will add a new caller.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/object.c b/object.c
index 14188453c5..07fcf23d7b 100644
--- a/object.c
+++ b/object.c
@@ -185,6 +185,24 @@ struct object *lookup_unknown_object(struct repository *r, const struct object_i
 	return obj;
 }
 
+struct object *lookup_object_by_type(struct repository *r,
+			    const struct object_id *oid,
+			    enum object_type type)
+{
+	switch (type) {
+	case OBJ_COMMIT:
+		return (struct object *)lookup_commit(r, oid);
+	case OBJ_TREE:
+		return (struct object *)lookup_tree(r, oid);
+	case OBJ_TAG:
+		return (struct object *)lookup_tag(r, oid);
+	case OBJ_BLOB:
+		return (struct object *)lookup_blob(r, oid);
+	default:
+		die("BUG: unknown object type %d", type);
+	}
+}
+
 struct object *parse_object_buffer(struct repository *r, const struct object_id *oid, enum object_type type, unsigned long size, void *buffer, int *eaten_p)
 {
 	struct object *obj;
diff --git a/object.h b/object.h
index eb7e481c39..3b38c9cc98 100644
--- a/object.h
+++ b/object.h
@@ -158,6 +158,13 @@ struct object *parse_object_buffer(struct repository *r, const struct object_id
  */
 struct object *lookup_unknown_object(struct repository *r, const struct object_id *oid);
 
+/*
+ * Dispatch to the appropriate lookup_blob(), lookup_commit(), etc, based on
+ * "type".
+ */
+struct object *lookup_object_by_type(struct repository *r, const struct object_id *oid,
+				     enum object_type type);
+
 struct object_list *object_list_insert(struct object *item,
 				       struct object_list **list_p);
 
diff --git a/reachable.c b/reachable.c
index c59847257a..84e3d0d75e 100644
--- a/reachable.c
+++ b/reachable.c
@@ -159,24 +159,6 @@ int add_unseen_recent_objects_to_traversal(struct rev_info *revs,
 				      FOR_EACH_OBJECT_LOCAL_ONLY);
 }
 
-static void *lookup_object_by_type(struct repository *r,
-				   const struct object_id *oid,
-				   enum object_type type)
-{
-	switch (type) {
-	case OBJ_COMMIT:
-		return lookup_commit(r, oid);
-	case OBJ_TREE:
-		return lookup_tree(r, oid);
-	case OBJ_TAG:
-		return lookup_tag(r, oid);
-	case OBJ_BLOB:
-		return lookup_blob(r, oid);
-	default:
-		die("BUG: unknown object type %d", type);
-	}
-}
-
 static int mark_object_seen(const struct object_id *oid,
 			     enum object_type type,
 			     int exclude,

commit 680ff910b0329c8482f98ad9d3c49f4628c1bafa
Author: Jeff King <peff@peff.net>
Date:   Thu Jan 28 01:34:31 2021 -0500

    rerere: use strmap to store rerere directories
    
    We store a struct for each directory we access under .git/rr-cache. The
    structs are kept in an array sorted by the binary hash associated with
    their name (and we do lookups with a binary search).
    
    This works OK, but there are a few small downsides:
    
     - the amount of code isn't huge, but it's more than we'd need using one
       of our other stock data structures
    
     - the insertion into a sorted array is quadratic (though in practice
       it's unlikely anybody has enough conflicts for this to matter)
    
     - it's intimately tied to the representation of an object hash. This
       isn't a big deal, as the conflict ids we generate use the same hash,
       but it produces a few awkward bits (e.g., we are the only user of
       hash_pos() that is not using object_id).
    
    Let's instead just treat the directory names as strings, and store them
    in a strmap. This is less code, and removes the use of hash_pos().
    
    Insertion is now non-quadratic, though we probably use a bit more
    memory. Besides the hash table overhead, and storing hex bytes instead
    of a binary hash, we actually store each name twice. Other code expects
    to access the name of a rerere_dir struct from the struct itself, so we
    need a copy there. But strmap keeps its own copy of the name, as well.
    
    Using a bare hashmap instead of strmap means we could use the name for
    both, but at the cost of extra code (e.g., our own comparison function).
    Likewise, strmap has a feature to use a pointer to the in-struct name at
    the cost of a little extra code. I didn't do either here, as simple code
    seemed more important than squeezing out a few bytes of efficiency.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/rerere.c b/rerere.c
index e92e305f96..dee60dc6df 100644
--- a/rerere.c
+++ b/rerere.c
@@ -11,6 +11,7 @@
 #include "pathspec.h"
 #include "object-store.h"
 #include "hash-lookup.h"
+#include "strmap.h"
 
 #define RESOLVED 0
 #define PUNTED 1
@@ -23,26 +24,27 @@ static int rerere_enabled = -1;
 /* automatically update cleanly resolved paths to the index */
 static int rerere_autoupdate;
 
-static int rerere_dir_nr;
-static int rerere_dir_alloc;
-
 #define RR_HAS_POSTIMAGE 1
 #define RR_HAS_PREIMAGE 2
-static struct rerere_dir {
-	unsigned char hash[GIT_MAX_HEXSZ];
+struct rerere_dir {
 	int status_alloc, status_nr;
 	unsigned char *status;
-} **rerere_dir;
+	char name[FLEX_ARRAY];
+};
+
+static struct strmap rerere_dirs = STRMAP_INIT;
 
 static void free_rerere_dirs(void)
 {
-	int i;
-	for (i = 0; i < rerere_dir_nr; i++) {
-		free(rerere_dir[i]->status);
-		free(rerere_dir[i]);
+	struct hashmap_iter iter;
+	struct strmap_entry *ent;
+
+	strmap_for_each_entry(&rerere_dirs, &iter, ent) {
+		struct rerere_dir *rr_dir = ent->value;
+		free(rr_dir->status);
+		free(rr_dir);
 	}
-	FREE_AND_NULL(rerere_dir);
-	rerere_dir_nr = rerere_dir_alloc = 0;
+	strmap_clear(&rerere_dirs, 0);
 }
 
 static void free_rerere_id(struct string_list_item *item)
@@ -52,7 +54,7 @@ static void free_rerere_id(struct string_list_item *item)
 
 static const char *rerere_id_hex(const struct rerere_id *id)
 {
-	return hash_to_hex(id->collection->hash);
+	return id->collection->name;
 }
 
 static void fit_variant(struct rerere_dir *rr_dir, int variant)
@@ -115,7 +117,7 @@ static int is_rr_file(const char *name, const char *filename, int *variant)
 static void scan_rerere_dir(struct rerere_dir *rr_dir)
 {
 	struct dirent *de;
-	DIR *dir = opendir(git_path("rr-cache/%s", hash_to_hex(rr_dir->hash)));
+	DIR *dir = opendir(git_path("rr-cache/%s", rr_dir->name));
 
 	if (!dir)
 		return;
@@ -133,39 +135,21 @@ static void scan_rerere_dir(struct rerere_dir *rr_dir)
 	closedir(dir);
 }
 
-static const unsigned char *rerere_dir_hash(size_t i, void *table)
-{
-	struct rerere_dir **rr_dir = table;
-	return rr_dir[i]->hash;
-}
-
 static struct rerere_dir *find_rerere_dir(const char *hex)
 {
-	unsigned char hash[GIT_MAX_RAWSZ];
 	struct rerere_dir *rr_dir;
-	int pos;
-
-	if (get_sha1_hex(hex, hash))
-		BUG("cannot parse rerere dir hex?");
-	pos = hash_pos(hash, rerere_dir, rerere_dir_nr, rerere_dir_hash);
-	if (pos < 0) {
-		rr_dir = xmalloc(sizeof(*rr_dir));
-		hashcpy(rr_dir->hash, hash);
+
+	rr_dir = strmap_get(&rerere_dirs, hex);
+	if (!rr_dir) {
+		FLEX_ALLOC_STR(rr_dir, name, hex);
 		rr_dir->status = NULL;
 		rr_dir->status_nr = 0;
 		rr_dir->status_alloc = 0;
-		pos = -1 - pos;
-
-		/* Make sure the array is big enough ... */
-		ALLOC_GROW(rerere_dir, rerere_dir_nr + 1, rerere_dir_alloc);
-		/* ... and add it in. */
-		rerere_dir_nr++;
-		MOVE_ARRAY(rerere_dir + pos + 1, rerere_dir + pos,
-			   rerere_dir_nr - pos - 1);
-		rerere_dir[pos] = rr_dir;
+		strmap_put(&rerere_dirs, hex, rr_dir);
+
 		scan_rerere_dir(rr_dir);
 	}
-	return rerere_dir[pos];
+	return rr_dir;
 }
 
 static int has_rerere_resolution(const struct rerere_id *id)

commit 918d8ff78099004c561e0da90fa04cd629bb3b0e
Author: Eric Sunshine <sunshine@sunshineco.com>
Date:   Fri Jul 31 19:32:14 2020 -0400

    worktree: retire special-case normalization of main worktree path
    
    In order for "git-worktree list" to present consistent results,
    get_main_worktree() performs manual normalization on the repository
    path (returned by get_common_dir()) after passing it through
    strbuf_add_absolute_path(). In particular, it cleans up the path for
    three distinct cases when the current working directory is (1) the main
    worktree, (2) the .git/ subdirectory, or (3) a bare repository.
    
    The need for such special-cases is a direct consequence of employing
    strbuf_add_absolute_path() which, for the sake of efficiency, doesn't
    bother normalizing the path (such as folding out redundant path
    components) after making it absolute. Lack of normalization is not
    typically a problem since redundant path elements make no difference
    when working with paths at the filesystem level. However, when preparing
    paths for presentation, possible redundant path components make it
    difficult to ensure consistency.
    
    Eliminate the need for these special cases by instead making the path
    absolute via strbuf_add_real_path() which normalizes the path for us.
    Once normalized, the only case we need to handle manually is converting
    it to the path of the main worktree by stripping the "/.git" suffix.
    This stripping of the "/.git" suffix is a regular idiom in
    worktree-related code; for instance, it is employed by
    get_linked_worktree(), as well.
    
    Signed-off-by: Eric Sunshine <sunshine@sunshineco.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/worktree.c b/worktree.c
index 355824bf87..62217b4a6b 100644
--- a/worktree.c
+++ b/worktree.c
@@ -49,10 +49,8 @@ static struct worktree *get_main_worktree(void)
 	struct worktree *worktree = NULL;
 	struct strbuf worktree_path = STRBUF_INIT;
 
-	strbuf_add_absolute_path(&worktree_path, get_git_common_dir());
-	if (!strbuf_strip_suffix(&worktree_path, "/.git/.") && /* in .git */
-	    !strbuf_strip_suffix(&worktree_path, "/.git")) /* in worktree */
-		strbuf_strip_suffix(&worktree_path, "/."); /* in bare repo */
+	strbuf_add_real_path(&worktree_path, get_git_common_dir());
+	strbuf_strip_suffix(&worktree_path, "/.git");
 
 	worktree = xcalloc(1, sizeof(*worktree));
 	worktree->path = strbuf_detach(&worktree_path, NULL);

commit 4f3bd5606a02260274555f41fd7d6368f2bea1d8
Author: Jeff King <peff@peff.net>
Date:   Fri Feb 14 13:22:36 2020 -0500

    pack-bitmap: implement BLOB_NONE filtering
    
    We can easily support BLOB_NONE filters with bitmaps. Since we know the
    types of all of the objects, we just need to clear the result bits of
    any blobs.
    
    Note two subtleties in the implementation (which I also called out in
    comments):
    
      - we have to include any blobs that were specifically asked for (and
        not reached through graph traversal) to match the non-bitmap version
    
      - we have to handle in-pack and "ext_index" objects separately.
        Arguably prepare_bitmap_walk() could be adding these ext_index
        objects to the type bitmaps. But it doesn't for now, so let's match
        the rest of the bitmap code here (it probably wouldn't be an
        efficiency improvement to do so since the cost of extending those
        bitmaps is about the same as our loop here, but it might make the
        code a bit simpler).
    
    Here are perf results for the new test on git.git:
    
      Test                                    HEAD^             HEAD
      --------------------------------------------------------------------------------
      5310.9: rev-list count with blob:none   1.67(1.62+0.05)   0.22(0.21+0.02) -86.8%
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/pack-bitmap.c b/pack-bitmap.c
index 48c8694f92..dcf8a9aadf 100644
--- a/pack-bitmap.c
+++ b/pack-bitmap.c
@@ -712,6 +712,73 @@ static int in_bitmapped_pack(struct bitmap_index *bitmap_git,
 	return 0;
 }
 
+static struct bitmap *find_tip_blobs(struct bitmap_index *bitmap_git,
+				     struct object_list *tip_objects)
+{
+	struct bitmap *result = bitmap_new();
+	struct object_list *p;
+
+	for (p = tip_objects; p; p = p->next) {
+		int pos;
+
+		if (p->item->type != OBJ_BLOB)
+			continue;
+
+		pos = bitmap_position(bitmap_git, &p->item->oid);
+		if (pos < 0)
+			continue;
+
+		bitmap_set(result, pos);
+	}
+
+	return result;
+}
+
+static void filter_bitmap_blob_none(struct bitmap_index *bitmap_git,
+				    struct object_list *tip_objects,
+				    struct bitmap *to_filter)
+{
+	struct eindex *eindex = &bitmap_git->ext_index;
+	struct bitmap *tips;
+	struct ewah_iterator it;
+	eword_t mask;
+	uint32_t i;
+
+	/*
+	 * The non-bitmap version of this filter never removes
+	 * blobs which the other side specifically asked for,
+	 * so we must match that behavior.
+	 */
+	tips = find_tip_blobs(bitmap_git, tip_objects);
+
+	/*
+	 * We can use the blob type-bitmap to work in whole words
+	 * for the objects that are actually in the bitmapped packfile.
+	 */
+	for (i = 0, init_type_iterator(&it, bitmap_git, OBJ_BLOB);
+	     i < to_filter->word_alloc && ewah_iterator_next(&mask, &it);
+	     i++) {
+		if (i < tips->word_alloc)
+			mask &= ~tips->words[i];
+		to_filter->words[i] &= ~mask;
+	}
+
+	/*
+	 * Clear any blobs that weren't in the packfile (and so would not have
+	 * been caught by the loop above. We'll have to check them
+	 * individually.
+	 */
+	for (i = 0; i < eindex->count; i++) {
+		uint32_t pos = i + bitmap_git->pack->num_objects;
+		if (eindex->objects[i]->type == OBJ_BLOB &&
+		    bitmap_get(to_filter, pos) &&
+		    !bitmap_get(tips, pos))
+			bitmap_unset(to_filter, pos);
+	}
+
+	bitmap_free(tips);
+}
+
 static int filter_bitmap(struct bitmap_index *bitmap_git,
 			 struct object_list *tip_objects,
 			 struct bitmap *to_filter,
@@ -720,6 +787,13 @@ static int filter_bitmap(struct bitmap_index *bitmap_git,
 	if (!filter || filter->choice == LOFC_DISABLED)
 		return 0;
 
+	if (filter->choice == LOFC_BLOB_NONE) {
+		if (bitmap_git)
+			filter_bitmap_blob_none(bitmap_git, tip_objects,
+						to_filter);
+		return 0;
+	}
+
 	/* filter choice not handled */
 	return -1;
 }
diff --git a/t/perf/p5310-pack-bitmaps.sh b/t/perf/p5310-pack-bitmaps.sh
index e52f66ec9e..936742314c 100755
--- a/t/perf/p5310-pack-bitmaps.sh
+++ b/t/perf/p5310-pack-bitmaps.sh
@@ -47,6 +47,11 @@ test_perf 'rev-list (objects)' '
 	git rev-list --all --use-bitmap-index --objects >/dev/null
 '
 
+test_perf 'rev-list count with blob:none' '
+	git rev-list --use-bitmap-index --count --objects --all \
+		--filter=blob:none >/dev/null
+'
+
 test_expect_success 'create partial bitmap state' '
 	# pick a commit to represent the repo tip in the past
 	cutoff=$(git rev-list HEAD~100 -1) &&
diff --git a/t/t6113-rev-list-bitmap-filters.sh b/t/t6113-rev-list-bitmap-filters.sh
index 977f8d0930..f4e6d582f0 100755
--- a/t/t6113-rev-list-bitmap-filters.sh
+++ b/t/t6113-rev-list-bitmap-filters.sh
@@ -21,4 +21,18 @@ test_expect_success 'filters fallback to non-bitmap traversal' '
 	test_cmp expect actual
 '
 
+test_expect_success 'blob:none filter' '
+	git rev-list --objects --filter=blob:none HEAD >expect &&
+	git rev-list --use-bitmap-index \
+		     --objects --filter=blob:none HEAD >actual &&
+	test_bitmap_traversal expect actual
+'
+
+test_expect_success 'blob:none filter with specified blob' '
+	git rev-list --objects --filter=blob:none HEAD HEAD:two.t >expect &&
+	git rev-list --use-bitmap-index \
+		     --objects --filter=blob:none HEAD HEAD:two.t >actual &&
+	test_bitmap_traversal expect actual
+'
+
 test_done

commit c8d521faf72590fd4cd9bab3d20eb3de139f69d5
Author: Jeff King <peff@peff.net>
Date:   Thu Aug 16 08:13:07 2018 +0200

    Add delta-islands.{c,h}
    
    Hosting providers that allow users to "fork" existing
    repos want those forks to share as much disk space as
    possible.
    
    Alternates are an existing solution to keep all the
    objects from all the forks into a unique central repo,
    but this can have some drawbacks. Especially when
    packing the central repo, deltas will be created
    between objects from different forks.
    
    This can make cloning or fetching a fork much slower
    and much more CPU intensive as Git might have to
    compute new deltas for many objects to avoid sending
    objects from a different fork.
    
    Because the inefficiency primarily arises when an
    object is deltified against another object that does
    not exist in the same fork, we partition objects into
    sets that appear in the same fork, and define
    "delta islands". When finding delta base, we do not
    allow an object outside the same island to be
    considered as its base.
    
    So "delta islands" is a way to store objects from
    different forks in the same repo and packfile without
    having deltas between objects from different forks.
    
    This patch implements the delta islands mechanism in
    "delta-islands.{c,h}", but does not yet make use of it.
    
    A few new fields are added in 'struct object_entry'
    in "pack-objects.h" though.
    
    The documentation will follow in a patch that actually
    uses delta islands in "builtin/pack-objects.c".
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/Makefile b/Makefile
index bc4fc8eeab..e7994888e8 100644
--- a/Makefile
+++ b/Makefile
@@ -841,6 +841,7 @@ LIB_OBJS += csum-file.o
 LIB_OBJS += ctype.o
 LIB_OBJS += date.o
 LIB_OBJS += decorate.o
+LIB_OBJS += delta-islands.o
 LIB_OBJS += diffcore-break.o
 LIB_OBJS += diffcore-delta.o
 LIB_OBJS += diffcore-order.o
diff --git a/delta-islands.c b/delta-islands.c
new file mode 100644
index 0000000000..e7123d44a3
--- /dev/null
+++ b/delta-islands.c
@@ -0,0 +1,493 @@
+#include "cache.h"
+#include "attr.h"
+#include "object.h"
+#include "blob.h"
+#include "commit.h"
+#include "tag.h"
+#include "tree.h"
+#include "delta.h"
+#include "pack.h"
+#include "tree-walk.h"
+#include "diff.h"
+#include "revision.h"
+#include "list-objects.h"
+#include "progress.h"
+#include "refs.h"
+#include "khash.h"
+#include "pack-bitmap.h"
+#include "pack-objects.h"
+#include "delta-islands.h"
+#include "sha1-array.h"
+#include "config.h"
+
+KHASH_INIT(str, const char *, void *, 1, kh_str_hash_func, kh_str_hash_equal)
+
+static khash_sha1 *island_marks;
+static unsigned island_counter;
+static unsigned island_counter_core;
+
+static kh_str_t *remote_islands;
+
+struct remote_island {
+	uint64_t hash;
+	struct oid_array oids;
+};
+
+struct island_bitmap {
+	uint32_t refcount;
+	uint32_t bits[FLEX_ARRAY];
+};
+
+static uint32_t island_bitmap_size;
+
+/*
+ * Allocate a new bitmap; if "old" is not NULL, the new bitmap will be a copy
+ * of "old". Otherwise, the new bitmap is empty.
+ */
+static struct island_bitmap *island_bitmap_new(const struct island_bitmap *old)
+{
+	size_t size = sizeof(struct island_bitmap) + (island_bitmap_size * 4);
+	struct island_bitmap *b = xcalloc(1, size);
+
+	if (old)
+		memcpy(b, old, size);
+
+	b->refcount = 1;
+	return b;
+}
+
+static void island_bitmap_or(struct island_bitmap *a, const struct island_bitmap *b)
+{
+	uint32_t i;
+
+	for (i = 0; i < island_bitmap_size; ++i)
+		a->bits[i] |= b->bits[i];
+}
+
+static int island_bitmap_is_subset(struct island_bitmap *self,
+		struct island_bitmap *super)
+{
+	uint32_t i;
+
+	if (self == super)
+		return 1;
+
+	for (i = 0; i < island_bitmap_size; ++i) {
+		if ((self->bits[i] & super->bits[i]) != self->bits[i])
+			return 0;
+	}
+
+	return 1;
+}
+
+#define ISLAND_BITMAP_BLOCK(x) (x / 32)
+#define ISLAND_BITMAP_MASK(x) (1 << (x % 32))
+
+static void island_bitmap_set(struct island_bitmap *self, uint32_t i)
+{
+	self->bits[ISLAND_BITMAP_BLOCK(i)] |= ISLAND_BITMAP_MASK(i);
+}
+
+static int island_bitmap_get(struct island_bitmap *self, uint32_t i)
+{
+	return (self->bits[ISLAND_BITMAP_BLOCK(i)] & ISLAND_BITMAP_MASK(i)) != 0;
+}
+
+int in_same_island(const struct object_id *trg_oid, const struct object_id *src_oid)
+{
+	khiter_t trg_pos, src_pos;
+
+	/* If we aren't using islands, assume everything goes together. */
+	if (!island_marks)
+		return 1;
+
+	/*
+	 * If we don't have a bitmap for the target, we can delta it
+	 * against anything -- it's not an important object
+	 */
+	trg_pos = kh_get_sha1(island_marks, trg_oid->hash);
+	if (trg_pos >= kh_end(island_marks))
+		return 1;
+
+	/*
+	 * if the source (our delta base) doesn't have a bitmap,
+	 * we don't want to base any deltas on it!
+	 */
+	src_pos = kh_get_sha1(island_marks, src_oid->hash);
+	if (src_pos >= kh_end(island_marks))
+		return 0;
+
+	return island_bitmap_is_subset(kh_value(island_marks, trg_pos),
+				kh_value(island_marks, src_pos));
+}
+
+int island_delta_cmp(const struct object_id *a, const struct object_id *b)
+{
+	khiter_t a_pos, b_pos;
+	struct island_bitmap *a_bitmap = NULL, *b_bitmap = NULL;
+
+	if (!island_marks)
+		return 0;
+
+	a_pos = kh_get_sha1(island_marks, a->hash);
+	if (a_pos < kh_end(island_marks))
+		a_bitmap = kh_value(island_marks, a_pos);
+
+	b_pos = kh_get_sha1(island_marks, b->hash);
+	if (b_pos < kh_end(island_marks))
+		b_bitmap = kh_value(island_marks, b_pos);
+
+	if (a_bitmap) {
+		if (!b_bitmap || !island_bitmap_is_subset(a_bitmap, b_bitmap))
+			return -1;
+	}
+	if (b_bitmap) {
+		if (!a_bitmap || !island_bitmap_is_subset(b_bitmap, a_bitmap))
+			return 1;
+	}
+
+	return 0;
+}
+
+static struct island_bitmap *create_or_get_island_marks(struct object *obj)
+{
+	khiter_t pos;
+	int hash_ret;
+
+	pos = kh_put_sha1(island_marks, obj->oid.hash, &hash_ret);
+	if (hash_ret)
+		kh_value(island_marks, pos) = island_bitmap_new(NULL);
+
+	return kh_value(island_marks, pos);
+}
+
+static void set_island_marks(struct object *obj, struct island_bitmap *marks)
+{
+	struct island_bitmap *b;
+	khiter_t pos;
+	int hash_ret;
+
+	pos = kh_put_sha1(island_marks, obj->oid.hash, &hash_ret);
+	if (hash_ret) {
+		/*
+		 * We don't have one yet; make a copy-on-write of the
+		 * parent.
+		 */
+		marks->refcount++;
+		kh_value(island_marks, pos) = marks;
+		return;
+	}
+
+	/*
+	 * We do have it. Make sure we split any copy-on-write before
+	 * updating.
+	 */
+	b = kh_value(island_marks, pos);
+	if (b->refcount > 1) {
+		b->refcount--;
+		b = kh_value(island_marks, pos) = island_bitmap_new(b);
+	}
+	island_bitmap_or(b, marks);
+}
+
+static void mark_remote_island_1(struct remote_island *rl, int is_core_island)
+{
+	uint32_t i;
+
+	for (i = 0; i < rl->oids.nr; ++i) {
+		struct island_bitmap *marks;
+		struct object *obj = parse_object(the_repository, &rl->oids.oid[i]);
+
+		if (!obj)
+			continue;
+
+		marks = create_or_get_island_marks(obj);
+		island_bitmap_set(marks, island_counter);
+
+		if (is_core_island && obj->type == OBJ_COMMIT)
+			obj->flags |= NEEDS_BITMAP;
+
+		/* If it was a tag, also make sure we hit the underlying object. */
+		while (obj && obj->type == OBJ_TAG) {
+			obj = ((struct tag *)obj)->tagged;
+			if (obj) {
+				parse_object(the_repository, &obj->oid);
+				marks = create_or_get_island_marks(obj);
+				island_bitmap_set(marks, island_counter);
+			}
+		}
+	}
+
+	if (is_core_island)
+		island_counter_core = island_counter;
+
+	island_counter++;
+}
+
+static int cmp_tree_depth(const void *va, const void *vb)
+{
+	struct object_entry *a = *(struct object_entry **)va;
+	struct object_entry *b = *(struct object_entry **)vb;
+	return a->tree_depth - b->tree_depth;
+}
+
+void resolve_tree_islands(int progress, struct packing_data *to_pack)
+{
+	struct progress *progress_state = NULL;
+	struct object_entry **todo;
+	int nr = 0;
+	int i;
+
+	if (!island_marks)
+		return;
+
+	/*
+	 * We process only trees, as commits and tags have already been handled
+	 * (and passed their marks on to root trees, as well. We must make sure
+	 * to process them in descending tree-depth order so that marks
+	 * propagate down the tree properly, even if a sub-tree is found in
+	 * multiple parent trees.
+	 */
+	ALLOC_ARRAY(todo, to_pack->nr_objects);
+	for (i = 0; i < to_pack->nr_objects; i++) {
+		if (oe_type(&to_pack->objects[i]) == OBJ_TREE)
+			todo[nr++] = &to_pack->objects[i];
+	}
+	QSORT(todo, nr, cmp_tree_depth);
+
+	if (progress)
+		progress_state = start_progress(_("Propagating island marks"), nr);
+
+	for (i = 0; i < nr; i++) {
+		struct object_entry *ent = todo[i];
+		struct island_bitmap *root_marks;
+		struct tree *tree;
+		struct tree_desc desc;
+		struct name_entry entry;
+		khiter_t pos;
+
+		pos = kh_get_sha1(island_marks, ent->idx.oid.hash);
+		if (pos >= kh_end(island_marks))
+			continue;
+
+		root_marks = kh_value(island_marks, pos);
+
+		tree = lookup_tree(the_repository, &ent->idx.oid);
+		if (!tree || parse_tree(tree) < 0)
+			die(_("bad tree object %s"), oid_to_hex(&ent->idx.oid));
+
+		init_tree_desc(&desc, tree->buffer, tree->size);
+		while (tree_entry(&desc, &entry)) {
+			struct object *obj;
+
+			if (S_ISGITLINK(entry.mode))
+				continue;
+
+			obj = lookup_object(the_repository, entry.oid->hash);
+			if (!obj)
+				continue;
+
+			set_island_marks(obj, root_marks);
+		}
+
+		free_tree_buffer(tree);
+
+		display_progress(progress_state, i+1);
+	}
+
+	stop_progress(&progress_state);
+	free(todo);
+}
+
+static regex_t *island_regexes;
+static unsigned int island_regexes_alloc, island_regexes_nr;
+static const char *core_island_name;
+
+static int island_config_callback(const char *k, const char *v, void *cb)
+{
+	if (!strcmp(k, "pack.island")) {
+		struct strbuf re = STRBUF_INIT;
+
+		if (!v)
+			return config_error_nonbool(k);
+
+		ALLOC_GROW(island_regexes, island_regexes_nr + 1, island_regexes_alloc);
+
+		if (*v != '^')
+			strbuf_addch(&re, '^');
+		strbuf_addstr(&re, v);
+
+		if (regcomp(&island_regexes[island_regexes_nr], re.buf, REG_EXTENDED))
+			die(_("failed to load island regex for '%s': %s"), k, re.buf);
+
+		strbuf_release(&re);
+		island_regexes_nr++;
+		return 0;
+	}
+
+	if (!strcmp(k, "pack.islandcore"))
+		return git_config_string(&core_island_name, k, v);
+
+	return 0;
+}
+
+static void add_ref_to_island(const char *island_name, const struct object_id *oid)
+{
+	uint64_t sha_core;
+	struct remote_island *rl = NULL;
+
+	int hash_ret;
+	khiter_t pos = kh_put_str(remote_islands, island_name, &hash_ret);
+
+	if (hash_ret) {
+		kh_key(remote_islands, pos) = xstrdup(island_name);
+		kh_value(remote_islands, pos) = xcalloc(1, sizeof(struct remote_island));
+	}
+
+	rl = kh_value(remote_islands, pos);
+	oid_array_append(&rl->oids, oid);
+
+	memcpy(&sha_core, oid->hash, sizeof(uint64_t));
+	rl->hash += sha_core;
+}
+
+static int find_island_for_ref(const char *refname, const struct object_id *oid,
+			       int flags, void *data)
+{
+	/*
+	 * We should advertise 'ARRAY_SIZE(matches) - 2' as the max,
+	 * so we can diagnose below a config with more capture groups
+	 * than we support.
+	 */
+	regmatch_t matches[16];
+	int i, m;
+	struct strbuf island_name = STRBUF_INIT;
+
+	/* walk backwards to get last-one-wins ordering */
+	for (i = island_regexes_nr - 1; i >= 0; i--) {
+		if (!regexec(&island_regexes[i], refname,
+			     ARRAY_SIZE(matches), matches, 0))
+			break;
+	}
+
+	if (i < 0)
+		return 0;
+
+	if (matches[ARRAY_SIZE(matches) - 1].rm_so != -1)
+		warning(_("island regex from config has "
+			  "too many capture groups (max=%d)"),
+			(int)ARRAY_SIZE(matches) - 2);
+
+	for (m = 1; m < ARRAY_SIZE(matches); m++) {
+		regmatch_t *match = &matches[m];
+
+		if (match->rm_so == -1)
+			continue;
+
+		if (island_name.len)
+			strbuf_addch(&island_name, '-');
+
+		strbuf_add(&island_name, refname + match->rm_so, match->rm_eo - match->rm_so);
+	}
+
+	add_ref_to_island(island_name.buf, oid);
+	strbuf_release(&island_name);
+	return 0;
+}
+
+static struct remote_island *get_core_island(void)
+{
+	if (core_island_name) {
+		khiter_t pos = kh_get_str(remote_islands, core_island_name);
+		if (pos < kh_end(remote_islands))
+			return kh_value(remote_islands, pos);
+	}
+
+	return NULL;
+}
+
+static void deduplicate_islands(void)
+{
+	struct remote_island *island, *core = NULL, **list;
+	unsigned int island_count, dst, src, ref, i = 0;
+
+	island_count = kh_size(remote_islands);
+	ALLOC_ARRAY(list, island_count);
+
+	kh_foreach_value(remote_islands, island, {
+		list[i++] = island;
+	});
+
+	for (ref = 0; ref + 1 < island_count; ref++) {
+		for (src = ref + 1, dst = src; src < island_count; src++) {
+			if (list[ref]->hash == list[src]->hash)
+				continue;
+
+			if (src != dst)
+				list[dst] = list[src];
+
+			dst++;
+		}
+		island_count = dst;
+	}
+
+	island_bitmap_size = (island_count / 32) + 1;
+	core = get_core_island();
+
+	for (i = 0; i < island_count; ++i) {
+		mark_remote_island_1(list[i], core && list[i]->hash == core->hash);
+	}
+
+	free(list);
+}
+
+void load_delta_islands(void)
+{
+	island_marks = kh_init_sha1();
+	remote_islands = kh_init_str();
+
+	git_config(island_config_callback, NULL);
+	for_each_ref(find_island_for_ref, NULL);
+	deduplicate_islands();
+
+	fprintf(stderr, _("Marked %d islands, done.\n"), island_counter);
+}
+
+void propagate_island_marks(struct commit *commit)
+{
+	khiter_t pos = kh_get_sha1(island_marks, commit->object.oid.hash);
+
+	if (pos < kh_end(island_marks)) {
+		struct commit_list *p;
+		struct island_bitmap *root_marks = kh_value(island_marks, pos);
+
+		parse_commit(commit);
+		set_island_marks(&get_commit_tree(commit)->object, root_marks);
+		for (p = commit->parents; p; p = p->next)
+			set_island_marks(&p->item->object, root_marks);
+	}
+}
+
+int compute_pack_layers(struct packing_data *to_pack)
+{
+	uint32_t i;
+
+	if (!core_island_name || !island_marks)
+		return 1;
+
+	for (i = 0; i < to_pack->nr_objects; ++i) {
+		struct object_entry *entry = &to_pack->objects[i];
+		khiter_t pos = kh_get_sha1(island_marks, entry->idx.oid.hash);
+
+		entry->layer = 1;
+
+		if (pos < kh_end(island_marks)) {
+			struct island_bitmap *bitmap = kh_value(island_marks, pos);
+
+			if (island_bitmap_get(bitmap, island_counter_core))
+				entry->layer = 0;
+		}
+	}
+
+	return 2;
+}
diff --git a/delta-islands.h b/delta-islands.h
new file mode 100644
index 0000000000..f9725730f4
--- /dev/null
+++ b/delta-islands.h
@@ -0,0 +1,11 @@
+#ifndef DELTA_ISLANDS_H
+#define DELTA_ISLANDS_H
+
+int island_delta_cmp(const struct object_id *a, const struct object_id *b);
+int in_same_island(const struct object_id *, const struct object_id *);
+void resolve_tree_islands(int progress, struct packing_data *to_pack);
+void load_delta_islands(void);
+void propagate_island_marks(struct commit *commit);
+int compute_pack_layers(struct packing_data *to_pack);
+
+#endif /* DELTA_ISLANDS_H */
diff --git a/pack-objects.h b/pack-objects.h
index edf74dabdd..8eecd67991 100644
--- a/pack-objects.h
+++ b/pack-objects.h
@@ -100,6 +100,10 @@ struct object_entry {
 	unsigned type_:TYPE_BITS;
 	unsigned no_try_delta:1;
 	unsigned in_pack_type:TYPE_BITS; /* could be delta */
+
+	unsigned int tree_depth; /* should be repositioned for packing? */
+	unsigned char layer;
+
 	unsigned preferred_base:1; /*
 				    * we do not pack this, but is available
 				    * to be used as the base object to delta

commit 29ef759d7ca039590240890a604be8308b30a069
Author: Stefan Beller <sbeller@google.com>
Date:   Mon Aug 13 18:41:20 2018 -0700

    diff: use emit_line_0 once per line
    
    All lines that use emit_line_0 multiple times per line, are combined
    into a single call to emit_line_0, making use of the 'set' argument.
    
    We gain a little efficiency here, as we can omit emission of color and
    accompanying reset if 'len == 0'.
    
    Signed-off-by: Stefan Beller <sbeller@google.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/diff.c b/diff.c
index 4ef6638928..4f430f44e6 100644
--- a/diff.c
+++ b/diff.c
@@ -656,12 +656,14 @@ static void emit_line_0(struct diff_options *o,
 			fputs(set_sign, file);
 		if (first && !nofirst)
 			fputc(first, file);
-		if (set && set != set_sign) {
-			if (set_sign)
-				fputs(reset, file);
-			fputs(set, file);
+		if (len) {
+			if (set && set != set_sign) {
+				if (set_sign)
+					fputs(reset, file);
+				fputs(set, file);
+			}
+			fwrite(line, len, 1, file);
 		}
-		fwrite(line, len, 1, file);
 		fputs(reset, file);
 	}
 	if (has_trailing_carriage_return)
@@ -1207,9 +1209,7 @@ static void emit_line_ws_markup(struct diff_options *o,
 	if (!ws && !set_sign)
 		emit_line_0(o, set, NULL, 0, reset, sign, line, len);
 	else if (!ws) {
-		/* Emit just the prefix, then the rest. */
-		emit_line_0(o, set_sign, NULL, !!set_sign, reset, sign, "", 0);
-		emit_line_0(o, set, NULL, 0, reset, 0, line, len);
+		emit_line_0(o, set_sign, set, !!set_sign, reset, sign, line, len);
 	} else if (blank_at_eof)
 		/* Blank line at EOF - paint '+' as well */
 		emit_line_0(o, ws, NULL, 0, reset, sign, line, len);
diff --git a/t/t3206-range-diff.sh b/t/t3206-range-diff.sh
index 31f6458f96..7dc7c80a1d 100755
--- a/t/t3206-range-diff.sh
+++ b/t/t3206-range-diff.sh
@@ -151,7 +151,7 @@ test_expect_success 'dual-coloring' '
 	:         s/4/A/<RESET>
 	:     <RESET>
 	:    <REVERSE><GREEN>+<RESET><BOLD>    Also a silly comment here!<RESET>
-	:    <REVERSE><GREEN>+<RESET><BOLD><RESET>
+	:    <REVERSE><GREEN>+<RESET>
 	:     diff --git a/file b/file<RESET>
 	:    <RED> --- a/file<RESET>
 	:    <GREEN> +++ b/file<RESET>

commit 765b496dc6963ad8aaf40e9ac5dee358aa7fea47
Author: Jeff King <peff@peff.net>
Date:   Tue Jul 24 06:51:39 2018 -0400

    pass st.st_size as hint for strbuf_readlink()
    
    When we initially added the strbuf_readlink() function in
    b11b7e13f4 (Add generic 'strbuf_readlink()' helper function,
    2008-12-17), the point was that we generally have a _guess_
    as to the correct size based on the stat information, but we
    can't necessarily trust it.
    
    Over the years, a few callers have grown up that simply pass
    in 0, even though they have the stat information. Let's have
    them pass in their hint for consistency (and in theory
    efficiency, since it may avoid an extra resize/syscall loop,
    but neither location is probably performance critical).
    
    Note that st.st_size is actually an off_t, so in theory we
    need xsize_t() here. But none of the other callsites use it,
    and since this is just a hint, it doesn't matter either way
    (if we wrap we'll simply start with a too-small hint and
    then eventually complain when we cannot allocate the
    memory).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/builtin/init-db.c b/builtin/init-db.c
index 4ecf909368..12ddda7e7b 100644
--- a/builtin/init-db.c
+++ b/builtin/init-db.c
@@ -73,7 +73,8 @@ static void copy_templates_1(struct strbuf *path, struct strbuf *template_path,
 			continue;
 		else if (S_ISLNK(st_template.st_mode)) {
 			struct strbuf lnk = STRBUF_INIT;
-			if (strbuf_readlink(&lnk, template_path->buf, 0) < 0)
+			if (strbuf_readlink(&lnk, template_path->buf,
+					    st_template.st_size) < 0)
 				die_errno(_("cannot readlink '%s'"), template_path->buf);
 			if (symlink(lnk.buf, path->buf))
 				die_errno(_("cannot symlink '%s' '%s'"),
diff --git a/refs/files-backend.c b/refs/files-backend.c
index a9a066dcfb..c110c2520c 100644
--- a/refs/files-backend.c
+++ b/refs/files-backend.c
@@ -363,7 +363,7 @@ static int files_read_raw_ref(struct ref_store *ref_store,
 	/* Follow "normalized" - ie "refs/.." symlinks by hand */
 	if (S_ISLNK(st.st_mode)) {
 		strbuf_reset(&sb_contents);
-		if (strbuf_readlink(&sb_contents, path, 0) < 0) {
+		if (strbuf_readlink(&sb_contents, path, st.st_size) < 0) {
 			if (errno == ENOENT || errno == EINVAL)
 				/* inconsistent with lstat; retry */
 				goto stat_ref;

commit df11e1964825b825e179ccdbc1b9e3a6fc09e67a
Author: Jonathan Tan <jonathantanmy@google.com>
Date:   Fri Dec 8 15:27:15 2017 +0000

    rev-list: support termination at promisor objects
    
    Teach rev-list to support termination of an object traversal at any
    object from a promisor remote (whether one that the local repo also has,
    or one that the local repo knows about because it has another promisor
    object that references it).
    
    This will be used subsequently in gc and in the connectivity check used
    by fetch.
    
    For efficiency, if an object is referenced by a promisor object, and is
    in the local repo only as a non-promisor object, object traversal will
    not stop there. This is to avoid building the list of promisor object
    references.
    
    (In list-objects.c, the case where obj is NULL in process_blob() and
    process_tree() do not need to be changed because those happen only when
    there is a conflict between the expected type and the existing object.
    If the object doesn't exist, an object will be synthesized, which is
    fine.)
    
    Signed-off-by: Jonathan Tan <jonathantanmy@google.com>
    Signed-off-by: Jeff Hostetler <jeffhost@microsoft.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/Documentation/rev-list-options.txt b/Documentation/rev-list-options.txt
index 8d8b7f492a..0ce8ccdd43 100644
--- a/Documentation/rev-list-options.txt
+++ b/Documentation/rev-list-options.txt
@@ -745,10 +745,21 @@ The form '--missing=allow-any' will allow object traversal to continue
 if a missing object is encountered.  Missing objects will silently be
 omitted from the results.
 +
+The form '--missing=allow-promisor' is like 'allow-any', but will only
+allow object traversal to continue for EXPECTED promisor missing objects.
+Unexpected missing objects will raise an error.
++
 The form '--missing=print' is like 'allow-any', but will also print a
 list of the missing objects.  Object IDs are prefixed with a ``?'' character.
 endif::git-rev-list[]
 
+--exclude-promisor-objects::
+	(For internal use only.)  Prefilter object traversal at
+	promisor boundary.  This is used with partial clone.  This is
+	stronger than `--missing=allow-promisor` because it limits the
+	traversal, rather than just silencing errors about missing
+	objects.
+
 --no-walk[=(sorted|unsorted)]::
 	Only show the given commits, but do not traverse their ancestors.
 	This has no effect if a range is specified. If the argument
diff --git a/builtin/rev-list.c b/builtin/rev-list.c
index 159b035a2c..48f922d709 100644
--- a/builtin/rev-list.c
+++ b/builtin/rev-list.c
@@ -15,6 +15,7 @@
 #include "progress.h"
 #include "reflog-walk.h"
 #include "oidset.h"
+#include "packfile.h"
 
 static const char rev_list_usage[] =
 "git rev-list [OPTION] <commit-id>... [ -- paths... ]\n"
@@ -67,6 +68,7 @@ enum missing_action {
 	MA_ERROR = 0,    /* fail if any missing objects are encountered */
 	MA_ALLOW_ANY,    /* silently allow ALL missing objects */
 	MA_PRINT,        /* print ALL missing objects in special section */
+	MA_ALLOW_PROMISOR, /* silently allow all missing PROMISOR objects */
 };
 static enum missing_action arg_missing_action;
 
@@ -197,6 +199,12 @@ static void finish_commit(struct commit *commit, void *data)
 
 static inline void finish_object__ma(struct object *obj)
 {
+	/*
+	 * Whether or not we try to dynamically fetch missing objects
+	 * from the server, we currently DO NOT have the object.  We
+	 * can either print, allow (ignore), or conditionally allow
+	 * (ignore) them.
+	 */
 	switch (arg_missing_action) {
 	case MA_ERROR:
 		die("missing blob object '%s'", oid_to_hex(&obj->oid));
@@ -209,25 +217,36 @@ static inline void finish_object__ma(struct object *obj)
 		oidset_insert(&missing_objects, &obj->oid);
 		return;
 
+	case MA_ALLOW_PROMISOR:
+		if (is_promisor_object(&obj->oid))
+			return;
+		die("unexpected missing blob object '%s'",
+		    oid_to_hex(&obj->oid));
+		return;
+
 	default:
 		BUG("unhandled missing_action");
 		return;
 	}
 }
 
-static void finish_object(struct object *obj, const char *name, void *cb_data)
+static int finish_object(struct object *obj, const char *name, void *cb_data)
 {
 	struct rev_list_info *info = cb_data;
-	if (obj->type == OBJ_BLOB && !has_object_file(&obj->oid))
+	if (obj->type == OBJ_BLOB && !has_object_file(&obj->oid)) {
 		finish_object__ma(obj);
+		return 1;
+	}
 	if (info->revs->verify_objects && !obj->parsed && obj->type != OBJ_COMMIT)
 		parse_object(&obj->oid);
+	return 0;
 }
 
 static void show_object(struct object *obj, const char *name, void *cb_data)
 {
 	struct rev_list_info *info = cb_data;
-	finish_object(obj, name, cb_data);
+	if (finish_object(obj, name, cb_data))
+		return;
 	display_progress(progress, ++progress_counter);
 	if (info->flags & REV_LIST_QUIET)
 		return;
@@ -315,11 +334,19 @@ static inline int parse_missing_action_value(const char *value)
 
 	if (!strcmp(value, "allow-any")) {
 		arg_missing_action = MA_ALLOW_ANY;
+		fetch_if_missing = 0;
 		return 1;
 	}
 
 	if (!strcmp(value, "print")) {
 		arg_missing_action = MA_PRINT;
+		fetch_if_missing = 0;
+		return 1;
+	}
+
+	if (!strcmp(value, "allow-promisor")) {
+		arg_missing_action = MA_ALLOW_PROMISOR;
+		fetch_if_missing = 0;
 		return 1;
 	}
 
@@ -344,6 +371,35 @@ int cmd_rev_list(int argc, const char **argv, const char *prefix)
 	init_revisions(&revs, prefix);
 	revs.abbrev = DEFAULT_ABBREV;
 	revs.commit_format = CMIT_FMT_UNSPECIFIED;
+
+	/*
+	 * Scan the argument list before invoking setup_revisions(), so that we
+	 * know if fetch_if_missing needs to be set to 0.
+	 *
+	 * "--exclude-promisor-objects" acts as a pre-filter on missing objects
+	 * by not crossing the boundary from realized objects to promisor
+	 * objects.
+	 *
+	 * Let "--missing" to conditionally set fetch_if_missing.
+	 */
+	for (i = 1; i < argc; i++) {
+		const char *arg = argv[i];
+		if (!strcmp(arg, "--exclude-promisor-objects")) {
+			fetch_if_missing = 0;
+			revs.exclude_promisor_objects = 1;
+			break;
+		}
+	}
+	for (i = 1; i < argc; i++) {
+		const char *arg = argv[i];
+		if (skip_prefix(arg, "--missing=", &arg)) {
+			if (revs.exclude_promisor_objects)
+				die(_("cannot combine --exclude-promisor-objects and --missing"));
+			if (parse_missing_action_value(arg))
+				break;
+		}
+	}
+
 	argc = setup_revisions(argc, argv, &revs, NULL);
 
 	memset(&info, 0, sizeof(info));
@@ -412,9 +468,10 @@ int cmd_rev_list(int argc, const char **argv, const char *prefix)
 			continue;
 		}
 
-		if (skip_prefix(arg, "--missing=", &arg) &&
-		    parse_missing_action_value(arg))
-			continue;
+		if (!strcmp(arg, "--exclude-promisor-objects"))
+			continue; /* already handled above */
+		if (skip_prefix(arg, "--missing=", &arg))
+			continue; /* already handled above */
 
 		usage(rev_list_usage);
 
diff --git a/list-objects.c b/list-objects.c
index d9e83d05e1..58621fc6ee 100644
--- a/list-objects.c
+++ b/list-objects.c
@@ -9,6 +9,7 @@
 #include "list-objects.h"
 #include "list-objects-filter.h"
 #include "list-objects-filter-options.h"
+#include "packfile.h"
 
 static void process_blob(struct rev_info *revs,
 			 struct blob *blob,
@@ -30,6 +31,20 @@ static void process_blob(struct rev_info *revs,
 	if (obj->flags & (UNINTERESTING | SEEN))
 		return;
 
+	/*
+	 * Pre-filter known-missing objects when explicitly requested.
+	 * Otherwise, a missing object error message may be reported
+	 * later (depending on other filtering criteria).
+	 *
+	 * Note that this "--exclude-promisor-objects" pre-filtering
+	 * may cause the actual filter to report an incomplete list
+	 * of missing objects.
+	 */
+	if (revs->exclude_promisor_objects &&
+	    !has_object_file(&obj->oid) &&
+	    is_promisor_object(&obj->oid))
+		return;
+
 	pathlen = path->len;
 	strbuf_addstr(path, name);
 	if (filter_fn)
@@ -91,6 +106,8 @@ static void process_tree(struct rev_info *revs,
 		all_entries_interesting: entry_not_interesting;
 	int baselen = base->len;
 	enum list_objects_filter_result r = LOFR_MARK_SEEN | LOFR_DO_SHOW;
+	int gently = revs->ignore_missing_links ||
+		     revs->exclude_promisor_objects;
 
 	if (!revs->tree_objects)
 		return;
@@ -98,9 +115,19 @@ static void process_tree(struct rev_info *revs,
 		die("bad tree object");
 	if (obj->flags & (UNINTERESTING | SEEN))
 		return;
-	if (parse_tree_gently(tree, revs->ignore_missing_links) < 0) {
+	if (parse_tree_gently(tree, gently) < 0) {
 		if (revs->ignore_missing_links)
 			return;
+
+		/*
+		 * Pre-filter known-missing tree objects when explicitly
+		 * requested.  This may cause the actual filter to report
+		 * an incomplete list of missing objects.
+		 */
+		if (revs->exclude_promisor_objects &&
+		    is_promisor_object(&obj->oid))
+			return;
+
 		die("bad tree object %s", oid_to_hex(&obj->oid));
 	}
 
diff --git a/object.c b/object.c
index b9a4a0e501..4c222d6260 100644
--- a/object.c
+++ b/object.c
@@ -252,7 +252,7 @@ struct object *parse_object(const struct object_id *oid)
 	if (obj && obj->parsed)
 		return obj;
 
-	if ((obj && obj->type == OBJ_BLOB) ||
+	if ((obj && obj->type == OBJ_BLOB && has_object_file(oid)) ||
 	    (!obj && has_object_file(oid) &&
 	     sha1_object_info(oid->hash, NULL) == OBJ_BLOB)) {
 		if (check_sha1_signature(repl, NULL, 0, NULL) < 0) {
diff --git a/revision.c b/revision.c
index d167223e69..05a7aac063 100644
--- a/revision.c
+++ b/revision.c
@@ -198,6 +198,8 @@ static struct object *get_reference(struct rev_info *revs, const char *name,
 	if (!object) {
 		if (revs->ignore_missing)
 			return object;
+		if (revs->exclude_promisor_objects && is_promisor_object(oid))
+			return NULL;
 		die("bad object %s", name);
 	}
 	object->flags |= flags;
@@ -790,9 +792,17 @@ static int add_parents_to_list(struct rev_info *revs, struct commit *commit,
 
 	for (parent = commit->parents; parent; parent = parent->next) {
 		struct commit *p = parent->item;
-
-		if (parse_commit_gently(p, revs->ignore_missing_links) < 0)
+		int gently = revs->ignore_missing_links ||
+			     revs->exclude_promisor_objects;
+		if (parse_commit_gently(p, gently) < 0) {
+			if (revs->exclude_promisor_objects &&
+			    is_promisor_object(&p->object.oid)) {
+				if (revs->first_parent_only)
+					break;
+				continue;
+			}
 			return -1;
+		}
 		if (revs->show_source && !p->util)
 			p->util = commit->util;
 		p->object.flags |= left_flag;
@@ -2088,6 +2098,10 @@ static int handle_revision_opt(struct rev_info *revs, int argc, const char **arg
 		revs->limited = 1;
 	} else if (!strcmp(arg, "--ignore-missing")) {
 		revs->ignore_missing = 1;
+	} else if (!strcmp(arg, "--exclude-promisor-objects")) {
+		if (fetch_if_missing)
+			die("BUG: exclude_promisor_objects can only be used when fetch_if_missing is 0");
+		revs->exclude_promisor_objects = 1;
 	} else {
 		int opts = diff_opt_parse(&revs->diffopt, argv, argc, revs->prefix);
 		if (!opts)
@@ -2830,6 +2844,16 @@ void reset_revision_walk(void)
 	clear_object_flags(SEEN | ADDED | SHOWN);
 }
 
+static int mark_uninteresting(const struct object_id *oid,
+			      struct packed_git *pack,
+			      uint32_t pos,
+			      void *unused)
+{
+	struct object *o = parse_object(oid);
+	o->flags |= UNINTERESTING | SEEN;
+	return 0;
+}
+
 int prepare_revision_walk(struct rev_info *revs)
 {
 	int i;
@@ -2858,6 +2882,11 @@ int prepare_revision_walk(struct rev_info *revs)
 	    (revs->limited && limiting_can_increase_treesame(revs)))
 		revs->treesame.name = "treesame";
 
+	if (revs->exclude_promisor_objects) {
+		for_each_packed_object(mark_uninteresting, NULL,
+				       FOR_EACH_OBJECT_PROMISOR_ONLY);
+	}
+
 	if (revs->no_walk != REVISION_WALK_NO_WALK_UNSORTED)
 		commit_list_sort_by_date(&revs->commits);
 	if (revs->no_walk)
diff --git a/revision.h b/revision.h
index 54761200ad..5f9a49ca66 100644
--- a/revision.h
+++ b/revision.h
@@ -121,7 +121,10 @@ struct rev_info {
 			bisect:1,
 			ancestry_path:1,
 			first_parent_only:1,
-			line_level_traverse:1;
+			line_level_traverse:1,
+
+			/* for internal use only */
+			exclude_promisor_objects:1;
 
 	/* Diff flags */
 	unsigned int	diff:1,
diff --git a/t/t0410-partial-clone.sh b/t/t0410-partial-clone.sh
index 8a90f6ab34..3ca6af5cda 100755
--- a/t/t0410-partial-clone.sh
+++ b/t/t0410-partial-clone.sh
@@ -160,6 +160,107 @@ test_expect_success 'fetching of missing objects' '
 	git verify-pack --verbose "$IDX" | grep "$HASH"
 '
 
+test_expect_success 'rev-list stops traversal at missing and promised commit' '
+	rm -rf repo &&
+	test_create_repo repo &&
+	test_commit -C repo foo &&
+	test_commit -C repo bar &&
+
+	FOO=$(git -C repo rev-parse foo) &&
+	promise_and_delete "$FOO" &&
+
+	git -C repo config core.repositoryformatversion 1 &&
+	git -C repo config extensions.partialclone "arbitrary string" &&
+	git -C repo rev-list --exclude-promisor-objects --objects bar >out &&
+	grep $(git -C repo rev-parse bar) out &&
+	! grep $FOO out
+'
+
+test_expect_success 'rev-list stops traversal at missing and promised tree' '
+	rm -rf repo &&
+	test_create_repo repo &&
+	test_commit -C repo foo &&
+	mkdir repo/a_dir &&
+	echo something >repo/a_dir/something &&
+	git -C repo add a_dir/something &&
+	git -C repo commit -m bar &&
+
+	# foo^{tree} (tree referenced from commit)
+	TREE=$(git -C repo rev-parse foo^{tree}) &&
+
+	# a tree referenced by HEAD^{tree} (tree referenced from tree)
+	TREE2=$(git -C repo ls-tree HEAD^{tree} | grep " tree " | head -1 | cut -b13-52) &&
+
+	promise_and_delete "$TREE" &&
+	promise_and_delete "$TREE2" &&
+
+	git -C repo config core.repositoryformatversion 1 &&
+	git -C repo config extensions.partialclone "arbitrary string" &&
+	git -C repo rev-list --exclude-promisor-objects --objects HEAD >out &&
+	grep $(git -C repo rev-parse foo) out &&
+	! grep $TREE out &&
+	grep $(git -C repo rev-parse HEAD) out &&
+	! grep $TREE2 out
+'
+
+test_expect_success 'rev-list stops traversal at missing and promised blob' '
+	rm -rf repo &&
+	test_create_repo repo &&
+	echo something >repo/something &&
+	git -C repo add something &&
+	git -C repo commit -m foo &&
+
+	BLOB=$(git -C repo hash-object -w something) &&
+	promise_and_delete "$BLOB" &&
+
+	git -C repo config core.repositoryformatversion 1 &&
+	git -C repo config extensions.partialclone "arbitrary string" &&
+	git -C repo rev-list --exclude-promisor-objects --objects HEAD >out &&
+	grep $(git -C repo rev-parse HEAD) out &&
+	! grep $BLOB out
+'
+
+test_expect_success 'rev-list stops traversal at promisor commit, tree, and blob' '
+	rm -rf repo &&
+	test_create_repo repo &&
+	test_commit -C repo foo &&
+	test_commit -C repo bar &&
+	test_commit -C repo baz &&
+
+	COMMIT=$(git -C repo rev-parse foo) &&
+	TREE=$(git -C repo rev-parse bar^{tree}) &&
+	BLOB=$(git hash-object repo/baz.t) &&
+	printf "%s\n%s\n%s\n" $COMMIT $TREE $BLOB | pack_as_from_promisor &&
+
+	git -C repo config core.repositoryformatversion 1 &&
+	git -C repo config extensions.partialclone "arbitrary string" &&
+	git -C repo rev-list --exclude-promisor-objects --objects HEAD >out &&
+	! grep $COMMIT out &&
+	! grep $TREE out &&
+	! grep $BLOB out &&
+	grep $(git -C repo rev-parse bar) out  # sanity check that some walking was done
+'
+
+test_expect_success 'rev-list accepts missing and promised objects on command line' '
+	rm -rf repo &&
+	test_create_repo repo &&
+	test_commit -C repo foo &&
+	test_commit -C repo bar &&
+	test_commit -C repo baz &&
+
+	COMMIT=$(git -C repo rev-parse foo) &&
+	TREE=$(git -C repo rev-parse bar^{tree}) &&
+	BLOB=$(git hash-object repo/baz.t) &&
+
+	promise_and_delete $COMMIT &&
+	promise_and_delete $TREE &&
+	promise_and_delete $BLOB &&
+
+	git -C repo config core.repositoryformatversion 1 &&
+	git -C repo config extensions.partialclone "arbitrary string" &&
+	git -C repo rev-list --exclude-promisor-objects --objects "$COMMIT" "$TREE" "$BLOB"
+'
+
 LIB_HTTPD_PORT=12345  # default port, 410, cannot be used as non-root
 . "$TEST_DIRECTORY"/lib-httpd.sh
 start_httpd

commit a2b22854bd5f252cd036636091a1d30141c35bce
Author: Jeff King <peff@peff.net>
Date:   Wed Jan 25 23:12:07 2017 -0500

    fsck: lazily load types under --connectivity-only
    
    The recent fixes to "fsck --connectivity-only" load all of
    the objects with their correct types. This keeps the
    connectivity-only code path close to the regular one, but it
    also introduces some unnecessary inefficiency. While getting
    the type of an object is cheap compared to actually opening
    and parsing the object (as the non-connectivity-only case
    would do), it's still not free.
    
    For reachable non-blob objects, we end up having to parse
    them later anyway (to see what they point to), making our
    type lookup here redundant.
    
    For unreachable objects, we might never hit them at all in
    the reachability traversal, making the lookup completely
    wasted. And in some cases, we might have quite a few
    unreachable objects (e.g., when alternates are used for
    shared object storage between repositories, it's normal for
    there to be objects reachable from other repositories but
    not the one running fsck).
    
    The comment in mark_object_for_connectivity() claims two
    benefits to getting the type up front:
    
      1. We need to know the types during fsck_walk(). (And not
         explicitly mentioned, but we also need them when
         printing the types of broken or dangling commits).
    
         We can address this by lazy-loading the types as
         necessary. Most objects never need this lazy-load at
         all, because they fall into one of these categories:
    
           a. Reachable from our tips, and are coerced into the
              correct type as we traverse (e.g., a parent link
              will call lookup_commit(), which converts OBJ_NONE
              to OBJ_COMMIT).
    
           b. Unreachable, but not at the tip of a chunk of
              unreachable history. We only mention the tips as
              "dangling", so an unreachable commit which links
              to hundreds of other objects needs only report the
              type of the tip commit.
    
      2. It serves as a cross-check that the coercion in (1a) is
         correct (i.e., we'll complain about a parent link that
         points to a blob). But we get most of this for free
         already, because right after coercing, we'll parse any
         non-blob objects. So we'd notice then if we expected a
         commit and got a blob.
    
         The one exception is when we expect a blob, in which
         case we never actually read the object contents.
    
         So this is a slight weakening, but given that the whole
         point of --connectivity-only is to sacrifice some data
         integrity checks for speed, this seems like an
         acceptable tradeoff.
    
    Here are before and after timings for an extreme case with
    ~5M reachable objects and another ~12M unreachable (it's the
    torvalds/linux repository on GitHub, connected to shared
    storage for all of the other kernel forks):
    
      [before]
      $ time git fsck --no-dangling --connectivity-only
      real  3m4.323s
      user  1m25.121s
      sys   1m38.710s
    
      [after]
      $ time git fsck --no-dangling --connectivity-only
      real  0m51.497s
      user  0m49.575s
      sys   0m1.776s
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/builtin/fsck.c b/builtin/fsck.c
index 3d5ced2d3a..140357b6fc 100644
--- a/builtin/fsck.c
+++ b/builtin/fsck.c
@@ -60,6 +60,12 @@ static const char *printable_type(struct object *obj)
 {
 	const char *ret;
 
+	if (obj->type == OBJ_NONE) {
+		enum object_type type = sha1_object_info(obj->oid.hash, NULL);
+		if (type > 0)
+			object_as_type(obj, type, 0);
+	}
+
 	ret = typename(obj->type);
 	if (!ret)
 		ret = "unknown";
@@ -595,57 +601,7 @@ static int fsck_cache_tree(struct cache_tree *it)
 
 static void mark_object_for_connectivity(const unsigned char *sha1)
 {
-	struct object *obj = lookup_object(sha1);
-
-	/*
-	 * Setting the object type here isn't strictly necessary for a
-	 * connectivity check. In most cases, our walk will expect a certain
-	 * type (e.g., a tree referencing a blob) and will use lookup_blob() to
-	 * assign the type. But doing it here has two advantages:
-	 *
-	 *   1. When the fsck_walk code looks at objects that _don't_ come from
-	 *      links (e.g., the tip of a ref), it may complain about the
-	 *      "unknown object type".
-	 *
-	 *   2. This serves as a nice cross-check that the graph links are
-	 *      sane. So --connectivity-only does not check that the bits of
-	 *      blobs are not corrupted, but it _does_ check that 100644 tree
-	 *      entries point to blobs, and so forth.
-	 *
-	 * Unfortunately we can't just use parse_object() here, because the
-	 * whole point of --connectivity-only is to avoid reading the object
-	 * data more than necessary.
-	 */
-	if (!obj || obj->type == OBJ_NONE) {
-		enum object_type type = sha1_object_info(sha1, NULL);
-		switch (type) {
-		case OBJ_BAD:
-			error("%s: unable to read object type",
-			      sha1_to_hex(sha1));
-			break;
-		case OBJ_COMMIT:
-			obj = (struct object *)lookup_commit(sha1);
-			break;
-		case OBJ_TREE:
-			obj = (struct object *)lookup_tree(sha1);
-			break;
-		case OBJ_BLOB:
-			obj = (struct object *)lookup_blob(sha1);
-			break;
-		case OBJ_TAG:
-			obj = (struct object *)lookup_tag(sha1);
-			break;
-		default:
-			error("%s: unknown object type %d",
-			      sha1_to_hex(sha1), type);
-		}
-
-		if (!obj || obj->type == OBJ_NONE) {
-			errors_found |= ERROR_OBJECT;
-			return;
-		}
-	}
-
+	struct object *obj = lookup_unknown_object(sha1);
 	obj->flags |= HAS_OBJ;
 }
 
diff --git a/fsck.c b/fsck.c
index 4a3069e204..939792752b 100644
--- a/fsck.c
+++ b/fsck.c
@@ -458,6 +458,10 @@ int fsck_walk(struct object *obj, void *data, struct fsck_options *options)
 {
 	if (!obj)
 		return -1;
+
+	if (obj->type == OBJ_NONE)
+		parse_object(obj->oid.hash);
+
 	switch (obj->type) {
 	case OBJ_BLOB:
 		return 0;

commit f26eef302fc315394d1016eb06360637ac86f62e
Author: Jeff King <peff@peff.net>
Date:   Fri Jul 15 06:26:29 2016 -0400

    check_everything_connected: always pass --quiet to rev-list
    
    The check_everything_connected function takes a "quiet"
    parameter which does two things if non-zero:
    
      1. redirect rev-list's stderr to /dev/null to avoid
         showing errors to the user
    
      2. pass "--quiet" to rev-list
    
    Item (1) is obviously useful. But item (2) is
    surprisingly not. For rev-list, "--quiet" does not have
    anything to do with chattiness on stderr; it tells rev-list
    not to bother writing the list of traversed objects to
    stdout, for efficiency.  And since we always redirect
    rev-list's stdout to /dev/null in this function, there is no
    point in asking it to ever write anything to stdout.
    
    The efficiency gains are modest; a best-of-five run of "git
    rev-list --objects --all" on linux.git dropped from 32.013s
    to 30.502s when adding "--quiet". That's only about 5%, but
    given how easy it is, it's worth doing.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/connected.c b/connected.c
index bf1b12e7ec..7560a31628 100644
--- a/connected.c
+++ b/connected.c
@@ -56,8 +56,7 @@ static int check_everything_connected_real(sha1_iterate_fn fn,
 	argv[ac++] = "--stdin";
 	argv[ac++] = "--not";
 	argv[ac++] = "--all";
-	if (quiet)
-		argv[ac++] = "--quiet";
+	argv[ac++] = "--quiet";
 	argv[ac] = NULL;
 
 	rev_list.argv = argv;

commit f5b2dec1657e09a22f8b2aefa25d022988e3e467
Author: Jeff King <peff@peff.net>
Date:   Mon Aug 10 05:36:19 2015 -0400

    refs.c: remove extra git_path calls from read_loose_refs
    
    In iterating over the loose refs in "refs/foo/", we keep a
    running strbuf with "refs/foo/one", "refs/foo/two", etc. But
    we also need to access these files in the filesystem, as
    ".git/refs/foo/one", etc. For this latter purpose, we make a
    series of independent calls to git_path(). These are safe
    (we only use the result to call stat()), but assigning the
    result of git_path is a suspicious pattern that we'd rather
    avoid.
    
    This patch keeps a running buffer with ".git/refs/foo/", and
    we can just append/reset each directory element as we loop.
    This matches how we handle the refnames. It should also be
    more efficient, as we do not keep formatting the same
    ".git/refs/foo" prefix (which can be arbitrarily deep).
    
    Technically we are dropping a call to strbuf_cleanup() on
    each generated filename, but that's OK; it wasn't doing
    anything, as we are putting in single-level names we read
    from the filesystem (so it could not possibly be cleaning up
    cruft like "./" in this instance).
    
    A clever reader may also note that the running refname
    buffer ("refs/foo/") is actually a subset of the filesystem
    path buffer (".git/refs/foo/"). We could get by with one
    buffer, indexing the length of $GIT_DIR when we want the
    refname. However, having tried this, the resulting code
    actually ends up a little more confusing, and the efficiency
    improvement is tiny (and almost certainly dwarfed by the
    system calls we are making).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/refs.c b/refs.c
index e70941a090..06f95c4c12 100644
--- a/refs.c
+++ b/refs.c
@@ -1352,19 +1352,23 @@ static void read_loose_refs(const char *dirname, struct ref_dir *dir)
 {
 	struct ref_cache *refs = dir->ref_cache;
 	DIR *d;
-	const char *path;
 	struct dirent *de;
 	int dirnamelen = strlen(dirname);
 	struct strbuf refname;
+	struct strbuf path = STRBUF_INIT;
+	size_t path_baselen;
 
 	if (*refs->name)
-		path = git_path_submodule(refs->name, "%s", dirname);
+		strbuf_git_path_submodule(&path, refs->name, "%s", dirname);
 	else
-		path = git_path("%s", dirname);
+		strbuf_git_path(&path, "%s", dirname);
+	path_baselen = path.len;
 
-	d = opendir(path);
-	if (!d)
+	d = opendir(path.buf);
+	if (!d) {
+		strbuf_release(&path);
 		return;
+	}
 
 	strbuf_init(&refname, dirnamelen + 257);
 	strbuf_add(&refname, dirname, dirnamelen);
@@ -1373,17 +1377,14 @@ static void read_loose_refs(const char *dirname, struct ref_dir *dir)
 		unsigned char sha1[20];
 		struct stat st;
 		int flag;
-		const char *refdir;
 
 		if (de->d_name[0] == '.')
 			continue;
 		if (ends_with(de->d_name, ".lock"))
 			continue;
 		strbuf_addstr(&refname, de->d_name);
-		refdir = *refs->name
-			? git_path_submodule(refs->name, "%s", refname.buf)
-			: git_path("%s", refname.buf);
-		if (stat(refdir, &st) < 0) {
+		strbuf_addstr(&path, de->d_name);
+		if (stat(path.buf, &st) < 0) {
 			; /* silently ignore */
 		} else if (S_ISDIR(st.st_mode)) {
 			strbuf_addch(&refname, '/');
@@ -1430,8 +1431,10 @@ static void read_loose_refs(const char *dirname, struct ref_dir *dir)
 					 create_ref_entry(refname.buf, sha1, flag, 0));
 		}
 		strbuf_setlen(&refname, dirnamelen);
+		strbuf_setlen(&path, path_baselen);
 	}
 	strbuf_release(&refname);
+	strbuf_release(&path);
 	closedir(d);
 }
 

commit 9e3751d4437b43e72497178774c74be1ceac28b9
Author: Jeff King <peff@peff.net>
Date:   Thu May 21 00:45:13 2015 -0400

    remote.c: drop "remote" pointer from "struct branch"
    
    When we create each branch struct, we fill in the
    "remote_name" field from the config, and then fill in the
    actual "remote" field (with a "struct remote") based on that
    name. However, it turns out that nobody really cares about
    the latter field. The only two sites that access it at all
    are:
    
      1. git-merge, which uses it to notice when the branch does
         not have a remote defined. But we can easily replace this
         with looking at remote_name instead.
    
      2. remote.c itself, when setting up the @{upstream} merge
         config. But we don't need to save the "remote" in the
         "struct branch" for that; we can just look it up for
         the duration of the operation.
    
    So there is no need to have both fields; they are redundant
    with each other (the struct remote contains the name, or you
    can look up the struct from the name). It would be nice to
    simplify this, especially as we are going to add matching
    pushremote config in a future patch (and it would be nice to
    keep them consistent).
    
    So which one do we keep and which one do we get rid of?
    
    If we had a lot of callers accessing the struct, it would be
    more efficient to keep it (since you have to do a lookup to
    go from the name to the struct, but not vice versa). But we
    don't have a lot of callers; we have exactly one, so
    efficiency doesn't matter. We can decide this based on
    simplicity and readability.
    
    And the meaning of the struct value is somewhat unclear. Is
    it always the remote matching remote_name? If remote_name is
    NULL (i.e., no per-branch config), does the struct fall back
    to the "origin" remote, or is it also NULL? These questions
    will get even more tricky with pushremotes, whose fallback
    behavior is more complicated. So let's just store the name,
    which pretty clearly represents the branch.*.remote config.
    Any lookup or fallback behavior can then be implemented in
    helper functions.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/Documentation/technical/api-remote.txt b/Documentation/technical/api-remote.txt
index 5d245aa9d1..2cfdd224a8 100644
--- a/Documentation/technical/api-remote.txt
+++ b/Documentation/technical/api-remote.txt
@@ -97,10 +97,6 @@ It contains:
 
 	The name of the remote listed in the configuration.
 
-`remote`::
-
-	The struct remote for that remote.
-
 `merge_name`::
 
 	An array of the "merge" lines in the configuration.
diff --git a/builtin/merge.c b/builtin/merge.c
index 3b0f8f96d4..1840317118 100644
--- a/builtin/merge.c
+++ b/builtin/merge.c
@@ -955,7 +955,7 @@ static int setup_with_upstream(const char ***argv)
 
 	if (!branch)
 		die(_("No current branch."));
-	if (!branch->remote)
+	if (!branch->remote_name)
 		die(_("No remote for the current branch."));
 	if (!branch->merge_nr)
 		die(_("No default upstream defined for the current branch."));
diff --git a/remote.c b/remote.c
index ac17e66c09..c298a43a1c 100644
--- a/remote.c
+++ b/remote.c
@@ -1632,6 +1632,7 @@ void set_ref_status_for_push(struct ref *remote_refs, int send_mirror,
 
 static void set_merge(struct branch *ret)
 {
+	struct remote *remote;
 	char *ref;
 	unsigned char sha1[20];
 	int i;
@@ -1649,11 +1650,13 @@ static void set_merge(struct branch *ret)
 		return;
 	}
 
+	remote = remote_get(ret->remote_name);
+
 	ret->merge = xcalloc(ret->merge_nr, sizeof(*ret->merge));
 	for (i = 0; i < ret->merge_nr; i++) {
 		ret->merge[i] = xcalloc(1, sizeof(**ret->merge));
 		ret->merge[i]->src = xstrdup(ret->merge_name[i]);
-		if (!remote_find_tracking(ret->remote, ret->merge[i]) ||
+		if (!remote_find_tracking(remote, ret->merge[i]) ||
 		    strcmp(ret->remote_name, "."))
 			continue;
 		if (dwim_ref(ret->merge_name[i], strlen(ret->merge_name[i]),
@@ -1673,8 +1676,6 @@ struct branch *branch_get(const char *name)
 		ret = current_branch;
 	else
 		ret = make_branch(name, 0);
-	if (ret && ret->remote_name)
-		ret->remote = remote_get(ret->remote_name);
 	set_merge(ret);
 	return ret;
 }
diff --git a/remote.h b/remote.h
index 02d66ceff5..4bb6672735 100644
--- a/remote.h
+++ b/remote.h
@@ -203,7 +203,6 @@ struct branch {
 	const char *refname;
 
 	const char *remote_name;
-	struct remote *remote;
 
 	const char **merge_name;
 	struct refspec **merge;

commit c33ddc2e33d51da9391a81206a1d9e4a92d97d10
Author: Jeff King <peff@peff.net>
Date:   Wed Aug 27 03:57:08 2014 -0400

    date: use strbufs in date-formatting functions
    
    Many of the date functions write into fixed-size buffers.
    This is a minor pain, as we have to take special
    precautions, and frequently end up copying the result into a
    strbuf or heap-allocated buffer anyway (for which we
    sometimes use strcpy!).
    
    Let's instead teach parse_date, datestamp, etc to write to a
    strbuf. The obvious downside is that we might need to
    perform a heap allocation where we otherwise would not need
    to. However, it turns out that the only two new allocations
    required are:
    
      1. In test-date.c, where we don't care about efficiency.
    
      2. In determine_author_info, which is not performance
         critical (and where the use of a strbuf will help later
         refactoring).
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/builtin/commit.c b/builtin/commit.c
index 5ed60364ce..8da0a9f3e3 100644
--- a/builtin/commit.c
+++ b/builtin/commit.c
@@ -520,19 +520,16 @@ static int sane_ident_split(struct ident_split *person)
 	return 1;
 }
 
-static int parse_force_date(const char *in, char *out, int len)
+static int parse_force_date(const char *in, struct strbuf *out)
 {
-	if (len < 1)
-		return -1;
-	*out++ = '@';
-	len--;
+	strbuf_addch(out, '@');
 
-	if (parse_date(in, out, len) < 0) {
+	if (parse_date(in, out) < 0) {
 		int errors = 0;
 		unsigned long t = approxidate_careful(in, &errors);
 		if (errors)
 			return -1;
-		snprintf(out, len, "%lu", t);
+		strbuf_addf(out, "%lu", t);
 	}
 
 	return 0;
@@ -542,7 +539,7 @@ static void determine_author_info(struct strbuf *author_ident)
 {
 	char *name, *email, *date;
 	struct ident_split author;
-	char date_buf[64];
+	struct strbuf date_buf = STRBUF_INIT;
 
 	name = getenv("GIT_AUTHOR_NAME");
 	email = getenv("GIT_AUTHOR_EMAIL");
@@ -588,9 +585,10 @@ static void determine_author_info(struct strbuf *author_ident)
 	}
 
 	if (force_date) {
-		if (parse_force_date(force_date, date_buf, sizeof(date_buf)))
+		strbuf_reset(&date_buf);
+		if (parse_force_date(force_date, &date_buf))
 			die(_("invalid date format: %s"), force_date);
-		date = date_buf;
+		date = date_buf.buf;
 	}
 
 	strbuf_addstr(author_ident, fmt_ident(name, email, date, IDENT_STRICT));
@@ -600,6 +598,8 @@ static void determine_author_info(struct strbuf *author_ident)
 		export_one("GIT_AUTHOR_EMAIL", author.mail_begin, author.mail_end, 0);
 		export_one("GIT_AUTHOR_DATE", author.date_begin, author.tz_end, '@');
 	}
+
+	strbuf_release(&date_buf);
 }
 
 static void split_ident_or_die(struct ident_split *id, const struct strbuf *buf)
diff --git a/cache.h b/cache.h
index fcb511db70..96ecc1f158 100644
--- a/cache.h
+++ b/cache.h
@@ -1044,10 +1044,10 @@ enum date_mode {
 const char *show_date(unsigned long time, int timezone, enum date_mode mode);
 void show_date_relative(unsigned long time, int tz, const struct timeval *now,
 			struct strbuf *timebuf);
-int parse_date(const char *date, char *buf, int bufsize);
+int parse_date(const char *date, struct strbuf *out);
 int parse_date_basic(const char *date, unsigned long *timestamp, int *offset);
 int parse_expiry_date(const char *date, unsigned long *timestamp);
-void datestamp(char *buf, int bufsize);
+void datestamp(struct strbuf *out);
 #define approxidate(s) approxidate_careful((s), NULL)
 unsigned long approxidate_careful(const char *, int *);
 unsigned long approxidate_relative(const char *date, const struct timeval *now);
diff --git a/date.c b/date.c
index 782de95d90..2c33468dfb 100644
--- a/date.c
+++ b/date.c
@@ -605,7 +605,7 @@ static int match_tz(const char *date, int *offp)
 	return end - date;
 }
 
-static int date_string(unsigned long date, int offset, char *buf, int len)
+static void date_string(unsigned long date, int offset, struct strbuf *buf)
 {
 	int sign = '+';
 
@@ -613,7 +613,7 @@ static int date_string(unsigned long date, int offset, char *buf, int len)
 		offset = -offset;
 		sign = '-';
 	}
-	return snprintf(buf, len, "%lu %c%02d%02d", date, sign, offset / 60, offset % 60);
+	strbuf_addf(buf, "%lu %c%02d%02d", date, sign, offset / 60, offset % 60);
 }
 
 /*
@@ -735,13 +735,14 @@ int parse_expiry_date(const char *date, unsigned long *timestamp)
 	return errors;
 }
 
-int parse_date(const char *date, char *result, int maxlen)
+int parse_date(const char *date, struct strbuf *result)
 {
 	unsigned long timestamp;
 	int offset;
 	if (parse_date_basic(date, &timestamp, &offset))
 		return -1;
-	return date_string(timestamp, offset, result, maxlen);
+	date_string(timestamp, offset, result);
+	return 0;
 }
 
 enum date_mode parse_date_format(const char *format)
@@ -766,7 +767,7 @@ enum date_mode parse_date_format(const char *format)
 		die("unknown date format %s", format);
 }
 
-void datestamp(char *buf, int bufsize)
+void datestamp(struct strbuf *out)
 {
 	time_t now;
 	int offset;
@@ -776,7 +777,7 @@ void datestamp(char *buf, int bufsize)
 	offset = tm_to_time_t(localtime(&now)) - now;
 	offset /= 60;
 
-	date_string(now, offset, buf, bufsize);
+	date_string(now, offset, out);
 }
 
 /*
diff --git a/fast-import.c b/fast-import.c
index d73f58cbe3..dc9f7a8ccb 100644
--- a/fast-import.c
+++ b/fast-import.c
@@ -1971,7 +1971,7 @@ static int parse_data(struct strbuf *sb, uintmax_t limit, uintmax_t *len_res)
 	return 1;
 }
 
-static int validate_raw_date(const char *src, char *result, int maxlen)
+static int validate_raw_date(const char *src, struct strbuf *result)
 {
 	const char *orig_src = src;
 	char *endp;
@@ -1989,11 +1989,10 @@ static int validate_raw_date(const char *src, char *result, int maxlen)
 		return -1;
 
 	num = strtoul(src + 1, &endp, 10);
-	if (errno || endp == src + 1 || *endp || (endp - orig_src) >= maxlen ||
-	    1400 < num)
+	if (errno || endp == src + 1 || *endp || 1400 < num)
 		return -1;
 
-	strcpy(result, orig_src);
+	strbuf_addstr(result, orig_src);
 	return 0;
 }
 
@@ -2001,7 +2000,7 @@ static char *parse_ident(const char *buf)
 {
 	const char *ltgt;
 	size_t name_len;
-	char *ident;
+	struct strbuf ident = STRBUF_INIT;
 
 	/* ensure there is a space delimiter even if there is no name */
 	if (*buf == '<')
@@ -2020,26 +2019,25 @@ static char *parse_ident(const char *buf)
 		die("Missing space after > in ident string: %s", buf);
 	ltgt++;
 	name_len = ltgt - buf;
-	ident = xmalloc(name_len + 24);
-	strncpy(ident, buf, name_len);
+	strbuf_add(&ident, buf, name_len);
 
 	switch (whenspec) {
 	case WHENSPEC_RAW:
-		if (validate_raw_date(ltgt, ident + name_len, 24) < 0)
+		if (validate_raw_date(ltgt, &ident) < 0)
 			die("Invalid raw date \"%s\" in ident: %s", ltgt, buf);
 		break;
 	case WHENSPEC_RFC2822:
-		if (parse_date(ltgt, ident + name_len, 24) < 0)
+		if (parse_date(ltgt, &ident) < 0)
 			die("Invalid rfc2822 date \"%s\" in ident: %s", ltgt, buf);
 		break;
 	case WHENSPEC_NOW:
 		if (strcmp("now", ltgt))
 			die("Date in ident must be 'now': %s", buf);
-		datestamp(ident + name_len, 24);
+		datestamp(&ident);
 		break;
 	}
 
-	return ident;
+	return strbuf_detach(&ident, NULL);
 }
 
 static void parse_and_store_blob(
diff --git a/ident.c b/ident.c
index 1d9b6e770d..9bcc4e11b8 100644
--- a/ident.c
+++ b/ident.c
@@ -9,7 +9,7 @@
 
 static struct strbuf git_default_name = STRBUF_INIT;
 static struct strbuf git_default_email = STRBUF_INIT;
-static char git_default_date[50];
+static struct strbuf git_default_date = STRBUF_INIT;
 
 #define IDENT_NAME_GIVEN 01
 #define IDENT_MAIL_GIVEN 02
@@ -129,9 +129,9 @@ const char *ident_default_email(void)
 
 static const char *ident_default_date(void)
 {
-	if (!git_default_date[0])
-		datestamp(git_default_date, sizeof(git_default_date));
-	return git_default_date;
+	if (!git_default_date.len)
+		datestamp(&git_default_date);
+	return git_default_date.buf;
 }
 
 static int crud(unsigned char c)
@@ -292,7 +292,6 @@ const char *fmt_ident(const char *name, const char *email,
 		      const char *date_str, int flag)
 {
 	static struct strbuf ident = STRBUF_INIT;
-	char date[50];
 	int strict = (flag & IDENT_STRICT);
 	int want_date = !(flag & IDENT_NO_DATE);
 	int want_name = !(flag & IDENT_NO_NAME);
@@ -320,15 +319,6 @@ const char *fmt_ident(const char *name, const char *email,
 		die("unable to auto-detect email address (got '%s')", email);
 	}
 
-	if (want_date) {
-		if (date_str && date_str[0]) {
-			if (parse_date(date_str, date, sizeof(date)) < 0)
-				die("invalid date format: %s", date_str);
-		}
-		else
-			strcpy(date, ident_default_date());
-	}
-
 	strbuf_reset(&ident);
 	if (want_name) {
 		strbuf_addstr_without_crud(&ident, name);
@@ -339,8 +329,14 @@ const char *fmt_ident(const char *name, const char *email,
 			strbuf_addch(&ident, '>');
 	if (want_date) {
 		strbuf_addch(&ident, ' ');
-		strbuf_addstr_without_crud(&ident, date);
+		if (date_str && date_str[0]) {
+			if (parse_date(date_str, &ident) < 0)
+				die("invalid date format: %s", date_str);
+		}
+		else
+			strbuf_addstr(&ident, ident_default_date());
 	}
+
 	return ident.buf;
 }
 
diff --git a/test-date.c b/test-date.c
index 10afaabbfa..94a6997a8f 100644
--- a/test-date.c
+++ b/test-date.c
@@ -19,19 +19,21 @@ static void show_dates(char **argv, struct timeval *now)
 
 static void parse_dates(char **argv, struct timeval *now)
 {
+	struct strbuf result = STRBUF_INIT;
+
 	for (; *argv; argv++) {
-		char result[100];
 		unsigned long t;
 		int tz;
 
-		result[0] = 0;
-		parse_date(*argv, result, sizeof(result));
-		if (sscanf(result, "%lu %d", &t, &tz) == 2)
+		strbuf_reset(&result);
+		parse_date(*argv, &result);
+		if (sscanf(result.buf, "%lu %d", &t, &tz) == 2)
 			printf("%s -> %s\n",
 			       *argv, show_date(t, tz, DATE_ISO8601));
 		else
 			printf("%s -> bad\n", *argv);
 	}
+	strbuf_release(&result);
 }
 
 static void parse_approxidate(char **argv, struct timeval *now)

commit 897e3e454062d2bb9d3c1e4068caf4971fd713ff
Author: Albert L. Lash, IV <alash3@bloomberg.net>
Date:   Sat Feb 8 15:41:36 2014 -0500

    docs/git-clone: clarify use of --no-hardlinks option
    
    Current text claims optimization, implying the use of
    hardlinks, when this option ratchets down the level of
    efficiency. This change explains the difference made by
    using this option, namely copying instead of hardlinking,
    and why it may be useful.
    
    Signed-off-by: Albert L. Lash, IV <alash3@bloomberg.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/Documentation/git-clone.txt b/Documentation/git-clone.txt
index bf3dac0cef..0363d0039b 100644
--- a/Documentation/git-clone.txt
+++ b/Documentation/git-clone.txt
@@ -55,15 +55,12 @@ repository is specified as a URL, then this flag is ignored (and we
 never use the local optimizations).  Specifying `--no-local` will
 override the default when `/path/to/repo` is given, using the regular
 Git transport instead.
-+
-To force copying instead of hardlinking (which may be desirable if you
-are trying to make a back-up of your repository), but still avoid the
-usual "Git aware" transport mechanism, `--no-hardlinks` can be used.
 
 --no-hardlinks::
-	Optimize the cloning process from a repository on a
-	local filesystem by copying files under `.git/objects`
-	directory.
+	Force the cloning process from a repository on a local
+	filesystem to copy the files under the `.git/objects`
+	directory instead of using hardlinks. This may be desirable
+	if you are trying to make a back-up of your repository.
 
 --shared::
 -s::

commit 050ef3655c8ea1dc7a2b3b843ca7c45dd94d9c88
Author: Jeff King <peff@peff.net>
Date:   Sat Sep 28 04:35:35 2013 -0400

    remote-curl: rewrite base url from info/refs redirects
    
    For efficiency and security reasons, an earlier commit in
    this series taught http_get_* to re-write the base url based
    on redirections we saw while making a specific request.
    
    This commit wires that option into the info/refs request,
    meaning that a redirect from
    
        http://example.com/foo.git/info/refs
    
    to
    
        https://example.com/bar.git/info/refs
    
    will behave as if "https://example.com/bar.git" had been
    provided to git in the first place.
    
    The tests bear some explanation. We introduce two new
    hierearchies into the httpd test config:
    
      1. Requests to /smart-redir-limited will work only for the
         initial info/refs request, but not any subsequent
         requests. As a result, we can confirm whether the
         client is re-rooting its requests after the initial
         contact, since otherwise it will fail (it will ask for
         "repo.git/git-upload-pack", which is not redirected).
    
      2. Requests to smart-redir-auth will redirect, and require
         auth after the redirection. Since we are using the
         redirected base for further requests, we also update
         the credential struct, in order not to mislead the user
         (or credential helpers) about which credential is
         needed. We can therefore check the GIT_ASKPASS prompts
         to make sure we are prompting for the new location.
         Because we have neither multiple servers nor https
         support in our test setup, we can only redirect between
         paths, meaning we need to turn on
         credential.useHttpPath to see the difference.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Jonathan Nieder <jrnieder@gmail.com>

diff --git a/remote-curl.c b/remote-curl.c
index 345fea8898..ef1684b9df 100644
--- a/remote-curl.c
+++ b/remote-curl.c
@@ -188,6 +188,7 @@ static struct discovery* discover_refs(const char *service, int for_push)
 	struct strbuf type = STRBUF_INIT;
 	struct strbuf buffer = STRBUF_INIT;
 	struct strbuf refs_url = STRBUF_INIT;
+	struct strbuf effective_url = STRBUF_INIT;
 	struct discovery *last = last_discovery;
 	int http_ret, maybe_smart = 0;
 	struct http_get_options options;
@@ -209,6 +210,8 @@ static struct discovery* discover_refs(const char *service, int for_push)
 
 	memset(&options, 0, sizeof(options));
 	options.content_type = &type;
+	options.effective_url = &effective_url;
+	options.base_url = &url;
 	options.no_cache = 1;
 	options.keep_error = 1;
 
@@ -268,6 +271,7 @@ static struct discovery* discover_refs(const char *service, int for_push)
 	strbuf_release(&refs_url);
 	strbuf_release(&exp);
 	strbuf_release(&type);
+	strbuf_release(&effective_url);
 	strbuf_release(&buffer);
 	last_discovery = last;
 	return last;
diff --git a/t/lib-httpd.sh b/t/lib-httpd.sh
index 895b9258b0..7059cc6c21 100644
--- a/t/lib-httpd.sh
+++ b/t/lib-httpd.sh
@@ -187,7 +187,8 @@ set_askpass() {
 }
 
 expect_askpass() {
-	dest=$HTTPD_DEST
+	dest=$HTTPD_DEST${3+/$3}
+
 	{
 		case "$1" in
 		none)
diff --git a/t/lib-httpd/apache.conf b/t/lib-httpd/apache.conf
index dd17e3a09d..4a261f13f5 100644
--- a/t/lib-httpd/apache.conf
+++ b/t/lib-httpd/apache.conf
@@ -102,6 +102,8 @@ ScriptAlias /broken_smart/ broken-smart-http.sh/
 RewriteEngine on
 RewriteRule ^/smart-redir-perm/(.*)$ /smart/$1 [R=301]
 RewriteRule ^/smart-redir-temp/(.*)$ /smart/$1 [R=302]
+RewriteRule ^/smart-redir-auth/(.*)$ /auth/smart/$1 [R=301]
+RewriteRule ^/smart-redir-limited/(.*)/info/refs$ /smart/$1/info/refs [R=301]
 
 <IfDefine SSL>
 LoadModule ssl_module modules/mod_ssl.so
diff --git a/t/t5551-http-fetch.sh b/t/t5551-http-fetch.sh
index 55a866af80..1b71bb5156 100755
--- a/t/t5551-http-fetch.sh
+++ b/t/t5551-http-fetch.sh
@@ -113,6 +113,10 @@ test_expect_success 'follow redirects (302)' '
 	git clone $HTTPD_URL/smart-redir-temp/repo.git --quiet repo-t
 '
 
+test_expect_success 'redirects re-root further requests' '
+	git clone $HTTPD_URL/smart-redir-limited/repo.git repo-redir-limited
+'
+
 test_expect_success 'clone from password-protected repository' '
 	echo two >expect &&
 	set_askpass user@host &&
@@ -146,6 +150,13 @@ test_expect_success 'no-op half-auth fetch does not require a password' '
 	expect_askpass none
 '
 
+test_expect_success 'redirects send auth to new location' '
+	set_askpass user@host &&
+	git -c credential.useHttpPath=true \
+	  clone $HTTPD_URL/smart-redir-auth/repo.git repo-redir-auth &&
+	expect_askpass both user@host auth/smart/repo.git
+'
+
 test_expect_success 'disable dumb http on server' '
 	git --git-dir="$HTTPD_DOCUMENT_ROOT_PATH/repo.git" \
 		config http.getanyfile false

commit 84054f79de35015fc92f73ec4780102dd820e452
Author: Jeff King <peff@peff.net>
Date:   Thu Jun 9 16:56:19 2011 -0400

    clone: accept config options on the command line
    
    Clone does all of init, "remote add", fetch, and checkout
    without giving the user a chance to intervene and set any
    configuration. This patch allows you to set config options
    in the newly created repository after the clone, but before
    we do any other operations.
    
    In many cases, this is a minor convenience over something
    like:
    
      git clone git://...
      git config core.whatever true
    
    But in some cases, it can bring extra efficiency by changing
    how the fetch or checkout work. For example, setting
    line-ending config before the checkout avoids having to
    re-checkout all of the contents with the correct line
    endings.
    
    It also provides a mechanism for passing information to remote
    helpers during a clone; the helpers may read the git config
    to influence how they operate.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/Documentation/git-clone.txt b/Documentation/git-clone.txt
index b093e45497..4b8b26b75e 100644
--- a/Documentation/git-clone.txt
+++ b/Documentation/git-clone.txt
@@ -159,6 +159,17 @@ objects from the source repository into a pack in the cloned repository.
 	Specify the directory from which templates will be used;
 	(See the "TEMPLATE DIRECTORY" section of linkgit:git-init[1].)
 
+--config <key>=<value>::
+-c <key>=<value>::
+	Set a configuration variable in the newly-created repository;
+	this takes effect immediately after the repository is
+	initialized, but before the remote history is fetched or any
+	files checked out.  The key is in the same format as expected by
+	linkgit:git-config[1] (e.g., `core.eol=true`). If multiple
+	values are given for the same key, each value will be written to
+	the config file. This makes it safe, for example, to add
+	additional fetch refspecs to the origin remote.
+
 --depth <depth>::
 	Create a 'shallow' clone with a history truncated to the
 	specified number of revisions.  A shallow repository has a
diff --git a/builtin/clone.c b/builtin/clone.c
index f579794d9a..a15784a7b8 100644
--- a/builtin/clone.c
+++ b/builtin/clone.c
@@ -46,6 +46,7 @@ static const char *real_git_dir;
 static char *option_upload_pack = "git-upload-pack";
 static int option_verbosity;
 static int option_progress;
+static struct string_list option_config;
 
 static struct option builtin_clone_options[] = {
 	OPT__VERBOSITY(&option_verbosity),
@@ -83,7 +84,8 @@ static struct option builtin_clone_options[] = {
 		    "create a shallow clone of that depth"),
 	OPT_STRING(0, "separate-git-dir", &real_git_dir, "gitdir",
 		   "separate git dir from working tree"),
-
+	OPT_STRING_LIST('c', "config", &option_config, "key=value",
+			"set config inside the new repository"),
 	OPT_END()
 };
 
@@ -364,6 +366,22 @@ static void write_remote_refs(const struct ref *local_refs)
 	clear_extra_refs();
 }
 
+static int write_one_config(const char *key, const char *value, void *data)
+{
+	return git_config_set_multivar(key, value ? value : "true", "^$", 0);
+}
+
+static void write_config(struct string_list *config)
+{
+	int i;
+
+	for (i = 0; i < config->nr; i++) {
+		if (git_config_parse_parameter(config->items[i].string,
+					       write_one_config, NULL) < 0)
+			die("unable to write parameters to config file");
+	}
+}
+
 int cmd_clone(int argc, const char **argv, const char *prefix)
 {
 	int is_bundle = 0, is_local;
@@ -482,6 +500,7 @@ int cmd_clone(int argc, const char **argv, const char *prefix)
 			printf(_("Cloning into %s...\n"), dir);
 	}
 	init_db(option_template, INIT_DB_QUIET);
+	write_config(&option_config);
 
 	/*
 	 * At this point, the config exists, so we do not need the
diff --git a/t/t5708-clone-config.sh b/t/t5708-clone-config.sh
new file mode 100755
index 0000000000..27d730c0a7
--- /dev/null
+++ b/t/t5708-clone-config.sh
@@ -0,0 +1,40 @@
+#!/bin/sh
+
+test_description='tests for git clone -c key=value'
+. ./test-lib.sh
+
+test_expect_success 'clone -c sets config in cloned repo' '
+	rm -rf child &&
+	git clone -c core.foo=bar . child &&
+	echo bar >expect &&
+	git --git-dir=child/.git config core.foo >actual &&
+	test_cmp expect actual
+'
+
+test_expect_success 'clone -c can set multi-keys' '
+	rm -rf child &&
+	git clone -c core.foo=bar -c core.foo=baz . child &&
+	{ echo bar; echo baz; } >expect &&
+	git --git-dir=child/.git config --get-all core.foo >actual &&
+	test_cmp expect actual
+'
+
+test_expect_success 'clone -c without a value is boolean true' '
+	rm -rf child &&
+	git clone -c core.foo . child &&
+	echo true >expect &&
+	git --git-dir=child/.git config --bool core.foo >actual &&
+	test_cmp expect actual
+'
+
+test_expect_success 'clone -c config is available during clone' '
+	echo content >file &&
+	git add file &&
+	git commit -m one &&
+	rm -rf child &&
+	git clone -c core.autocrlf . child &&
+	printf "content\\r\\n" >expect &&
+	test_cmp expect child/file
+'
+
+test_done

commit ebc9529f0358bdb10192fa27bc75f5d4e452ce90
Author: Christian Couder <chriscool@tuxfamily.org>
Date:   Sat Jun 13 07:21:06 2009 +0200

    bisect: use a PRNG with a bias when skipping away from untestable commits
    
    Using a PRNG (pseudo random number generator) with a bias should be better
    than alternating between 3 fixed ratios.
    
    In repositories with many untestable commits it should prevent alternating
    between areas where many commits are untestable. The bias should favor
    commits that can give more information, so that the bisection process
    should not loose much efficiency.
    
    HPA suggested to use a PRNG and found that the best bias is to raise a
    ratio between 0 and 1 given by the PRNG to the power 1.5.
    
    An integer square root function is implemented to avoid including
    <math.h> and linking with -lm.
    
    A PRNG function is implemented to get the same number sequence on
    different machines as suggested by "man 3 rand".
    
    Signed-off-by: Christian Couder <chriscool@tuxfamily.org>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/bisect.c b/bisect.c
index 6fdff05722..095b55eba6 100644
--- a/bisect.c
+++ b/bisect.c
@@ -585,16 +585,49 @@ struct commit_list *filter_skipped(struct commit_list *list,
 	return filtered;
 }
 
-static struct commit_list *apply_skip_ratio(struct commit_list *list,
-					    int count,
-					    int skip_num, int skip_denom)
+#define PRN_MODULO 32768
+
+/*
+ * This is a pseudo random number generator based on "man 3 rand".
+ * It is not used properly because the seed is the argument and it
+ * is increased by one between each call, but that should not matter
+ * for this application.
+ */
+int get_prn(int count) {
+	count = count * 1103515245 + 12345;
+	return ((unsigned)(count/65536) % PRN_MODULO);
+}
+
+/*
+ * Custom integer square root from
+ * http://en.wikipedia.org/wiki/Integer_square_root
+ */
+static int sqrti(int val)
+{
+	float d, x = val;
+
+	if (val == 0)
+		return 0;
+
+	do {
+		float y = (x + (float)val / x) / 2;
+		d = (y > x) ? y - x : x - y;
+		x = y;
+	} while (d >= 0.5);
+
+	return (int)x;
+}
+
+static struct commit_list *skip_away(struct commit_list *list, int count)
 {
-	int index, i;
 	struct commit_list *cur, *previous;
+	int prn, index, i;
+
+	prn = get_prn(count);
+	index = (count * prn / PRN_MODULO) * sqrti(prn) / sqrti(PRN_MODULO);
 
 	cur = list;
 	previous = NULL;
-	index = count * skip_num / skip_denom;
 
 	for (i = 0; cur; cur = cur->next, i++) {
 		if (i == index) {
@@ -614,7 +647,6 @@ static struct commit_list *managed_skipped(struct commit_list *list,
 					   struct commit_list **tried)
 {
 	int count, skipped_first;
-	int skip_num, skip_denom;
 
 	*tried = NULL;
 
@@ -626,11 +658,7 @@ static struct commit_list *managed_skipped(struct commit_list *list,
 	if (!skipped_first)
 		return list;
 
-	/* Use alternatively 1/5, 2/5 and 3/5 as skip ratio. */
-	skip_num = count % 3 + 1;
-	skip_denom = 5;
-
-	return apply_skip_ratio(list, count, skip_num, skip_denom);
+	return skip_away(list, count);
 }
 
 static void bisect_rev_setup(struct rev_info *revs, const char *prefix,
diff --git a/t/t6030-bisect-porcelain.sh b/t/t6030-bisect-porcelain.sh
index 4556cdd8d2..1315bab595 100755
--- a/t/t6030-bisect-porcelain.sh
+++ b/t/t6030-bisect-porcelain.sh
@@ -563,8 +563,8 @@ test_expect_success 'skipping away from skipped commit' '
 	hash7=$(git rev-parse --verify HEAD) &&
 	test "$hash7" = "$HASH7" &&
         git bisect skip &&
-	hash3=$(git rev-parse --verify HEAD) &&
-	test "$hash3" = "$HASH3"
+	para3=$(git rev-parse --verify HEAD) &&
+	test "$para3" = "$PARA_HASH3"
 '
 
 #

commit 8d2dfc49b199c7da6faefd7993630f24bd37fee0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 10 17:27:58 2009 -0700

    process_{tree,blob}: show objects without buffering
    
    Here's a less trivial thing, and slightly more dubious one.
    
    I was looking at that "struct object_array objects", and wondering why we
    do that. I have honestly totally forgotten. Why not just call the "show()"
    function as we encounter the objects? Rather than add the objects to the
    object_array, and then at the very end going through the array and doing a
    'show' on all, just do things more incrementally.
    
    Now, there are possible downsides to this:
    
     - the "buffer using object_array" _can_ in theory result in at least
       better I-cache usage (two tight loops rather than one more spread out
       one). I don't think this is a real issue, but in theory..
    
     - this _does_ change the order of the objects printed. Instead of doing a
       "process_tree(revs, commit->tree, &objects, NULL, "");" in the loop
       over the commits (which puts all the root trees _first_ in the object
       list, this patch just adds them to the list of pending objects, and
       then we'll traverse them in that order (and thus show each root tree
       object together with the objects we discover under it)
    
       I _think_ the new ordering actually makes more sense, but the object
       ordering is actually a subtle thing when it comes to packing
       efficiency, so any change in order is going to have implications for
       packing. Good or bad, I dunno.
    
     - There may be some reason why we did it that odd way with the object
       array, that I have simply forgotten.
    
    Anyway, now that we don't buffer up the objects before showing them
    that may actually result in lower memory usage during that whole
    traverse_commit_list() phase.
    
    This is seriously not very deeply tested. It makes sense to me, it seems
    to pass all the tests, it looks ok, but...
    
    Does anybody remember why we did that "object_array" thing? It used to be
    an "object_list" a long long time ago, but got changed into the array due
    to better memory usage patterns (those linked lists of obejcts are
    horrible from a memory allocation standpoint). But I wonder why we didn't
    do this back then. Maybe there's a reason for it.
    
    Or maybe there _used_ to be a reason, and no longer is.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/builtin-pack-objects.c b/builtin-pack-objects.c
index a6adc8c271..dde8cc3f01 100644
--- a/builtin-pack-objects.c
+++ b/builtin-pack-objects.c
@@ -1856,13 +1856,17 @@ static void show_commit(struct commit *commit)
 	commit->object.flags |= OBJECT_ADDED;
 }
 
-static void show_object(struct object_array_entry *p)
+static void show_object(struct object *obj, const char *name)
 {
-	add_preferred_base_object(p->name);
-	add_object_entry(p->item->sha1, p->item->type, p->name, 0);
-	p->item->flags |= OBJECT_ADDED;
-	free((char *)p->name);
-	p->name = NULL;
+	add_preferred_base_object(name);
+	add_object_entry(obj->sha1, obj->type, name, 0);
+	obj->flags |= OBJECT_ADDED;
+
+	/*
+	 * We will have generated the hash from the name,
+	 * but not saved a pointer to it - we can free it
+	 */
+	free((char *)name);
 }
 
 static void show_edge(struct commit *commit)
diff --git a/builtin-rev-list.c b/builtin-rev-list.c
index facaff288d..759e6714ce 100644
--- a/builtin-rev-list.c
+++ b/builtin-rev-list.c
@@ -169,27 +169,27 @@ static void finish_commit(struct commit *commit)
 	commit->buffer = NULL;
 }
 
-static void finish_object(struct object_array_entry *p)
+static void finish_object(struct object *obj, const char *name)
 {
-	if (p->item->type == OBJ_BLOB && !has_sha1_file(p->item->sha1))
-		die("missing blob object '%s'", sha1_to_hex(p->item->sha1));
+	if (obj->type == OBJ_BLOB && !has_sha1_file(obj->sha1))
+		die("missing blob object '%s'", sha1_to_hex(obj->sha1));
 }
 
-static void show_object(struct object_array_entry *p)
+static void show_object(struct object *obj, const char *name)
 {
 	/* An object with name "foo\n0000000..." can be used to
 	 * confuse downstream "git pack-objects" very badly.
 	 */
-	const char *ep = strchr(p->name, '\n');
+	const char *ep = strchr(name, '\n');
 
-	finish_object(p);
+	finish_object(obj, name);
 	if (ep) {
-		printf("%s %.*s\n", sha1_to_hex(p->item->sha1),
-		       (int) (ep - p->name),
-		       p->name);
+		printf("%s %.*s\n", sha1_to_hex(obj->sha1),
+		       (int) (ep - name),
+		       name);
 	}
 	else
-		printf("%s %s\n", sha1_to_hex(p->item->sha1), p->name);
+		printf("%s %s\n", sha1_to_hex(obj->sha1), name);
 }
 
 static void show_edge(struct commit *commit)
diff --git a/list-objects.c b/list-objects.c
index dd243c7c66..5a4af62bdc 100644
--- a/list-objects.c
+++ b/list-objects.c
@@ -10,7 +10,7 @@
 
 static void process_blob(struct rev_info *revs,
 			 struct blob *blob,
-			 struct object_array *p,
+			 show_object_fn show,
 			 struct name_path *path,
 			 const char *name)
 {
@@ -23,7 +23,7 @@ static void process_blob(struct rev_info *revs,
 	if (obj->flags & (UNINTERESTING | SEEN))
 		return;
 	obj->flags |= SEEN;
-	add_object(obj, p, path, name);
+	show(obj, path_name(path, name));
 }
 
 /*
@@ -50,7 +50,7 @@ static void process_blob(struct rev_info *revs,
  */
 static void process_gitlink(struct rev_info *revs,
 			    const unsigned char *sha1,
-			    struct object_array *p,
+			    show_object_fn show,
 			    struct name_path *path,
 			    const char *name)
 {
@@ -59,7 +59,7 @@ static void process_gitlink(struct rev_info *revs,
 
 static void process_tree(struct rev_info *revs,
 			 struct tree *tree,
-			 struct object_array *p,
+			 show_object_fn show,
 			 struct name_path *path,
 			 const char *name)
 {
@@ -77,7 +77,7 @@ static void process_tree(struct rev_info *revs,
 	if (parse_tree(tree) < 0)
 		die("bad tree object %s", sha1_to_hex(obj->sha1));
 	obj->flags |= SEEN;
-	add_object(obj, p, path, name);
+	show(obj, path_name(path, name));
 	me.up = path;
 	me.elem = name;
 	me.elem_len = strlen(name);
@@ -88,14 +88,14 @@ static void process_tree(struct rev_info *revs,
 		if (S_ISDIR(entry.mode))
 			process_tree(revs,
 				     lookup_tree(entry.sha1),
-				     p, &me, entry.path);
+				     show, &me, entry.path);
 		else if (S_ISGITLINK(entry.mode))
 			process_gitlink(revs, entry.sha1,
-					p, &me, entry.path);
+					show, &me, entry.path);
 		else
 			process_blob(revs,
 				     lookup_blob(entry.sha1),
-				     p, &me, entry.path);
+				     show, &me, entry.path);
 	}
 	free(tree->buffer);
 	tree->buffer = NULL;
@@ -134,16 +134,20 @@ void mark_edges_uninteresting(struct commit_list *list,
 	}
 }
 
+static void add_pending_tree(struct rev_info *revs, struct tree *tree)
+{
+	add_pending_object(revs, &tree->object, "");
+}
+
 void traverse_commit_list(struct rev_info *revs,
 			  void (*show_commit)(struct commit *),
-			  void (*show_object)(struct object_array_entry *))
+			  void (*show_object)(struct object *, const char *))
 {
 	int i;
 	struct commit *commit;
-	struct object_array objects = { 0, 0, NULL };
 
 	while ((commit = get_revision(revs)) != NULL) {
-		process_tree(revs, commit->tree, &objects, NULL, "");
+		add_pending_tree(revs, commit->tree);
 		show_commit(commit);
 	}
 	for (i = 0; i < revs->pending.nr; i++) {
@@ -154,25 +158,22 @@ void traverse_commit_list(struct rev_info *revs,
 			continue;
 		if (obj->type == OBJ_TAG) {
 			obj->flags |= SEEN;
-			add_object_array(obj, name, &objects);
+			show_object(obj, name);
 			continue;
 		}
 		if (obj->type == OBJ_TREE) {
-			process_tree(revs, (struct tree *)obj, &objects,
+			process_tree(revs, (struct tree *)obj, show_object,
 				     NULL, name);
 			continue;
 		}
 		if (obj->type == OBJ_BLOB) {
-			process_blob(revs, (struct blob *)obj, &objects,
+			process_blob(revs, (struct blob *)obj, show_object,
 				     NULL, name);
 			continue;
 		}
 		die("unknown pending object %s (%s)",
 		    sha1_to_hex(obj->sha1), name);
 	}
-	for (i = 0; i < objects.nr; i++)
-		show_object(&objects.objects[i]);
-	free(objects.objects);
 	if (revs->pending.nr) {
 		free(revs->pending.objects);
 		revs->pending.nr = 0;
diff --git a/list-objects.h b/list-objects.h
index 0f41391ecc..13b0dd998e 100644
--- a/list-objects.h
+++ b/list-objects.h
@@ -2,7 +2,7 @@
 #define LIST_OBJECTS_H
 
 typedef void (*show_commit_fn)(struct commit *);
-typedef void (*show_object_fn)(struct object_array_entry *);
+typedef void (*show_object_fn)(struct object *, const char *);
 typedef void (*show_edge_fn)(struct commit *);
 
 void traverse_commit_list(struct rev_info *revs, show_commit_fn, show_object_fn);
diff --git a/revision.c b/revision.c
index 45fd7a3660..f95104b080 100644
--- a/revision.c
+++ b/revision.c
@@ -14,7 +14,7 @@
 
 volatile show_early_output_fn_t show_early_output;
 
-static char *path_name(struct name_path *path, const char *name)
+char *path_name(struct name_path *path, const char *name)
 {
 	struct name_path *p;
 	char *n, *m;
diff --git a/revision.h b/revision.h
index 91f194478b..6fcfb8ce0c 100644
--- a/revision.h
+++ b/revision.h
@@ -141,6 +141,8 @@ struct name_path {
 	const char *elem;
 };
 
+char *path_name(struct name_path *path, const char *name);
+
 extern void add_object(struct object *obj,
 		       struct object_array *p,
 		       struct name_path *path,
diff --git a/upload-pack.c b/upload-pack.c
index e5adbc011e..bdbd67bc1d 100644
--- a/upload-pack.c
+++ b/upload-pack.c
@@ -78,20 +78,20 @@ static void show_commit(struct commit *commit)
 	commit->buffer = NULL;
 }
 
-static void show_object(struct object_array_entry *p)
+static void show_object(struct object *obj, const char *name)
 {
 	/* An object with name "foo\n0000000..." can be used to
 	 * confuse downstream git-pack-objects very badly.
 	 */
-	const char *ep = strchr(p->name, '\n');
+	const char *ep = strchr(name, '\n');
 	if (ep) {
-		fprintf(pack_pipe, "%s %.*s\n", sha1_to_hex(p->item->sha1),
-		       (int) (ep - p->name),
-		       p->name);
+		fprintf(pack_pipe, "%s %.*s\n", sha1_to_hex(obj->sha1),
+		       (int) (ep - name),
+		       name);
 	}
 	else
 		fprintf(pack_pipe, "%s %s\n",
-				sha1_to_hex(p->item->sha1), p->name);
+				sha1_to_hex(obj->sha1), name);
 }
 
 static void show_edge(struct commit *commit)

commit 6ed807f8432c558ef102c94cb2e8ae4e03c48d4e
Author: Johannes Sixt <johannes.sixt@telecom.at>
Date:   Sat Dec 1 22:00:56 2007 +0100

    Windows: A rudimentary poll() emulation.
    
    This emulation of poll() is by far not general. It assumes that the
    fds that are to be waited for are connected to pipes. The pipes are
    polled in a loop until data becomes available in at least one of them.
    If only a single fd is waited for, the implementation actually does
    not wait at all, but assumes that a subsequent read() will block.
    
    In order not to needlessly burn CPU time, the CPU is yielded to other
    processes before the next round in the poll loop using Sleep(0). Note that
    any sleep timeout greater than zero will reduce the efficiency by a
    magnitude.
    
    Signed-off-by: Johannes Sixt <johannes.sixt@telecom.at>

diff --git a/compat/mingw.c b/compat/mingw.c
index 7f89a6cb87..2677e78626 100644
--- a/compat/mingw.c
+++ b/compat/mingw.c
@@ -101,7 +101,62 @@ int pipe(int filedes[2])
 
 int poll(struct pollfd *ufds, unsigned int nfds, int timeout)
 {
-	return -1;
+	int i, pending;
+
+	if (timeout != -1)
+		return errno = EINVAL, error("poll timeout not supported");
+
+	/* When there is only one fd to wait for, then we pretend that
+	 * input is available and let the actual wait happen when the
+	 * caller invokes read().
+	 */
+	if (nfds == 1) {
+		if (!(ufds[0].events & POLLIN))
+			return errno = EINVAL, error("POLLIN not set");
+		ufds[0].revents = POLLIN;
+		return 0;
+	}
+
+repeat:
+	pending = 0;
+	for (i = 0; i < nfds; i++) {
+		DWORD avail = 0;
+		HANDLE h = (HANDLE) _get_osfhandle(ufds[i].fd);
+		if (h == INVALID_HANDLE_VALUE)
+			return -1;	/* errno was set */
+
+		if (!(ufds[i].events & POLLIN))
+			return errno = EINVAL, error("POLLIN not set");
+
+		/* this emulation works only for pipes */
+		if (!PeekNamedPipe(h, NULL, 0, NULL, &avail, NULL)) {
+			int err = GetLastError();
+			if (err == ERROR_BROKEN_PIPE) {
+				ufds[i].revents = POLLHUP;
+				pending++;
+			} else {
+				errno = EINVAL;
+				return error("PeekNamedPipe failed,"
+					" GetLastError: %u", err);
+			}
+		} else if (avail) {
+			ufds[i].revents = POLLIN;
+			pending++;
+		} else
+			ufds[i].revents = 0;
+	}
+	if (!pending) {
+		/* The only times that we spin here is when the process
+		 * that is connected through the pipes is waiting for
+		 * its own input data to become available. But since
+		 * the process (pack-objects) is itself CPU intensive,
+		 * it will happily pick up the time slice that we are
+		 * relinguishing here.
+		 */
+		Sleep(0);
+		goto repeat;
+	}
+	return 0;
 }
 
 struct tm *gmtime_r(const time_t *timep, struct tm *result)

commit b76f6b627802d0a3c8bbf66fba0c090dbe56d509
Author: Junio C Hamano <junkio@cox.net>
Date:   Thu Feb 23 23:04:52 2006 -0800

    pack-objects: allow "thin" packs to exceed depth limits
    
    When creating a new pack to be used in .git/objects/pack/
    directory, we carefully count the depth of deltified objects to
    be reused, so that the generated pack does not to exceed the
    specified depth limit for runtime efficiency.  However, when we
    are generating a thin pack that does not contain base objects,
    such a pack can only be used during network transfer that is
    expanded on the other end upon reception, so being careful and
    artificially cutting the delta chain does not buy us anything
    except increased bandwidth requirement.  This patch disables the
    delta chain depth limit check when reusing an existing delta.
    
    Signed-off-by: Junio C Hamano <junkio@cox.net>

diff --git a/pack-objects.c b/pack-objects.c
index 3a16b7e4ce..2320bcf310 100644
--- a/pack-objects.c
+++ b/pack-objects.c
@@ -663,10 +663,23 @@ static void get_object_details(void)
 	prepare_pack_ix();
 	for (i = 0, entry = objects; i < nr_objects; i++, entry++)
 		check_object(entry);
-	for (i = 0, entry = objects; i < nr_objects; i++, entry++)
-		if (!entry->delta && entry->delta_child)
-			entry->delta_limit =
-				check_delta_limit(entry, 1);
+
+	if (nr_objects == nr_result) {
+		/*
+		 * Depth of objects that depend on the entry -- this
+		 * is subtracted from depth-max to break too deep
+		 * delta chain because of delta data reusing.
+		 * However, we loosen this restriction when we know we
+		 * are creating a thin pack -- it will have to be
+		 * expanded on the other end anyway, so do not
+		 * artificially cut the delta chain and let it go as
+		 * deep as it wants.
+		 */
+		for (i = 0, entry = objects; i < nr_objects; i++, entry++)
+			if (!entry->delta && entry->delta_child)
+				entry->delta_limit =
+					check_delta_limit(entry, 1);
+	}
 }
 
 typedef int (*entry_sort_t)(const struct object_entry *, const struct object_entry *);

commit 461cf59f8924f174d7a0dcc3d77f576d93ed29a4
Author: Linus Torvalds <torvalds@osdl.org>
Date:   Wed Jan 18 14:47:30 2006 -0800

    rev-list: stop when the file disappears
    
    The one thing I've considered doing (I really should) is to add a "stop
    when you don't find the file" option to "git-rev-list". This patch does
    some of the work towards that: it removes the "parent" thing when the
    file disappears, so a "git annotate" could do do something like
    
            git-rev-list --remove-empty --parents HEAD -- "$filename"
    
    and it would get a good graph that stops when the filename disappears
    (it's not perfect though: it won't remove all the unintersting commits).
    
    It also simplifies the logic of finding tree differences a bit, at the
    cost of making it a tad less efficient.
    
    The old logic was two-phase: it would first simplify _only_ merges tree as
    it traversed the tree, and then simplify the linear parts of the remainder
    independently. That was pretty optimal from an efficiency standpoint
    because it avoids doing any comparisons that we can see are unnecessary,
    but it made it much harder to understand than it really needed to be.
    
    The new logic is a lot more straightforward, and compares the trees as it
    traverses the graph (ie everything is a single phase). That makes it much
    easier to stop graph traversal at any point where a file disappears.
    
    As an example, let's say that you have a git repository that has had a
    file called "A" some time in the past. That file gets renamed to B, and
    then gets renamed back again to A. The old "git-rev-list" would show two
    commits: the commit that renames B to A (because it changes A) _and_ as
    its parent the commit that renames A to B (because it changes A).
    
    With the new --remove-empty flag, git-rev-list will show just the commit
    that renames B to A as the "root" commit, and stop traversal there
    (because that's what you want for "annotate" - you want to stop there, and
    for every "root" commit you then separately see if it really is a new
    file, or if the paths history disappeared because it was renamed from some
    other file).
    
    With this patch, you should be able to basically do a "poor mans 'git
    annotate'" with a fairly simple loop:
    
            push("HEAD", "$filename")
            while (revision,filename = pop()) {
                    for each i in $(git-rev-list --parents --remove-empty $revision -- "$filename")
    
                    pseudo-parents($i) = git-rev-list parents for that line
    
                    if (pseudo-parents($i) is non-empty) {
                            show diff of $i against pseudo-parents
                            continue
                    }
    
                    /* See if the _real_ parents of $i had a rename */
                    parent($i) = real-parent($i)
                    if (find-rename in $parent($i)->$i)
                            push $parent($i), "old-name"
            }
    
    which should be doable in perl or something (doing stacks in shell is just
    too painful to be worth it, so I'm not going to do this).
    
    Anybody want to try?
    
                    Linus

diff --git a/rev-list.c b/rev-list.c
index e00e6fc76d..7d3ddc6ad5 100644
--- a/rev-list.c
+++ b/rev-list.c
@@ -54,6 +54,7 @@ static int stop_traversal = 0;
 static int topo_order = 0;
 static int no_merges = 0;
 static const char **paths = NULL;
+static int remove_empty_trees = 0;
 
 static void show_commit(struct commit *commit)
 {
@@ -424,14 +425,33 @@ static void mark_edges_uninteresting(struct commit_list *list)
 	}
 }
 
-static int is_different = 0;
+#define TREE_SAME	0
+#define TREE_NEW	1
+#define TREE_DIFFERENT	2
+static int tree_difference = TREE_SAME;
 
 static void file_add_remove(struct diff_options *options,
 		    int addremove, unsigned mode,
 		    const unsigned char *sha1,
 		    const char *base, const char *path)
 {
-	is_different = 1;
+	int diff = TREE_DIFFERENT;
+
+	/*
+	 * Is it an add of a new file? It means that
+	 * the old tree didn't have it at all, so we
+	 * will turn "TREE_SAME" -> "TREE_NEW", but
+	 * leave any "TREE_DIFFERENT" alone (and if
+	 * it already was "TREE_NEW", we'll keep it
+	 * "TREE_NEW" of course).
+	 */
+	if (addremove == '+') {
+		diff = tree_difference;
+		if (diff != TREE_SAME)
+			return;
+		diff = TREE_NEW;
+	}
+	tree_difference = diff;
 }
 
 static void file_change(struct diff_options *options,
@@ -440,7 +460,7 @@ static void file_change(struct diff_options *options,
 		 const unsigned char *new_sha1,
 		 const char *base, const char *path)
 {
-	is_different = 1;
+	tree_difference = TREE_DIFFERENT;
 }
 
 static struct diff_options diff_opt = {
@@ -449,12 +469,16 @@ static struct diff_options diff_opt = {
 	.change = file_change,
 };
 
-static int same_tree(struct tree *t1, struct tree *t2)
+static int compare_tree(struct tree *t1, struct tree *t2)
 {
-	is_different = 0;
+	if (!t1)
+		return TREE_NEW;
+	if (!t2)
+		return TREE_DIFFERENT;
+	tree_difference = TREE_SAME;
 	if (diff_tree_sha1(t1->object.sha1, t2->object.sha1, "", &diff_opt) < 0)
-		return 0;
-	return !is_different;
+		return TREE_DIFFERENT;
+	return tree_difference;
 }
 
 static int same_tree_as_empty(struct tree *t1)
@@ -474,28 +498,55 @@ static int same_tree_as_empty(struct tree *t1)
 	empty.buf = "";
 	empty.size = 0;
 
-	is_different = 0;
+	tree_difference = 0;
 	retval = diff_tree(&empty, &real, "", &diff_opt);
 	free(tree);
 
-	return retval >= 0 && !is_different;
+	return retval >= 0 && !tree_difference;
 }
 
-static struct commit *try_to_simplify_merge(struct commit *commit, struct commit_list *parent)
+static void try_to_simplify_commit(struct commit *commit)
 {
+	struct commit_list **pp, *parent;
+
 	if (!commit->tree)
-		return NULL;
+		return;
 
-	while (parent) {
+	if (!commit->parents) {
+		if (!same_tree_as_empty(commit->tree))
+			commit->object.flags |= TREECHANGE;
+		return;
+	}
+
+	pp = &commit->parents;
+	while ((parent = *pp) != NULL) {
 		struct commit *p = parent->item;
-		parent = parent->next;
+
+		if (p->object.flags & UNINTERESTING) {
+			pp = &parent->next;
+			continue;
+		}
+
 		parse_commit(p);
-		if (!p->tree)
+		switch (compare_tree(p->tree, commit->tree)) {
+		case TREE_SAME:
+			parent->next = NULL;
+			commit->parents = parent;
+			return;
+
+		case TREE_NEW:
+			if (remove_empty_trees && same_tree_as_empty(p->tree)) {
+				*pp = parent->next;
+				continue;
+			}
+		/* fallthrough */
+		case TREE_DIFFERENT:
+			pp = &parent->next;
 			continue;
-		if (same_tree(commit->tree, p->tree))
-			return p;
+		}
+		die("bad tree compare for commit %s", sha1_to_hex(commit->object.sha1));
 	}
-	return NULL;
+	commit->object.flags |= TREECHANGE;
 }
 
 static void add_parents_to_list(struct commit *commit, struct commit_list **list)
@@ -531,20 +582,14 @@ static void add_parents_to_list(struct commit *commit, struct commit_list **list
 	}
 
 	/*
-	 * Ok, the commit wasn't uninteresting. If it
-	 * is a merge, try to find the parent that has
-	 * no differences in the path set if one exists.
+	 * Ok, the commit wasn't uninteresting. Try to
+	 * simplify the commit history and find the parent
+	 * that has no differences in the path set if one exists.
 	 */
-	if (paths && parent && parent->next) {
-		struct commit *preferred;
-
-		preferred = try_to_simplify_merge(commit, parent);
-		if (preferred) {
-			parent->item = preferred;
-			parent->next = NULL;
-		}
-	}
+	if (paths)
+		try_to_simplify_commit(commit);
 
+	parent = commit->parents;
 	while (parent) {
 		struct commit *p = parent->item;
 
@@ -558,33 +603,6 @@ static void add_parents_to_list(struct commit *commit, struct commit_list **list
 	}
 }
 
-static void compress_list(struct commit_list *list)
-{
-	while (list) {
-		struct commit *commit = list->item;
-		struct commit_list *parent = commit->parents;
-		list = list->next;
-
-		if (!parent) {
-			if (!same_tree_as_empty(commit->tree))
-				commit->object.flags |= TREECHANGE;
-			continue;
-		}
-
-		/*
-		 * Exactly one parent? Check if it leaves the tree
-		 * unchanged
-		 */
-		if (!parent->next) {
-			struct tree *t1 = commit->tree;
-			struct tree *t2 = parent->item->tree;
-			if (!t1 || !t2 || same_tree(t1, t2))
-				continue;
-		}
-		commit->object.flags |= TREECHANGE;
-	}
-}
-
 static struct commit_list *limit_list(struct commit_list *list)
 {
 	struct commit_list *newlist = NULL;
@@ -614,8 +632,6 @@ static struct commit_list *limit_list(struct commit_list *list)
 	}
 	if (tree_objects)
 		mark_edges_uninteresting(newlist);
-	if (paths && dense)
-		compress_list(newlist);
 	if (bisect_list)
 		newlist = find_bisection(newlist);
 	return newlist;
@@ -808,6 +824,10 @@ int main(int argc, const char **argv)
 			dense = 0;
 			continue;
 		}
+		if (!strcmp(arg, "--remove-empty")) {
+			remove_empty_trees = 1;
+			continue;
+		}
 		if (!strcmp(arg, "--")) {
 			i++;
 			break;
commit 3585d0ea232b1a9c5498ab5785b11f61e93967c8
Author: Elijah Newren <newren@gmail.com>
Date:   Wed Jun 30 17:30:00 2021 +0000

    merge-recursive: handle rename-to-self case
    
    Directory rename detection can cause transitive renames, e.g. if the two
    different sides of history each do one half of:
        A/file -> B/file
        B/     -> C/
    then directory rename detection transitively renames to give us
        A/file -> C/file
    
    However, when C/ == A/, note that this gives us
        A/file -> A/file.
    
    merge-recursive assumed that any rename D -> E would have D != E.  While
    that is almost always true, the above is a special case where it is not.
    So we cannot do things like delete the rename source, we cannot assume
    that a file existing at path E implies a rename/add conflict and we have
    to be careful about what stages end up in the output.
    
    This change feels a bit hackish.  It took me surprisingly many hours to
    find, and given merge-recursive's design causing it to attempt to
    enumerate all combinations of edge and corner cases with special code
    for each combination, I'm worried there are other similar fixes needed
    elsewhere if we can just come up with the right special testcase.
    Perhaps an audit would rule it out, but I have not the energy.
    merge-recursive deserves to die, and since it is on its way out anyway,
    fixing this particular bug narrowly will have to be good enough.
    
    Reported-by: Anders Kaseorg <andersk@mit.edu>
    Signed-off-by: Elijah Newren <newren@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/merge-recursive.c b/merge-recursive.c
index d146bb116f..c895145a8f 100644
--- a/merge-recursive.c
+++ b/merge-recursive.c
@@ -2804,12 +2804,19 @@ static int process_renames(struct merge_options *opt,
 			int renamed_stage = a_renames == renames1 ? 2 : 3;
 			int other_stage =   a_renames == renames1 ? 3 : 2;
 
+			/*
+			 * Directory renames have a funny corner case...
+			 */
+			int renamed_to_self = !strcmp(ren1_src, ren1_dst);
+
 			/* BUG: We should only remove ren1_src in the base
 			 * stage and in other_stage (think of rename +
 			 * add-source case).
 			 */
-			remove_file(opt, 1, ren1_src,
-				    renamed_stage == 2 || !was_tracked(opt, ren1_src));
+			if (!renamed_to_self)
+				remove_file(opt, 1, ren1_src,
+					    renamed_stage == 2 ||
+					    !was_tracked(opt, ren1_src));
 
 			oidcpy(&src_other.oid,
 			       &ren1->src_entry->stages[other_stage].oid);
@@ -2823,6 +2830,9 @@ static int process_renames(struct merge_options *opt,
 			    ren1->dir_rename_original_type == 'A') {
 				setup_rename_conflict_info(RENAME_VIA_DIR,
 							   opt, ren1, NULL);
+			} else if (renamed_to_self) {
+				setup_rename_conflict_info(RENAME_NORMAL,
+							   opt, ren1, NULL);
 			} else if (oideq(&src_other.oid, null_oid())) {
 				setup_rename_conflict_info(RENAME_DELETE,
 							   opt, ren1, NULL);
@@ -3180,7 +3190,6 @@ static int handle_rename_normal(struct merge_options *opt,
 	struct rename *ren = ci->ren1;
 	struct merge_file_info mfi;
 	int clean;
-	int side = (ren->branch == opt->branch1 ? 2 : 3);
 
 	/* Merge the content and write it out */
 	clean = handle_content_merge(&mfi, opt, path, was_dirty(opt, path),
@@ -3190,9 +3199,7 @@ static int handle_rename_normal(struct merge_options *opt,
 	    opt->detect_directory_renames == MERGE_DIRECTORY_RENAMES_CONFLICT &&
 	    ren->dir_rename_original_dest) {
 		if (update_stages(opt, path,
-				  NULL,
-				  side == 2 ? &mfi.blob : NULL,
-				  side == 2 ? NULL : &mfi.blob))
+				  &mfi.blob, &mfi.blob, &mfi.blob))
 			return -1;
 		clean = 0; /* not clean, but conflicted */
 	}
diff --git a/t/t6423-merge-rename-directories.sh b/t/t6423-merge-rename-directories.sh
index 316339cb6c..4d056f0465 100755
--- a/t/t6423-merge-rename-directories.sh
+++ b/t/t6423-merge-rename-directories.sh
@@ -5000,7 +5000,7 @@ test_setup_12i () {
 	)
 }
 
-test_expect_merge_algorithm failure success '12i: Directory rename causes rename-to-self' '
+test_expect_success '12i: Directory rename causes rename-to-self' '
 	test_setup_12i &&
 	(
 		cd 12i &&
@@ -5058,7 +5058,7 @@ test_setup_12j () {
 	)
 }
 
-test_expect_merge_algorithm failure success '12j: Directory rename to root causes rename-to-self' '
+test_expect_success '12j: Directory rename to root causes rename-to-self' '
 	test_setup_12j &&
 	(
 		cd 12j &&
@@ -5116,7 +5116,7 @@ test_setup_12k () {
 	)
 }
 
-test_expect_merge_algorithm failure success '12k: Directory rename with sibling causes rename-to-self' '
+test_expect_success '12k: Directory rename with sibling causes rename-to-self' '
 	test_setup_12k &&
 	(
 		cd 12k &&

commit 4463ce75b7eea47f9b484b05957def655d3f46d5
Author: Johannes Schindelin <johannes.schindelin@gmx.de>
Date:   Thu Oct 8 15:29:35 2020 +0000

    ci: do not skip tagged revisions in GitHub workflows
    
    When `master` is tagged, and then both `master` and the tag are pushed,
    Travis CI will happily build both. That is a waste of energy, which is
    why we skip the build for `master` in that case.
    
    Our GitHub workflow is also triggered by tags. However, the run would
    fail because the `windows-test` jobs are _not_ skipped on tags, but the
    `windows-build` job _is skipped (and therefore fails to upload the
    build artifacts needed by the test jobs).
    
    In addition, we just added logic to our GitHub workflow that will skip
    runs altogether if there is already a successful run for the same commit
    or at least for the same tree.
    
    Let's just change the GitHub workflow to no longer specifically skip
    tagged revisions.
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/ci/lib.sh b/ci/lib.sh
index 821e3660d6..38c0eac351 100755
--- a/ci/lib.sh
+++ b/ci/lib.sh
@@ -149,6 +149,7 @@ then
 	CI_REPO_SLUG="$GITHUB_REPOSITORY"
 	CI_JOB_ID="$GITHUB_RUN_ID"
 	CC="${CC:-gcc}"
+	DONT_SKIP_TAGS=t
 
 	cache_dir="$HOME/none"
 
@@ -167,6 +168,7 @@ good_trees_file="$cache_dir/good-trees"
 
 mkdir -p "$cache_dir"
 
+test -n "${DONT_SKIP_TAGS-}" ||
 skip_branch_tip_with_tag
 skip_good_tree
 

commit 7d78d5fc1a91b683dde970e5e48b6d9a873cfd99
Author: Johannes Schindelin <johannes.schindelin@gmx.de>
Date:   Thu Oct 8 15:29:34 2020 +0000

    ci: skip GitHub workflow runs for already-tested commits/trees
    
    When pushing a commit that has already passed a CI or PR build
    successfully, it makes sense to save some energy and time and skip the
    new build.
    
    Let's teach our GitHub workflow to do that.
    
    For good measure, we also compare the tree ID, which is what we actually
    test (the commit ID might have changed due to a reworded commit message,
    which should not affect the outcome of the run).
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/.github/workflows/main.yml b/.github/workflows/main.yml
index fcfd138ff1..0a9acb6a19 100644
--- a/.github/workflows/main.yml
+++ b/.github/workflows/main.yml
@@ -9,7 +9,7 @@ jobs:
   ci-config:
     runs-on: ubuntu-latest
     outputs:
-      enabled: ${{ steps.check-ref.outputs.enabled }}
+      enabled: ${{ steps.check-ref.outputs.enabled }}${{ steps.skip-if-redundant.outputs.enabled }}
     steps:
       - name: try to clone ci-config branch
         run: |
@@ -34,6 +34,43 @@ jobs:
             enabled=no
           fi
           echo "::set-output name=enabled::$enabled"
+      - name: skip if the commit or tree was already tested
+        id: skip-if-redundant
+        uses: actions/github-script@v3
+        if: steps.check-ref.outputs.enabled == 'yes'
+        with:
+          github-token: ${{secrets.GITHUB_TOKEN}}
+          script: |
+            // Figure out workflow ID, commit and tree
+            const { data: run } = await github.actions.getWorkflowRun({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              run_id: context.runId,
+            });
+            const workflow_id = run.workflow_id;
+            const head_sha = run.head_sha;
+            const tree_id = run.head_commit.tree_id;
+
+            // See whether there is a successful run for that commit or tree
+            const { data: runs } = await github.actions.listWorkflowRuns({
+              owner: context.repo.owner,
+              repo: context.repo.repo,
+              per_page: 500,
+              status: 'success',
+              workflow_id,
+            });
+            for (const run of runs.workflow_runs) {
+              if (head_sha === run.head_sha) {
+                core.warning(`Successful run for the commit ${head_sha}: ${run.html_url}`);
+                core.setOutput('enabled', ' but skip');
+                break;
+              }
+              if (tree_id === run.head_commit.tree_id) {
+                core.warning(`Successful run for the tree ${tree_id}: ${run.html_url}`);
+                core.setOutput('enabled', ' but skip');
+                break;
+              }
+            }
 
   windows-build:
     needs: ci-config

commit 6081d3898fe5e33e739cc0771f6df102b30b1db6
Author: Johannes Schindelin <johannes.schindelin@gmx.de>
Date:   Sat Apr 11 00:18:11 2020 +0700

    ci: retire the Azure Pipelines definition
    
    We have GitHub Actions now. Running the same builds and tests in Azure
    Pipelines would be redundant, and a waste of energy.
    
    Signed-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>
    Signed-off-by: Đoàn Trần Công Danh <congdanhqx@gmail.com>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/README.md b/README.md
index e2e00ae249..eb8115e6b0 100644
--- a/README.md
+++ b/README.md
@@ -1,5 +1,4 @@
 [![Build status](https://github.com/git/git/workflows/CI/PR/badge.svg)](https://github.com/git/git/actions?query=branch%3Amaster+event%3Apush)
-[![Build Status](https://dev.azure.com/git/git/_apis/build/status/git.git)](https://dev.azure.com/git/git/_build/latest?definitionId=11)
 
 Git - fast, scalable, distributed revision control system
 =========================================================
diff --git a/azure-pipelines.yml b/azure-pipelines.yml
deleted file mode 100644
index 11413f66f8..0000000000
--- a/azure-pipelines.yml
+++ /dev/null
@@ -1,558 +0,0 @@
-variables:
-  Agent.Source.Git.ShallowFetchDepth: 1
-
-jobs:
-- job: windows_build
-  displayName: Windows Build
-  condition: succeeded()
-  pool:
-    vmImage: windows-latest
-  timeoutInMinutes: 240
-  steps:
-  - powershell: |
-      if ("$GITFILESHAREPWD" -ne "" -and "$GITFILESHAREPWD" -ne "`$`(gitfileshare.pwd)") {
-        net use s: \\gitfileshare.file.core.windows.net\test-cache "$GITFILESHAREPWD" /user:AZURE\gitfileshare /persistent:no
-        cmd /c mklink /d "$(Build.SourcesDirectory)\test-cache" S:\
-      }
-    displayName: 'Mount test-cache'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - powershell: |
-      $urlbase = "https://dev.azure.com/git-for-windows/git/_apis/build/builds"
-      $id = ((Invoke-WebRequest -UseBasicParsing "${urlbase}?definitions=22&statusFilter=completed&resultFilter=succeeded&`$top=1").content | ConvertFrom-JSON).value[0].id
-      $downloadUrl = ((Invoke-WebRequest -UseBasicParsing "${urlbase}/$id/artifacts").content | ConvertFrom-JSON).value[1].resource.downloadUrl
-      (New-Object Net.WebClient).DownloadFile($downloadUrl,"git-sdk-64-minimal.zip")
-      Expand-Archive git-sdk-64-minimal.zip -DestinationPath . -Force
-      Remove-Item git-sdk-64-minimal.zip
-
-      # Let Git ignore the SDK and the test-cache
-      "/git-sdk-64-minimal/`n/test-cache/`n" | Out-File -NoNewLine -Encoding ascii -Append "$(Build.SourcesDirectory)\.git\info\exclude"
-    displayName: 'Download git-sdk-64-minimal'
-  - powershell: |
-      & git-sdk-64-minimal\usr\bin\bash.exe -lc @"
-        ci/make-test-artifacts.sh artifacts
-      "@
-      if (!$?) { exit(1) }
-    displayName: Build
-    env:
-      HOME: $(Build.SourcesDirectory)
-      MSYSTEM: MINGW64
-      DEVELOPER: 1
-      NO_PERL: 1
-  - task: PublishPipelineArtifact@0
-    displayName: 'Publish Pipeline Artifact: test artifacts'
-    inputs:
-      artifactName: 'windows-artifacts'
-      targetPath: '$(Build.SourcesDirectory)\artifacts'
-  - task: PublishPipelineArtifact@0
-    displayName: 'Publish Pipeline Artifact: git-sdk-64-minimal'
-    inputs:
-      artifactName: 'git-sdk-64-minimal'
-      targetPath: '$(Build.SourcesDirectory)\git-sdk-64-minimal'
-  - powershell: |
-      if ("$GITFILESHAREPWD" -ne "" -and "$GITFILESHAREPWD" -ne "`$`(gitfileshare.pwd)") {
-        cmd /c rmdir "$(Build.SourcesDirectory)\test-cache"
-      }
-    displayName: 'Unmount test-cache'
-    condition: true
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-
-- job: windows_test
-  displayName: Windows Test
-  dependsOn: windows_build
-  condition: succeeded()
-  pool:
-    vmImage: windows-latest
-  timeoutInMinutes: 240
-  strategy:
-    parallel: 10
-  steps:
-  - powershell: |
-      if ("$GITFILESHAREPWD" -ne "" -and "$GITFILESHAREPWD" -ne "`$`(gitfileshare.pwd)") {
-        net use s: \\gitfileshare.file.core.windows.net\test-cache "$GITFILESHAREPWD" /user:AZURE\gitfileshare /persistent:no
-        cmd /c mklink /d "$(Build.SourcesDirectory)\test-cache" S:\
-      }
-    displayName: 'Mount test-cache'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: DownloadPipelineArtifact@0
-    displayName: 'Download Pipeline Artifact: test artifacts'
-    inputs:
-      artifactName: 'windows-artifacts'
-      targetPath: '$(Build.SourcesDirectory)'
-  - task: DownloadPipelineArtifact@0
-    displayName: 'Download Pipeline Artifact: git-sdk-64-minimal'
-    inputs:
-      artifactName: 'git-sdk-64-minimal'
-      targetPath: '$(Build.SourcesDirectory)\git-sdk-64-minimal'
-  - powershell: |
-      & git-sdk-64-minimal\usr\bin\bash.exe -lc @"
-        test -f artifacts.tar.gz || {
-          echo No test artifacts found\; skipping >&2
-          exit 0
-        }
-        tar xf artifacts.tar.gz || exit 1
-
-        # Let Git ignore the SDK and the test-cache
-        printf '%s\n' /git-sdk-64-minimal/ /test-cache/ >>.git/info/exclude
-
-        ci/run-test-slice.sh `$SYSTEM_JOBPOSITIONINPHASE `$SYSTEM_TOTALJOBSINPHASE || {
-          ci/print-test-failures.sh
-          exit 1
-        }
-      "@
-      if (!$?) { exit(1) }
-    displayName: 'Test (parallel)'
-    env:
-      HOME: $(Build.SourcesDirectory)
-      MSYSTEM: MINGW64
-      NO_SVN_TESTS: 1
-      GIT_TEST_SKIP_REBASE_P: 1
-  - powershell: |
-      if ("$GITFILESHAREPWD" -ne "" -and "$GITFILESHAREPWD" -ne "`$`(gitfileshare.pwd)") {
-        cmd /c rmdir "$(Build.SourcesDirectory)\test-cache"
-      }
-    displayName: 'Unmount test-cache'
-    condition: true
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: PublishTestResults@2
-    displayName: 'Publish Test Results **/TEST-*.xml'
-    inputs:
-      mergeTestResults: true
-      testRunTitle: 'windows'
-      platform: Windows
-      publishRunAttachments: false
-    condition: succeededOrFailed()
-  - task: PublishBuildArtifacts@1
-    displayName: 'Publish trash directories of failed tests'
-    condition: failed()
-    inputs:
-      PathtoPublish: t/failed-test-artifacts
-      ArtifactName: failed-test-artifacts
-
-- job: vs_build
-  displayName: Visual Studio Build
-  condition: succeeded()
-  pool:
-    vmImage: windows-latest
-  timeoutInMinutes: 240
-  steps:
-  - powershell: |
-      if ("$GITFILESHAREPWD" -ne "" -and "$GITFILESHAREPWD" -ne "`$`(gitfileshare.pwd)") {
-        net use s: \\gitfileshare.file.core.windows.net\test-cache "$GITFILESHAREPWD" /user:AZURE\gitfileshare /persistent:no
-        cmd /c mklink /d "$(Build.SourcesDirectory)\test-cache" S:\
-      }
-    displayName: 'Mount test-cache'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - powershell: |
-      $urlbase = "https://dev.azure.com/git-for-windows/git/_apis/build/builds"
-      $id = ((Invoke-WebRequest -UseBasicParsing "${urlbase}?definitions=22&statusFilter=completed&resultFilter=succeeded&`$top=1").content | ConvertFrom-JSON).value[0].id
-      $downloadUrl = ((Invoke-WebRequest -UseBasicParsing "${urlbase}/$id/artifacts").content | ConvertFrom-JSON).value[1].resource.downloadUrl
-      (New-Object Net.WebClient).DownloadFile($downloadUrl,"git-sdk-64-minimal.zip")
-      Expand-Archive git-sdk-64-minimal.zip -DestinationPath . -Force
-      Remove-Item git-sdk-64-minimal.zip
-
-      # Let Git ignore the SDK and the test-cache
-      "/git-sdk-64-minimal/`n/test-cache/`n" | Out-File -NoNewLine -Encoding ascii -Append "$(Build.SourcesDirectory)\.git\info\exclude"
-    displayName: 'Download git-sdk-64-minimal'
-  - powershell: |
-      & git-sdk-64-minimal\usr\bin\bash.exe -lc @"
-        make NDEBUG=1 DEVELOPER=1 vcxproj
-      "@
-      if (!$?) { exit(1) }
-    displayName: Generate Visual Studio Solution
-    env:
-      HOME: $(Build.SourcesDirectory)
-      MSYSTEM: MINGW64
-      DEVELOPER: 1
-      NO_PERL: 1
-      GIT_CONFIG_PARAMETERS: "'user.name=CI' 'user.email=ci@git'"
-  - powershell: |
-      $urlbase = "https://dev.azure.com/git/git/_apis/build/builds"
-      $id = ((Invoke-WebRequest -UseBasicParsing "${urlbase}?definitions=9&statusFilter=completed&resultFilter=succeeded&`$top=1").content | ConvertFrom-JSON).value[0].id
-      $downloadUrl = ((Invoke-WebRequest -UseBasicParsing "${urlbase}/$id/artifacts").content | ConvertFrom-JSON).value[0].resource.downloadUrl
-      (New-Object Net.WebClient).DownloadFile($downloadUrl, "compat.zip")
-      Expand-Archive compat.zip -DestinationPath . -Force
-      Remove-Item compat.zip
-    displayName: 'Download vcpkg artifacts'
-  - task: MSBuild@1
-    inputs:
-      solution: git.sln
-      platform: x64
-      configuration: Release
-      maximumCpuCount: 4
-      msbuildArguments: /p:PlatformToolset=v142
-  - powershell: |
-      & compat\vcbuild\vcpkg_copy_dlls.bat release
-      if (!$?) { exit(1) }
-      & git-sdk-64-minimal\usr\bin\bash.exe -lc @"
-        mkdir -p artifacts &&
-        eval \"`$(make -n artifacts-tar INCLUDE_DLLS_IN_ARTIFACTS=YesPlease ARTIFACTS_DIRECTORY=artifacts | grep ^tar)\"
-      "@
-      if (!$?) { exit(1) }
-    displayName: Bundle artifact tar
-    env:
-      HOME: $(Build.SourcesDirectory)
-      MSYSTEM: MINGW64
-      DEVELOPER: 1
-      NO_PERL: 1
-      MSVC: 1
-      VCPKG_ROOT: $(Build.SourcesDirectory)\compat\vcbuild\vcpkg
-  - powershell: |
-      $tag = (Invoke-WebRequest -UseBasicParsing "https://gitforwindows.org/latest-tag.txt").content
-      $version = (Invoke-WebRequest -UseBasicParsing "https://gitforwindows.org/latest-version.txt").content
-      $url = "https://github.com/git-for-windows/git/releases/download/${tag}/PortableGit-${version}-64-bit.7z.exe"
-      (New-Object Net.WebClient).DownloadFile($url,"PortableGit.exe")
-      & .\PortableGit.exe -y -oartifacts\PortableGit
-      # Wait until it is unpacked
-      while (-not @(Remove-Item -ErrorAction SilentlyContinue PortableGit.exe; $?)) { sleep 1 }
-    displayName: Download & extract portable Git
-  - task: PublishPipelineArtifact@0
-    displayName: 'Publish Pipeline Artifact: MSVC test artifacts'
-    inputs:
-      artifactName: 'vs-artifacts'
-      targetPath: '$(Build.SourcesDirectory)\artifacts'
-  - powershell: |
-      if ("$GITFILESHAREPWD" -ne "" -and "$GITFILESHAREPWD" -ne "`$`(gitfileshare.pwd)") {
-        cmd /c rmdir "$(Build.SourcesDirectory)\test-cache"
-      }
-    displayName: 'Unmount test-cache'
-    condition: true
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-
-- job: vs_test
-  displayName: Visual Studio Test
-  dependsOn: vs_build
-  condition: succeeded()
-  pool:
-    vmImage: windows-latest
-  timeoutInMinutes: 240
-  strategy:
-    parallel: 10
-  steps:
-  - powershell: |
-      if ("$GITFILESHAREPWD" -ne "" -and "$GITFILESHAREPWD" -ne "`$`(gitfileshare.pwd)") {
-        net use s: \\gitfileshare.file.core.windows.net\test-cache "$GITFILESHAREPWD" /user:AZURE\gitfileshare /persistent:no
-        cmd /c mklink /d "$(Build.SourcesDirectory)\test-cache" S:\
-      }
-    displayName: 'Mount test-cache'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: DownloadPipelineArtifact@0
-    displayName: 'Download Pipeline Artifact: VS test artifacts'
-    inputs:
-      artifactName: 'vs-artifacts'
-      targetPath: '$(Build.SourcesDirectory)'
-  - powershell: |
-      & PortableGit\git-cmd.exe --command=usr\bin\bash.exe -lc @"
-        test -f artifacts.tar.gz || {
-          echo No test artifacts found\; skipping >&2
-          exit 0
-        }
-        tar xf artifacts.tar.gz || exit 1
-
-        # Let Git ignore the SDK and the test-cache
-        printf '%s\n' /PortableGit/ /test-cache/ >>.git/info/exclude
-
-        cd t &&
-        PATH=\"`$PWD/helper:`$PATH\" &&
-        test-tool.exe run-command testsuite --jobs=10 -V -x --write-junit-xml \
-                `$(test-tool.exe path-utils slice-tests \
-                        `$SYSTEM_JOBPOSITIONINPHASE `$SYSTEM_TOTALJOBSINPHASE t[0-9]*.sh)
-      "@
-      if (!$?) { exit(1) }
-    displayName: 'Test (parallel)'
-    env:
-      HOME: $(Build.SourcesDirectory)
-      MSYSTEM: MINGW64
-      NO_SVN_TESTS: 1
-      GIT_TEST_SKIP_REBASE_P: 1
-  - powershell: |
-      if ("$GITFILESHAREPWD" -ne "" -and "$GITFILESHAREPWD" -ne "`$`(gitfileshare.pwd)") {
-        cmd /c rmdir "$(Build.SourcesDirectory)\test-cache"
-      }
-    displayName: 'Unmount test-cache'
-    condition: true
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: PublishTestResults@2
-    displayName: 'Publish Test Results **/TEST-*.xml'
-    inputs:
-      mergeTestResults: true
-      testRunTitle: 'vs'
-      platform: Windows
-      publishRunAttachments: false
-    condition: succeededOrFailed()
-  - task: PublishBuildArtifacts@1
-    displayName: 'Publish trash directories of failed tests'
-    condition: failed()
-    inputs:
-      PathtoPublish: t/failed-test-artifacts
-      ArtifactName: failed-vs-test-artifacts
-
-- job: linux_clang
-  displayName: linux-clang
-  condition: succeeded()
-  pool:
-    vmImage: ubuntu-latest
-  steps:
-  - bash: |
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || ci/mount-fileshare.sh //gitfileshare.file.core.windows.net/test-cache gitfileshare "$GITFILESHAREPWD" "$HOME/test-cache" || exit 1
-
-       sudo apt-get update &&
-       sudo apt-get -y install git gcc make libssl-dev libcurl4-openssl-dev libexpat-dev tcl tk gettext git-email zlib1g-dev apache2-bin &&
-
-       export CC=clang || exit 1
-
-       ci/install-dependencies.sh || exit 1
-       ci/run-build-and-tests.sh || {
-           ci/print-test-failures.sh
-           exit 1
-       }
-
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || sudo umount "$HOME/test-cache" || exit 1
-    displayName: 'ci/run-build-and-tests.sh'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: PublishTestResults@2
-    displayName: 'Publish Test Results **/TEST-*.xml'
-    inputs:
-      mergeTestResults: true
-      testRunTitle: 'linux-clang'
-      platform: Linux
-      publishRunAttachments: false
-    condition: succeededOrFailed()
-  - task: PublishBuildArtifacts@1
-    displayName: 'Publish trash directories of failed tests'
-    condition: failed()
-    inputs:
-      PathtoPublish: t/failed-test-artifacts
-      ArtifactName: failed-test-artifacts
-
-- job: linux_gcc
-  displayName: linux-gcc
-  condition: succeeded()
-  pool:
-    vmImage: ubuntu-latest
-  steps:
-  - bash: |
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || ci/mount-fileshare.sh //gitfileshare.file.core.windows.net/test-cache gitfileshare "$GITFILESHAREPWD" "$HOME/test-cache" || exit 1
-
-       sudo add-apt-repository ppa:ubuntu-toolchain-r/test &&
-       sudo apt-get update &&
-       sudo apt-get -y install git gcc make libssl-dev libcurl4-openssl-dev libexpat-dev tcl tk gettext git-email zlib1g-dev apache2 language-pack-is git-svn gcc-8 || exit 1
-
-       ci/install-dependencies.sh || exit 1
-       ci/run-build-and-tests.sh || {
-           ci/print-test-failures.sh
-           exit 1
-       }
-
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || sudo umount "$HOME/test-cache" || exit 1
-    displayName: 'ci/run-build-and-tests.sh'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: PublishTestResults@2
-    displayName: 'Publish Test Results **/TEST-*.xml'
-    inputs:
-      mergeTestResults: true
-      testRunTitle: 'linux-gcc'
-      platform: Linux
-      publishRunAttachments: false
-    condition: succeededOrFailed()
-  - task: PublishBuildArtifacts@1
-    displayName: 'Publish trash directories of failed tests'
-    condition: failed()
-    inputs:
-      PathtoPublish: t/failed-test-artifacts
-      ArtifactName: failed-test-artifacts
-
-- job: osx_clang
-  displayName: osx-clang
-  condition: succeeded()
-  pool:
-    vmImage: macOS-latest
-  steps:
-  - bash: |
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || ci/mount-fileshare.sh //gitfileshare.file.core.windows.net/test-cache gitfileshare "$GITFILESHAREPWD" "$HOME/test-cache" || exit 1
-
-       export CC=clang
-
-       ci/install-dependencies.sh || exit 1
-       ci/run-build-and-tests.sh || {
-           ci/print-test-failures.sh
-           exit 1
-       }
-
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || umount "$HOME/test-cache" || exit 1
-    displayName: 'ci/run-build-and-tests.sh'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: PublishTestResults@2
-    displayName: 'Publish Test Results **/TEST-*.xml'
-    inputs:
-      mergeTestResults: true
-      testRunTitle: 'osx-clang'
-      platform: macOS
-      publishRunAttachments: false
-    condition: succeededOrFailed()
-  - task: PublishBuildArtifacts@1
-    displayName: 'Publish trash directories of failed tests'
-    condition: failed()
-    inputs:
-      PathtoPublish: t/failed-test-artifacts
-      ArtifactName: failed-test-artifacts
-
-- job: osx_gcc
-  displayName: osx-gcc
-  condition: succeeded()
-  pool:
-    vmImage: macOS-latest
-  steps:
-  - bash: |
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || ci/mount-fileshare.sh //gitfileshare.file.core.windows.net/test-cache gitfileshare "$GITFILESHAREPWD" "$HOME/test-cache" || exit 1
-
-       ci/install-dependencies.sh || exit 1
-       ci/run-build-and-tests.sh || {
-           ci/print-test-failures.sh
-           exit 1
-       }
-
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || umount "$HOME/test-cache" || exit 1
-    displayName: 'ci/run-build-and-tests.sh'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: PublishTestResults@2
-    displayName: 'Publish Test Results **/TEST-*.xml'
-    inputs:
-      mergeTestResults: true
-      testRunTitle: 'osx-gcc'
-      platform: macOS
-      publishRunAttachments: false
-    condition: succeededOrFailed()
-  - task: PublishBuildArtifacts@1
-    displayName: 'Publish trash directories of failed tests'
-    condition: failed()
-    inputs:
-      PathtoPublish: t/failed-test-artifacts
-      ArtifactName: failed-test-artifacts
-
-- job: gettext_poison
-  displayName: GETTEXT_POISON
-  condition: succeeded()
-  pool:
-    vmImage: ubuntu-latest
-  steps:
-  - bash: |
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || ci/mount-fileshare.sh //gitfileshare.file.core.windows.net/test-cache gitfileshare "$GITFILESHAREPWD" "$HOME/test-cache" || exit 1
-
-       sudo apt-get update &&
-       sudo apt-get -y install git gcc make libssl-dev libcurl4-openssl-dev libexpat-dev tcl tk gettext git-email zlib1g-dev &&
-
-       export jobname=GETTEXT_POISON || exit 1
-
-       ci/run-build-and-tests.sh || {
-           ci/print-test-failures.sh
-           exit 1
-       }
-
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || sudo umount "$HOME/test-cache" || exit 1
-    displayName: 'ci/run-build-and-tests.sh'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: PublishTestResults@2
-    displayName: 'Publish Test Results **/TEST-*.xml'
-    inputs:
-      mergeTestResults: true
-      testRunTitle: 'gettext-poison'
-      platform: Linux
-      publishRunAttachments: false
-    condition: succeededOrFailed()
-  - task: PublishBuildArtifacts@1
-    displayName: 'Publish trash directories of failed tests'
-    condition: failed()
-    inputs:
-      PathtoPublish: t/failed-test-artifacts
-      ArtifactName: failed-test-artifacts
-
-- job: linux32
-  displayName: Linux32
-  condition: succeeded()
-  pool:
-    vmImage: ubuntu-latest
-  steps:
-  - bash: |
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || ci/mount-fileshare.sh //gitfileshare.file.core.windows.net/test-cache gitfileshare "$GITFILESHAREPWD" "$HOME/test-cache" || exit 1
-
-       res=0
-       sudo AGENT_OS="$AGENT_OS" BUILD_BUILDNUMBER="$BUILD_BUILDNUMBER" BUILD_REPOSITORY_URI="$BUILD_REPOSITORY_URI" BUILD_SOURCEBRANCH="$BUILD_SOURCEBRANCH" BUILD_SOURCEVERSION="$BUILD_SOURCEVERSION" SYSTEM_PHASENAME="$SYSTEM_PHASENAME" SYSTEM_TASKDEFINITIONSURI="$SYSTEM_TASKDEFINITIONSURI" SYSTEM_TEAMPROJECT="$SYSTEM_TEAMPROJECT" CC=$CC MAKEFLAGS="$MAKEFLAGS" jobname=Linux32 bash -lxc ci/run-docker.sh || res=1
-
-       sudo chmod a+r t/out/TEST-*.xml
-       test ! -d t/failed-test-artifacts || sudo chmod a+r t/failed-test-artifacts
-
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || sudo umount "$HOME/test-cache" || res=1
-       exit $res
-    displayName: 'jobname=Linux32 ci/run-docker.sh'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-  - task: PublishTestResults@2
-    displayName: 'Publish Test Results **/TEST-*.xml'
-    inputs:
-      mergeTestResults: true
-      testRunTitle: 'linux32'
-      platform: Linux
-      publishRunAttachments: false
-    condition: succeededOrFailed()
-  - task: PublishBuildArtifacts@1
-    displayName: 'Publish trash directories of failed tests'
-    condition: failed()
-    inputs:
-      PathtoPublish: t/failed-test-artifacts
-      ArtifactName: failed-test-artifacts
-
-- job: static_analysis
-  displayName: StaticAnalysis
-  condition: succeeded()
-  pool:
-    vmImage: ubuntu-latest
-  steps:
-  - bash: |
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || ci/mount-fileshare.sh //gitfileshare.file.core.windows.net/test-cache gitfileshare "$GITFILESHAREPWD" "$HOME/test-cache" || exit 1
-
-       sudo apt-get update &&
-       sudo apt-get install -y coccinelle libcurl4-openssl-dev libssl-dev libexpat-dev gettext &&
-
-       export jobname=StaticAnalysis &&
-
-       ci/run-static-analysis.sh || exit 1
-
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || sudo umount "$HOME/test-cache" || exit 1
-    displayName: 'ci/run-static-analysis.sh'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)
-
-- job: documentation
-  displayName: Documentation
-  condition: succeeded()
-  pool:
-    vmImage: ubuntu-latest
-  steps:
-  - bash: |
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || ci/mount-fileshare.sh //gitfileshare.file.core.windows.net/test-cache gitfileshare "$GITFILESHAREPWD" "$HOME/test-cache" || exit 1
-
-       sudo apt-get update &&
-       sudo apt-get install -y asciidoc xmlto asciidoctor docbook-xsl-ns &&
-
-       export ALREADY_HAVE_ASCIIDOCTOR=yes. &&
-       export jobname=Documentation &&
-
-       ci/test-documentation.sh || exit 1
-
-       test "$GITFILESHAREPWD" = '$(gitfileshare.pwd)' || sudo umount "$HOME/test-cache" || exit 1
-    displayName: 'ci/test-documentation.sh'
-    env:
-      GITFILESHAREPWD: $(gitfileshare.pwd)

commit e4da43b1f063d227b5f7d2922d27458748763a2d
Author: Jeff King <peff@peff.net>
Date:   Mon Mar 20 21:28:49 2017 -0400

    prefix_filename: return newly allocated string
    
    The prefix_filename() function returns a pointer to static
    storage, which makes it easy to use dangerously. We already
    fixed one buggy caller in hash-object recently, and the
    calls in apply.c are suspicious (I didn't dig in enough to
    confirm that there is a bug, but we call the function once
    in apply_all_patches() and then again indirectly from
    parse_chunk()).
    
    Let's make it harder to get wrong by allocating the return
    value. For simplicity, we'll do this even when the prefix is
    empty (and we could just return the original file pointer).
    That will cause us to allocate sometimes when we wouldn't
    otherwise need to, but this function isn't called in
    performance critical code-paths (and it already _might_
    allocate on any given call, so a caller that cares about
    performance is questionable anyway).
    
    The downside is that the callers need to remember to free()
    the result to avoid leaking. Most of them already used
    xstrdup() on the result, so we know they are OK. The
    remainder have been converted to use free() as appropriate.
    
    I considered retaining a prefix_filename_unsafe() for cases
    where we know the static lifetime is OK (and handling the
    cleanup is awkward). This is only a handful of cases,
    though, and it's not worth the mental energy in worrying
    about whether the "unsafe" variant is OK to use in any
    situation.
    
    Signed-off-by: Jeff King <peff@peff.net>
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/abspath.c b/abspath.c
index c6f480993d..4addd1fde0 100644
--- a/abspath.c
+++ b/abspath.c
@@ -246,20 +246,18 @@ char *absolute_pathdup(const char *path)
 	return strbuf_detach(&sb, NULL);
 }
 
-const char *prefix_filename(const char *pfx, const char *arg)
+char *prefix_filename(const char *pfx, const char *arg)
 {
-	static struct strbuf path = STRBUF_INIT;
+	struct strbuf path = STRBUF_INIT;
 	size_t pfx_len = pfx ? strlen(pfx) : 0;
 
 #ifndef GIT_WINDOWS_NATIVE
 	if (!pfx_len || is_absolute_path(arg))
-		return arg;
-	strbuf_reset(&path);
+		return xstrdup(arg);
 	strbuf_add(&path, pfx, pfx_len);
 	strbuf_addstr(&path, arg);
 #else
 	/* don't add prefix to absolute paths, but still replace '\' by '/' */
-	strbuf_reset(&path);
 	if (is_absolute_path(arg))
 		pfx_len = 0;
 	else if (pfx_len)
@@ -267,5 +265,5 @@ const char *prefix_filename(const char *pfx, const char *arg)
 	strbuf_addstr(&path, arg);
 	convert_slashes(path.buf + pfx_len);
 #endif
-	return path.buf;
+	return strbuf_detach(&path, NULL);
 }
diff --git a/apply.c b/apply.c
index b8bd5a4bef..e6dbab26ad 100644
--- a/apply.c
+++ b/apply.c
@@ -2046,7 +2046,7 @@ static void prefix_one(struct apply_state *state, char **name)
 	char *old_name = *name;
 	if (!old_name)
 		return;
-	*name = xstrdup(prefix_filename(state->prefix, *name));
+	*name = prefix_filename(state->prefix, *name);
 	free(old_name);
 }
 
@@ -4805,6 +4805,7 @@ int apply_all_patches(struct apply_state *state,
 
 	for (i = 0; i < argc; i++) {
 		const char *arg = argv[i];
+		char *to_free = NULL;
 		int fd;
 
 		if (!strcmp(arg, "-")) {
@@ -4814,19 +4815,21 @@ int apply_all_patches(struct apply_state *state,
 			errs |= res;
 			read_stdin = 0;
 			continue;
-		} else if (0 < state->prefix_length)
-			arg = prefix_filename(state->prefix, arg);
+		} else
+			arg = to_free = prefix_filename(state->prefix, arg);
 
 		fd = open(arg, O_RDONLY);
 		if (fd < 0) {
 			error(_("can't open patch '%s': %s"), arg, strerror(errno));
 			res = -128;
+			free(to_free);
 			goto end;
 		}
 		read_stdin = 0;
 		set_default_whitespace_mode(state);
 		res = apply_patch(state, fd, arg, options);
 		close(fd);
+		free(to_free);
 		if (res < 0)
 			goto end;
 		errs |= res;
diff --git a/builtin/config.c b/builtin/config.c
index 74f6c34d11..4f49a0edb9 100644
--- a/builtin/config.c
+++ b/builtin/config.c
@@ -527,8 +527,7 @@ int cmd_config(int argc, const char **argv, const char *prefix)
 	else if (given_config_source.file) {
 		if (!is_absolute_path(given_config_source.file) && prefix)
 			given_config_source.file =
-				xstrdup(prefix_filename(prefix,
-							given_config_source.file));
+				prefix_filename(prefix, given_config_source.file);
 	}
 
 	if (respect_includes == -1)
diff --git a/builtin/hash-object.c b/builtin/hash-object.c
index 2ea36909d2..bbeaf20bcc 100644
--- a/builtin/hash-object.c
+++ b/builtin/hash-object.c
@@ -145,7 +145,7 @@ int cmd_hash_object(int argc, const char **argv, const char *prefix)
 		char *to_free = NULL;
 
 		if (prefix)
-			arg = to_free = xstrdup(prefix_filename(prefix, arg));
+			arg = to_free = prefix_filename(prefix, arg);
 		hash_object(arg, type, no_filters ? NULL : vpath ? vpath : arg,
 			    flags, literally);
 		free(to_free);
diff --git a/builtin/log.c b/builtin/log.c
index bfdc7a23d3..670229cbb4 100644
--- a/builtin/log.c
+++ b/builtin/log.c
@@ -1084,7 +1084,7 @@ static const char *set_outdir(const char *prefix, const char *output_directory)
 	if (!output_directory)
 		return prefix;
 
-	return xstrdup(prefix_filename(prefix, output_directory));
+	return prefix_filename(prefix, output_directory);
 }
 
 static const char * const builtin_format_patch_usage[] = {
diff --git a/builtin/mailinfo.c b/builtin/mailinfo.c
index 681f07f54d..cfb667a594 100644
--- a/builtin/mailinfo.c
+++ b/builtin/mailinfo.c
@@ -11,13 +11,6 @@
 static const char mailinfo_usage[] =
 	"git mailinfo [-k | -b] [-m | --message-id] [-u | --encoding=<encoding> | -n] [--scissors | --no-scissors] <msg> <patch> < mail >info";
 
-static char *prefix_copy(const char *prefix, const char *filename)
-{
-	if (!prefix || is_absolute_path(filename))
-		return xstrdup(filename);
-	return xstrdup(prefix_filename(prefix, filename));
-}
-
 int cmd_mailinfo(int argc, const char **argv, const char *prefix)
 {
 	const char *def_charset;
@@ -60,8 +53,8 @@ int cmd_mailinfo(int argc, const char **argv, const char *prefix)
 	mi.input = stdin;
 	mi.output = stdout;
 
-	msgfile = prefix_copy(prefix, argv[1]);
-	patchfile = prefix_copy(prefix, argv[2]);
+	msgfile = prefix_filename(prefix, argv[1]);
+	patchfile = prefix_filename(prefix, argv[2]);
 
 	status = !!mailinfo(&mi, msgfile, patchfile);
 	clear_mailinfo(&mi);
diff --git a/builtin/merge-file.c b/builtin/merge-file.c
index 63cd943587..47dde7c39c 100644
--- a/builtin/merge-file.c
+++ b/builtin/merge-file.c
@@ -65,11 +65,18 @@ int cmd_merge_file(int argc, const char **argv, const char *prefix)
 	}
 
 	for (i = 0; i < 3; i++) {
-		const char *fname = prefix_filename(prefix, argv[i]);
+		char *fname;
+		int ret;
+
 		if (!names[i])
 			names[i] = argv[i];
-		if (read_mmfile(mmfs + i, fname))
+
+		fname = prefix_filename(prefix, argv[i]);
+		ret = read_mmfile(mmfs + i, fname);
+		free(fname);
+		if (ret)
 			return -1;
+
 		if (mmfs[i].size > MAX_XDIFF_SIZE ||
 		    buffer_is_binary(mmfs[i].ptr, mmfs[i].size))
 			return error("Cannot merge binary files: %s",
@@ -86,7 +93,7 @@ int cmd_merge_file(int argc, const char **argv, const char *prefix)
 
 	if (ret >= 0) {
 		const char *filename = argv[0];
-		const char *fpath = prefix_filename(prefix, argv[0]);
+		char *fpath = prefix_filename(prefix, argv[0]);
 		FILE *f = to_stdout ? stdout : fopen(fpath, "wb");
 
 		if (!f)
@@ -98,6 +105,7 @@ int cmd_merge_file(int argc, const char **argv, const char *prefix)
 		else if (fclose(f))
 			ret = error_errno("Could not close %s", filename);
 		free(result.ptr);
+		free(fpath);
 	}
 
 	if (ret > 127)
diff --git a/builtin/rev-parse.c b/builtin/rev-parse.c
index c8035331e2..7cd01c2819 100644
--- a/builtin/rev-parse.c
+++ b/builtin/rev-parse.c
@@ -228,7 +228,9 @@ static int show_file(const char *arg, int output_prefix)
 	if ((filter & (DO_NONFLAGS|DO_NOREV)) == (DO_NONFLAGS|DO_NOREV)) {
 		if (output_prefix) {
 			const char *prefix = startup_info->prefix;
-			show(prefix_filename(prefix, arg));
+			char *fname = prefix_filename(prefix, arg);
+			show(fname);
+			free(fname);
 		} else
 			show(arg);
 		return 1;
diff --git a/builtin/worktree.c b/builtin/worktree.c
index e38325e44b..9993ded41a 100644
--- a/builtin/worktree.c
+++ b/builtin/worktree.c
@@ -318,7 +318,8 @@ static int add(int ac, const char **av, const char *prefix)
 {
 	struct add_opts opts;
 	const char *new_branch_force = NULL;
-	const char *path, *branch;
+	char *path;
+	const char *branch;
 	struct option options[] = {
 		OPT__FORCE(&opts.force, N_("checkout <branch> even if already checked out in other worktree")),
 		OPT_STRING('b', NULL, &opts.new_branch, N_("branch"),
diff --git a/cache.h b/cache.h
index 0b53aef0ed..aa6a0fb91a 100644
--- a/cache.h
+++ b/cache.h
@@ -537,10 +537,10 @@ extern char *prefix_path_gently(const char *prefix, int len, int *remaining, con
  * not have to interact with index entry; i.e. name of a random file
  * on the filesystem.
  *
- * The return value may point to static storage which will be overwritten by
- * further calls.
+ * The return value is always a newly allocated string (even if the
+ * prefix was empty).
  */
-extern const char *prefix_filename(const char *prefix, const char *path);
+extern char *prefix_filename(const char *prefix, const char *path);
 
 extern int check_filename(const char *prefix, const char *name);
 extern void verify_filename(const char *prefix,
diff --git a/diff-no-index.c b/diff-no-index.c
index 5f7317ced9..79229382b0 100644
--- a/diff-no-index.c
+++ b/diff-no-index.c
@@ -266,7 +266,7 @@ void diff_no_index(struct rev_info *revs,
 			 */
 			p = file_from_standard_input;
 		else if (prefix)
-			p = xstrdup(prefix_filename(prefix, p));
+			p = prefix_filename(prefix, p);
 		paths[i] = p;
 	}
 
diff --git a/diff.c b/diff.c
index 70870b4b69..58cb72d7e7 100644
--- a/diff.c
+++ b/diff.c
@@ -4023,8 +4023,7 @@ int diff_opt_parse(struct diff_options *options,
 	else if (!strcmp(arg, "--pickaxe-regex"))
 		options->pickaxe_opts |= DIFF_PICKAXE_REGEX;
 	else if ((argcount = short_opt('O', av, &optarg))) {
-		const char *path = prefix_filename(prefix, optarg);
-		options->orderfile = xstrdup(path);
+		options->orderfile = prefix_filename(prefix, optarg);
 		return argcount;
 	}
 	else if ((argcount = parse_long_opt("diff-filter", av, &optarg))) {
@@ -4071,13 +4070,14 @@ int diff_opt_parse(struct diff_options *options,
 	else if (!strcmp(arg, "--no-function-context"))
 		DIFF_OPT_CLR(options, FUNCCONTEXT);
 	else if ((argcount = parse_long_opt("output", av, &optarg))) {
-		const char *path = prefix_filename(prefix, optarg);
+		char *path = prefix_filename(prefix, optarg);
 		options->file = fopen(path, "w");
 		if (!options->file)
 			die_errno("Could not open '%s'", path);
 		options->close_file = 1;
 		if (options->use_color != GIT_COLOR_ALWAYS)
 			options->use_color = GIT_COLOR_NEVER;
+		free(path);
 		return argcount;
 	} else
 		return 0;
diff --git a/parse-options.c b/parse-options.c
index ba6cc30b26..a23a1e67f0 100644
--- a/parse-options.c
+++ b/parse-options.c
@@ -40,7 +40,7 @@ static void fix_filename(const char *prefix, const char **file)
 	if (!file || !*file || !prefix || is_absolute_path(*file)
 	    || !strcmp("-", *file))
 		return;
-	*file = xstrdup(prefix_filename(prefix, *file));
+	*file = prefix_filename(prefix, *file);
 }
 
 static int opt_command_mode_error(const struct option *opt,
diff --git a/setup.c b/setup.c
index a76379e0ce..5c7946d2b4 100644
--- a/setup.c
+++ b/setup.c
@@ -135,6 +135,7 @@ int path_inside_repo(const char *prefix, const char *path)
 int check_filename(const char *prefix, const char *arg)
 {
 	const char *name;
+	char *to_free = NULL;
 	struct stat st;
 
 	if (starts_with(arg, ":/")) {
@@ -142,13 +143,17 @@ int check_filename(const char *prefix, const char *arg)
 			return 1;
 		name = arg + 2;
 	} else if (prefix)
-		name = prefix_filename(prefix, arg);
+		name = to_free = prefix_filename(prefix, arg);
 	else
 		name = arg;
-	if (!lstat(name, &st))
+	if (!lstat(name, &st)) {
+		free(to_free);
 		return 1; /* file exists */
-	if (errno == ENOENT || errno == ENOTDIR)
+	}
+	if (errno == ENOENT || errno == ENOTDIR) {
+		free(to_free);
 		return 0; /* file does not exist */
+	}
 	die_errno("failed to stat '%s'", arg);
 }
 
diff --git a/worktree.c b/worktree.c
index 42dd3d52b0..bae787cf8d 100644
--- a/worktree.c
+++ b/worktree.c
@@ -250,16 +250,19 @@ struct worktree *find_worktree(struct worktree **list,
 {
 	struct worktree *wt;
 	char *path;
+	char *to_free = NULL;
 
 	if ((wt = find_worktree_by_suffix(list, arg)))
 		return wt;
 
-	arg = prefix_filename(prefix, arg);
+	if (prefix)
+		arg = to_free = prefix_filename(prefix, arg);
 	path = real_pathdup(arg, 1);
 	for (; *list; list++)
 		if (!fspathcmp(path, real_path((*list)->path)))
 			break;
 	free(path);
+	free(to_free);
 	return *list;
 }
 

commit e2eb527345d48881dac0d88e6bdfc0a267a2eb62
Author: Junio C Hamano <gitster@pobox.com>
Date:   Wed May 4 19:11:18 2011 -0700

    require-work-tree wants more than what its name says
    
    Somebody tried "git pull" from a random place completely outside the work
    tree, while exporting GIT_DIR and GIT_WORK_TREE that are set to correct
    places, e.g.
    
        GIT_WORK_TREE=$HOME/git.git
        GIT_DIR=$GIT_WORK_TREE/.git
        export GIT_WORK_TREE GIT_DIR
        cd /tmp
        git pull
    
    At the beginning of git-pull, we check "require-work-tree" and then
    "cd-to-toplevel".  I _think_ the original intention when I wrote the
    command was "we MUST have a work tree, our $(cwd) might not be at the
    top-level directory of it", and no stronger than that.  That check is a
    very sensible thing to do before doing cd-to-toplevel.  We check that the
    place we would want to go exists, and then go there.
    
    But the implementation of require_work_tree we have today is quite
    different.  I don't have energy to dig the history, but currently it says:
    
        test "$(git rev-parse --is-inside-work-tree 2>/dev/null)" = true ||
        die "fatal: $0 cannot be used without a working tree."
    
    Which is completely bogus.  Even though we may happen to be just outside
    of it right now, we may have a working tree that we can cd_to_toplevel
    back to.
    
    Add a function "require_work_tree_exists" that implements the check
    this function originally intended (this is so that third-party scripts
    that rely on the current behaviour do not have to get broken).
    
    For now, update _no_ in-tree scripts, not even "git pull", as nobody on
    the list seems to really care about the above corner case workflow that
    triggered this. Scripts can be updated after vetting that they do want the
    "we want to make sure the place we are going to go actually exists"
    semantics.
    
    Signed-off-by: Junio C Hamano <gitster@pobox.com>

diff --git a/Documentation/git-sh-setup.txt b/Documentation/git-sh-setup.txt
index 3da241304b..1f02c4b6ea 100644
--- a/Documentation/git-sh-setup.txt
+++ b/Documentation/git-sh-setup.txt
@@ -58,9 +58,14 @@ cd_to_toplevel::
 	runs chdir to the toplevel of the working tree.
 
 require_work_tree::
-	checks if the repository is a bare repository, and dies
-	if so.  Used by scripts that require working tree
-	(e.g. `checkout`).
+	checks if the current directory is within the working tree
+	of the repository, and otherwise dies.
+
+require_work_tree_exists::
+	checks if the working tree associated with the repository
+	exists, and otherwise dies.  Often done before calling
+	cd_to_toplevel, which is impossible to do if there is no
+	working tree.
 
 get_author_ident_from_commit::
 	outputs code for use with eval to set the GIT_AUTHOR_NAME,
diff --git a/git-sh-setup.sh b/git-sh-setup.sh
index aa16b83565..94e26ed5e8 100644
--- a/git-sh-setup.sh
+++ b/git-sh-setup.sh
@@ -140,6 +140,13 @@ cd_to_toplevel () {
 	}
 }
 
+require_work_tree_exists () {
+	if test "z$(git rev-parse --is-bare-repository)" != zfalse
+	then
+		die "fatal: $0 cannot be used without a working tree."
+	fi
+}
+
 require_work_tree () {
 	test "$(git rev-parse --is-inside-work-tree 2>/dev/null)" = true ||
 	die "fatal: $0 cannot be used without a working tree."

commit 8c989ec5288021e07c265882f86ac3999b44c142
Author: Junio C Hamano <junkio@cox.net>
Date:   Thu Apr 13 00:17:19 2006 -0700

    Makefile: $(MAKE) check-docs
    
    This target lists undocumented commands, and/or whose document
    is not referenced from the main git documentation.
    
    For now, there are some exceptions I added primarily because I
    lack the energy to document them myself:
    
     - merge backends (we should really document them)
     - ssh-push/ssh-pull (does anybody still use them?)
     - annotate and blame (maybe after one of them eats the other ;-)
    
    Signed-off-by: Junio C Hamano <junkio@cox.net>

diff --git a/Makefile b/Makefile
index e6ef41d702..1130af4f38 100644
--- a/Makefile
+++ b/Makefile
@@ -665,3 +665,23 @@ clean:
 .PHONY: all install clean strip
 .PHONY: .FORCE-GIT-VERSION-FILE TAGS tags
 
+### Check documentation
+#
+check-docs::
+	@for v in $(ALL_PROGRAMS) $(BUILT_INS) git$X gitk; \
+	do \
+		case "$$v" in \
+		git-annotate | git-blame | \
+		git-merge-octopus | git-merge-ours | git-merge-recursive | \
+		git-merge-resolve | git-merge-stupid | \
+		git-ssh-pull | git-ssh-push ) continue ;; \
+		esac ; \
+		test -f "Documentation/$$v.txt" || \
+		echo "no doc: $$v"; \
+		grep -q "^gitlink:$$v\[[0-9]\]::" Documentation/git.txt || \
+		case "$$v" in \
+		git) ;; \
+		*) echo "no link: $$v";; \
+		esac ; \
+	done | sort
+
